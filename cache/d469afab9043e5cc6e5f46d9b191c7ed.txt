
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/li-jian-Lee/p/18703414" title="发布于 2025-02-07 22:29">
    <span role="heading" aria-level="2">DeepSeek-R1 技术全景解析：从原理到实践的“炼金术配方” ——附多阶段训练流程图与核心误区澄清</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<blockquote>
<p><strong>字数：约3200字｜预计阅读时间：8分钟</strong></p>
</blockquote>
<p>（调试着R1的API接口，看着控制台瀑布般流淌的思维链日志）此刻我仿佛看到AlphaGo的棋谱在代码世界重生——<strong>这是属于推理模型的AlphaZero时刻</strong>。</p>
<p>DeepSeek 发布的 V3、R1-Zero、R1 三大模型，代表了一条从通用基座到专用推理的完整技术路径。许多读者对三者的关系存在困惑，本文将通过<strong>流程图解、差异对比、训练逻辑拆解</strong>三大模块，彻底厘清它们的定位与联系。</p>
<p><img src="https://nxgwqnqda8m.feishu.cn/space/api/box/stream/download/asynccode/?code=NDZkYWFmMzhjNDgzNGRkM2U5MzhlZjg1Nzg4NzI4ODNfNGRFZjFDR25DaWQwZFNhYW14MWM2YWpEa3FudlQxODlfVG9rZW46QWVkWmJCb05Ub3RPRU14ZWFiUWN6bUtHbml4XzE3Mzg5MzY3NTc6MTczODk0MDM1N19WNA" alt="" loading="lazy"></p>
<h2 id="一模型定位与技术差异">一、模型定位与技术差异</h2>
<table>
<thead>
<tr>
<th><strong>维度</strong></th>
<th><strong>DeepSeek-V3</strong></th>
<th><strong>R1-Zero</strong></th>
<th><strong>DeepSeek-R1</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>定位</td>
<td>通用基座模型</td>
<td>纯 RL 训练的推理实验模型</td>
<td>多阶段优化的商用推理模型</td>
</tr>
<tr>
<td>训练方法</td>
<td>预训练 + SFT</td>
<td>纯强化学习（GRPO 算法）</td>
<td>SFT → RL → SFT → RL与SFT混合训练</td>
</tr>
<tr>
<td>数据依赖</td>
<td>通用语料 + 标注数据</td>
<td>数学/代码数据（无需标注）</td>
<td>RL 生成数据 + 人类偏好数据</td>
</tr>
<tr>
<td>推理能力</td>
<td>基础问答</td>
<td>强推理但语言混杂</td>
<td>强推理 + 语言规范</td>
</tr>
<tr>
<td>可用性</td>
<td>通用场景</td>
<td>实验性（不可直接商用）</td>
<td>全场景适配（客服、编程等）</td>
</tr>
<tr>
<td>开源状态</td>
<td>开源</td>
<td>未开源</td>
<td>开源</td>
</tr>
</tbody>
</table>
<h2 id="二训练关系全流程图解">二、训练关系全流程图解</h2>
<p><img src="https://nxgwqnqda8m.feishu.cn/space/api/box/stream/download/asynccode/?code=YmQ4NmMwODZkN2EzZDNlZjlkOWViZDIxYmYyNTU3ODVfa2RxYll0emE5bmxkbGRFNXlsVUZJMW80a0JnclIzTVFfVG9rZW46RU81R2JQUHIzb05PWE94aWRKdWNMbUxObjRlXzE3Mzg5MzY3NTc6MTczODk0MDM1N19WNA" alt="" loading="lazy"></p>
<p><strong>流程图解读</strong>：</p>
<ol>
<li><strong>V3 是起点</strong>：作为通用基座模型，提供基础语言能力。</li>
<li><strong>R1-Zero 是过渡实验体</strong>：通过纯 RL 训练验证推理能力，但语言混乱不可用。</li>
<li><strong>R1 是终极形态</strong>：融合冷启动、RL 锻造、数据反哺、人类偏好四阶段，兼顾能力与实用性。</li>
</ol>
<p><img src="https://nxgwqnqda8m.feishu.cn/space/api/box/stream/download/asynccode/?code=ZTg3M2YxYzVjNTdhOThkY2QyZjM5ZWFkZDVlNmExZDhfSjdmcG1ZOGFlaWNiTHU5WHRubmlEZ2R6UTZta2pMZnFfVG9rZW46VVQyamJiYUFkb0gwNm14THN1TGN5SDFqbmxlXzE3Mzg5MzY3NTc6MTczODk0MDM1N19WNA" alt="" loading="lazy"></p>
<h2 id="三技术演进逻辑拆解">三、技术演进逻辑拆解</h2>
<h3 id="1-基座奠基deepseek-v3-的原始积累">1. 基座奠基：DeepSeek-V3 的“原始积累”</h3>
<ul>
<li><strong>核心能力</strong>：通用文本生成、基础问答、多任务处理。</li>
<li><strong>短板暴露</strong>：
<ul>
<li>无法生成连贯的思维链（CoT）。</li>
<li>依赖监督微调（SFT），推理能力天花板低。</li>
</ul>
</li>
<li><strong>破局方向</strong>：引入<strong>强化学习</strong>（<strong>RL</strong>），让模型通过奖励机制自主探索推理路径。</li>
</ul>
<p><img src="https://nxgwqnqda8m.feishu.cn/space/api/box/stream/download/asynccode/?code=MjMyNzdmZWQwMmZkMWY0NWY2NmU2ZDA2OTE4M2Y1OWVfRlVxUElwekdQR25sek96RjdxMU1WZ2ZFNlNnS3pvYlhfVG9rZW46S1M5MmJvVEdCb3RsWmV4S1VKaGN6ZlZmbjRFXzE3Mzg5MzY3NTc6MTczODk0MDM1N19WNA" alt="" loading="lazy"></p>
<h3 id="2-纯-rl-试炼r1-zero-的无监督觉醒">2. 纯 RL 试炼：R1-Zero 的“无监督觉醒”</h3>
<ul>
<li><strong>设计理念</strong>：<strong>“不依赖人类标注，仅靠</strong> <strong>RL</strong> <strong>自我进化”</strong></li>
<li><strong>训练逻辑</strong>：
<ul>
<li><strong>数据选择</strong>：数学推导与代码执行（每一步可客观验证）。</li>
<li><strong>算法核心</strong>：GRPO（组内奖励对比优化），避免训练额外评判模型。</li>
<li><strong>成果与代价</strong>：推理指标超越 V3，但语言混杂、格式混乱。</li>
</ul>
</li>
</ul>
<p><img src="https://nxgwqnqda8m.feishu.cn/space/api/box/stream/download/asynccode/?code=M2U0YzQzNjhkZWQ3N2Y2ZTQ5NmI4M2RkMzQ1ZmI1NjlfRzEzVHRBU2lzR3doYWVOUGU1TGJuVjBBWVBuRE95QmRfVG9rZW46V2VCV2JmSjl2b0tsUUd4UGRpTmN2VndUblNnXzE3Mzg5MzY3NTc6MTczODk0MDM1N19WNA" alt="" loading="lazy"></p>
<p><img src="https://nxgwqnqda8m.feishu.cn/space/api/box/stream/download/asynccode/?code=MDY2MjlmNzI5MWM5MjM1NzE0ZjU5YjY1M2M4NWFmMjhfZ0dpaWZtaXZoNkllVjVPT0xNRFBJdDVBQkZqNnh2aWRfVG9rZW46SWdRNGI4NTBZbzFjdzV4SlVIWWN3ZW1HbjRaXzE3Mzg5MzY3NTc6MTczODk0MDM1N19WNA" alt="" loading="lazy"></p>
<h3 id="3-最终形态满血r1">3. 最终形态满血R1</h3>
<p>DeepSeek-R1 通过四大阶段实现了能力与实用性的完美平衡：</p>
<ul>
<li><strong>冷启动</strong>：利用少量高质量的 CoT 数据，使模型学会标准答案格式。</li>
<li><strong>RL</strong> <strong>锻造</strong>：引入 GRPO 算法，让模型在多种推理路径中自主选择最优策略。</li>
<li><strong>数据反哺</strong>：模型自生成高质量数据，减少对人工标注的依赖。</li>
<li><strong>人机融合</strong>：引入人类偏好奖励，确保输出结果不仅推理精准，更符合实际使用需求。</li>
</ul>
<p><img src="https://nxgwqnqda8m.feishu.cn/space/api/box/stream/download/asynccode/?code=MDMzODBkMWQyYTliM2MyNmM2YjA0MGYwNjU3YWUwY2RfakFZNDdrUGNCY3dmbGFJME5oMHlzVFRHMHZJbWExZ0hfVG9rZW46QTZYbWJFQmw2b2xRSmp4Y3JsUGNncHJlbnVjXzE3Mzg5MzY3NTc6MTczODk0MDM1N19WNA" alt="" loading="lazy"></p>
<p>当看到DeepSeek论文中训练曲线图时我突然意识到：<strong>强化学习</strong> <strong>正在重写游戏规则</strong>。就像AlphaZero抛弃人类棋谱</p>
<h2 id="四deepseek-r1-的四步炼金术">四、DeepSeek-R1 的“四步炼金术”</h2>
<p>DeepSeek-R1 的训练过程可以分为四大步骤，每一步都像是炼金术中的独门秘技：</p>
<h3 id="step-1冷启动抄作业阶段">Step 1：冷启动——“抄作业”阶段</h3>
<p>这个阶段，DeepSeek-R1 还只是个“新手”。它要做的就是“抄作业”——学习少量高质量的 CoT（思维链）数据。这些数据就像“武功秘籍”中的“图解”，告诉 DeepSeek-R1 什么是正确的推理过程。</p>
<p>（对照自己调试Agent的经历）初始阶段如同给新生儿植入基础反射：</p>
<pre><code class="language-HTML">收集1000+高质量CoT数据（相当于婴儿的看图识字卡）
精细调整prompt格式（建立神经元的「输入输出规范」）
引入语言一致性奖励（避免中英文混杂的「精神分裂」）
</code></pre>
<p>这个阶段的核心矛盾是：<strong>如何在最小化人工干预的前提下，建立可扩展的推理</strong> <strong>范式</strong>。DeepSeek的方案像给模型安装「脚手架」，既约束探索方向，又不限制创新空间。</p>
<ul>
<li><strong>目标</strong>：防止 RL 初期盲目探索，奠定基础推理格式。</li>
<li><strong>核心操作</strong>：
<ul>
<li><strong>数据精选</strong>：少量高质量长思维链（Long-CoT）数据，包含清晰推理步骤。</li>
<li><strong>SFT</strong> <strong>预热</strong>：对 V3 微调，使其初步学会“抄写”标准答案。</li>
</ul>
</li>
</ul>
<blockquote>
<p>如同教孩童写字，先临摹字帖，再迈向自主创作。</p>
</blockquote>
<p><img src="https://nxgwqnqda8m.feishu.cn/space/api/box/stream/download/asynccode/?code=ODk4MTlkNmE2NjNiOWZkOGQ0ZmVlN2U1NzZhYTY3NTNfb3pVV0RzTlpiY3NwMWNpUlJVTGZBcGl4djZkdkRXbHNfVG9rZW46R1dEcmJrWUJPb01rdDh4Y2lsMWNiVmZZblVlXzE3Mzg5MzY3NTc6MTczODk0MDM1N19WNA" alt="" loading="lazy"></p>
<h3 id="step-2rorl实战演练阶段">Step 2：RORL——“实战演练”阶段</h3>
<p>有了“冷启动”的基础，DeepSeek-R1 进入了“实战演练”阶段——RORL（推理导向的强化学习）。</p>
<p>这个阶段，DeepSeek-R1 不再只是“抄作业”，而是要自己“解题”。它会尝试各种推理路径，并通过 GRPO（Group Relative Policy Optimization）算法来评估自己的表现。</p>
<p>GRPO 就像一位“裁判”，根据 DeepSeek-R1 的“答题”情况打分，并指导它如何改进。</p>
<p>这个阶段，DeepSeek-R1 主要依靠规则奖励（Rule-based Reward）来“修炼”。规则奖励就像“武功秘籍”中的“口诀”，告诉 DeepSeek-R1 哪些是“正确”的推理步骤。</p>
<ul>
<li><strong>算法核心</strong>：GRPO 的三大创新设计
<ul>
<li><strong>组内基线估计</strong>：同一批输出的奖励对比，降低训练开销。</li>
<li><strong>规则奖励</strong>：答案正确性、推理格式规范性（如步骤编号、符号统一）。</li>
<li><strong>语言一致性奖励</strong>：强制中英文分离，解决 R1-Zero 的“语言混搭”问题。</li>
</ul>
</li>
<li><strong>效果验证</strong>：模型逐步涌现长推理链能力，甚至能自我修正错误步骤（“Aha Moment”）。</li>
</ul>
<p><img src="https://nxgwqnqda8m.feishu.cn/space/api/box/stream/download/asynccode/?code=MzE1NDkzOWIzOWY4ZGM0YzE5YmZhNWQzOGVkZDI3ZTVfQkJZd2NOeEhDWGZFazYxY0FQUURrRTBOZzFuWXlRaUtfVG9rZW46U1BzM2JiUHNWb242dWl4eGFkWWN5UHh3blZDXzE3Mzg5MzY3NTc6MTczODk0MDM1N19WNA" alt="" loading="lazy"></p>
<h3 id="step-3重构自创武功阶段">Step 3：重构——“自创武功”阶段</h3>
<p>经过“实战演练”，DeepSeek-R1 已经具备了一定的推理能力。接下来，它开始“自创武功”——<strong>生成高质量的</strong> <strong>训练数据</strong>。</p>
<p>这个阶段，DeepSeek-R1 会利用拒绝采样（Rejection Sampling）和 CoT 提示（CoT Prompting）来生成数据。拒绝采样就像“筛选器”，确保生成的数据符合要求；CoT 提示则像“模板”，帮助 DeepSeek-R1 生成各种类型的 SFT 数据。</p>
<ul>
<li><strong>拒绝采样（Rejection Sampling）</strong>：
<ul>
<li><strong>生成</strong>：模型输出推理过程。</li>
<li><strong>筛选</strong>：规则校验（格式）→ V3 模型二次过滤 → 保留优质数据。</li>
</ul>
</li>
<li><strong>CoT</strong> <strong>提示工程</strong>：生成非推理任务数据（如写作、对话），增强泛化性。</li>
</ul>
<blockquote>
<p>类似厨师研发新菜后，将成功配方整理成食谱，供团队学习。</p>
</blockquote>
<p><img src="https://nxgwqnqda8m.feishu.cn/space/api/box/stream/download/asynccode/?code=OWZiNDNhYTg1NGU1MDUzZGVhMmMxZGQzZjIyMzBkZmNfVzdqYjhuVDhrbm5Udks0MkZNcFViVzlLUDQ3bXRiUXZfVG9rZW46Ujc3V2JSdUJDb2dtR1p4YnlmdGNkUTNUblRoXzE3Mzg5MzY3NTc6MTczODk0MDM1N19WNA" alt="" loading="lazy"></p>
<h3 id="step-4最终进化融会贯通阶段">Step 4：最终进化——“融会贯通”阶段</h3>
<p>这个阶段，DeepSeek-R1 将之前学到的所有“招式”融会贯通。它会再次进行 SFT 微调，并引入人类偏好奖励（Human Preference Reward），让自己的推理能力更上一层楼。</p>
<p>人类偏好奖励就像“武林大会”的“观众投票”，让 DeepSeek-R1 知道什么样的推理结果更受欢迎。</p>
<ul>
<li><strong>混合奖励信号</strong>：
<ul>
<li><strong>规则奖励</strong>：针对推理任务（如数学解题）。</li>
<li><strong>人类偏好奖励</strong>：引入 Helpfulness（有用性）与 Harmlessness（无害性）评估。</li>
</ul>
</li>
<li><strong>数据多样性</strong>：覆盖多场景提示（客服、编程、创意写作），避免“过拟合推理”。</li>
</ul>
<blockquote>
<p>正如武林高手在大赛中通过观众投票验证实力，DeepSeek-R1 经过这一阶段实现了真正的能力整合。</p>
</blockquote>
<p><img src="https://nxgwqnqda8m.feishu.cn/space/api/box/stream/download/asynccode/?code=YWNhYzBlYTJmYjkwNDdjNmJhMjgyNzU3ZjhlZWEzNDRfWDllT21zSDlueGZWQ3ViOFJjWmhBR1lHMEdjem9kdXRfVG9rZW46RlRkc2IyVGZOb1BVMVl4RGQzcmMwU1VBbnZnXzE3Mzg5MzY3NTc6MTczODk0MDM1N19WNA" alt="" loading="lazy"></p>
<h2 id="五核心误区澄清打破-rl-训练的神话滤镜">五、核心误区澄清：打破 RL 训练的“神话滤镜”</h2>
<p>在推理模型的演进过程中，常见以下几个误区：</p>
<ol>
<li><strong>“无需</strong> <strong>SFT</strong> <strong>”意味着完全抛弃</strong> <strong>监督学习</strong>？
<ol>
<li>事实：SFT 在冷启动和数据反哺阶段都是不可或缺的润滑剂，为 RL 提供了正确的格式指引。</li>
</ol>
</li>
<li><strong>RL</strong> <strong>训练成本必然高于</strong> <strong>SFT****？</strong>
<ol>
<li>事实：得益于 GRPO 算法的组内奖励对比机制，R1 的训练成本仅为传统 RLHF 的约 1/3。</li>
</ol>
</li>
<li><strong>推理模型必须依赖过程监督（<strong><strong>PRM</strong></strong>）？</strong>
<ol>
<li>事实：DeepSeek-R1 证明，仅凭规则奖励和 GRPO 也能实现性能突破；不过 PRM 可在加速收敛上起到一定作用。</li>
</ol>
</li>
</ol>
<p>此外，对于如下常见问题：</p>
<ul>
<li><strong>Q1：为何不直接发布 R1-Zero？</strong>  纯 RL 训练固然让推理能力飞速提升，但语言表达存在混乱，因此 R1-Zero 更多用于技术验证而非产品化。</li>
<li><strong>Q2：DeepSeek-V3 是否已被淘汰？</strong>  V3 作为通用基座在非推理场景依然具有优势，而 R1 则是 V3 的“推理特化版”，二者各有侧重。</li>
<li><strong>Q3：普通开发者应如何选择模型？</strong>
<ul>
<li>通用任务：DeepSeek-V3</li>
<li>复杂推理：DeepSeek-R1</li>
<li>学术研究：R1-Zero（需具备相应权限）</li>
</ul>
</li>
</ul>
<p><img src="https://nxgwqnqda8m.feishu.cn/space/api/box/stream/download/asynccode/?code=MDY2MzBkN2VlMmExNjQ3Y2NlZmZiMzRmMDViMDAwZTBfSXN6WTB0Tll3Qk1rdm1JMVJ6bjJNUzJmeUFwNnkwMzVfVG9rZW46V0V1emJlVnVNbzlzVUd4UzliaWNNYlNDbmplXzE3Mzg5MzY3NTc6MTczODk0MDM1N19WNA" alt="" loading="lazy"></p>
<h2 id="六技术启示开源社区的破壁宣言">六、技术启示：开源社区的“破壁宣言”</h2>
<ol>
<li>范式革新：RL 主导的后训练时代</li>
</ol>
<p>传统模型依赖海量标注数据，而 RL 训练则让模型从“被动模仿”转变为“主动探索”，极大突破了数据瓶颈，让推理能力迈向新高。</p>
<ol start="2">
<li>工程哲学：简单即美</li>
</ol>
<p>拒绝复杂设计，DeepSeek-R1 未采用诸如 MCTS 复杂搜索，而是依托清晰的规则奖励与 Scaling Law，在追求性能的同时兼顾工程效率。</p>
<ol start="3">
<li>未来预言</li>
</ol>
<ul>
<li><strong>推理即服务</strong>：未来两年，50% 的 AI 应用将内置本地推理引擎，R1 类模型将成为关键的“推理中间件”。</li>
<li><strong>RL</strong> <strong>主导训练</strong>：预计到 2025 年，80% 的顶尖模型将优先采用 RL 策略，监督学习将在辅助工具的地位上出现。</li>
<li><strong>开源定义标准</strong>：正如 Linux 定义了服务器操作系统，相信 R1 将成为 AI 时代推理接口的标准之一。</li>
</ul>
<p><img src="https://nxgwqnqda8m.feishu.cn/space/api/box/stream/download/asynccode/?code=OWE0OWJhZDJhOWU1NjRhMDM4N2U0MzczM2EzOTUxMTJfdVZ2NnZyTkc4VU8yaTFEV1pJVVRnMWVuUWRUU1JSWHlfVG9rZW46QWV2TmIyUTZEb1pOZTN4THc0WGNqN1JhbjZjXzE3Mzg5MzY3NTc6MTczODk0MDM1N19WNA" alt="" loading="lazy"></p>
<h2 id="结语一场重新定义智能的旅程">结语：一场重新定义“智能”的旅程</h2>
<p>（保存完最后一个DS案例的日志）看到R1设身处地的为我的粗糙的提示词思考时，我突然想起邱锡鹏教授的话：「强推理模型的终点是Agent」。而今天，我们正站在这个转折点上——这不是终结，而是新炼金时代的开始。</p>
<blockquote>
<p>技术永远在追问：我们能否做得更优雅？<br>
而R1的回答是：让强化学习重新定义可能性的边界。</p>
</blockquote>
<p>从 V3 到 R1，DeepSeek 完成了一条<strong>基座赋能 → 纯</strong>  <strong>RL</strong> <strong>验证 → 多阶段优化</strong>的完整技术路径。这场演进不仅是算法的胜利，更是开源精神的体现——<strong>它证明：最强的推理能力，可以诞生于开放协作的土壤。</strong></p>
<p>正如 DeepSeek 写道：</p>
<blockquote>
<p><strong>“</strong> <strong>AI</strong> <strong>的终极目标不是取代人类，而是让机器学会思考，人类学会协作。”</strong></p>
</blockquote>
<p><img src="https://nxgwqnqda8m.feishu.cn/space/api/box/stream/download/asynccode/?code=YmQzYTA1ZjlkMTY2NDMzMjdkNDJkYTA5NDhjZjc1YjFfS0FXTjVQQkozY3F6ZTlBSzZ2aU9kWGhsREV1TGRDcmVfVG9rZW46VDFlOWJRem1Lb25mUTV4blhUU2NwT0ZobnhXXzE3Mzg5MzY3NTc6MTczODk0MDM1N19WNA" alt="" loading="lazy"></p>
<hr>
<p><strong>参考资料</strong></p>
<ul>
<li>Guo D, Yang D, Zhang H, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning[J]. arXiv preprint arXiv:2501.12948, 2025.</li>
<li>Liu A, Feng B, Xue B, et al. Deepseek-v3 technical report[J]. arXiv preprint arXiv:2412.19437, 2024.</li>
<li>Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., &amp; Cao, Y. (2022). React: Synergizing reasoning and acting in language models. <em>arXiv</em> <em>preprint</em> <em>arXiv:2210.03629</em>.</li>
<li>Muennighoff N, Yang Z, Shi W, et al. s1: Simple test-time scaling[J]. arXiv preprint arXiv:2501.19393, 2025.</li>
<li>[DeepSeek-R1] (<a href="https://github.com/deepseek-ai/DeepSeek-R1?tab=readme-ov-file" target="_blank" rel="noopener nofollow">https://github.com/deepseek-ai/DeepSeek-R1?tab=readme-ov-file</a>)</li>
<li>[大白话聊聊Deepseek R1背后的来龙去脉，必读] (<a href="https://mp.weixin.qq.com/s/je7BId1DZYaZZJHUShFpqA" target="_blank" rel="noopener nofollow">https://mp.weixin.qq.com/s/je7BId1DZYaZZJHUShFpqA</a>)</li>
<li>[DeepSeek最强专业拆解来了，清交复教授超硬核解读] (<a href="https://mp.weixin.qq.com/s/LsMOIgQinPZBnsga0imcvA" target="_blank" rel="noopener nofollow">https://mp.weixin.qq.com/s/LsMOIgQinPZBnsga0imcvA</a>)</li>
</ul>
<hr>
<p><strong>（本文同步发布于[遇健李的幸运] (<a href="https://www.cnblogs.com/li-jian-Lee" target="_blank">https://www.cnblogs.com/li-jian-Lee</a>)，点击“阅读原文”直达技术博客）</strong></p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.07515593553703703" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-02-07 22:29">2025-02-07 22:29</span>&nbsp;
<a href="https://www.cnblogs.com/li-jian-Lee">遇健李的幸运</a>&nbsp;
阅读(<span id="post_view_count">34</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18703414" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18703414);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18703414', targetLink: 'https://www.cnblogs.com/li-jian-Lee/p/18703414', title: 'DeepSeek-R1 技术全景解析：从原理到实践的“炼金术配方” ——附多阶段训练流程图与核心误区澄清' })">举报</a>
</div>
        