
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/johnnyzen/p/18881700" title="发布于 2025-05-17 23:09">
    <span role="heading" aria-level="2">[虚拟化/云原生] Kubernetes 安装部署指南</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h1 id="0-序">0 序</h1>
<ul>
<li>先说我个人对当前ICT行业技术演进趋势的最大定论：大数据/云计算(云原生)/AI时代，<code>K8s</code>/<code>docker</code>在现在和未来都必将是<strong>云</strong>里头最重要的<strong>基础设施</strong>之一。</li>
<li><code>docker</code> 自己玩了很久；在公司里，k8s集群、rancher也用了几年了。</li>
<li><code>docker</code> / <code>docker compose</code> / <code>k8s</code>的架构、设计理念等，也均有所涉猎。</li>
</ul>
<blockquote>
<blockquote>
<p>感兴趣的朋友，可拉到文章底部【推荐文献】查阅。</p>
</blockquote>
</blockquote>
<ul>
<li>唯一的遗憾是，自己从没有动手安装部署过 <code>k8s</code>。</li>
<li>这不，最近在研究云边协同架构，不得不倒逼着我要亲手来安装一次 k8s 了。</li>
</ul>
<blockquote>
<p>目前已安装 ok 了，本文做个完整的总结。</p>
</blockquote>
<h1 id="1-kubernetes-安装部署指南">1 Kubernetes 安装部署指南</h1>
<h2 id="11-前置工作与注意事项">1.1 前置工作与注意事项</h2>
<h3 id="部署规划">部署规划</h3>
<ul>
<li>服务器规划</li>
</ul>
<table>
<thead>
<tr>
<th>IP</th>
<th>Role</th>
<th>Hostname</th>
<th>resource</th>
<th>Remark</th>
</tr>
</thead>
<tbody>
<tr>
<td>172.16.1.72</td>
<td>master 和 node</td>
<td>2c , 4g, 10GB(个人建议至少2c,2g)</td>
<td>k8s-master-1</td>
<td>资源有限，此台机器同时作为 master 和 node 节点</td>
</tr>
</tbody>
</table>
<ul>
<li>版本</li>
</ul>
<blockquote>
<ul>
<li>cpu : x86-64</li>
<li>os : centos 7.9</li>
<li>docker : 26.1.4</li>
</ul>
<blockquote>
<p>也可换为其他的<strong>容器运行时</strong>（Container runtime）</p>
</blockquote>
<ul>
<li>k8s : 1.25.0 ≥ 1.24.0 (docker 涉及适配 CRI-O)</li>
</ul>
<blockquote>
<p>安装过程中涉及安装的3大命令模块: <code>kubelet</code> / <code>kubeadm</code> / <code>kubectl</code><br>
<a href="https://kubernetes.io/zh-cn/docs/concepts/architecture/" target="_blank" rel="noopener nofollow">Kubernetes 架构 - kubernetes.io</a></p>
</blockquote>
</blockquote>
<p><img src="https://img2024.cnblogs.com/blog/1173617/202505/1173617-20250517232221649-906922872.png" alt="" loading="lazy"></p>
<p><img src="https://img2024.cnblogs.com/blog/1173617/202505/1173617-20250517232259582-376043154.png" alt="" loading="lazy"></p>
<blockquote>
<ul>
<li>kubeEdge : 1.16.6 [x/本文不涉及]</li>
</ul>
</blockquote>
<h3 id="机器配置">机器配置</h3>
<ul>
<li>系统配置</li>
</ul>
<blockquote>
<ul>
<li>要求：2c 2g 20g 以上</li>
<li>cpu 至少为2核，否则k8s初始化无法成功。</li>
<li>内存 建议master节点内存给4g</li>
</ul>
</blockquote>
<h3 id="操作系统">操作系统</h3>
<ul>
<li>OS</li>
</ul>
<blockquote>
<ul>
<li>CENTOS 7.9</li>
<li>不同版本的系统对k8s影响较大，具体看实际情况而定。 有的还需要更新系统内核</li>
</ul>
</blockquote>
<h3 id="hosts--hostname">hosts / hostname</h3>
<ul>
<li>hosts</li>
</ul>
<pre><code class="language-shell">vim /etc/hosts

172.16.1.72 k8s-master-1 k8s-slave-1
</code></pre>
<blockquote>
<p>1台机器 1行 host 配置</p>
</blockquote>
<ul>
<li>hostname配置</li>
</ul>
<blockquote>
<p>○ 每台机器都需配置，这里我们以 172.16.1.72 为例，我们需要设置hostname为 “k8s-master-1” ，与hosts 相匹配。<br>
○ 如果不配置hostname 默认会配置为 <code>localhost.localdomain</code> ，k8s 运行时会报错<code>Error getting node" err="node \"localhost.localdomain\" not found</code></p>
</blockquote>
<pre><code class="language-shell"># 设置当前机器的hostname
hostnamectl set-hostname k8s-master-1	

# 查看当前机器hostname
hostname
</code></pre>
<p><img src="https://img2024.cnblogs.com/blog/1173617/202505/1173617-20250517222835282-1286116762.png" alt="" loading="lazy"></p>
<ul>
<li>ping 一下</li>
</ul>
<pre><code class="language-shell">ping k8s-master-1
ping k8s-slave-1
</code></pre>
<h3 id="其他注意事项">其他注意事项</h3>
<ul>
<li>k8s安装有多种方式</li>
</ul>
<blockquote>
<ul>
<li>使用<code>minikube</code>安装单节点集群，用于测试</li>
<li>采用工具<code>kubeadm</code> – 我们使用的这种方式（开发环境，机器比较少(几十台以下)）</li>
<li>使用<code>kubespray</code>, google官方提供的工具</li>
<li><strong>全手动</strong>: 二进制安装(运维)</li>
<li><strong>全自动安装</strong>: rancher、kubesphere (大型生产环境，百台，万台机器)</li>
</ul>
</blockquote>
<ul>
<li>k8s health会依赖一些端口，为了不出现网络问题，我们在虚拟机(master)中开放以下端口：</li>
</ul>
<blockquote>
<ul>
<li>6443 主要</li>
<li>2379</li>
<li>2380</li>
</ul>
</blockquote>
<ul>
<li>kubeadm</li>
</ul>
<blockquote>
<ul>
<li>ca 证书：帮助我们安装的ca 证书时限是一年，所以不推荐正式环境使用，或需要手动配置ca证书。</li>
</ul>
</blockquote>
<h2 id="12-安装步骤">1.2 安装步骤</h2>
<h3 id="121-初始准备-master--node">1.2.1 初始准备 [master + node]</h3>
<h4 id="设置-hostname">设置 hostname</h4>
<pre><code class="language-shell"># 设置当前机器的hostname
hostnamectl set-hostname k8s-master-1

# 查看当前机器hostname
hostname
</code></pre>
<h4 id="基础依赖包的安装">基础依赖包的安装</h4>
<pre><code class="language-shell"># 基础依赖包安装
yum -y install wget vim net-tools ntpdate bash-completion
</code></pre>
<h4 id="系统时钟同步">系统时钟同步</h4>
<pre><code class="language-shell"># 向阿里云服务器同步时间
ntpdate time1.aliyun.com

# 删除本地时间并设置时区为上海
rm -rf /etc/localtime
ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime

# 查看时间
date -R || date
</code></pre>
<h4 id="关闭防火墙selinux">关闭防火墙、selinux</h4>
<pre><code class="language-shell">systemctl stop firewalld
systemctl disable firewalld

# 临时关闭
setenforce 0
# 永久关闭
sed -i 's/enforcing/disabled/' /etc/selinux/config
</code></pre>
<h4 id="关闭-swap">关闭 swap</h4>
<blockquote>
<p>K8S 不能使用swap</p>
</blockquote>
<pre><code class="language-shell"># 临时关闭Swap
swapoff -a

# 修改 /etc/fstab 删除或者注释掉swap的挂载，可永久关闭swap
sed -i '/swap/s/^/#/' /etc/fstab

# 修改完后我们检测一下，看最后一行swap 都是0 就成功了
free -m
#----------------start----------------------
 total used free shared buff/cache available
Mem: 1837 721 69 10 1046 944
Swap: 0 0 0
#-----------------end---------------------
</code></pre>
<h4 id="网桥过滤">网桥过滤</h4>
<pre><code class="language-shell"># 网桥过滤
vim /etc/sysctl.conf

net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-arptables = 1
net.ipv4.ip_forward=1
net.ipv4.ip_forward_use_pmtu = 0

# 生效命令
sysctl --system

# 查看效果
sysctl -a|grep "ip_forward"
</code></pre>
<h4 id="开启ipvs">开启ipvs</h4>
<ul>
<li>开启ipvs</li>
</ul>
<blockquote>
<ul>
<li><code>kubernetes 1.8</code>版本开始，新增了<code>kube-proxy</code>对<code>ipvs</code>的支持，性能和追踪问题比iptable强</li>
</ul>
<blockquote>
<p>此步骤为选填项，如果不执行那么默认使用<code>iptables</code></p>
</blockquote>
</blockquote>
<pre><code class="language-shell"># 安装IPVS
yum -y install ipset ipvsdm

# 编译ipvs.modules文件
vi /etc/sysconfig/modules/ipvs.modules
# 文件内容如下
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4

# 赋予权限并执行
chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp;lsmod | grep -e ip_vs -e nf_conntrack_ipv4

# 重启电脑，检查是否生效
reboot
lsmod | grep ip_vs_rr
</code></pre>
<h4 id="修改hosts">修改hosts</h4>
<ul>
<li>修改hosts文件</li>
</ul>
<blockquote>
<p>添加 dns 解析</p>
</blockquote>
<pre><code class="language-shell">vim /etc/hosts
	
172.16.1.72 k8s-master-1 k8s-slave-1
</code></pre>
<h3 id="122-docker-安装-master--node">1.2.2 Docker 安装 [master + node]</h3>
<h4 id="docker-换源">docker 换源</h4>
<pre><code class="language-shell"># 安装yum utils ( 安装 yum-config-manager 命令 )
yum install -y yum-utils

# yum docker-ce config 换源
#yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo

ls -la /etc/yum.repos.d/
</code></pre>
<h4 id="安装docker">安装docker</h4>
<ul>
<li>安装 docker</li>
</ul>
<pre><code class="language-shell"># 安装 docker (亦可指定版本)
yum -y install docker-ce docker-ce-cli containerd.io

# 启动docker， enable 为必须，k8s会检测docker.service
systemctl enable docker &amp;&amp; systemctl start docker
</code></pre>
<ul>
<li>查验 docker 版本</li>
</ul>
<pre><code class="language-sh">docker version
</code></pre>
<h4 id="配置镜像加速">配置镜像加速</h4>
<pre><code class="language-shell"># 创建docker目录
mkdir -p /etc/docker

# 设置镜像源, exec-opts必须指定否则k8s启动报错（cgroup、systemd）
tee /etc/docker/daemon.json &lt;&lt;-'EOF'
{
 "exec-opts": ["native.cgroupdriver=systemd"],
 "registry-mirrors": [" https://fl791z1h.mirror.aliyuncs.com"]
}
EOF

# 重启docke并生效镜像加速
systemctl daemon-reload &amp;&amp; systemctl restart docker

# 查看 docker 运行的进程和镜像
docker ps
docker images
</code></pre>
<h3 id="123-k8s安装">1.2.3 K8S安装</h3>
<h4 id="step1-配置源-master--node">step1 配置源 [master + node]</h4>
<pre><code class="language-shell">cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
</code></pre>
<h4 id="step2-安装-kuberneteskubeletkubeadmkubectl-masternode">step2 安装 kubernetes(kubelet/kubeadm/kubectl) [master+node]</h4>
<blockquote>
<p>注意：<strong>阿里源</strong>并未与官网同步<code>gpg</code>（由于官网未开放同步方式, 可能会有索引<code>gpg</code>检查失败的情况，这时请用如下命令安装）</p>
</blockquote>
<ul>
<li>安装 kubernetes, 最好指定版本，否则会使用最新版本。</li>
</ul>
<blockquote>
<p>k8s 每个版本的变化都比较大，这里我们的k8s使用 1.25.0 版本</p>
</blockquote>
<pre><code class="language-shell"># 检测可用的k8s版本(--nogpgcheck 忽略gpg检测)
yum list --nogpgcheck --showduplicates kubeadm --disableexcludes=kubernetes

# 找到我们想要安装的版本，并安装(这里可以用 1.23.8 / 1.25.0 等版本，目前≥1.24.0 版本安装时启用docker，需要特别安装 CRI，尽量使用 docker 与 k8s 兼容的区间版本
# 安装 kubelet、kubeadm、kubectl 组件
# yum install --nogpgcheck kubelet-1.23.8 kubeadm-1.23.8 kubectl-1.23.8
yum -y install --nogpgcheck kubelet-1.25.0 kubeadm-1.25.0 kubectl-1.25.0

# 此时还不能启动 kubelet ：因为此时配置还不能，现在仅仅可设置开机自启动
$ systemctl enable kubelet
</code></pre>
<blockquote>
<p>Created symlink from /etc/systemd/system/multi-user.target.wants/kubelet.service to /usr/lib/systemd/system/kubelet.service.</p>
</blockquote>
<ul>
<li>安装完成后，我们检查一下</li>
</ul>
<pre><code class="language-shell"># 检查 kubectl 版本
kubectl version
##########show start############
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short. Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"25", GitVersion:"v1.25.0", GitCommit:"a866cbe2e5bbaa01cfd5e969aa3e033f3282a8a2", GitTreeState:"clean", BuildDate:"2022-08-23T17:44:59Z", GoVersion:"go1.19", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v4.5.7
The connection to the server localhost:8080 was refused - did you specify the right host or port?
###########show end###########

# 检查 kubeadm 版本
kubeadm version
##########show start############
kubeadm version: &amp;version.Info{Major:"1", Minor:"25", GitVersion:"v1.25.0", GitCommit:"a866cbe2e5bbaa01cfd5e969aa3e033f3282a8a2", GitTreeState:"clean", BuildDate:"2022-08-23T17:43:25Z", GoVersion:"go1.19", Compiler:"gc", Platform:"linux/amd64"}
##########show end############
</code></pre>
<h4 id="step3-安装容器运行时cri-master--node">step3 安装容器运行时(CRI) [master + node]</h4>
<blockquote>
<p>【特别注意】<br>
k8s 1.24及更高版本不能直接兼容docker引擎， Docker Engine 没有实现 CRI， 而这是容器运行时在 Kubernetes 中工作所需要的。<br>
为此，若K8s版本≥1.24时，必须安装一个额外的服务cri-dockerd。 cri-dockerd 是一个基于传统的内置 Docker 引擎支持的项目， 它在 1.24 版本从 kubelet 中移除。<br>
你需要在集群内每个节点上安装一个容器运行时以使Pod可以运行在上面。<br>
Kubernetes 中几个常见的容器运行时：</p>
<ul>
<li>containerd</li>
<li>CRI-O</li>
<li>Docker Engine</li>
<li>Mirantis Container Runtime</li>
</ul>
</blockquote>
<blockquote>
<p>以下是使用 <code>cri-dockerd</code> <strong>适配器</strong>*来将 Docker Engine 与 Kubernetes 集成。</p>
</blockquote>
<h5 id="安装cri-dockerd">安装cri-dockerd</h5>
<pre><code class="language-shell">wget https://github.com/Mirantis/cri-dockerd/releases/download/v0.2.6/cri-dockerd-0.2.6.amd64.tgz
tar -xf cri-dockerd-0.2.6.amd64.tgz
cp cri-dockerd/cri-dockerd /usr/bin/
chmod +x /usr/bin/cri-dockerd
</code></pre>
<h5 id="配置启动服务cri-dockerservice">配置启动服务(cri-docker.service)</h5>
<pre><code class="language-shell">cat &lt;&lt;"EOF" &gt; /usr/lib/systemd/system/cri-docker.service
[Unit]
Description=CRI Interface for Docker Application Container Engine
Documentation=https://docs.mirantis.com
After=network-online.target firewalld.service docker.service
Wants=network-online.target
Requires=cri-docker.socket
[Service]
Type=notify
ExecStart=/usr/bin/cri-dockerd --network-plugin=cni --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.8
ExecReload=/bin/kill -s HUP $MAINPID
TimeoutSec=0
RestartSec=2
Restart=always
StartLimitBurst=3
StartLimitInterval=60s
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity
TasksMax=infinity
Delegate=yes
KillMode=process
[Install]
WantedBy=multi-user.target
EOF
</code></pre>
<blockquote>
<p>主要是通过命令：<br>
<code>ExecStart=/usr/bin/cri-dockerd --network-plugin=cni --pod-infra-container-image=http://registry.aliyuncs.com/google_containers/pause:3.8</code></p>
</blockquote>
<blockquote>
<p>注意：<code>pause</code>的版本可通过: <code>kubeadm config images list</code> (此命令执行时，可能会有点点慢)</p>
</blockquote>
<pre><code class="language-shell">kubeadm config images list
</code></pre>
<h5 id="生成-cri-dockersocket-文件">生成 cri-docker.socket 文件</h5>
<pre><code class="language-shell">cat &lt;&lt;"EOF" &gt; /usr/lib/systemd/system/cri-docker.socket
[Unit]
Description=CRI Docker Socket for the API
PartOf=cri-docker.service
[Socket]
ListenStream=%t/cri-dockerd.sock
SocketMode=0660
SocketUser=root
SocketGroup=docker
[Install]
WantedBy=sockets.target
EOF
</code></pre>
<h5 id="启动-cri-docker-服务并配置开机启动">启动 cri-docker 服务、并配置开机启动</h5>
<pre><code class="language-shell">systemctl daemon-reload
systemctl enable cri-docker
systemctl start cri-docker
systemctl is-active cri-docker
</code></pre>
<h4 id="step4-部署kubernetes-master">step4 部署Kubernetes [master]</h4>
<blockquote>
<p>kubeadm init</p>
</blockquote>
<blockquote>
<p>注意：<br>
node节点不需要执行kubeadm init<br>
这里考虑的是单master，多slave</p>
</blockquote>
<ul>
<li>创建<code>kubeadm.yaml</code>文件，命令如下：</li>
</ul>
<pre><code class="language-shell">kubeadm init \
--apiserver-advertise-address=172.16.1.72 \
--image-repository registry.aliyuncs.com/google_containers \
--kubernetes-version v1.25.0 \
--service-cidr=10.10.0.0/12 \
--pod-network-cidr=10.244.0.0/16 \
--ignore-preflight-errors=all \
--cri-socket unix:///var/run/cri-dockerd.sock

##########show start############
[init] Using Kubernetes version: v1.25.0
[preflight] Running pre-flight checks
 [WARNING CRI]: container runtime is not running: output: time="2025-05-17T16:37:27+08:00" level=fatal msg="validate service connection: CRI v1 runtime API is not implemented for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService"
, error: exit status 1
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
 [WARNING ImagePull]: failed to pull image registry.aliyuncs.com/google_containers/kube-apiserver:v1.25.0: output: time="2025-05-17T16:37:27+08:00" level=fatal msg="validate service connection: CRI v1 image API is not implemented for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.ImageService"
, error: exit status 1
 [WARNING ImagePull]: failed to pull image registry.aliyuncs.com/google_containers/kube-controller-manager:v1.25.0: output: time="2025-05-17T16:37:27+08:00" level=fatal msg="validate service connection: CRI v1 image API is not implemented for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.ImageService"
, error: exit status 1
 [WARNING ImagePull]: failed to pull image registry.aliyuncs.com/google_containers/kube-scheduler:v1.25.0: output: time="2025-05-17T16:37:27+08:00" level=fatal msg="validate service connection: CRI v1 image API is not implemented for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.ImageService"
, error: exit status 1
 [WARNING ImagePull]: failed to pull image registry.aliyuncs.com/google_containers/kube-proxy:v1.25.0: output: time="2025-05-17T16:37:27+08:00" level=fatal msg="validate service connection: CRI v1 image API is not implemented for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.ImageService"
, error: exit status 1
 [WARNING ImagePull]: failed to pull image registry.aliyuncs.com/google_containers/pause:3.8: output: time="2025-05-17T16:37:27+08:00" level=fatal msg="validate service connection: CRI v1 image API is not implemented for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.ImageService"
, error: exit status 1
 [WARNING ImagePull]: failed to pull image registry.aliyuncs.com/google_containers/etcd:3.5.4-0: output: time="2025-05-17T16:37:27+08:00" level=fatal msg="validate service connection: CRI v1 image API is not implemented for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.ImageService"
, error: exit status 1
 [WARNING ImagePull]: failed to pull image registry.aliyuncs.com/google_containers/coredns:v1.9.3: output: time="2025-05-17T16:37:27+08:00" level=fatal msg="validate service connection: CRI v1 image API is not implemented for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.ImageService"
, error: exit status 1
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s-master-1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.0.0.1 172.16.1.72]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s-master-1 localhost] and IPs [172.16.1.72 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s-master-1 localhost] and IPs [172.16.1.72 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 24.503463 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node k8s-master-1 as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node k8s-master-1 as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: 67yn2j.pwv6zn6ci59bw8ma
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

 mkdir -p $HOME/.kube
 sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
 sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

 export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
 https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 172.16.1.72:6443 --token 67yn2j.pwv6zn6ci59bw8ma \
 --discovery-token-ca-cert-hash sha256:ef39262b36489b6f6e8df35af5d921cdee7a4de01148544e2f28c18f5c3d7d1d 
##########show end############
</code></pre>
<blockquote>
<p>浏览器可访问: <a href="https://172.16.1.72:6443" target="_blank" rel="noopener nofollow">https://172.16.1.72:6443</a></p>
</blockquote>
<blockquote>
<p><code>--apiserver-advertise-address=</code> master节点IP<br>
<code>--pod-network-cidr=10.244.0.0/16</code> 要与后面<code>kube-flannel.yml</code>里的ip一致</p>
<blockquote>
<p>也就是使用<code>10.244.0.0/16</code>不要改它。</p>
</blockquote>
</blockquote>
<table>
<thead>
<tr>
<th>参数名</th>
<th>示例值</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>--kubernetes-version</code></td>
<td>v1.25.0</td>
<td>版本</td>
</tr>
<tr>
<td><code>--apiserver-advertise-address</code></td>
<td><code>172.16.1.72</code></td>
<td>当前机器节点IP</td>
</tr>
<tr>
<td><code>--image-repository</code></td>
<td><code>registry.aliyuncs.com/google_containers</code></td>
<td>镜像仓库</td>
</tr>
<tr>
<td><code>--service-cidr</code></td>
<td>10.10.0.0/12</td>
<td><code>service</code>&nbsp;网段</td>
</tr>
<tr>
<td><code>--pod-network-cidr</code></td>
<td>10.244.0.0/16</td>
<td>k8s内部pod节点的网段，不能和&nbsp;<code>--service-cidr</code>&nbsp;相同</td>
</tr>
</tbody>
</table>
<blockquote>
<p>成功后末尾输出信息如下：</p>
</blockquote>
<blockquote>
<p>【注意】<br>
这里是通过<code>kubeadm init</code>安装，所以执行后会下载相应的docker镜像，一般会发现在控制台卡着不动很久，这时就是在下载镜像，你可以 <code>docker images</code> 查看是不是有新的镜像增加。</p>
</blockquote>
<pre><code class="language-shell">docker images
</code></pre>
<h4 id="step5-配置-kubeconfig-master--node">step5 配置 <code>.kube/config</code> [master + node]</h4>
<h5 id="配置-kubeconfig">配置 <code>.kube/config</code></h5>
<ul>
<li>使用kubectl工具，kubeadm安装好后,控制台也会有提示执行以下命令，照着执行。</li>
</ul>
<blockquote>
<p>这也是上一步最后控制台输出的）</p>
</blockquote>
<p><img src="https://img2024.cnblogs.com/blog/1173617/202505/1173617-20250517224929718-1611964696.png" alt="" loading="lazy"></p>
<pre><code class="language-shell">mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
</code></pre>
<h5 id="配置-kubeconfig-环境变量">配置 KUBECONFIG 环境变量</h5>
<pre><code class="language-shell">vim /etc/profile
# 加入以下变量
export KUBECONFIG=/etc/kubernetes/admin.conf

source /etc/profile
</code></pre>
<h5 id="基于-kubectl-查验集群节点">基于 kubectl 查验集群节点</h5>
<ul>
<li>测试一下 kubectl</li>
</ul>
<pre><code class="language-shell">kubectl get nodes
##########show start############
NAME STATUS ROLES AGE VERSION
k8s-master-1 NotReady control-plane 11m v1.25.0
##########show end############
</code></pre>
<blockquote>
<p>一般来说状态先会是NotReady ，可能程序还在启动中，过一会再看看就会变成Ready</p>
</blockquote>
<h4 id="step6-安装pod网络插件-flannel-master--node">step6 安装Pod网络插件 flannel [master + node]</h4>
<ul>
<li>安装pod网络插件 flannel</li>
</ul>
<blockquote>
<p><code>flannel</code> 和 <code>calico</code> 是常用的网络插件。故也有网友推荐 callico。</p>
</blockquote>
<pre><code class="language-shell">kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

或 手动下载下来后，执行： kubectl apply -f /root/kube-flannel.yml

##########show start############
namespace/kube-flannel created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds created
##########show end############
</code></pre>
<blockquote>
<p>【注意】<br>
若报错：The connection to the server&nbsp;<a href="http://raw.githubusercontent.com" target="_blank" rel="noopener nofollow">http://raw.githubusercontent.com</a>&nbsp;was refused - did you specify the right host or port?<br>
原因：国外资源访问不了<br>
解决办法：host配置可以访问的ip（例如下述操作）</p>
</blockquote>
<pre><code class="language-shell">vim /etc/hosts 
# 在/etc/hosts增加以下这条
199.232.28.133 raw.githubusercontent.com
</code></pre>
<h4 id="step7-node节点加入master-node">step7 node节点加入master [node]</h4>
<blockquote>
<p>此步骤即执行 <code>kubeadm init</code>步骤中控制台输出的内容</p>
</blockquote>
<h5 id="提前验证网络连通性">提前验证网络连通性</h5>
<pre><code class="language-shell">ssh 172.16.1.72 -p 6443 -v
</code></pre>
<h5 id="node-加入-master">node 加入 master</h5>
<pre><code class="language-shell">kubeadm join 172.16.1.72:6443 --token ebe5w8.hfd3b59u9ww1r966 \
  --discovery-token-ca-cert-hash sha256:b3683deac5daa34a5778ede0ac0210bfbefce78a380c738aac7c2304c1cb1e4f \
  --ignore-preflight-errors=all \
  --cri-socket unix:///var/run/cri-dockerd.sock
</code></pre>
<blockquote>
<p>【注意】<br>
<code>--ignore-preflight-errors=all</code><br>
<code>--cri-socket unix:///var/run/cri-dockerd.sock</code></p>
<blockquote>
<p>这两行配置命令，一定要加上，以避免各种报错：</p>
</blockquote>
</blockquote>
<blockquote>
<p>由于本文中 master 和 node 实际上，均是同一台机器，故本步骤便不再执行，避免报错：</p>
</blockquote>
<pre><code class="language-log">//运行情况:
[preflight] Running pre-flight checks
 [WARNING FileAvailable--etc-kubernetes-kubelet.conf]: /etc/kubernetes/kubelet.conf already exists
 [WARNING CRI]: container runtime is not running: output: time="2025-05-17T17:26:39+08:00" level=fatal msg="validate service connection: CRI v1 runtime API is not implemented for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService"
, error: exit status 1
 [WARNING Port-10250]: Port 10250 is in use
 [WARNING FileAvailable--etc-kubernetes-pki-ca.crt]: /etc/kubernetes/pki/ca.crt already exists

error execution phase preflight: couldn't validate the identity of the API Server: could not find a JWS signature in the cluster-info ConfigMap for token ID "ebe5w8"
To see the stack trace of this error execute with --v=5 or higher
</code></pre>
<ul>
<li>在master可查看集群的node情况</li>
</ul>
<pre><code class="language-shell">kubectl get nodes
</code></pre>
<p><img src="https://img2024.cnblogs.com/blog/1173617/202505/1173617-20250517225423852-29347952.png" alt="" loading="lazy"></p>
<blockquote>
<p>注：网络截图↑</p>
</blockquote>
<p><img src="https://img2024.cnblogs.com/blog/1173617/202505/1173617-20250517225439583-822453438.png" alt="" loading="lazy"></p>
<blockquote>
<p>注：此处的实际截图</p>
</blockquote>
<ul>
<li>查看 pod 情况</li>
</ul>
<pre><code class="language-shell">kubectl get pod --all-namespaces
##########show start############
NAMESPACE NAME READY STATUS RESTARTS AGE
kube-flannel kube-flannel-ds-lgrbd 1/1 Running 0 50m
kube-system coredns-c676cc86f-nzj2x 1/1 Running 0 80m
kube-system coredns-c676cc86f-s88tw 1/1 Running 0 80m
kube-system etcd-k8s-master-1 1/1 Running 0 80m
kube-system kube-apiserver-k8s-master-1 1/1 Running 0 80m
kube-system kube-controller-manager-k8s-master-1 1/1 Running 0 80m
kube-system kube-proxy-l9g4h 1/1 Running 0 80m
kube-system kube-scheduler-k8s-master-1 1/1 Running 0 80m
##########show end############
</code></pre>
<ul>
<li>到这里，整个k8s集群环境就基本搭建完成了。</li>
</ul>
<blockquote>
<p>【注意】</p>
<ol>
<li>安装时，注意自己安装的程序版本</li>
<li>k8s组件也是以 docker 容器 的形式存在。所以，会下载很多 <code>dokcer image</code></li>
<li>一般安装时可能会出现不少问题，可用 <code>tailf /var/log/messages</code> 跟踪下日志</li>
<li>最好把几台机器的系统时间同步下，节点通讯中的 token 与时间有关</li>
</ol>
</blockquote>
<h3 id="124-检查系统资源消耗">1.2.4 检查系统资源消耗</h3>
<blockquote>
<p>此时，查看下系统的资源消耗情况：</p>
<blockquote>
<p>经实际对比，安装 k8s 前，至现在安装完成k8s后，内存增加了 932-196 = 736 MB；CPU：2核。</p>
</blockquote>
</blockquote>
<pre><code class="language-shell">free -h
</code></pre>
<h3 id="125-安装dashboard-可选步骤">1.2.5 安装dashboard (可选步骤)</h3>
<ul>
<li>详情参见：</li>
</ul>
<blockquote>
<ul>
<li><a href="https://blog.csdn.net/feiyuchuixue/article/details/126373118" target="_blank" rel="noopener nofollow">kubernetes 的安装与部署 - CSDN</a></li>
</ul>
</blockquote>
<ul>
<li>效果预览：（实际，本步骤未执行）<br>
<img src="https://img2024.cnblogs.com/blog/1173617/202505/1173617-20250517225806190-192296024.png" alt="" loading="lazy"></li>
</ul>
<blockquote>
<p>网友的效果图</p>
</blockquote>
<h1 id="k8s核心组件的解读">K8s核心组件的解读</h1>
<h2 id="k8s集群组件">K8s集群组件</h2>
<p><img src="https://img2024.cnblogs.com/blog/1173617/202505/1173617-20250518001730474-359393725.png" alt="" loading="lazy"></p>
<blockquote>
<p><a href="https://kubernetes.io/zh-cn/docs/concepts/overview/components/" target="_blank" rel="noopener nofollow">Kubernetes 组件 - Kubernetes</a></p>
</blockquote>
<h2 id="kubelet--每个工作节点上运行的节点代理">Kubelet : 每个工作节点上运行的节点代理</h2>
<ul>
<li><code>kubelet</code> 是在每个节点上运行的<strong>主要 “节点代理”</strong>。</li>
</ul>
<blockquote>
<p>即： <code>Kubelet</code> 是 kubernetes <strong>工作节点</strong>上的一个<strong>代理组件</strong>，运行在每个<strong>节点</strong>上。</p>
</blockquote>
<p><img src="https://img2024.cnblogs.com/blog/1173617/202505/1173617-20250518001331945-1887343418.png" alt="" loading="lazy"></p>
<blockquote>
<p><code>Kubelet</code>是<strong>工作节点</strong>上的<strong>主要服务</strong>，定期从<code>kube-apiserver</code>组件接收新的或修改的<code>Pod</code><strong>规范</strong>，并确保<code>Pod</code>及其<strong>容器</strong>在期望规范下运行。<br>
同时该组件作为工作节点的<strong>监控组件</strong>，向<code>kube-apiserver</code>汇报主机的运行状况。</p>
</blockquote>
<p><img src="https://img2024.cnblogs.com/blog/1173617/202505/1173617-20250518001436238-1085123765.png" alt="" loading="lazy"></p>
<ul>
<li>它可以使用以下方式之一向 <strong>API 服务器</strong>注册：</li>
</ul>
<blockquote>
<ul>
<li>主机名（hostname）；</li>
<li>覆盖主机名的参数；</li>
<li>特定于某云驱动的逻辑。</li>
</ul>
</blockquote>
<ul>
<li><code>kubelet</code> 是基于 <code>PodSpec</code> 来工作的。</li>
</ul>
<blockquote>
<ul>
<li>每个 <code>PodSpec</code> 是一个描述 Pod 的 YAML 或 JSON 对象。</li>
<li><code>kubelet</code> 接受通过各种机制（主要是通过 <code>apiserver</code>）提供的一组 <code>PodSpec</code>，并确保这些 <code>PodSpec</code> 中描述的容器处于运行状态且运行状况良好。</li>
<li><code>kubelet</code> 不管理不是由 <code>Kubernetes</code> 创建的容器。</li>
</ul>
</blockquote>
<ul>
<li>除了来自 API 服务器的 <code>PodSpec</code> 之外，还可以通过以下两种方式将<strong>容器清单</strong>（<code>manifest</code>）提供给 <code>kubelet</code>。</li>
</ul>
<blockquote>
<ul>
<li><strong>文件</strong>（File）：利用命令行参数传递路径。kubelet 周期性地监视此路径下的文件是否有更新。 监视周期默认为 20s，且可通过参数进行配置。</li>
<li><strong>HTTP 端点</strong>（HTTP endpoint）：利用命令行参数指定 HTTP 端点。 此端点的监视周期默认为 20 秒，也可以使用参数进行配置。</li>
</ul>
</blockquote>
<h3 id="参考文献">参考文献</h3>
<ul>
<li><a href="https://kubernetes.io/zh-cn/docs/reference/command-line-tools-reference/kubelet/" target="_blank" rel="noopener nofollow">kubelet - kubernetes</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/338462784" target="_blank" rel="noopener nofollow">一文看懂 Kubelet - Zhihu</a></li>
</ul>
<h2 id="kubeadm--k8s-部署工具">Kubeadm : k8s 部署工具</h2>
<h3 id="组件简介">组件简介</h3>
<ul>
<li><code>Kubeadm</code> 是一个提供了 <code>kubeadm init</code> 和 <code>kubeadm join</code> 的工具， 作为创建 <code>Kubernetes</code> 集群的 “快捷途径” 的最佳实践。</li>
<li><code>kubeadm</code> 通过执行必要的操作来<strong>启动和运行最小可用集群</strong>。</li>
</ul>
<blockquote>
<p>按照设计，它只关注<strong>启动引导</strong>，而非配置机器。<br>
同样的， 安装各种 “锦上添花” 的扩展，例如 Kubernetes Dashboard、 监控方案、以及特定云平台的扩展，都不在讨论范围内。<br>
相反，k8s官方希望在 <code>kubeadm</code> 之上构建更高级别以及更加合规的工具， 理想情况下，使用 <code>kubeadm</code> 作为<strong>所有部署工作的基准</strong>将会更加易于<strong>创建一致性集群</strong>。</p>
</blockquote>
<h3 id="常用命令">常用命令</h3>
<ul>
<li><code>kubeadm init</code> 用于搭建控制平面节点</li>
<li><code>kubeadm join</code> 用于搭建工作节点并将其加入到集群中</li>
</ul>
<pre><code class="language-shell">kubeadm join 172.16.1.72:6443 --token 67yn2j.pwv6zn6ci59bw8ma \
 --discovery-token-ca-cert-hash sha256:ef39262b36489b6f6e8df35af5d921cdee7a4de01148544e2f28c18f5c3d7d1d 
</code></pre>
<ul>
<li><code>kubeadm upgrade</code> 用于升级 Kubernetes 集群到新版本</li>
<li><code>kubeadm config</code> 如果你使用了 v1.7.x 或更低版本的 kubeadm 版本初始化你的集群，则使用 kubeadm upgrade 来配置你的集群</li>
<li><code>kubeadm token</code> 用于管理 kubeadm join 使用的令牌</li>
</ul>
<pre><code class="language-shell"># 查看历史的token
kubeadm token list

# 再生成一个token
kubeadm token create --print-join-command
</code></pre>
<ul>
<li><code>kubeadm reset</code> 用于恢复通过 kubeadm init 或者 kubeadm join 命令对节点进行的任何变更</li>
<li><code>kubeadm certs</code> 用于管理 Kubernetes 证书</li>
<li><code>kubeadm kubeconfig</code> 用于管理 kubeconfig 文件</li>
<li><code>kubeadm version</code> 用于打印 kubeadm 的版本信息</li>
<li><code>kubeadm alpha</code> 用于预览一组可用于收集社区反馈的特性</li>
</ul>
<h3 id="参考文献-1">参考文献</h3>
<ul>
<li><a href="https://kubernetes.io/zh-cn/docs/reference/setup-tools/kubeadm/" target="_blank" rel="noopener nofollow">Kubeadm - kubernetes</a></li>
</ul>
<h2 id="kubectl--k8s-命令行工具">Kubectl : k8s 命令行工具</h2>
<h3 id="组件简介-1">组件简介</h3>
<ul>
<li><code>Kubernetes</code> 提供 <code>kubectl</code> 是使用 <code>Kubernetes API</code> 与 <code>Kubernetes</code> <strong>集群</strong>的<strong>控制面板</strong>进行通信的<strong>【命令行工具】</strong>。</li>
</ul>
<blockquote>
<p>这个工具叫做 <code>kubectl</code>。</p>
</blockquote>
<ul>
<li>针对配置信息，<code>kubectl</code> 在 <code>$HOME/.kube</code> 目录中查找一个名为 <code>config</code> 的配置文件。</li>
</ul>
<blockquote>
<p>你可以通过设置 <code>KUBECONFIG</code> 环境变量或设置 <code>--kubeconfig</code> 参数来<strong>指定其它</strong> <code>kubeconfig</code> 文件。</p>
</blockquote>
<ul>
<li>有关每个命令的详细信息，包括所有受支持的参数和子命令， 请参阅 kubectl 参考文档。</li>
</ul>
<blockquote>
<p>有关安装说明，请参见安装 kubectl； 如需快速指南，请参见备忘单。<br>
如果你更习惯使用 docker 命令行工具， Docker 用户的 kubectl 介绍了一些 Kubernetes 的等价命令。</p>
</blockquote>
<h3 id="使用语法">使用语法</h3>
<ul>
<li>使用以下语法从终端窗口运行 kubectl 命令：</li>
</ul>
<pre><code class="language-shell">kubectl [command] [TYPE] [NAME] [flags]
</code></pre>
<ul>
<li>其中 <code>command</code>、<code>TYPE</code>、<code>NAME</code> 和 <code>flags</code> 分别是：</li>
</ul>
<blockquote>
<ul>
<li><code>command</code>：指定要对一个或多个资源执行的操作，例如 create、get、describe、delete。</li>
<li><code>TYPE</code>：指定资源类型。资源类型不区分大小写， 可以指定单数、复数或缩写形式。</li>
<li><code>NAME</code>：指定资源的名称。名称区分大小写。 如果省略名称，则显示所有资源的详细信息。例如：<code>kubectl get pods</code>。</li>
</ul>
<blockquote>
<p>在对多个资源执行操作时，你可以按类型和名称指定每个资源，或指定一个或多个文件：</p>
</blockquote>
</blockquote>
<blockquote>
<ul>
<li>要按类型(TYPE)和名称(NAME)指定资源：</li>
<li>要对所有类型(TYPE)相同的资源进行分组，请执行以下操作：<code>TYPE1 name1 name2 name&lt;#&gt;</code>。</li>
</ul>
<blockquote>
<p>例子：<code>kubectl get pod example-pod1 example-pod2</code></p>
</blockquote>
</blockquote>
<blockquote>
<ul>
<li>分别指定多个资源类型：<code>TYPE1/name1 TYPE1/name2 TYPE2/name3 TYPE&lt;#&gt;/name&lt;#&gt;</code>。<br>
例子：kubectl get pod/example-pod1 replicationcontroller/example-rc1</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>用一个或多个文件指定资源：<code>-f file1 -f file2 -f file&lt;#&gt;</code></li>
<li>使用 <code>YAML</code> 而不是 <code>JSON</code></li>
</ul>
<blockquote>
<p>因为 YAML 对用户更友好, 特别是对于配置文件。<br>
例子：<code>kubectl get -f ./pod.yaml</code></p>
</blockquote>
</blockquote>
<blockquote>
<ul>
<li><code>flags</code>： 指定可选的参数。例如，可以使用 <code>-s</code> 或 <code>--server</code> 参数指定 <code>Kubernetes API</code> 服务器的地址和端口。</li>
</ul>
</blockquote>
<ul>
<li>
<p>注意，从<strong>命令行指定的参数</strong>会覆盖默认值和任何相应的<strong>环境变量</strong>。</p>
</li>
<li>
<p>如果你需要帮助，在终端窗口中运行 <code>kubectl help</code></p>
</li>
</ul>
<h3 id="常用命令-1">常用命令</h3>
<h4 id="查询pod">查询pod</h4>
<pre><code class="language-shell">kubectl get pod pod1
kubectl get pods pod1
kubectl get po pod1
</code></pre>
<ul>
<li>按命名空间查询pod</li>
</ul>
<pre><code class="language-shell"># 查询所有命名空间下的 pod
kubectl get pod --all-namespaces

# 根据指定命名空间，查询其下的 pod | 不带等同  -n default
kubectl get pods -n kube-system
</code></pre>
<h4 id="kubectl-apply--以文件或标准输入为准应用或更新资源"><code>kubectl apply</code> : 以文件或标准输入为准应用或更新资源</h4>
<pre><code class="language-shell"># 使用 example-service.yaml 中的定义创建 Service。
kubectl apply -f example-service.yaml

# 使用 example-controller.yaml 中的定义创建 replication controller。
kubectl apply -f example-controller.yaml

# 使用 &lt;directory&gt; 路径下的任意 .yaml、.yml 或 .json 文件 创建对象。
kubectl apply -f &lt;directory&gt;
</code></pre>
<h4 id="kubectl-get--列出一个或多个资源podrcserviceds"><code>kubectl get</code> : 列出一个或多个资源(pod/rc/service/ds)</h4>
<pre><code class="language-shell"># 以纯文本输出格式列出所有 Pod。
kubectl get pods

# 以纯文本输出格式列出所有 Pod，并包含附加信息(如节点名)。
kubectl get pods -o wide

# 以纯文本输出格式列出具有指定名称的副本控制器。提示：你可以使用别名 'rc' 缩短和替换 'replicationcontroller' 资源类型。
kubectl get replicationcontroller &lt;rc-name&gt;

# 以纯文本输出格式列出所有副本控制器和 Service。
kubectl get rc,services

# 以纯文本输出格式列出所有守护程序集，包括未初始化的守护程序集。
kubectl get ds --include-uninitialized

# 列出在节点 server01 上运行的所有 Pod
kubectl get pods --field-selector=spec.nodeName=server01
</code></pre>
<h4 id="kubectl-describe--显示一个或多个资源的详细状态默认情况下包括未初始化的资源"><code>kubectl describe</code> : 显示一个或多个资源的详细状态，默认情况下包括未初始化的资源</h4>
<pre><code class="language-shell"># 显示名为 &lt;pod-name&gt; 的 Pod 的详细信息。
kubectl describe nodes &lt;node-name&gt;

# 显示名为 &lt;pod-name&gt; 的 Pod 的详细信息。
kubectl describe pods/&lt;pod-name&gt;

# 显示由名为 &lt;rc-name&gt; 的副本控制器管理的所有 Pod 的详细信息。
# 记住：副本控制器创建的任何 Pod 都以副本控制器的名称为前缀。
kubectl describe pods &lt;rc-name&gt;

# 描述所有的 Pod
kubectl describe pods
</code></pre>
<h4 id="kubectl-logs--打印-pod-中容器的日志"><code>kubectl logs</code> : 打印 Pod 中容器的日志</h4>
<pre><code class="language-shell"># 返回 Pod &lt;pod-name&gt; 的日志快照。
kubectl logs &lt;pod-name&gt;

# 从 Pod &lt;pod-name&gt; 开始流式传输日志。这类似于 'tail -f' Linux 命令。
kubectl logs -f &lt;pod-name&gt;
</code></pre>
<h3 id="参考文献-2">参考文献</h3>
<ul>
<li><a href="https://kubernetes.io/zh-cn/docs/reference/kubectl/" target="_blank" rel="noopener nofollow">Kubectl - kubernetes</a></li>
<li><a href="https://www.cnblogs.com/johnnyzen/p/18813835" target="_blank">[云原生] K9S：与 Kubernetes 集群交互的开源终端 UI - 博客园/千千寰宇</a></li>
</ul>
<blockquote>
<blockquote>
<p><code>k9s</code> 的部分功能，依赖 <code>kubectl</code> / <code>kubectl</code> 在 windows 的安装与配置</p>
</blockquote>
</blockquote>
<h1 id="faq-for-k8s安装部署">FAQ for K8s安装部署</h1>
<h2 id="qk8s在kubeadm-init后没有记录kubeadm-join如何查询">Q：K8S在kubeadm init后，没有记录kubeadm join如何查询？</h2>
<pre><code class="language-shell"># 再生成一个token即可
kubeadm token create --print-join-command

# 可通过如下命令，查看历史的token
kubeadm token list
</code></pre>
<h2 id="qnode节点kubeadm-join失败后要重新join怎么办">Q：node节点kubeadm join失败后，要重新join怎么办？</h2>
<pre><code class="language-shell"># 先执行
kubeadm -y reset

# 再执行
kubeadm join  xx.....
</code></pre>
<h2 id="q重启kubelet">Q：重启kubelet？</h2>
<pre><code class="language-shell">systemctl daemon-reload
systemctl restart kubelet
</code></pre>
<h2 id="q查询k8s集群的-node--pod">Q：查询K8s集群的 node / pod？</h2>
<pre><code class="language-shell"># 查询节点
kubectl get nodes

# 查询pods 一般要带上"-n"即命名空间。不带等同  -n default
kubectl get pods -n kube-system
</code></pre>
<h2 id="qk8s弃用docker">Q：k8s"弃用"docker?</h2>
<ul>
<li>记得当时“k8s弃用docker”解读满天飞，很多文章说docker已死。后来也出来一波说并不是完全弃用docker只是移除了docker作为容器运行时的支持。</li>
<li>k8s去掉的其实是dockershim，这是一个在kubelet和docker之间的适配器，用来将docker的接口转换为k8s所需的CRI（容器运行时接口）。这样做是为了简化k8s的架构，提高性能和安全性，以及支持更多的容器运行时。</li>
<li>k8s并没有完全弃用docker，而是默认弃用了docker作为容器运行时的支持。这意味着k8s将不再使用docker来创建和运行容器，而是使用其他符合CRI标准的运行时，如containerd或CRI-O123。这样做的原因是docker不符合CRI标准，而且需要一个叫做dockershim的中间层来适配k8s的API。</li>
<li>k8s去掉docker并不意味着docker就没有用了，或者你不能或者不应该用docker作为开发工具。docker仍然是构建容器镜像的非常有用的工具，而且它生成的镜像是符合OCI（开放容器倡议）标准的。这意味着任何用docker构建的镜像都可以在k8s中与其他容器运行时正常工作。所以，你不需要担心你的docker镜像会失效或者不兼容。</li>
</ul>
<h2 id="qk8s常用运维命令">Q：k8s常用运维命令？</h2>
<pre><code class="language-shell"># 查看k8s 运行日志命令
# 比较有用，在k8s 启动、kubeadm init、kubeadm join 阶段可以辅助分析问题。
journalctl -xefu kubelet 

# 查看k8s驱动
systemctl show --property=Environment kubelet |cat
##########show start##########
Environment=KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml
##########show end##########

# 重启k8s
systemctl restart kubelet

# 启动k8s
systemctl start kubelet
# 停止k8s
systemctl stop kubelet
# 开机自启k8s
systemctl enable kubelet

# dashboard 获取token
kubectl describe secret admin-user -n kubernetes-dashboard

# kubeadm 重置
# 有些时候我们在使用kubeadm init 命令时会报错，我们根据错误提示修复问题后需要重新进行 init 操作，因此需要进行reset重置
kubeadm reset
</code></pre>
<h2 id="q环境信息">Q：环境信息？</h2>
<pre><code class="language-shell"># k8s 安装目录
/etc/kubernetes/
##########show start##########
total 44
drwxr-xr-x 4 root root 4096 May 17 16:37 .
drwxr-xr-x. 83 root root 4096 May 17 17:08 ..
-rw------- 1 root root 5635 May 17 16:37 admin.conf
-rw------- 1 root root 5667 May 17 16:37 controller-manager.conf
-rw------- 1 root root 1983 May 17 16:37 kubelet.conf
drwxr-xr-x 2 root root 4096 May 17 16:37 manifests
drwxr-xr-x 3 root root 4096 May 17 16:37 pki
-rw------- 1 root root 5615 May 17 16:37 scheduler.conf
##########show end##########

# 组件配置文件目录
/etc/kubernetes/manifests/
##########show start##########
[root@k8s-master-1 ~]# ls -la /etc/kubernetes/manifests/
total 24
drwxr-xr-x 2 root root 4096 May 17 16:37 .
drwxr-xr-x 4 root root 4096 May 17 16:37 ..
-rw------- 1 root root 2400 May 17 16:37 etcd.yaml
-rw------- 1 root root 3362 May 17 16:37 kube-apiserver.yaml
-rw------- 1 root root 2878 May 17 16:37 kube-controller-manager.yaml
-rw------- 1 root root 1464 May 17 16:37 kube-scheduler.yaml
##########show end##########

# 自定义dashboard yaml文件目录 (此目录，因实际未安装 dashboard，故不存在)
/opt/kube-dashboard/conf/
##########show start##########
总用量 8
-rw-r--r--. 1 root root 1124 6月 29 08:41 admin-user-dashboard.yaml
-rw-r--r--. 1 root root 285 6月 29 08:25 k8s-dashboard.yaml
##########show end##########
</code></pre>
<h1 id="y-推荐文献">Y 推荐文献</h1>
<ul>
<li>Kubernetes</li>
</ul>
<blockquote>
<ul>
<li><a href="https://kubernetes.io/zh-cn/docs/concepts/architecture/" target="_blank" rel="noopener nofollow">Kubernetes 架构 - Kubernetes</a></li>
<li><a href="https://kubernetes.io/zh-cn/docs/concepts/overview/components/" target="_blank" rel="noopener nofollow">Kubernetes 组件 - Kubernetes</a></li>
</ul>
</blockquote>
<ul>
<li><a href="https://www.cnblogs.com/johnnyzen/p/17439295.html" target="_blank">[Docker] 基于CENTOS7安装Docker环境 - 博客园/千千寰宇</a></li>
</ul>
<blockquote>
<ul>
<li><a href="https://www.cnblogs.com/johnnyzen/p/18746970" target="_blank">[Docker] 如何给 Docker 配置网络代理?  - 博客园/千千寰宇</a></li>
</ul>
</blockquote>
<ul>
<li><a href="https://www.cnblogs.com/johnnyzen/p/17439306.html" target="_blank">[Docker] Docker 基础教程（概念/原理/基础操作） - 博客园/千千寰宇</a></li>
<li><a href="https://www.cnblogs.com/johnnyzen/p/17683863.html" target="_blank">[虚拟化] K8S概述 - 博客园/千千寰宇</a></li>
<li><a href="https://www.cnblogs.com/johnnyzen/p/18864204" target="_blank">[K8s/云原生] 开源 Kubernetes Web UI 管理工具 - 博客园/千千寰宇</a></li>
<li></li>
<li><a href="https://blog.csdn.net/MacWx/article/details/129530878" target="_blank" rel="noopener nofollow">kubernetes+KubeEdge云边环境的安装与部署 - CSDN</a></li>
<li><strong>KubeEdge与Kubernets的版本兼容性</strong></li>
</ul>
<p><img src="https://img2024.cnblogs.com/blog/1173617/202505/1173617-20250517231005751-837221032.png" alt="" loading="lazy"></p>
<blockquote>
<ul>
<li><a href="https://github.com/kubeedge/kubeedge#kubernetes-compatibility" target="_blank" rel="noopener nofollow">GitHub - kubeedge/kubeedge: Kubernetes Native Edge Computing Framework (project under CNCF)</a></li>
</ul>
<blockquote>
<p><a href="https://github.com/kubeedge/kubeedge#kubernetes-compatibility" target="_blank" rel="noopener nofollow">https://github.com/kubeedge/kubeedge#kubernetes-compatibility</a></p>
</blockquote>
</blockquote>
<blockquote>
<p>k8s 1.17 - 1.23</p>
</blockquote>
<table>
<thead>
<tr>
<th></th>
<th>Kubernetes 1.17</th>
<th>Kubernetes 1.18</th>
<th>Kubernetes 1.19</th>
<th>Kubernetes 1.20</th>
<th>Kubernetes 1.21</th>
<th>Kubernetes 1.22</th>
<th>Kubernetes 1.23</th>
</tr>
</thead>
<tbody>
<tr>
<td>KubeEdge 1.11</td>
<td>✔</td>
<td>✔</td>
<td>✔</td>
<td>✔</td>
<td>✔</td>
<td>✔</td>
<td>-</td>
</tr>
<tr>
<td>KubeEdge 1.12</td>
<td>✔</td>
<td>✔</td>
<td>✔</td>
<td>✔</td>
<td>✔</td>
<td>✔</td>
<td>-</td>
</tr>
<tr>
<td>KubeEdge 1.13</td>
<td>✔</td>
<td>✔</td>
<td>✔</td>
<td>✔</td>
<td>✔</td>
<td>✔</td>
<td>✔</td>
</tr>
</tbody>
</table>
<blockquote>
<p>k8s 1.25 - 1.30</p>
</blockquote>
<table>
<thead>
<tr>
<th>Kubernetes 1.25</th>
<th>Kubernetes 1.26</th>
<th>Kubernetes 1.27</th>
<th>Kubernetes 1.28</th>
<th>Kubernetes 1.29</th>
<th>Kubernetes 1.30</th>
</tr>
</thead>
<tbody>
<tr>
<td>KubeEdge 1.16</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>KubeEdge 1.17</td>
<td>+</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>-</td>
</tr>
<tr>
<td>KubeEdge 1.18</td>
<td>+</td>
<td>+</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
</tr>
<tr>
<td>KubeEdge 1.19</td>
<td>+</td>
<td>+</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
</tr>
<tr>
<td>KubeEdge 1.20</td>
<td>+</td>
<td>+</td>
<td>+</td>
<td>✓</td>
<td>✓</td>
</tr>
<tr>
<td>KubeEdge HEAD (master)</td>
<td>+</td>
<td>+</td>
<td>+</td>
<td>✓</td>
<td>✓</td>
</tr>
</tbody>
</table>
<pre><code class="language-txt">√ KubeEdge和Kubernetes版本完全兼容。
+ KubeEdge具有Kubernetes版本中可能不存在的功能或API对象。
- Kubernetes版本有KubeEdge无法使用的特性或API对象。
</code></pre>
<h1 id="x-参考文献">X 参考文献</h1>
<ul>
<li><a href="https://blog.csdn.net/feiyuchuixue/article/details/126373118" target="_blank" rel="noopener nofollow">kubernetes 的安装与部署 - CSDN</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/654344714" target="_blank" rel="noopener nofollow">Kubernetes v1.25.0集群搭建实战案例(新版本含Docker容器运行时) - Zhihu</a></li>
</ul>
<blockquote>
<p>其他阅读过的，有一定启发的文献：</p>
</blockquote>
<ul>
<li><a href="https://cloud.tencent.com/developer/article/1704686" target="_blank" rel="noopener nofollow">centos7安装kubernetes教程 - 腾讯云</a> 【参考】</li>
<li><a href="https://zhuanlan.zhihu.com/p/654344714" target="_blank" rel="noopener nofollow">Kubernetes v1.25.0集群搭建实战案例(新版本含Docker容器运行时) - Zhihu</a></li>
<li></li>
<li><a href="https://blog.csdn.net/m0_64284147/article/details/128124322" target="_blank" rel="noopener nofollow">Found multiple CRI endpoints on the host. Please define which one do you wish to use by setting..... - CSDN</a></li>
<li><a href="https://blog.csdn.net/chen_haoren/article/details/108580338" target="_blank" rel="noopener nofollow">部署k8s的时候kube-flannel.yml下载不下来解决 - CSDN</a></li>
</ul>
<blockquote>
<p>wget <a href="https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml" target="_blank" rel="noopener nofollow">https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</a></p>
</blockquote>
<ul>
<li><a href="https://developer.aliyun.com/article/1354243" target="_blank" rel="noopener nofollow">【K8S系列】深入解析k8s网络插件—Flannel - 阿里云</a></li>
</ul>
<blockquote>
<p>Flannel：Flannel是一个流行的CNI插件，它使用虚拟网络覆盖技术（overlay network）来连接不同节点上的容器。Flannel支持多种后端驱动，如VXLAN、UDP、Host-GW等</p>
</blockquote>
<ul>
<li><a href="https://blog.csdn.net/weixin_42104154/article/details/123961616" target="_blank" rel="noopener nofollow">k8s踩坑——网络插件fannel安装 - CSDN</a></li>
</ul>
<blockquote>
<p>搭建好k8s集群后，集群中的节点status全部都是NotReady，最后一查资料，原来是网络插件没有安装，看到有安装fannel</p>
</blockquote>
<ul>
<li><a href="https://blog.csdn.net/weixin_52156647/article/details/129765134" target="_blank" rel="noopener nofollow">kubernetes新版本使用kubeadm init的超全问题解决和建议 - CSDN</a></li>
<li><a href="https://blog.csdn.net/qq_41490274/article/details/124663897" target="_blank" rel="noopener nofollow">解决Docker Unable to connect to the server: EOF - CSDN</a></li>
</ul>
<blockquote>
<p><code>export no_proxy=kubernetes.docker.internal,docker-for-desktop,$no_proxy</code></p>
</blockquote>
<ul>
<li><a href="https://blog.csdn.net/ZYK213/article/details/109075445" target="_blank" rel="noopener nofollow">k8s Unable to connect to the server: EOF 问题解决 - CSDN</a></li>
</ul>

</div>
<div id="MySignature" role="contentinfo">
    <div class="essaySuffix-box">
    <div class="essaySuffix-box-left" style=" margin: 6px auto; ">
        <img src="https://blog-static.cnblogs.com/files/johnnyzen/cnblogs-qq-group-qrcode.gif?t=1679679148" alt="QQ沟通交流群" onload="changeImg(this,200,100)">
    </div>
<div class="essaySuffix-box-right">
    <span class="essaySuffix-right-title">本文作者</span>：
        <strong><span><a href="https://github.com/Johnny-ZTSD" target="_blank">千千寰宇</a></span></strong>
    <br>
    <span style="font-weight: bold; white-space:nowrap;">本文链接</span>：
        <a href="https://www.cnblogs.com/johnnyzen" target="_blank" id="articleLinkElement"> https://www.cnblogs.com/johnnyzen</a>
    <br>
    <span class="essaySuffix-right-title">关于博文</span>：评论和私信会在第一时间回复，或<a href="https://msg.cnblogs.com/msg/send/johnnyzen" target="_blank">直接私信</a>我。
    <br>
    <span class="essaySuffix-right-title">版权声明</span>：本博客所有文章除特别声明外，均采用 <a title="https://creativecommons.org/licenses/by-nc-nd/4.0/" href="http://blog.sina.com.cn/s/blog_896327b90102y6c6.html" alt="BY-NC-SA" target="_blank">BY-NC-SA</a> 
    许可协议。转载请注明出处！<br>
    <span class="essaySuffix-right-title">日常交流</span>：大数据与软件开发-QQ交流群: 774386015<strong>
        <span style="color: #ff0000; font-size: 12pt;">【<a id="post-up" onclick="votePost(getArticleNumber(),'Digg')" href="javascript:void(0);">入群二维码</a>】</span></strong>参见左下角。您的支持、鼓励<span style="color: #ff0000; font-size: 12pt;"></span>是博主技术写作的重要动力！
    <br>
</div>
<div style="clear: both;">
</div>
</div>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.1646701034849537" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-05-18 00:19">2025-05-17 23:09</span>&nbsp;
<a href="https://www.cnblogs.com/johnnyzen">千千寰宇</a>&nbsp;
阅读(<span id="post_view_count">15</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18881700);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18881700', targetLink: 'https://www.cnblogs.com/johnnyzen/p/18881700', title: '[虚拟化/云原生] Kubernetes 安装部署指南' })">举报</a>
</div>
        