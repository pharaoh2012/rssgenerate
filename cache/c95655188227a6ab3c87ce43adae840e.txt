
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/tshaaa/p/18670731" title="发布于 2025-01-14 14:38">
    <span role="heading" aria-level="2">用于决策的世界模型 -- 论文 World Models (2018) &amp; PlaNet (2019) 讲解</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                    <div id="cnblogs_post_description" style="display: none">
        
        介绍了两篇世界模型的文章 World Models (2018) 和 Learning Latent Dynamics for Planning from Pixels (2019)，主要侧重点是世界模型在决策和规划中的应用。
    </div>
<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p>参考资料：</p>
<ul>
<li><a href="https://arxiv.org/abs/2411.14499" target="_blank" rel="noopener nofollow">[2411.14499] Understanding World or Predicting Future? A Comprehensive Survey of World Models</a></li>
<li><a href="https://arxiv.org/abs/1803.10122" target="_blank" rel="noopener nofollow">[1803.10122] World Models</a></li>
<li><a href="https://proceedings.mlr.press/v97/hafner19a.html" target="_blank" rel="noopener nofollow">Learning Latent Dynamics for Planning from Pixels</a></li>
<li><a href="https://github.com/Kaixhin/PlaNet" target="_blank" rel="noopener nofollow">Kaixhin/PlaNet: Deep Planning Network: Control from pixels by latent planning with learned dynamics</a></li>
</ul>
<h1 id="世界模型">世界模型</h1>
<h2 id="简介">简介</h2>
<p><strong>世界模型</strong>：一种<strong>理解世界当前状态</strong>或<strong>预测其未来动态</strong>的工具。</p>
<p><strong>世界模型的两个主要功能</strong>：</p>
<ol>
<li>构建内部表征以理解世界运作机制。</li>
<li>预测未来状态以模拟和指导决策。</li>
</ol>
<h2 id="分类">分类</h2>
<p><img src="https://img2024.cnblogs.com/blog/2568389/202501/2568389-20250114142642821-713581888.png" alt="image" loading="lazy"></p>
<blockquote>
<p>图片来自<a href="https://arxiv.org/abs/2411.14499" target="_blank" rel="noopener nofollow">[2411.14499] Understanding World or Predicting Future? A Comprehensive Survey of World Models</a>.</p>
</blockquote>
<ul>
<li>作者按照模型的侧重点不同，将世界模型分成两个大类，即：
<ul>
<li>Internal Representations.</li>
<li>Future Predictions.</li>
</ul>
</li>
<li>经常能在网上刷到的LeCun力推世界模型，说的是JEPA.</li>
<li>左边分支的世界模型也可以做"future prediction"，作为学习模型参数过程的一个副产物吧 (视觉模块的reconstruction)。</li>
</ul>
<p>这里讨论的是两篇world model for decision-making的文章。</p>
<h1 id="world-models-2018">World Models (2018)</h1>
<p><img src="https://img2024.cnblogs.com/blog/2568389/202501/2568389-20250114142708213-408319658.png" alt="image" loading="lazy"></p>
<blockquote>
<p>AI社区中，首篇系统性介绍世界模型的文章。</p>
</blockquote>
<h2 id="人类的心理模型">人类的心理模型</h2>
<p>简单可以概括成以下几点：</p>
<ul>
<li>对于外部世界的大量信息流，人脑能够学习到外部世界时空信息的抽象表示，作为我们对外部世界的"建模"。</li>
<li>我们所看到一切都基于脑中模型对未来的预测。</li>
</ul>
<blockquote>
<p><img src="https://img2024.cnblogs.com/blog/2568389/202501/2568389-20250114142721701-900426175.png" alt="image" loading="lazy"></p>
</blockquote>
<ul>
<li>我们能够基于这个预测模型本能地行动，在面对危险时做出快速的反射性行为。</li>
</ul>
<blockquote>
<p>打棒球的例子： 击球手需要在毫秒级别的时间内决定如何挥棒 —— 这比视觉信号到大脑的时间还要短。</p>
</blockquote>
<p>在之后的世界模型结构和实验中，都可以看到这个心理模型的影子。</p>
<h2 id="模型结构">模型结构</h2>
<p><img src="https://img2024.cnblogs.com/blog/2568389/202501/2568389-20250114142747482-1125662320.png" alt="image" loading="lazy"></p>
<p>世界模型主要由两个模块组成：视觉模块、记忆模块。<br>
1. 视觉模块：将外部世界的高维观测，压缩成低维的特征。<br>
2. 记忆模块：整合历史信息，预测未来。</p>
<p>控制器会利用世界模型给出的信息进行决策。</p>
<h3 id="视觉模块">视觉模块</h3>
<p>作者在文章中使用VAE的Encoder部分作为视觉模块。<br>
<img src="https://img2024.cnblogs.com/blog/2568389/202501/2568389-20250114142758399-1498002179.png" alt="image" loading="lazy"></p>
<h3 id="记忆模块">记忆模块</h3>
<p>作者在文章中使用MDN-RNN作为记忆模块。<br>
<img src="https://img2024.cnblogs.com/blog/2568389/202501/2568389-20250114142808228-149081435.png" alt="image" loading="lazy"></p>
<ul>
<li>MDN指的是mixture density networks，就是一个建模混合模型的网络，文中使用的是高斯混合模型 (GMM)，此时神经网络除了输出每个高斯分布的均值和标准差，还需要输出用于选择高斯分布的类别分布。</li>
<li>MDN会接受一个temperature参数<span class="math inline">\(\tau\)</span>，用于调整不确定性。</li>
<li>在图中，MDN-RNN建模的是<span class="math inline">\(P(z_{t+1}\mid a_t, z_t, h_t)\)</span>.</li>
<li>除了隐状态之外，记忆模块可能还需要建模其他东西，比如奖励<span class="math inline">\(P(r_{t+1} \mid a_t, z_t, h_t)\)</span>，游戏结束的信号<span class="math inline">\(P(\text{done}_{t+1} \mid a_t, z_t, h_t)\)</span>.</li>
</ul>
<p><strong>NOTE</strong>：为什么要使用混合模型，即使VAE的隐变量空间只是一个对角高斯？作者的解释是：混合模型中的离散部分 (选择哪一个高斯组分)，有利于建模环境中的离散随机事件。比如说NPC在平静状态和警觉状态下的表现不同。</p>
<h3 id="控制器">控制器</h3>
<p>作者将整个模型的复杂性都集中到了视觉和记忆模块，有意使得控制器的结构尽可能简单：</p>
<p></p><div class="math display">\[a_t = W_c[z_t~~h_t] + b_c
\]</div><p></p><p>就是单层的神经网络。</p>
<h2 id="模型训练和实验">模型训练和实验</h2>
<p>文章官网<a href="https://worldmodels.github.io/" target="_blank" rel="noopener nofollow">World Models</a>，有gif演示，而且可以试玩模型"梦中"的游戏。</p>
<h3 id="训练">训练</h3>
<p>两个实验都是先单独训练世界模型 (无监督)：</p>
<ol>
<li>使用<strong>随机策略</strong>收集一系列的游戏图像。</li>
<li>使用这些图像训练好VAE。</li>
<li>在训练好的VAE基础上，训练好MDN-RNN。</li>
</ol>
<p>之后部署世界模型并训练控制器。两个实验的主要区别在部署：</p>
<ul>
<li>
<p>Car Racing实验：直接在实际环境部署，训练好了控制器之后，又给出了在模型"梦中"的模拟。<img src="https://img2024.cnblogs.com/blog/2568389/202501/2568389-20250114142843517-727695625.png" alt="image" loading="lazy"></p>
</li>
<li>
<p>VizDoom实验：先在"梦中"部署，训练好了控制器之后，再将整个模型转移到实际环境查看效果。<img src="https://img2024.cnblogs.com/blog/2568389/202501/2568389-20250114142849871-994795477.png" alt="image" loading="lazy"></p>
</li>
</ul>
<p><strong>NOTE</strong>：在两个实验中，世界模型都没有建模环境的奖励。第一个实验中，奖励只在训练控制器的时候由实际环境给出；第二个实验中，指标是存活时间，不需要奖励。</p>
<p><strong>REMARK</strong>：训练成功之后，模型实际上成为了游戏的"模拟器"，学习到了游戏逻辑 (角色中弹后会重新开始)、敌人行为 (按一定时间间隔发射子弹)、物理机制 (子弹飞行速度)等。</p>
<h3 id="实验">实验</h3>
<p>Car Racing：<br>
<img src="https://img2024.cnblogs.com/blog/2568389/202501/2568389-20250114143030680-190631163.png" alt="image" loading="lazy"></p>
<p>VizDoom：<br>
<img src="https://img2024.cnblogs.com/blog/2568389/202501/2568389-20250114143033831-280185515.png" alt="image" loading="lazy"></p>
<p><strong>消融实验1 -- 视觉模块+记忆模块的优越性</strong><br>
在Car Racing中，消融实验显示，单独的视觉模块效果不如一整个的世界模型 (但是也已经超过了DQN和A3C)</p>
<p><strong>消融实验2 -- 用tau调整随机性</strong><br>
在VizDoom实验中，由于模型并非完全精确，控制器可能会利用模型的缺陷来在模拟器中达到高分，一旦部署到实际环境，控制器就不行了。</p>
<p>为了防止这一点，MDN-RNN预测的是具有随机性的环境，并通过调整不确定性参数<span class="math inline">\(\tau\)</span>来控制随机性。在实验中，<span class="math inline">\(\tau=1.15\)</span>时效果最好。</p>
<p>当<span class="math inline">\(\tau=0.1\)</span>时，模型几乎是确定性的，这时候敌人甚至无法发射子弹，所以出现了在模拟器中非常高分，实际环境中却非常低分的情况。</p>
<p><strong>跑分对比实验</strong></p>
<ul>
<li>Car Racing实验：取得的分数超过了先前的基于深度强化学习的方法，如DQN、A3C.</li>
<li>VizDoom实验：在梦中学会了如何躲避怪物的子弹，部署到实际环境后的存活时长也超过了先前。</li>
</ul>
<h2 id="迭代训练过程">迭代训练过程</h2>
<p>本文的实验环境简单，所以是使用随机策略采样，分别训练三个模块。面对更复杂的任务，可能需要三个模块一起训练，但是本文只是提了一下记忆模块和控制器一起训练的流程：<br>
<img src="https://img2024.cnblogs.com/blog/2568389/202501/2568389-20250114143108458-431068432.png" alt="image" loading="lazy"></p>
<p>三个模块一起训练的好处是：</p>
<ol>
<li>视觉模块会倾向于学习到有利于当前任务的特征。</li>
<li>记忆模块可以对控制器进行学习，控制器又可以基于记忆模块继续改进，如此往复。</li>
<li>可以使用训练中的控制器进行轨迹采样而不是随机策略。</li>
</ol>
<h1 id="learning-latent-dynamics-for-planning-from-pixels-2019">Learning Latent Dynamics for Planning from Pixels (2019)</h1>
<p><img src="https://img2024.cnblogs.com/blog/2568389/202501/2568389-20250114143123028-1901635449.png" alt="image" loading="lazy"></p>
<p>相对于上一篇，这篇的改进：</p>
<ol>
<li>假定了环境是部分可观测马尔可夫决策过程 (POMDP)，世界模型就是在学习这个POMDP.</li>
<li>给出了一套结合模型预测控制 (MPC) 方法的训练过程 —— Deep Planning Network (PlaNet).</li>
<li>提出基于确定性和随机性结合的状态空间模型 (RSSM)，而不是仅有确定性状态的RNN和仅有随机性状态的SSM.</li>
<li>给出了适用于多步预测的变分推断方法 —— latent overshooting.</li>
</ol>
<h2 id="problem-setup">Problem setup</h2>
<p>假定实际的环境是一个POMDP：<br>
<img src="https://img2024.cnblogs.com/blog/2568389/202501/2568389-20250114143139112-1412168778.png" alt="image" loading="lazy"></p>
<p>目标是学习到一个策略，能够最大化期望累积回报<span class="math inline">\(\mathbb E[\sum r_t]\)</span>。</p>
<h2 id="deep-planning-network">Deep planning network</h2>
<p>这里先讲世界模型+MPC的学习和规划算法。</p>
<p><img src="https://img2024.cnblogs.com/blog/2568389/202501/2568389-20250114143148492-559969927.png" alt="image" loading="lazy"></p>
<p>while循环内部，总体上分成三个部分：模型学习，实时规划+数据收集，更新数据库。</p>
<h3 id="模型学习">模型学习</h3>
<p>从数据库中随机抽取观测序列的小批量，然后使用梯度方法学习。</p>
<h3 id="实时规划数据收集">实时规划+数据收集</h3>
<p>总体上就是一个有限时间域的MPC框架，在每个time step按三步走：</p>
<ol>
<li><strong>Observe</strong>：获得当前时刻的状态。由于这里在隐状态空间进行规划，所以需要从历史的观测数据中推断当前状态 (通过隐变量的后验概率)。</li>
<li><strong>Predict and plan</strong>：利用当前学习到的模型，解一个有限时间域的最优控制问题，获得一串动作序列。本文中的planner使用的是cross entropy method (CEM).</li>
<li><strong>Act</strong>：对环境使用这串动作序列的第一个动作<span class="math inline">\(a_t\)</span>，移动到下一个time step. 这里用了一个trick，把取得的动作<span class="math inline">\(a_t\)</span>重复了<span class="math inline">\(R\)</span>次 (用相同的action，连续走了<span class="math inline">\(R\)</span>步)，取reward的总和作为当前时刻的reward，取最终的第<span class="math inline">\(R\)</span>观测<span class="math inline">\(o_{t+1}^R\)</span>作为下一个时刻的观测<span class="math inline">\(o_{t+1}\)</span>。</li>
</ol>
<h3 id="更新数据库">更新数据库</h3>
<p>将上一个部分收集到的观测序列加入到数据库中，以供世界模型的进一步更新。</p>
<p><strong>NOTE</strong>：相对于model-free RL算法，model-based planning的一大优势就是数据利用率提高了。体现在planning取得的观测序列可以反复用于世界模型的学习。</p>
<h2 id="rssm">RSSM</h2>
<blockquote>
<p>这种模型也叫：Non-linear Kalman filter, sequential VAE, deep variational bayes filter，看了一眼相关的文章，好像要从头到尾讲明白 (像VAE那样) 比较复杂。</p>
</blockquote>
<p>这里浅浅讲一下世界模型的结构以及训练的Loss。</p>
<h3 id="latent-state-space-model">Latent state-space model</h3>
<p><img src="https://img2024.cnblogs.com/blog/2568389/202501/2568389-20250114143215548-379011025.png" alt="image" loading="lazy"></p>
<p>使用下面的encoder来近似后验概率：</p>
<p></p><div class="math display">\[q(s_{\le t} \mid o_{\le t},a_{&lt;t}) = \prod_{t=1}^T q(s_t\mid s_{t-1},a_{t-1},o_t)
\]</div><p></p><p>都使用神经网络参数化的高斯分布表示，其中observation model和encoder用的是卷积网络。</p>
<h3 id="training-objective">Training Objective</h3>
<p>通过最大化log Evidence来训练：</p>
<p></p><div class="math display">\[\arg\max \ln p(o_{\le t} \mid a_{&lt;t})
\]</div><p></p><p>接下来推导ELBO.</p>
<p>先拆成边际化的形式</p>
<p></p><div class="math display">\[\ln p(o_{\le T} \mid a_{&lt;T}) = \ln \int p(o_{\le T}, s_{\le T} \mid a_{&lt;T}) \text{d}s
\]</div><p></p><p>把联合概率拆开</p>
<p></p><div class="math display">\[\ln p(o_{\le T} \mid a_{&lt;T}) = \ln \int p(o_{\le T} \mid s_{\le T}, a_{&lt;T})  p(s_{\le T} \mid a_{&lt;T}) \text{d}s
\]</div><p></p><p>写成期望的形式</p>
<p></p><div class="math display">\[\ln p(o_{\le t} \mid a_{&lt;t}) = \ln \mathbb E_{p(s_{\le t}\mid a_{&lt;t})}[ p(o_{\le t} \mid s_{\le t}, a_{&lt;t})  ]
\]</div><p></p><p>利用重要性采样方法，转变成从encoder采样</p>
<p></p><div class="math display">\[\ln p(o_{\le t} \mid a_{&lt;t}) = \ln \mathbb E_{q(s_{\le t}\mid o_{\le t},a_{&lt;t})}[ p(o_{\le t} \mid s_{\le t}, a_{&lt;t})  p(s_{\le t}\mid a_{&lt;t}) / q(s_{\le t}\mid o_{\le t},a_{&lt;t})]
\]</div><p></p><p>链式分解，并利用模型的条件独立性化简 (概率图参考下面的)</p>
<p></p><div class="math display">\[\ln p(o_{\le t} \mid a_{&lt;t}) = \ln \mathbb E_{q(s_{\le t}\mid o_{\le t},a_{&lt;t})}[\prod p(o_t \mid s_t)  p(s_t \mid s_{t-1},a_{t-1}) / q(s_{\le t}\mid o_{\le t},a_{&lt;t})]
\]</div><p></p><p>根据Jensen不等式，<span class="math inline">\(\ln \mathbb E[x] \ge \mathbb E[\ln(x)]\)</span></p>
<p></p><div class="math display">\[\ln p(o_{\le t} \mid a_{&lt;t}) \ge \mathbb E_{q(s_{\le t}\mid o_{\le t},a_{&lt;t})}[\sum_t \ln p(o_t \mid s_t) + \ln p(s_t \mid s_{t-1},a_{t-1}) - \ln q(s_{\le t}\mid o_{\le t},a_{&lt;t})]
\]</div><p></p><p>右边可以写成reconstruction + KL的形式，最后就是<br>
<img src="https://img2024.cnblogs.com/blog/2568389/202501/2568389-20250114143239537-2006458350.png" alt="image" loading="lazy"></p>
<h3 id="确定性和随机性结合---rssm">确定性和随机性结合 - RSSM</h3>
<p><img src="https://img2024.cnblogs.com/blog/2568389/202501/2568389-20250114143233683-658382136.png" alt="image" loading="lazy"></p>
<ul>
<li>纯确定性的世界模型：模型难以预测多种可能的未来情况；容易被planner利用模型缺陷 (在World Models中，通过MDN添加随机性来缓解这一点，但本质还是确定性的)</li>
<li>纯随机性的世界模型：模型难以记住信息，导致产生前后不一致的预测结果。</li>
</ul>
<p>所以作者考虑将确定性和随机性结合，称这种结构为RSSM.</p>
<p>相对于上一篇，把记忆模块换成了RSSM。</p>
<h3 id="latent-overshooting">Latent Overshooting</h3>
<p>之前讨论的都是<span class="math inline">\(s_t \to s_{t+1}\)</span>的单步预测，如果每次单步预测都准确无误，那多步预测肯定也没问题。但是由于模型本身有局限，所以不一定能很好的推广到多部预测。</p>
<p>于是作者考虑了直接进行跨步的预测，先通过对中间几步隐变量边际化得到了跨步预测的转移<br>
<img src="https://img2024.cnblogs.com/blog/2568389/202501/2568389-20250114143311439-390607623.png" alt="image" loading="lazy"></p>
<p>并且推导了针对跨步预测的变分bound<br>
<img src="https://img2024.cnblogs.com/blog/2568389/202501/2568389-20250114143315867-2118418894.png" alt="image" loading="lazy"></p>
<p>把考虑不同的步幅<span class="math inline">\(d\)</span>，求和，就得到latent overshooting的目标函数<br>
<img src="https://img2024.cnblogs.com/blog/2568389/202501/2568389-20250114143319662-800952295.png" alt="image" loading="lazy"></p>
<h2 id="实验结果">实验结果</h2>
<p><img src="https://img2024.cnblogs.com/blog/2568389/202501/2568389-20250114143328534-1965438482.png" alt="image" loading="lazy"></p>
<p>DeepMind control suite环境：图像作为观测，连续动作空间。</p>
<p><strong>消融实验</strong></p>
<ul>
<li>
<p>验证PlaNet的数据收集过程有优势。Random Collection指的是用随机策略收集数据而不是通过MPC；Random shooting指的是使用了MPC框架，但是不使用CEM，而是直接从1000条随机采的动作序列里选最好的那条。最后PlaNet在大部分情况都明显好于另外两种。<br>
<img src="https://img2024.cnblogs.com/blog/2568389/202501/2568389-20250114143345149-1170742572.png" alt="image" loading="lazy"></p>
</li>
<li>
<p>RSSM和SSM、GRU的对比。观察到RSSM明显好于后两者，表明了确定性+随机性结合的优势。<br>
<img src="https://img2024.cnblogs.com/blog/2568389/202501/2568389-20250114143351955-1681982149.png" alt="image" loading="lazy"></p>
</li>
<li>
<p>是否加入latent overshooting作为变分目标。观察到Latent overshooting使RSSM的表现轻微变差，但是在一些任务上让DRNN的表现变好了。<br>
<img src="https://img2024.cnblogs.com/blog/2568389/202501/2568389-20250114143401024-1592317341.png" alt="image" loading="lazy"></p>
</li>
</ul>
<p><strong>跑分对比实验</strong><br>
<img src="https://img2024.cnblogs.com/blog/2568389/202501/2568389-20250114143409379-825872652.png" alt="image" loading="lazy"></p>
<ul>
<li>PlaNet的分数能打败A3C。</li>
<li>PlaNet的分数总体不如D4PG，但是大部分任务相差不多。</li>
<li>PlaNet在所有任务上，数据利用率都好于D4PG.</li>
<li>PlaNet (CEM + 世界模型) 和 CEM + true simulator对比只差了一些，体现出世界模型较好地学习到了环境。</li>
</ul>
<p><strong>六个任务一起训练</strong><br>
每次循环中，agent面对的可能是不同的环境，所以数据库中抽取出来的轨迹也是打乱的。<br>
<img src="https://img2024.cnblogs.com/blog/2568389/202501/2568389-20250114143419829-2122368465.png" alt="image" loading="lazy"></p>
<p>最后跑分不如单独训练，但是体现出了<strong>agent能够自己判断出面对的是哪个任务</strong>了。</p>
<h2 id="代码选讲">代码选讲</h2>
<blockquote>
<p>代码来自：<a href="https://github.com/Kaixhin/PlaNet" target="_blank" rel="noopener nofollow">Kaixhin/PlaNet: Deep Planning Network: Control from pixels by latent planning with learned dynamics</a></p>
</blockquote>
<p>主要是看看transition model和模型训练过程。解释都在注释里，有部分注释是代码库原有的。</p>
<p><strong>Transition model</strong></p>
<pre><code class="language-python">class TransitionModel(jit.ScriptModule):
  __constants__ = ['min_std_dev']

  def __init__(self, belief_size, state_size, action_size, hidden_size, embedding_size, activation_function='relu', min_std_dev=0.1):
    super().__init__()
    self.act_fn = getattr(F, activation_function)
    self.min_std_dev = min_std_dev
    self.fc_embed_state_action = nn.Linear(state_size + action_size, belief_size) # combine s_t and a_t to comb(s_t, a_t)
    self.rnn = nn.GRUCell(belief_size, belief_size) # from comb(s_t, a_t), h_t to h_t+1
    self.fc_embed_belief_prior = nn.Linear(belief_size, hidden_size) # from h_t to z_t
    self.fc_state_prior = nn.Linear(hidden_size, 2 * state_size) # parameterized prior of s_t, from z_t to mean and std
    self.fc_embed_belief_posterior = nn.Linear(belief_size + embedding_size, hidden_size) # from h_t and e_t to z_t
    self.fc_state_posterior = nn.Linear(hidden_size, 2 * state_size) # parameterized posterior of s_t, from z_t to mean and std

  # Operates over (previous) state, (previous) actions, (previous) belief, (previous) nonterminals (mask), and (current) observations
  # Diagram of expected inputs and outputs for T = 5 (-x- signifying beginning of output belief/state that gets sliced off):
  # t :  0  1  2  3  4  5
  # o :    -X--X--X--X--X-  设置了初始的隐状态是None，所以不考虑0时刻的obs
  # a : -X--X--X--X--X-     不考虑最后一个action，因为最后一个action没有后续的obs
  # n : -X--X--X--X--X-
  # pb: -X-
  # ps: -X-
  # b : -x--X--X--X--X--X-
  # s : -x--X--X--X--X--X-

  # 输入的shape都是(time_step, batch_size, *)
  @jit.script_method
  def forward(self, prev_state:torch.Tensor, actions:torch.Tensor, prev_belief:torch.Tensor, observations:Optional[torch.Tensor]=None, nonterminals:Optional[torch.Tensor]=None) -&gt; List[torch.Tensor]:
    # 后面都是动态更新，为了保留grad，不能使用单个tensor作为buffer，所以创建了几个list
    T = actions.size(0) + 1 # 实际需要的list长度，参考上面的图
    beliefs, prior_states, prior_means, prior_std_devs, posterior_states, posterior_means, posterior_std_devs = \
      [torch.empty(0)] * T, [torch.empty(0)] * T, [torch.empty(0)] * T, [torch.empty(0)] * T, [torch.empty(0)] * T, [torch.empty(0)] * T, [torch.empty(0)] * T
    beliefs[0], prior_states[0], posterior_states[0] = prev_belief, prev_state, prev_state # 0时刻赋初值

    # 每次循环开始，是已知t时刻的信息，进一步计算t+1时刻的信息
    for t in range(T - 1):
      # 根据情况合适的s，因为模型可以在脱离observations的情况下自己预测
      # 如果observations为None，则使用先验状态 (模型一步步生成出来的)，否则使用后验状态 (根据历史的obs和action推断出来的)
      _state = prior_states[t] if observations is None else posterior_states[t] 
      # terminal则说明这段序列已经结束了，所以把状态mask掉 (就是0)
      _state = _state if nonterminals is None else _state * nonterminals[t]  

      # 注意下面每一块的hidden是临时变量，表示的是不同的意思

      # 计算确定性隐状态h = f(s_t, a_t, h_t)
      hidden = self.act_fn(self.fc_embed_state_action(torch.cat([_state, actions[t]], dim=1))) # s和a先拼在一起
      beliefs[t + 1] = self.rnn(hidden, beliefs[t]) # 对应概率图中从s,a,h到h的实线

      # 计算隐状态s的先验 p(s_t|s_t-1,a_t-1)
      hidden = self.act_fn(self.fc_embed_belief_prior(beliefs[t + 1])) # 对应概率图中从h到s的实线
      prior_means[t + 1], _prior_std_dev = torch.chunk(self.fc_state_prior(hidden), 2, dim=1)
      prior_std_devs[t + 1] = F.softplus(_prior_std_dev) + self.min_std_dev # Trick: 使用softplus来保证std_devs为正，并且使用min_std_dev来保证std_devs不会太小
      prior_states[t + 1] = prior_means[t + 1] + prior_std_devs[t + 1] * torch.randn_like(prior_means[t + 1])     

      # 计算隐状态s的后验 q(s_t|o≤t,a&lt;t)
      if observations is not None: # 只有observations不为None时，才计算后验
        t_ = t - 1  # 这是实现的问题，因为传进来的是obs[1:]，所以应该用t_+1才能索引到对应的obs
        hidden = self.act_fn(self.fc_embed_belief_posterior(torch.cat([beliefs[t + 1], observations[t_ + 1]], dim=1))) # 对应概率图中的两条虚线
        posterior_means[t + 1], _posterior_std_dev = torch.chunk(self.fc_state_posterior(hidden), 2, dim=1)
        posterior_std_devs[t + 1] = F.softplus(_posterior_std_dev) + self.min_std_dev
        posterior_states[t + 1] = posterior_means[t + 1] + posterior_std_devs[t + 1] * torch.randn_like(posterior_means[t + 1])

    # 返回h，s，以及先验和后验的均值和方差
    hidden = [torch.stack(beliefs[1:], dim=0), torch.stack(prior_states[1:], dim=0), torch.stack(prior_means[1:], dim=0), torch.stack(prior_std_devs[1:], dim=0)]
    if observations is not None:
      hidden += [torch.stack(posterior_states[1:], dim=0), torch.stack(posterior_means[1:], dim=0), torch.stack(posterior_std_devs[1:], dim=0)]
    return hidden
</code></pre>
<p><strong>世界模型训练</strong><br>
只截取了一小部分，重点看loss func是如何计算的。</p>
<pre><code class="language-python">  # Model fitting
  losses = []
  for s in tqdm(range(args.collect_interval)):
    # Draw sequence chunks {(o_t, a_t, r_t+1, terminal_t+1)} ~ D uniformly at random from the dataset (including terminal flags)
    observations, actions, rewards, nonterminals = D.sample(args.batch_size, args.chunk_size)  # Transitions start at time t = 0

    # Create initial belief and state for time t = 0
    init_belief, init_state = torch.zeros(args.batch_size, args.belief_size, device=args.device), torch.zeros(args.batch_size, args.state_size, device=args.device)

    # Update belief/state using posterior from previous belief/state, previous action and current observation (over entire sequence at once)
    # 一次把整个隐状态序列全部计算出来
    beliefs, prior_states, prior_means, prior_std_devs, posterior_states, posterior_means, posterior_std_devs =\
      transition_model(init_state, actions[:-1], init_belief, bottle(encoder, (observations[1:], )), nonterminals[:-1])

    # Calculate observation likelihood, reward likelihood and KL losses (for t = 0 only for latent overshooting); sum over final dims, average over batch and time (original implementation, though paper seems to miss 1/T scaling?)
    # Reconstruction loss都使用MSE
    # mean(dim=(0, 1))对batch和time进行平均
    observation_loss =\
      F.mse_loss(bottle(observation_model, (beliefs, posterior_states)), observations[1:], reduction='none').sum(dim=2 if args.symbolic_env else (2, 3, 4)).mean(dim=(0, 1))
    reward_loss =\
      F.mse_loss(bottle(reward_model, (beliefs, posterior_states)), rewards[:-1], reduction='none').mean(dim=(0, 1))
    # KL loss, 计算了后验q(s_t|o≤t,a&lt;t)和先验p(s_t|s_t-1,a_t-1)的KL散度
    kl_loss =\
      torch.max(kl_divergence(Normal(posterior_means, posterior_std_devs), Normal(prior_means, prior_std_devs)).sum(dim=2), free_nats).mean(dim=(0, 1))  # Note that normalisation by overshooting distance and weighting by overshooting distance cancel out

"""
后面的部分略
"""
</code></pre>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.40201928293287037" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-01-14 14:39">2025-01-14 14:38</span>&nbsp;
<a href="https://www.cnblogs.com/tshaaa">伊犁纯流莱</a>&nbsp;
阅读(<span id="post_view_count">55</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18670731" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18670731);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18670731', targetLink: 'https://www.cnblogs.com/tshaaa/p/18670731', title: '用于决策的世界模型 -- 论文 World Models (2018) &amp;amp; PlaNet (2019) 讲解' })">举报</a>
</div>
        