
	<div class="postTitle">
		<a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/jellyuci/p/19026432/quick_model_quantization" title="发布于 2025-08-07 15:25">
    <span role="heading" aria-level="2">【模型压缩系列-1】一篇文章带你全面了解模型量化（Model Quantization ）——全局篇</span>
    

</a>

	</div>
	    <div id="cnblogs_post_description" style="display: none">
        
        本文通过五个方面系统介绍了大型模型量化技术：首先阐述量化的基本概念，以低比特（INT8/4/2/1）取代 FP32 的压缩与加速原理；其次按时间维度区分 PTQ、QAT 与 QAF 三种策略，明确何时量化；随后按对象维度梳理权重、激活、梯度、KV-Cache 及偏置的量化差异；再从粒度维度比较 per-tensor、per-channel、per-group、per-token 的精度与开销权衡；最后结合位宽与对象给出 W8A16、W4A8、KV4 等典型组合，完整呈现量化在模型大小、推理速度与部署场景中的综合优化路径。
    </div>
<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p>量化（Quantization）是一种关键策略，用于优化大型机器学习模型（特别是深度神经网络），使其在资源受限的硬件（如移动设备、边缘设备，或为了云端的快速推理）上部署得更加高效。</p>
<h2 id="什么是量化">什么是量化？</h2>
<p>量化是指<strong>降低</strong>用于表示模型参数（权重）和激活值的<strong>数值精度</strong>的过程。<br>
它不再使用 32 位浮点数（FP32），而通常采用更低精度的格式，例如 16 位浮点（FP16）、8 位整数（INT8）甚至更低。</p>
<blockquote>
<p>把 FP32 → INT8/INT4/FP4 等低 bit 类型。</p>
</blockquote>
<h2 id="什么要对大型模型进行量化">什么要对大型模型进行量化？</h2>
<p>大型模型（LLMs）在推理过程中会消耗大量的内存和计算资源。而量化之后的模型可以实现以下功能：</p>
<ul>
<li><strong>缩小模型体积：</strong> 低精度数值占用更少内存。</li>
<li><strong>加速推理：</strong> 许多硬件加速器（CPU、GPU、NPU）对低精度数据的处理速度更快，因为定点运算相比浮点运算通常更简单、更快。</li>
<li><strong>降低功耗：</strong> 由于量化后的数据占用的存储空间更小，计算量和内存访问减少，能耗随之降低。</li>
<li><strong>实现边缘部署：</strong> 许多硬件设备（如专用的 AI 芯片、GPU 等）对低精度计算提供了专门的硬件优化，可以高效地处理量化后的神经网络运算，因此可以在资源受限的设备上运行大型模型。</li>
</ul>
<h2 id="量化原理">量化原理</h2>
<p>量化的基本原理，即把模型的参数（weights）等从浮点数（如float32）转换为定点数（如int8）。在计算时，再将定点数据反量化回浮点数据。</p>
<p>量化的两个重要过程，一个是<font color="#6495ED"><strong>量化（Quantize）</strong></font>，另一个是<font color="#6495ED"><strong>反量化（Dequantize）</strong></font>：</p>
<ul>
<li>量化就是将浮点型实数量化为整型数（FP32-&gt;INT8）</li>
<li>反量化就是将整型数转换为浮点型实数（INT8-&gt;FP32）</li>
</ul>
<h2 id="量化类型"><strong>量化类型</strong></h2>
<p>那么具体是如何转换数值的呢？通常有以下两种转换方式：</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>子类</th>
<th>特点与适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>线性量化</strong></td>
<td><font color="#6495ED"><strong>对称量化</strong></font></td>
<td>零点为0，适合权重，计算高效，硬件友好。</td>
</tr>
<tr>
<td></td>
<td><font color="#6495ED"><strong>非对称量化</strong></font></td>
<td>引入零点(zero-point)，适合激活值，精度高但计算复杂。</td>
</tr>
<tr>
<td><strong>非线性量化</strong></td>
<td>—</td>
<td>如对数量化、矢量量化、查找表量化等，适用于极端压缩或非均匀分布数据。</td>
</tr>
</tbody>
</table>
<p>不同的方法使用不同的量化公式，得到不同的量化参数：</p>
<ul>
<li><font color="#6495ED"><strong>⚖️scale</strong></font></li>
<li><font color="#6495ED"><strong>0️⃣zero-point</strong></font></li>
</ul>
<p>具体的会在数学篇进行介绍（先挖一个坑，后面填吧）</p>
<h2 id="量化策略"><strong>量化策略</strong></h2>
<h3 id="训练前后">训练前后</h3>
<p>从训练视角来看，我们可以在模型的训练前或训练后进行量化，据此可以分为以下几种：</p>
<table>
<thead>
<tr>
<th>策略</th>
<th>阶段</th>
<th>是否需要重训练</th>
<th>精度</th>
<th>适用</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>PTQ</strong> (Post-Training Quant.)</td>
<td>训练后</td>
<td>❌</td>
<td>稍低</td>
<td>快速部署</td>
</tr>
<tr>
<td><strong>QAT</strong> (Quant.-Aware Training)</td>
<td>训练中</td>
<td>✅</td>
<td>高</td>
<td>极致精度</td>
</tr>
<tr>
<td><strong>QAF</strong> (Quant.-Aware Fine-tuning)</td>
<td>微调阶段</td>
<td>✅</td>
<td>中高</td>
<td>资源有限</td>
</tr>
</tbody>
</table>
<p>1.<strong>训练后量化(PTQ)</strong> ：在不重新训练的情况下，对已训练好的模型进行量化。</p>
<p>2.<strong>量化感知训练（QAT）</strong>：在训练过程中模拟量化，并更新量化参数。</p>
<ul>
<li>通常比 PTQ 精度更高，特别适用于大型或复杂模型。</li>
<li>计算开销更大，因为需要重新训练。</li>
</ul>
<p>3.<strong>量化感知微调（QAF）</strong>：从一个已训练好的 FP32 模型出发，在模拟量化的同时进行少量微调，使权重适应低比特表示。</p>
<ul>
<li>在 PTQ 与完整 QAT 之间取得折中：比从头训练快得多，但通常比 PTQ 恢复更多精度——尤其当原始 PTQ 出现明显下降时。</li>
</ul>
<h3 id="推理前后">推理前后</h3>
<p>从推理视角来看，根据量化过程在推理前后，可以分为：</p>
<table>
<thead>
<tr>
<th>分类</th>
<th>说明</th>
<th>子类</th>
<th>子类说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>离线</td>
<td>推理前量化</td>
<td><font color="#6495ED"><strong>静态</strong></font></td>
<td>用校准集一次性算好量化参数</td>
</tr>
<tr>
<td></td>
<td></td>
<td><font color="#6495ED"><strong>动态</strong></font></td>
<td>每次前向实时算激活值</td>
</tr>
<tr>
<td>在线</td>
<td>推理时量化</td>
<td>—</td>
<td></td>
</tr>
</tbody>
</table>
<p>1.<strong>离线量化</strong>：<strong>上线前</strong>完成全部量化，即提前确定好激活值的量化参数 $ S $ 和 $ Z $，在推理时直接使用。</p>
<ul>
<li>比如之前我们提到PTQ，是离线量化里最常见的实现方式。在大多数情况下，离线量化指的就是PTQ。</li>
</ul>
<blockquote>
<p><strong>离线量化 ≈ PTQ（Post-Training Quantization）</strong></p>
</blockquote>
<ul>
<li>离线量化又可以细分为：
<ul>
<li><strong>静态量化</strong>（Static Quantization）：同时量化权重和激活值，<strong>推理前</strong>用校准数据集一次性算好量化参数。</li>
</ul>
<blockquote>
<p>因为属于离线量化之PTQ，所以也叫静态离线量化（PTQ-Static）</p>
</blockquote>
<ul>
<li><strong>动态量化</strong>（Dynamic Quantization）：仅量化权重，激活值在<strong>推理时</strong>实时量化。</li>
</ul>
<blockquote>
<p>因为属于离线量化之PTQ，所以也叫动态离线量化（PTQ-Dynamic）</p>
</blockquote>
</li>
</ul>
<p>2.<strong>在线量化</strong>：<strong>推理时</strong>才量化，即在推理过程中动态计算量化参数 $ S(scale) $ 和 $ Z(zero-point) $。</p>
<h2 id="量化对象和量化层级"><strong>量化对象和量化层级</strong></h2>
<p>根据量化的对象的不同，可以分为不同的层级：</p>
<ul>
<li><strong>权重量化（Weight Quantization）：</strong> 仅量化模型权重。</li>
</ul>
<blockquote>
<p>因为只量化权重，也称为weight-only quantization</p>
</blockquote>
<ul>
<li><strong>激活量化（Activation Quantization）：</strong> 也对各层输出（激活值）进行量化。</li>
<li><strong>梯度量化（Gradient Quantization）：</strong> 训练时对梯度进行量化以减少通信开销。</li>
<li><strong>KV缓存量化（KV Cache Quantization）：</strong> 对注意力中的KV缓存进行量化以降低显存占用。</li>
<li><strong>偏置量化（Bias Quantization）：</strong> 有时也对偏置进行量化，但通常保持较高精度。</li>
</ul>
<p>也就是说，在模型量化过程中，量化可以应用于模型的多个部分，包括：</p>
<ul>
<li><strong>模型参数（weights）</strong>：如权重矩阵，这些是模型训练过程中学习到的参数。</li>
<li><strong>激活值（activations）</strong>：如神经元的输出值，这些值在前向传播过程中动态生成。</li>
<li><strong>梯度（gradient）</strong>：如反向传播过程中计算的梯度值，用于更新模型参数。</li>
<li><strong>KV Cache</strong>：在 Transformer 的自回归解码阶段，KV Cache 用于缓存每一层的键（Key）和值（Value）张量，以避免重复计算，从而显著提升长序列生成的效率。</li>
<li><strong>偏置（Bias）</strong>：指模型中各层加性偏置项（如线性层、卷积层后的 bias）。由于偏置参数量远小于权重（百万级 vs 十亿级），其对整体模型大小的影响有限，因此<strong>通常不量化或仅使用较高精度（如 INT16或 FP16）</strong>，仅在极端压缩需求下（如边缘设备部署），才考虑与权重一并量化至 INT8。</li>
</ul>
<table>
<thead>
<tr>
<th>量化对象</th>
<th>是否常被量化</th>
<th>量化方式举例</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>模型参数（weights）</strong></td>
<td>✅ 是</td>
<td>INT8/INT4，对称或非对称量化，GPTQ/AWQ 等</td>
<td>直接决定模型大小与推理速度</td>
</tr>
<tr>
<td><strong>激活值（activations）</strong></td>
<td>✅ 是</td>
<td>动态或静态量化，per-token/per-tensor</td>
<td>显著降低显存，需校准分布误差</td>
</tr>
<tr>
<td><strong>梯度（gradient）</strong></td>
<td>✅/❓ 可选</td>
<td>2–8 bit 均匀量化，Top-K 稀疏化</td>
<td>主要用于训练加速与分布式通信压缩</td>
</tr>
<tr>
<td><strong>KV Cache</strong></td>
<td>✅ 是</td>
<td>INT8/INT4，混合精度保留关键 token</td>
<td>显著降低显存，提升吞吐</td>
</tr>
<tr>
<td><strong>Bias</strong></td>
<td>❌ 通常否</td>
<td>保留为 FP16/INT16，极端场景下低比特量化</td>
<td>参数量小，量化收益低</td>
</tr>
</tbody>
</table>
<h2 id="量化粒度granularity"><strong>量化粒度（Granularity）</strong></h2>
<table>
<thead>
<tr>
<th>粒度</th>
<th>解释</th>
<th>适用对象</th>
</tr>
</thead>
<tbody>
<tr>
<td>per-tensor / per-layer</td>
<td>整层共享一个 scale &amp; zero-point</td>
<td>通用</td>
</tr>
<tr>
<td>per-channel</td>
<td>每个输出通道各自 scale</td>
<td>权重</td>
</tr>
<tr>
<td>per-token</td>
<td>每个 token（行）各自 scale</td>
<td>激活</td>
</tr>
<tr>
<td>per-group / sub-channel</td>
<td>每连续 N 个元素为一组 scale</td>
<td>权重/激活</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>逐层量化（per-tensor）</strong>：整个层的所有权重使用相同的缩放因子 $ S $ 和偏移量 $ Z $。</li>
<li><strong>逐通道量化（per-channel）</strong>：每个通道单独使用一组 $ S $ 和 $ Z $。</li>
<li><strong>逐组量化（per-group）</strong>：将权重按组划分，每个组使用一组 $ S $ 和 $ Z $。</li>
<li><strong>逐 token 量化（per-token）</strong>：对输入序列中的 <strong>每一个 token（即矩阵的每一行）</strong> 单独计算并使用一组缩放因子 <em>S</em> 和偏移量 <em>Z</em>。</li>
</ul>
<h2 id="量化位宽">量化位宽</h2>
<p>根据存储一个权重元素所需的位数，可以分为8bit量化、4bit量化、2bit量化和1bit量化。</p>
<table>
<thead>
<tr>
<th>位宽</th>
<th>特点与适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>8-bit</strong></td>
<td>最常用，精度损失小，广泛支持（INT8/FP8）。</td>
</tr>
<tr>
<td><strong>4-bit</strong></td>
<td>极限压缩，适合大模型部署（如 AWQ、GPTQ）。</td>
</tr>
<tr>
<td><strong>2-bit</strong></td>
<td>极端压缩，精度损失大，需配合误差补偿机制。</td>
</tr>
<tr>
<td><strong>1-bit</strong></td>
<td>极限压缩，仅限特定任务或研究使用（如 BNN）。</td>
</tr>
</tbody>
</table>
<h2 id="对象位宽组合">对象×位宽组合</h2>
<p>根据量化对象和量化位宽的不同组合，可以分为：</p>
<table>
<thead>
<tr>
<th>方案</th>
<th>组合名</th>
<th>含义</th>
<th>示例</th>
</tr>
</thead>
<tbody>
<tr>
<td>仅权重</td>
<td><strong>W8A16</strong></td>
<td>权重8bit，激活16bit（未量化，保持原精度）</td>
<td></td>
</tr>
<tr>
<td></td>
<td><strong>W4A16</strong></td>
<td>权重4bit，激活16bit（未量化，保持原精度）</td>
<td></td>
</tr>
<tr>
<td>权重+激活</td>
<td><strong>W8A8</strong></td>
<td>权重8bit，激活8bit</td>
<td>SmoothQuant、ZeroQuant</td>
</tr>
<tr>
<td></td>
<td><strong>W4A8</strong></td>
<td>权重4bit，激活8bit</td>
<td>QoQ</td>
</tr>
<tr>
<td></td>
<td><strong>W4A4</strong></td>
<td>权重4bit，激活4bit</td>
<td>Atom、QuaRot、OmniQuant</td>
</tr>
<tr>
<td>KV Cache</td>
<td><strong>KV8</strong></td>
<td>KV缓存8bit</td>
<td>LMDeploy、TensorRT-LLM</td>
</tr>
<tr>
<td></td>
<td><strong>KV4</strong></td>
<td>KV缓存4bit</td>
<td>Atom、QuaRot、QoQ</td>
</tr>
<tr>
<td></td>
<td><strong>KV2</strong></td>
<td>KV缓存2bit</td>
<td>KIVI、KVQuant</td>
</tr>
</tbody>
</table>
<h2 id="summary">Summary</h2>
<p>根据以上介绍的所有内容，为了方便理解记忆总结了一张图：<br>
<img src="https://img2024.cnblogs.com/blog/2808080/202508/2808080-20250807145458369-1213717222.png" alt="image" loading="lazy"></p>

</div>
<div class="clear"></div>

	<div class="postDesc">posted on 
<span id="post-date" data-last-update-days="0.005555555555555556" data-date-updated="2025-08-07 15:33">2025-08-07 15:25</span>&nbsp;
<a href="https://www.cnblogs.com/jellyuci">汤佘</a>&nbsp;
阅读(<span id="post_view_count">18</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(19026432);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '19026432', targetLink: 'https://www.cnblogs.com/jellyuci/p/19026432/quick_model_quantization', title: '【模型压缩系列-1】一篇文章带你全面了解模型量化（Model Quantization ）——全局篇' })">举报</a>
</div>
