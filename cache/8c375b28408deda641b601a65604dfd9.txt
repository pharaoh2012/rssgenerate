
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/zhanggaoxing/p/18811224" title="å‘å¸ƒäº 2025-04-06 16:00">
    <span role="heading" aria-level="2">å¼ é«˜å…´çš„å¤§æ¨¡å‹å¼€å‘å®æˆ˜ï¼šï¼ˆå››ï¼‰ä½¿ç”¨ LangGraph å®ç°å¤šæ™ºèƒ½ä½“åº”ç”¨</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p></p><div class="toc"><div class="toc-container-header">ç›®å½•</div><ul><li><a href="#ç¯å¢ƒæ­å»ºä¸é…ç½®" rel="noopener nofollow">ç¯å¢ƒæ­å»ºä¸é…ç½®</a></li><li><a href="#å®šä¹‰æ™ºèƒ½ä½“" rel="noopener nofollow">å®šä¹‰æ™ºèƒ½ä½“</a><ul><li><a href="#åŠ è½½æ¨¡å‹" rel="noopener nofollow">åŠ è½½æ¨¡å‹</a></li><li><a href="#æå–å…³é”®è¯" rel="noopener nofollow">æå–å…³é”®è¯</a></li><li><a href="#ç”Ÿæˆå›ç­”" rel="noopener nofollow">ç”Ÿæˆå›ç­”</a></li></ul></li><li><a href="#è¿æ¥æ™ºèƒ½ä½“" rel="noopener nofollow">è¿æ¥æ™ºèƒ½ä½“</a><ul><li><a href="#å®šä¹‰å›¾çš„çŠ¶æ€" rel="noopener nofollow">å®šä¹‰å›¾çš„çŠ¶æ€</a></li><li><a href="#å®šä¹‰èŠ‚ç‚¹æ–¹æ³•" rel="noopener nofollow">å®šä¹‰èŠ‚ç‚¹æ–¹æ³•</a><ul><li><a href="#æ ¹æ®æŒ‡ä»¤è·¯ç”±" rel="noopener nofollow">æ ¹æ®æŒ‡ä»¤è·¯ç”±</a></li><li><a href="#ç”Ÿæˆå›ç­”-1" rel="noopener nofollow">ç”Ÿæˆå›ç­”</a></li><li><a href="#æ–‡ä»¶å¤„ç†" rel="noopener nofollow">æ–‡ä»¶å¤„ç†</a></li><li><a href="#æå–å…³é”®è¯-1" rel="noopener nofollow">æå–å…³é”®è¯</a></li><li><a href="#ç½‘ç»œæœç´¢" rel="noopener nofollow">ç½‘ç»œæœç´¢</a></li></ul></li><li><a href="#å®šä¹‰å›¾çš„ç»“æ„" rel="noopener nofollow">å®šä¹‰å›¾çš„ç»“æ„</a></li><li><a href="#è¿è¡Œå›¾" rel="noopener nofollow">è¿è¡Œå›¾</a></li></ul></li><li><a href="#è¿è¡ŒæŒ‡å—" rel="noopener nofollow">è¿è¡ŒæŒ‡å—</a><ul><li><a href="#åœ¨æ§åˆ¶å°ä¸­æµ‹è¯•ç¨‹åº" rel="noopener nofollow">åœ¨æ§åˆ¶å°ä¸­æµ‹è¯•ç¨‹åº</a></li><li><a href="#ä½¿ç”¨-streamlit-æ„å»ºå‰ç«¯é¡µé¢" rel="noopener nofollow">ä½¿ç”¨ Streamlit æ„å»ºå‰ç«¯é¡µé¢</a></li></ul></li></ul></div><p></p>
<p>éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œäººä»¬æœŸæœ›åˆ©ç”¨ LLM è§£å†³å„ç§å¤æ‚é—®é¢˜ï¼Œåœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œæ„å»ºæ™ºèƒ½ä½“ï¼ˆAgentï¼‰åº”ç”¨å—åˆ°äº†å¹¿æ³›å…³æ³¨ã€‚ç”¨æˆ·ä¸ LLM çš„äº¤äº’å¯ä»¥è¢«è§†ä¸ºä¸€ç§ <strong>å•æ™ºèƒ½ä½“ï¼ˆSingle-Agentï¼‰</strong> è¡Œä¸ºï¼šç”¨æˆ·é€šè¿‡æç¤ºè¯ï¼ˆpromptï¼‰ä¸é€šç”¨ LLM è¿›è¡Œå¯¹è¯ï¼ŒLLM ç†è§£é—®é¢˜å¹¶æä¾›åé¦ˆã€‚ç„¶è€Œï¼Œå•ä¸€æ™ºèƒ½ä½“åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶å­˜åœ¨æ˜æ˜¾å±€é™æ€§ï¼Œä¾‹å¦‚éœ€è¦ç”¨æˆ·å¤šæ¬¡å¼•å¯¼ã€ç¼ºä¹å¯¹å¤–éƒ¨ç¯å¢ƒçš„æ„ŸçŸ¥èƒ½åŠ›ã€å¯¹è¯å†å²è®°å¿†æœ‰é™ç­‰ã€‚</p>
<p>è¯•æƒ³ä»¥ä¸‹åœºæ™¯ï¼šåœ¨ä¸åŒå¤„ç†é˜¶æ®µè°ƒç”¨ä¸åŒçš„æ¨¡å‹ï¼›å½“ LLM æ— æ³•å®Œæˆä»»åŠ¡æ—¶ï¼Œè‡ªåŠ¨æŸ¥è¯¢å¤–éƒ¨çŸ¥è¯†åº“ï¼›æˆ–è€…ç”± LLM è‡ªä¸»çº æ­£ç”Ÿæˆå†…å®¹ä¸­çš„å¹»è§‰å’Œé”™è¯¯ã€‚è¿™äº›éœ€æ±‚å¦‚ä½•å®ç°ï¼Ÿ <strong>å¤šæ™ºèƒ½ä½“ï¼ˆMulti-Agentï¼‰</strong> ç³»ç»Ÿæ­£æ˜¯è§£å†³è¿™ç±»é—®é¢˜çš„æœ‰æ•ˆå·¥å…·ã€‚é€šè¿‡æç¤ºè¯æ¨¡æ¿ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“åˆ†é…è§’è‰²å¹¶è§„èŒƒå…¶è¡Œä¸ºï¼Œå¤šä¸ªæ™ºèƒ½ä½“ç›¸äº’åä½œï¼Œä»è€Œå®Œæˆå¤æ‚çš„ä»»åŠ¡ã€‚</p>
<p>ç„¶è€Œï¼Œæ„å»ºå¤šæ™ºèƒ½ä½“åº”ç”¨å¹¶éæ˜“äº‹ï¼Œå¼€å‘è€…éœ€è¦é¢å¯¹æ™ºèƒ½ä½“è®¾è®¡ã€é€šä¿¡åè®®ã€åè°ƒç­–ç•¥ç­‰å¤šæ–¹é¢çš„é—®é¢˜ã€‚LangGraph æä¾›äº†ä¸€ç§ä»¥å›¾ï¼ˆgraphï¼‰ä¸ºæ ¸å¿ƒçš„è§£å†³æ–¹æ¡ˆï¼Œæ¸…æ™°å®šä¹‰äº†æ™ºèƒ½ä½“ä¹‹é—´çš„å…³ç³»ä¸äº¤äº’è§„åˆ™ï¼Œå¹¶é€šè¿‡å†…ç½®çš„é€šä¿¡æ¥å£å’Œåè°ƒç­–ç•¥ï¼Œå¸®åŠ©å¼€å‘è€…å¿«é€Ÿæ„å»ºé«˜æ•ˆä¸”å¯æ‰©å±•çš„åˆ†å¸ƒå¼æ™ºèƒ½ç³»ç»Ÿã€‚</p>
<p><img src="https://img2024.cnblogs.com/blog/1085877/202504/1085877-20250406155907600-36102623.png" alt="" loading="lazy"></p>
<p><strong>æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†é€šè¿‡ä¸€ä¸ªå®ä¾‹å±•ç¤ºå¦‚ä½•ä½¿ç”¨ <code>LangGraph</code> æ„å»ºä¸€ä¸ªå¤šæ™ºèƒ½ä½“åº”ç”¨ï¼Œå¹¶ç»“åˆ <code>Streamlit</code> å®ç°ç”¨æˆ·å‹å¥½çš„å‰ç«¯ç•Œé¢ã€‚</strong> è¯¥åº”ç”¨å…·å¤‡ä»¥ä¸‹åŠŸèƒ½ï¼š</p>
<ol>
<li>æ ¹æ®å¯¹è¯ç±»å‹å°†è¯·æ±‚è·¯ç”±åˆ°é€‚å½“çš„å¤„ç†èŠ‚ç‚¹ã€‚</li>
<li>æ”¯æŒè”ç½‘æœç´¢ï¼Œè·å–å®æ—¶ä¿¡æ¯ã€‚</li>
<li>æ ¹æ®é—®é¢˜å’Œå¯¹è¯å†å²ç”Ÿæˆä¼˜åŒ–çš„æœç´¢æç¤ºè¯ã€‚</li>
<li>æ”¯æŒæ–‡ä»¶ä¸Šä¼ ä¸å¤„ç†ã€‚</li>
<li>åˆ©ç”¨ç¼–ç¨‹ä¸“ç”¨çš„ LLM è§£å†³ä»£ç ç›¸å…³é—®é¢˜ã€‚</li>
<li>åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹ï¼Œæ€»ç»“ç”Ÿæˆç­”æ¡ˆã€‚</li>
</ol>
<p><img src="https://img2024.cnblogs.com/blog/1085877/202504/1085877-20250406155913943-35085412.png" alt="" loading="lazy"></p>
<h2 id="ç¯å¢ƒæ­å»ºä¸é…ç½®">ç¯å¢ƒæ­å»ºä¸é…ç½®</h2>
<p>é¡¹ç›®ç»“æ„å¦‚ä¸‹ï¼š</p>
<pre><code class="language-shell">.
â”œâ”€â”€ .streamlit  # Streamlit é…ç½®
â”‚&nbsp;&nbsp; â””â”€â”€ config.toml
â”œâ”€â”€ chains  # æ™ºèƒ½ä½“
â”‚&nbsp;&nbsp; â”œâ”€â”€ generate.py
â”‚&nbsp;&nbsp; â”œâ”€â”€ models.py
â”‚&nbsp;&nbsp; â””â”€â”€ summary.py
â”œâ”€â”€ graph   # å›¾ç»“æ„
â”‚&nbsp;&nbsp; â”œâ”€â”€ graph.py
â”‚&nbsp;&nbsp; â””â”€â”€ graph_state.py
â”œâ”€â”€ upload_files    # ä¸Šä¼ çš„æ–‡ä»¶
â”‚&nbsp;&nbsp; â””â”€â”€ .keep
â”œâ”€â”€ .env   # ç¯å¢ƒå˜é‡é…ç½®
â”œâ”€â”€ app.py  # Streamlit åº”ç”¨
â”œâ”€â”€ main.py # å‘½ä»¤è¡Œç¨‹åº
â””â”€â”€ requirements.txt    # ä¾èµ–
</code></pre>
<p><code>requirements.txt</code> ä¸­åˆ—å‡ºäº†ç¨‹åºå¿…è¦çš„åŒ…ï¼Œä½¿ç”¨å‘½ä»¤ <code>pip install -r requirements.txt</code> å®‰è£…ä¾èµ–ã€‚</p>
<pre><code class="language-shell"># LangChain ç›¸å…³åŒ…
langchain
langchain-ollama
langchain-chroma
langchain-community
langgraph
chromadb
tavily-python
python-dotenv

# æ–‡æ¡£å¤„ç†ç›¸å…³åŒ…
marker-pdf
weasyprint
mammoth
openpyxl
unstructured[all-docs]
libmagic

# Streamlit ç›¸å…³åŒ…
streamlit
streamlit-chat
streamlit-extras

# æ–‡æ¡£ä½¿ç”¨GPUå¤„ç†æ—¶ï¼Œå®‰è£…GPUç‰ˆPyTorch
# use 'pip install -r requirements.txt --proxy=127.0.0.1:23474' to accelerate download speed
# --extra-index-url https://download.pytorch.org/whl/cu124
# torch==2.6.0+cu124
# torchvision==0.21.0+cu124
</code></pre>
<p>ç›¸å…³çš„ç¯å¢ƒå˜é‡é…ç½®åœ¨ <code>.env</code> æ–‡ä»¶ä¸­ï¼Œåœ¨ç¨‹åºä¸­é€šè¿‡ <code>dotenv</code> åŒ…è¯»å–ã€‚</p>
<pre><code class="language-shell">TAVILY_API_KEY=tvly-dev-xxxxxx  # Tavily API å¯†é’¥
OMP_NUM_THREADS=8   # è®¾ç½®çº¿ç¨‹æ•°
</code></pre>
<p>å…¶ä¸­ <code>TAVILY_API_KEY</code> æ˜¯ Tavily çš„ API å¯†é’¥ï¼Œç”¨äºç½‘ç»œæœç´¢æœåŠ¡ï¼Œéœ€è¦åœ¨ <a href="https://app.tavily.com/home" target="_blank" rel="noopener nofollow">https://app.tavily.com/home</a> æ³¨å†Œå¹¶è·å–ï¼Œæ¯æœˆæœ‰ 1000 æ¬¡çš„å…è´¹é¢åº¦ã€‚</p>
<p><img src="https://img2024.cnblogs.com/blog/1085877/202504/1085877-20250406155948515-749727306.png" alt="" loading="lazy"></p>
<h2 id="å®šä¹‰æ™ºèƒ½ä½“">å®šä¹‰æ™ºèƒ½ä½“</h2>
<p>åœ¨ LangChain ä¸­ï¼Œä½¿ç”¨ <strong>é“¾ï¼ˆchainï¼‰</strong> æ¥å®šä¹‰ç”¨æˆ·ä¸ LLM äº¤äº’çš„è¡Œä¸ºï¼Œå³æ™ºèƒ½ä½“ã€‚é“¾æ˜¯ä¸€ä¸ªå¯è°ƒç”¨çš„å¯¹è±¡ï¼Œæ¥æ”¶è¾“å…¥å¹¶è¿”å›è¾“å‡ºã€‚åœ¨ <code>chains</code> ç›®å½•ä¸‹ï¼Œå®šä¹‰äº†ä¸¤ä¸ªé“¾ï¼š<code>summary.py</code> å’Œ <code>generate.py</code>ï¼Œåˆ†åˆ«ç”¨äºæå–å…³é”®è¯å’Œç”Ÿæˆå›ç­”ã€‚</p>
<pre><code class="language-shell">.
â”œâ”€â”€ chains  # æ™ºèƒ½ä½“
â”‚&nbsp;&nbsp; â”œâ”€â”€ generate.py
â”‚&nbsp;&nbsp; â”œâ”€â”€ models.py
â”‚&nbsp;&nbsp; â””â”€â”€ summary.py
</code></pre>
<h3 id="åŠ è½½æ¨¡å‹">åŠ è½½æ¨¡å‹</h3>
<p>åœ¨å®šä¹‰æ™ºèƒ½ä½“ä¹‹å‰ï¼Œéœ€è¦å…ˆå®šä¹‰å¥½åŠ è½½æ¨¡å‹çš„æ–¹æ³•ã€‚<code>models.py</code> æ–‡ä»¶è´Ÿè´£æ ¹æ®æä¾›çš„æ¨¡å‹åç§°åŠ è½½ç›¸åº”çš„æ¨¡å‹ã€‚</p>
<pre><code class="language-python">from langchain_ollama import ChatOllama, OllamaEmbeddings
from langchain_core.vectorstores import InMemoryVectorStore

def load_model(model_name: str) -&gt; ChatOllama:
    """
    åŠ è½½è¯­è¨€æ¨¡å‹

    å‚æ•°:
        model_name (str): æ¨¡å‹åç§°

    è¿”å›:
        ChatOllamaå®ä¾‹ï¼Œç”¨äºç”Ÿæˆæ–‡æœ¬å’Œå›ç­”é—®é¢˜
    """
    return ChatOllama(model=model_name)
    
def load_embeddings(model_name: str) -&gt; OllamaEmbeddings:
    """
    åŠ è½½åµŒå…¥æ¨¡å‹

    å‚æ•°:
        model_name (str): æ¨¡å‹åç§°

    è¿”å›:
        OllamaEmbeddingså®ä¾‹ï¼Œç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤º
    """
    return OllamaEmbeddings(model=model_name)

def load_vector_store(model_name: str) -&gt; InMemoryVectorStore:
    """
    åˆ›å»ºå†…å­˜å‘é‡å­˜å‚¨

    å‚æ•°:
        model_name (str): ç”¨äºç”ŸæˆåµŒå…¥çš„æ¨¡å‹åç§°

    è¿”å›:
        InMemoryVectorStoreå®ä¾‹ï¼Œç”¨äºå­˜å‚¨å’Œæ£€ç´¢å‘é‡åŒ–çš„æ–‡æœ¬
    """
    embeddings = load_embeddings(model_name)
    return InMemoryVectorStore(embeddings)
</code></pre>
<h3 id="æå–å…³é”®è¯">æå–å…³é”®è¯</h3>
<p>åœ¨ <code>summary.py</code> æ–‡ä»¶ä¸­ï¼Œå®šä¹‰äº† <code>SummaryChain</code> ç±»ï¼Œç”¨äºä»ç”¨æˆ·é—®é¢˜å’ŒèŠå¤©è®°å½•ä¸­æå–å…³é”®è¯ï¼Œå¹¶ç”Ÿæˆé«˜æ•ˆçš„æœç´¢æŸ¥è¯¢ã€‚</p>
<pre><code class="language-python">from langchain.prompts import ChatPromptTemplate
from chains.models import load_model

class SummaryChain:
    """
    ä¸€ä¸ªç”¨äºç”Ÿæˆæœç´¢æŸ¥è¯¢çš„ç±»ã€‚
    å®ƒä»ç”¨æˆ·é—®é¢˜å’ŒèŠå¤©è®°å½•ä¸­æå–å…³é”®è¯ï¼Œå¹¶ç”Ÿæˆé«˜æ•ˆçš„æœç´¢æŸ¥è¯¢ã€‚
    """
    def __init__(self, model_name):
        """
        åˆå§‹åŒ– SummaryChain ç±»ï¼Œå¹¶åŠ è½½æŒ‡å®šçš„è¯­è¨€æ¨¡å‹ã€‚

        å‚æ•°:
            model_name (str): è¦åŠ è½½çš„è¯­è¨€æ¨¡å‹çš„åç§°ã€‚
        """
        self.llm = load_model(model_name)
        self.prompt = ChatPromptTemplate.from_template(
            "You are a professional assistant specializing in extracting keywords from user questions and chat histories. Extract keywords and connect them with spaces to output a efficient and precise search query. Be careful not answer the question directly, just output the search query.\n\nHistories: {history}\n\nQuestion: {question}"
        )
        self.chain = self.prompt | self.llm

    def invoke(self, input_data):
        """
        ä½¿ç”¨æä¾›çš„è¾“å…¥æ•°æ®è°ƒç”¨é“¾ä»¥ç”Ÿæˆæœç´¢æŸ¥è¯¢ã€‚

        å‚æ•°:
            input_data (dict): åŒ…å« 'history' å’Œ 'question' é”®çš„å­—å…¸ã€‚

        è¿”å›:
            str: é“¾ç”Ÿæˆçš„æœç´¢æŸ¥è¯¢ã€‚
        """
        return self.chain.invoke(input_data)
</code></pre>
<h3 id="ç”Ÿæˆå›ç­”">ç”Ÿæˆå›ç­”</h3>
<p>åœ¨ <code>generate.py</code> æ–‡ä»¶ä¸­ï¼Œå®šä¹‰äº† <code>GenerateChain</code> ç±»ï¼Œæ ¹æ®ç”¨æˆ·é—®é¢˜ã€èŠå¤©è®°å½•å’Œæ–‡æ¡£å†…å®¹ç”Ÿæˆå›ç­”ã€‚</p>
<pre><code class="language-python">from langchain.prompts import ChatPromptTemplate
from chains.models import load_model

class GenerateChain:
    """
    ä¸€ä¸ªç”¨äºç”Ÿæˆé—®ç­”ä»»åŠ¡å“åº”çš„ç±»ã€‚
    å®ƒä½¿ç”¨è¯­è¨€æ¨¡å‹å’Œæç¤ºæ¨¡æ¿æ¥å¤„ç†è¾“å…¥æ•°æ®ã€‚
    """
    def __init__(self, model_name):
        """
        åˆå§‹åŒ– GenerateChain ç±»ï¼Œå¹¶åŠ è½½æŒ‡å®šçš„è¯­è¨€æ¨¡å‹ã€‚

        å‚æ•°:
            model_name (str): è¦åŠ è½½çš„è¯­è¨€æ¨¡å‹çš„åç§°ã€‚
        """
        self.llm = load_model(model_name)
        self.prompt = ChatPromptTemplate.from_template("You are an assistant for question-answering tasks. Use the following documents or chat histories to answer the question. If the documents or chat histories is empty, answer the question based on your own knowledge. If you don't know the answer, just say that you don't know.\n\nDocuments: {documents}\n\nHistories: {history}\n\nQuestion: {question}")

        self.chain = self.prompt | self.llm

    def invoke(self, input_data):
        """
        ä½¿ç”¨æä¾›çš„è¾“å…¥æ•°æ®è°ƒç”¨é“¾ä»¥ç”Ÿæˆå“åº”ã€‚

        å‚æ•°:
            input_data (dict): åŒ…å« 'documents'ã€'history' å’Œ 'question' é”®çš„å­—å…¸ã€‚

        è¿”å›:
            str: é“¾ç”Ÿæˆçš„å“åº”ã€‚
        """
        return self.chain.invoke(input_data)
</code></pre>
<h2 id="è¿æ¥æ™ºèƒ½ä½“">è¿æ¥æ™ºèƒ½ä½“</h2>
<p>åœ¨ LangGraph ä¸­ï¼Œæ™ºèƒ½ä½“ä¹‹é—´çš„è¿æ¥é€šè¿‡ <strong>çŠ¶æ€å›¾ï¼ˆgraphï¼‰</strong> æ¥å®ç°ï¼Œä½¿ç”¨ <strong>çŠ¶æ€ï¼ˆstateï¼‰</strong> å­˜å‚¨äº¤äº’çš„ä¿¡æ¯ã€‚å›¾ç”±èŠ‚ç‚¹ï¼ˆnodeï¼‰å’Œè¾¹ï¼ˆedgeï¼‰ç»„æˆï¼ŒèŠ‚ç‚¹è¡¨ç¤ºæ™ºèƒ½ä½“ï¼Œè¾¹è¡¨ç¤ºæ™ºèƒ½ä½“ä¹‹é—´çš„å…³ç³»ã€‚åœ¨ <code>graph</code> ç›®å½•ä¸‹å®šä¹‰äº†ä¸¤ä¸ªæ–‡ä»¶ï¼š<code>graph.py</code> å’Œ <code>graph_state.py</code>ã€‚</p>
<pre><code class="language-shell">.
â”œâ”€â”€ graph   # å›¾ç»“æ„
â”‚&nbsp;&nbsp; â”œâ”€â”€ graph.py
â”‚&nbsp;&nbsp; â””â”€â”€ graph_state.py
</code></pre>
<h3 id="å®šä¹‰å›¾çš„çŠ¶æ€">å®šä¹‰å›¾çš„çŠ¶æ€</h3>
<p>åœ¨ <code>graph_state.py</code> æ–‡ä»¶ä¸­ï¼Œå®šä¹‰äº† <code>GraphState</code> ç±»ï¼Œç”¨äºå­˜å‚¨å›¾çš„çŠ¶æ€ä¿¡æ¯ã€‚</p>
<pre><code class="language-python">from typing import Literal, Annotated, Optional
from typing_extensions import TypedDict
from langgraph.graph.message import add_messages

class GraphState(TypedDict):
    """
    å®šä¹‰å›¾çŠ¶æ€çš„ç±»å‹å­—å…¸ã€‚
    ç”¨äºè¡¨ç¤ºå›¾ä¸­çš„çŠ¶æ€ä¿¡æ¯ã€‚
    """
    model_name: str  # ä½¿ç”¨çš„æ¨¡å‹åç§°
    type: Literal["websearch", "file", "chat"]  # æ“ä½œç±»å‹ï¼ŒåŒ…æ‹¬è”ç½‘æœç´¢ã€ä¸Šä¼ æ–‡ä»¶å’ŒèŠå¤©
    messages: Annotated[list, add_messages]  # æ¶ˆæ¯åˆ—è¡¨ï¼Œä½¿ç”¨add_messagesæ³¨è§£å¤„ç†æ¶ˆæ¯è¿½åŠ 
    documents: Optional[list] = []  # æ–‡æ¡£åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºç©ºåˆ—è¡¨
</code></pre>
<h3 id="å®šä¹‰èŠ‚ç‚¹æ–¹æ³•">å®šä¹‰èŠ‚ç‚¹æ–¹æ³•</h3>
<p>åœ¨ <code>graph.py</code> æ–‡ä»¶ä¸­ï¼Œå®šä¹‰äº†å¤šä¸ªæ–¹æ³•ï¼Œè¡¨ç¤ºå›¾çš„ç»“æ„å’Œè¡Œä¸ºï¼Œç”¨äºå¤„ç†ä¸åŒç±»å‹çš„è¯·æ±‚ã€‚å…ˆå¼•å…¥æ‰€éœ€è¦çš„åŒ…ã€‚</p>
<pre><code class="language-python">import os
from langchain.schema import Document
from langchain_core.runnables import RunnableConfig
from langchain_community.document_loaders import TextLoader
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
from langgraph.graph.state import StateGraph, CompiledStateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from marker.converters.pdf import PdfConverter
from marker.models import create_model_dict
from marker.output import text_from_rendered
from graph.graph_state import GraphState
from chains.summary import SummaryChain
from chains.generate import GenerateChain
</code></pre>
<h4 id="æ ¹æ®æŒ‡ä»¤è·¯ç”±">æ ¹æ®æŒ‡ä»¤è·¯ç”±</h4>
<p><code>route_question()</code> æ–¹æ³•æ ¹æ® <code>GraphState</code> ç±»ä¸­çš„æ“ä½œç±»å‹å°†è¯·æ±‚è·¯ç”±åˆ°ç›¸åº”çš„å¤„ç†èŠ‚ç‚¹ã€‚</p>
<pre><code class="language-python">def route_question(state: GraphState) -&gt; str:
    """
    æ ¹æ®æ“ä½œç±»å‹è·¯ç”±åˆ°ç›¸åº”çš„å¤„ç†èŠ‚ç‚¹ã€‚

    å‚æ•°:
        state (GraphState): å½“å‰å›¾çš„çŠ¶æ€

    è¿”å›:
        str: ä¸‹ä¸€ä¸ªè¦è°ƒç”¨çš„èŠ‚ç‚¹åç§°
    """

    print("--- ROUTE QUESTION ---")
    if state['type'] == 'websearch':
        print("--- ROUTE QUESTION TO EXTRACT KEYWORDS ---")
        return "extract_keywords"
    if state['type'] == 'file':
        print("--- ROUTE QUESTION TO FILE PROCESS ---")
        return "file_process"
    elif state['type'] == 'chat':
        print("--- ROUTE QUESTION TO GENERATE ---")
        return "generate"
</code></pre>
<p>å½“ç„¶ï¼Œä¹Ÿå¯ä»¥å°†è·¯ç”±äº¤ç»™ LLM å†³å®šï¼Œåªéœ€è¦å†™å¥½ç›¸åº”çš„æç¤ºè¯å³å¯ï¼Œä¾‹å¦‚ä¸‹é¢çš„æç¤ºè¯å°†ç”± LLM å†³å®šæ˜¯è¿›è¡ŒçŸ¥è¯†åº“æŸ¥è¯¢è¿˜æ˜¯ç½‘ç»œæœç´¢ã€‚</p>
<pre><code class="language-python">from langchain_core.output_parsers import JsonOutputParser

prompt = ChatPromptTemplate.from_template("You are an expert at routing a user question to a vectorstore or web search. Use the vectorstore for questions on LangChain and LangGraph. You do not need to be stringent with the keywords in the question related to these topics. Otherwise, use web-search. Give a binary choice 'web_search' or 'vectorstore' based on the question. Return the a JSON with a single key 'datasource' and no premable or explaination. Question to route: {question}")
router = prompt | llm | JsonOutputParser()

source = router.invoke({"question": question})
if source['datasource'] == 'web_search':
    # TODO: route to web search
elif source['datasource'] == 'vectorstore':
    # TODO: route to vectorstore
</code></pre>
<h4 id="ç”Ÿæˆå›ç­”-1">ç”Ÿæˆå›ç­”</h4>
<p><code>generate()</code> æ–¹æ³•æ ¹æ®ç”¨æˆ·é—®é¢˜ã€èŠå¤©è®°å½•å’Œæ–‡æ¡£å†…å®¹ç”Ÿæˆå›ç­”ã€‚</p>
<pre><code class="language-python">def generate(state: GraphState) -&gt; GraphState:
    """
    æ ¹æ®æ–‡æ¡£å’Œå¯¹è¯å†å²ç”Ÿæˆç­”æ¡ˆã€‚

    å‚æ•°:
        state (GraphState): å½“å‰å›¾çš„çŠ¶æ€

    è¿”å›:
        state (GraphState): è¿”å›æ·»åŠ äº†LLMç”Ÿæˆå†…å®¹çš„æ–°çŠ¶æ€
    """

    print("--- GENERATE ---")
    chain = GenerateChain(state["model_name"])
    messages = state["messages"]
    state["messages"] = chain.invoke({"question": messages[-1].content, "history": messages[:-1], "documents": state["documents"]})
    return state
</code></pre>
<h4 id="æ–‡ä»¶å¤„ç†">æ–‡ä»¶å¤„ç†</h4>
<p><code>file_process()</code> æ–¹æ³•å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶ï¼Œæå–æ–‡æœ¬å†…å®¹å¹¶è¿›è¡Œè¯åµŒå…¥ï¼ˆembeddingï¼‰ï¼Œç„¶åå°†å‘é‡å­˜å‚¨è‡³å†…å­˜æ•°æ®åº“ä¸­ã€‚<code>config</code> æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œå­˜å‚¨ LLM è¿è¡Œæ—¶çš„é…ç½®å‚æ•°ï¼Œä¼šåœ¨è°ƒç”¨ LLM æ—¶ä¼ å…¥ã€‚</p>
<pre><code class="language-python">def file_process(state: GraphState, config: RunnableConfig) -&gt; GraphState:
    """
    å¤„ç†æ–‡ä»¶ã€‚

    å‚æ•°:
        state (GraphState): å½“å‰å›¾çš„çŠ¶æ€
        config (RunnableConfig): å¯è¿è¡Œé…ç½®

    è¿”å›:
        state (GraphState): è¿”å›å›¾çŠ¶æ€ï¼Œå°†æ–‡æ¡£æ·»åŠ  config ä¸­çš„å‘é‡å­˜å‚¨
    """

    print("--- FILE PROCESS ---")
    vector_store = config["configurable"]["vectorstore"]

    for doc in state["documents"]:
        file_path: str = doc.page_content
        if os.path.exists(file_path):
            split_docs: list[Document] = None
            if file_path.endswith(".txt") or file_path.endswith(".md"):
                # å¤„ç†æ–‡æœ¬æˆ–Markdownæ–‡ä»¶
                docs = TextLoader(file_path, autodetect_encoding=True).load()
                # æ–‡æœ¬åˆ†å‰²
                splitter = RecursiveCharacterTextSplitter(separators=["\n\n", "\n", " ", ".", ",", "\u200B", "\uff0c", "\u3001", "\uff0e", "\u3002", ""], chunk_size=1000, chunk_overlap=100, add_start_index=True)
                split_docs = splitter.split_documents(docs)
            else: 
                # ä½¿ç”¨ marker-pdf å¤„ç†å…¶ä»–æ–‡ä»¶
                converter = PdfConverter(artifact_dict=create_model_dict())
                rendered = converter(file_path)
                docs, _, _ = text_from_rendered(rendered)
                splitter = MarkdownHeaderTextSplitter([("#", "Header 1"), ("##", "Header 2"), ("###", "Header 3")], strip_headers = False)
                split_docs = splitter.split_text(docs)
            # å°†å¤„ç†åçš„æ–‡æ¡£æ·»åŠ åˆ°å‘é‡å­˜å‚¨ä¸­
            vector_store.add_documents(split_docs)
    return state
</code></pre>
<h4 id="æå–å…³é”®è¯-1">æå–å…³é”®è¯</h4>
<p><code>extract_keywords()</code> æ–¹æ³•ä»ç”¨æˆ·é—®é¢˜å’ŒèŠå¤©è®°å½•ä¸­æå–å…³é”®è¯ï¼Œå¹¶ç”Ÿæˆé«˜æ•ˆçš„æœç´¢æŸ¥è¯¢ã€‚</p>
<pre><code class="language-python">def extract_keywords(state: GraphState, config: RunnableConfig) -&gt; GraphState:
    """
    ä»é—®é¢˜ä¸­æå–å…³é”®è¯ã€‚

    å‚æ•°:
        state (GraphState): å½“å‰å›¾çš„çŠ¶æ€
        config (RunnableConfig): å¯è¿è¡Œé…ç½®

    è¿”å›:
        state (GraphState): è¿”å›æ·»åŠ äº†æå–å…³é”®è¯çš„æ–°çŠ¶æ€
    """

    print("--- EXTRACT KEYWORDS ---")
    chain = SummaryChain(state["model_name"])
    messages = state["messages"]
    query = chain.invoke({"question": messages[-1].content, "history": messages[:-1]})
    print(query.content)

    if state["type"] == "websearch":
        # å°†ç”Ÿæˆçš„æœç´¢æŸ¥è¯¢æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨ä¸­ï¼Œä¸‹ä¸€ä¸ªèŠ‚ç‚¹å°†ä¼šä½¿ç”¨
        state["messages"] = query
    elif state["type"] == "file":
        # ä½¿ç”¨ç”Ÿæˆçš„æœç´¢æŸ¥è¯¢åœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢
        docs = config["configurable"]["vectorstore"].max_marginal_relevance_search(query.content)
        state["documents"] = docs
    return state
</code></pre>
<p>å¯¹äºâ€œä¸Šä¼ æ–‡ä»¶â€ï¼Œâ€œæå–å…³é”®è¯â€æ—¶å·²ç»è¿›è¡Œäº†æŸ¥è¯¢å¤„ç†ï¼Œå¯ä»¥ç›´æ¥è¿›è¡Œâ€œç”Ÿæˆå›ç­”â€ï¼›å¯¹äºâ€œç½‘ç»œæœç´¢â€ï¼Œâ€œæå–å…³é”®è¯â€è¿›è¡Œæœç´¢åï¼Œæ‰èƒ½è¿›è¡Œâ€œç”Ÿæˆå›ç­”â€ã€‚æ‰§è¡Œè·¯å¾„ä¸åŒï¼Œè¿˜éœ€è¦è¿›è¡Œåˆ¤æ–­ã€‚</p>
<pre><code class="language-python">def decide_to_generate(state: GraphState) -&gt; str:
    """
    å†³å®šæ˜¯è¿›è¡Œç½‘ç»œæœç´¢è¿˜æ˜¯ç›´æ¥ç”Ÿæˆå›ç­”ã€‚

    å‚æ•°:
        state (GraphState): å½“å‰å›¾çš„çŠ¶æ€

    è¿”å›:
        str: ä¸‹ä¸€ä¸ªè¦è°ƒç”¨çš„èŠ‚ç‚¹åç§°
    """

    if state["type"] == "websearch":
        print("--- DECIDE TO WEB SEARCH ---")
        return "websearch"
    elif state["type"] == "file":
        print("--- DECIDE TO GENERATE ---")
        return "generate"
</code></pre>
<h4 id="ç½‘ç»œæœç´¢">ç½‘ç»œæœç´¢</h4>
<p><code>web_search()</code> æ–¹æ³•ä½¿ç”¨ Tavily API è¿›è¡Œç½‘ç»œæœç´¢ï¼Œè·å–å®æ—¶ä¿¡æ¯ã€‚</p>
<pre><code class="language-python">def web_search(state: GraphState) -&gt; GraphState:
    """
    åŸºäºé—®é¢˜è¿›è¡Œç½‘ç»œæœç´¢ã€‚

    å‚æ•°:
        state (GraphState): å½“å‰å›¾çš„çŠ¶æ€

    è¿”å›:
        state (GraphState): è¿”å›æ·»åŠ äº†ç½‘ç»œæœç´¢ç»“æœçš„æ–°çŠ¶æ€
    """

    print("--- WEB SEARCH ---")
    web_search_tool = TavilySearchResults(k=3)
    documents = state["documents"]
    try:
        docs = web_search_tool.invoke({"query": state["messages"][-1].content})
        web_results = "\n".join([d["content"] for d in docs])
        web_results = Document(page_content=web_results)
        documents.append(web_results)
        state["documents"] = documents
    except:
        pass
    return state
</code></pre>
<h3 id="å®šä¹‰å›¾çš„ç»“æ„">å®šä¹‰å›¾çš„ç»“æ„</h3>
<p>åœ¨ LangGraph ä¸­ï¼Œå›¾çš„è¾¹åˆ†ä¸ºæ™®é€šè¾¹å’Œæ¡ä»¶è¾¹ã€‚æ™®é€šè¾¹è¡¨ç¤ºä¸¤ä¸ªèŠ‚ç‚¹ä¹‹é—´çš„ç›´æ¥è¿æ¥ï¼Œè€Œæ¡ä»¶è¾¹åˆ™æ ¹æ®ç‰¹å®šæ¡ä»¶å†³å®šæ˜¯å¦è¿æ¥ä¸¤ä¸ªèŠ‚ç‚¹ã€‚åœ¨ <code>create_graph()</code> æ–¹æ³•ä¸­å®šä¹‰äº†å›¾çš„ç»“æ„ï¼š<code>add_node()</code> æ–¹æ³•å°†å®šä¹‰çš„èŠ‚ç‚¹æ–¹æ³•æ·»åŠ åˆ°å›¾ä¸­ï¼›<code>add_edge()</code> æ–¹æ³•å®šä¹‰äº†èŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥å…³ç³»ï¼Œä¹Ÿå°±æ˜¯æ™®é€šè¾¹ï¼›<code>add_conditional_edges()</code> æ–¹æ³•å®šä¹‰äº†æ¡ä»¶è¾¹çš„è¿æ¥å…³ç³»ï¼›<code>set_conditional_entry_point()</code> æ–¹æ³•å®šä¹‰äº†å›¾çš„æ¡ä»¶å…¥å£èŠ‚ç‚¹ã€‚</p>
<p><img src="https://img2024.cnblogs.com/blog/1085877/202504/1085877-20250406155939600-1605236821.png" alt="" loading="lazy"></p>
<pre><code class="language-python">def create_graph() -&gt; CompiledStateGraph:
    """
    åˆ›å»ºå¹¶é…ç½®çŠ¶æ€å›¾å·¥ä½œæµã€‚

    è¿”å›:
        CompiledStateGraph: ç¼–è¯‘å¥½çš„çŠ¶æ€å›¾
    """

    workflow = StateGraph(GraphState)
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("websearch", web_search)
    workflow.add_node("extract_keywords", extract_keywords)
    workflow.add_node("file_process", file_process)
    workflow.add_node("generate", generate)
    # æ·»åŠ è¾¹
    workflow.set_conditional_entry_point(
        route_question,
        {
            "extract_keywords": "extract_keywords",
            "generate": "generate",
            "file_process": "file_process",
        },
    )
    workflow.add_edge("file_process", "extract_keywords")
    workflow.add_conditional_edges(
        "extract_keywords",
        decide_to_generate,
        {
            "websearch": "websearch",
            "generate": "generate",
        },
    )
    workflow.add_edge("websearch", "generate")
    workflow.add_edge("generate", END)

    # åˆ›å»ºå›¾ï¼Œå¹¶ä½¿ç”¨ `MemorySaver()` åœ¨å†…å­˜ä¸­ä¿å­˜çŠ¶æ€
    return workflow.compile(checkpointer=MemorySaver())
</code></pre>
<h3 id="è¿è¡Œå›¾">è¿è¡Œå›¾</h3>
<p>æœ€åé€šè¿‡ <code>stream_graph_updates()</code> æ–¹æ³•è¿è¡Œå›¾ï¼Œå¹¶æµå¼è¿”å›ç»“æœå†…å®¹ã€‚</p>
<pre><code class="language-python">def stream_graph_updates(graph: CompiledStateGraph, user_input: GraphState, config: dict):
    """
    æµå¼å¤„ç†å›¾æ›´æ–°å¹¶è¿”å›æœ€ç»ˆç»“æœã€‚

    å‚æ•°:
        graph (CompiledStateGraph): ç¼–è¯‘å¥½çš„çŠ¶æ€å›¾
        user_input (GraphState): ç”¨æˆ·è¾“å…¥çš„çŠ¶æ€
        config (dict): é…ç½®å­—å…¸

    è¿”å›:
        generator: ç”Ÿæˆå™¨å¯¹è±¡ï¼Œé€æ­¥è¿”å›å›¾æ›´æ–°çš„å†…å®¹
    """

    for chunk, _ in graph.stream(user_input, config, stream_mode="messages"):
        yield chunk.content
</code></pre>
<h2 id="è¿è¡ŒæŒ‡å—">è¿è¡ŒæŒ‡å—</h2>
<h3 id="åœ¨æ§åˆ¶å°ä¸­æµ‹è¯•ç¨‹åº">åœ¨æ§åˆ¶å°ä¸­æµ‹è¯•ç¨‹åº</h3>
<p>åœ¨ <code>main.py</code> æ–‡ä»¶ä¸­ï¼Œå®šä¹‰äº†ä¸€ä¸ªå‘½ä»¤è¡Œç¨‹åºï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡è¾“å…¥é—®é¢˜ä¸æ™ºèƒ½ä½“è¿›è¡Œäº¤äº’ã€‚</p>
<pre><code class="language-python">import uuid
from dotenv import load_dotenv
from langchain.schema import Document
from langchain_core.messages import AIMessage, HumanMessage
from chains.models import load_vector_store
from graph.graph import create_graph, stream_graph_updates, GraphState

def main():
    # langchain.debug = True  # å¯ç”¨langchainè°ƒè¯•æ¨¡å¼ï¼Œå¯ä»¥è·å¾—å¦‚å®Œæ•´æç¤ºè¯ç­‰ä¿¡æ¯
    load_dotenv(verbose=True)  # åŠ è½½ç¯å¢ƒå˜é‡é…ç½®

    # åˆ›å»ºçŠ¶æ€å›¾ä»¥åŠå¯¹è¯ç›¸å…³çš„è®¾ç½®
    config = {"configurable": {"thread_id": uuid.uuid4().hex, "vectorstore": load_vector_store("nomic-embed-text")}}  
    state = GraphState(
        model_name="qwen2.5:7b",
        type="chat",
        documents=[Document(page_content="upload_files/test.pdf")],
    )
    graph = create_graph()

    # å¯¹è¯
    while True:
        user_input = input("User: ")
        if user_input.lower() in ["exit", "quit"]:
            break
        state["messages"] = HumanMessage(user_input)
        # æµå¼è·å–AIçš„å›å¤
        for answer in stream_graph_updates(graph, state, config):
            print(answer, end="")
        print()

    # æ‰“å°å¯¹è¯å†å²
    print("\nHistory: ")
    for message in graph.get_state(config).values["messages"]:
        if isinstance(message, AIMessage):
            prefix = "AI"
        else:
            prefix = "User"
        print(f"{prefix}: {message.content}")

if __name__ == "__main__":
    main()
</code></pre>
<h3 id="ä½¿ç”¨-streamlit-æ„å»ºå‰ç«¯é¡µé¢">ä½¿ç”¨ Streamlit æ„å»ºå‰ç«¯é¡µé¢</h3>
<p>åœ¨ <code>app.py</code> æ–‡ä»¶ä¸­ï¼Œä½¿ç”¨ <code>Streamlit</code> æ„å»ºäº†ä¸€ä¸ªç®€å•çš„å‰ç«¯ç•Œé¢ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡è¾“å…¥æ¡†ä¸æ™ºèƒ½ä½“è¿›è¡Œäº¤äº’ã€‚å®Œæ•´çš„ç¨‹åºä»£ç å¦‚ä¸‹ï¼š</p>
<p><img src="https://img2024.cnblogs.com/blog/1085877/202504/1085877-20250406155913943-35085412.png" alt="" loading="lazy"></p>
<pre><code class="language-python">import uuid
import datetime
from dotenv import load_dotenv
from langchain.schema import Document
import streamlit as st
from streamlit_extras.bottom_container import bottom
from chains.models import load_vector_store
from graph.graph import create_graph, stream_graph_updates, GraphState

# è®¾ç½®ä¸Šä¼ æ–‡ä»¶çš„å­˜å‚¨è·¯å¾„
file_path = "upload_files/"
# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(verbose=True)

def upload_pdf(file):
    """ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶å¹¶è¿”å›æ–‡ä»¶è·¯å¾„"""
    with open(file_path + file.name, "wb") as f:
        f.write(file.getbuffer())
        return file_path + file.name

# è®¾ç½®é¡µé¢é…ç½®ä¿¡æ¯
st.set_page_config(
    page_title="AI-Powerwd Assistant",
    page_icon="ğŸŒ",
    layout="wide"
)

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€å˜é‡ï¼Œåˆ›å»ºå›¾
if "graph" not in st.session_state:
    st.session_state.graph = create_graph()
# åˆå§‹åŒ–ä¼šè¯IDå’Œå‘é‡å­˜å‚¨
if "config" not in st.session_state:
    st.session_state.config = {"configurable": {"thread_id": uuid.uuid4().hex, "vectorstore": load_vector_store("nomic-embed-text")}}
# åˆå§‹åŒ–å¯¹è¯å†å²è®°å½•
if "history" not in st.session_state:
    st.session_state.history = []
# åˆå§‹åŒ–ä¸Šä¼ çŠ¶æ€ã€æ¨¡å‹åç§°å’Œå¯¹è¯ç±»å‹
if "settings" not in st.session_state:
    st.session_state.settings = {"uploaded": False, "model_name": "qwen2.5:7b", "type": "chat"}

# æ˜¾ç¤ºåº”ç”¨æ ‡é¢˜
st.header("ğŸ‘½ AI-Powerwd Assistant")

# å®šä¹‰å¯é€‰çš„æ¨¡å‹
model_options = {"é€šä¹‰åƒé—® 2.5 7B": "qwen2.5:7b", "DeepSeek R1 7B": "deepseek-r1:7b"}
with st.sidebar:
    # ä¾§è¾¹æ è®¾ç½®éƒ¨åˆ†
    st.header("è®¾ç½®")
    # æ¨¡å‹é€‰æ‹©ä¸‹æ‹‰æ¡†
    st.session_state.settings["model_name"] = model_options[st.selectbox("é€‰æ‹©æ¨¡å‹", model_options, index=list(model_options.values()).index(st.session_state.settings["model_name"]))]

    st.divider()

    # æ˜¾ç¤ºç‰ˆæœ¬ä¿¡æ¯
    st.text(f"{datetime.datetime.now().strftime('%Y.%m.%d')} - ZHANG GAOXING")

# å®šä¹‰å¯¹è¯ç±»å‹é€‰é¡¹
type_options = {"ğŸ¤– å¯¹è¯": "chat", "ğŸ” è”ç½‘æœç´¢": "websearch", "ğŸ‘¾ ä»£ç æ¨¡å¼": "code"}
question = None
with bottom():
    # åº•éƒ¨å®¹å™¨ï¼ŒåŒ…å«å·¥å…·é€‰æ‹©ã€æ–‡ä»¶ä¸Šä¼ å’Œè¾“å…¥æ¡†
    st.session_state.settings["type"] = type_options[st.radio("å·¥å…·é€‰æ‹©", type_options.keys(), horizontal=True, label_visibility="collapsed", index=list(type_options.values()).index(st.session_state.settings["type"]))]
    # æ–‡ä»¶ä¸Šä¼ ç»„ä»¶
    uploaded_file = st.file_uploader("ä¸Šä¼ æ–‡ä»¶", type=["pdf", "docx", "xlsx", "txt", "md"], accept_multiple_files=False, label_visibility="collapsed")
    # èŠå¤©è¾“å…¥æ¡†
    question = st.chat_input('è¾“å…¥ä½ è¦è¯¢é—®çš„å†…å®¹')

# æ˜¾ç¤ºå†å²å¯¹è¯å†…å®¹
for message in st.session_state.history:
    with st.chat_message(message["role"]):
      st.markdown(message["content"])  

# å¤„ç†ç”¨æˆ·æé—®
if question: 
    # æ˜¾ç¤ºç”¨æˆ·é—®é¢˜
    with st.chat_message("user"):
        st.markdown(question)

    # å‡†å¤‡è¯·æ±‚çŠ¶æ€
    state = []
    if st.session_state.settings["type"] == "code":
        # ä»£ç æ¨¡å¼ä½¿ç”¨ä¸“é—¨çš„ä»£ç æ¨¡å‹
        state = {"model_name": "qwen2.5-coder:7b", "messages": [{"role": "user", "content": question}], "type": "chat", "documents": []}
    else:
        # å…¶ä»–æ¨¡å¼ä½¿ç”¨é€‰æ‹©çš„æ¨¡å‹
        state = {"model_name": st.session_state.settings["model_name"], "messages": [{"role": "user", "content": question}], "type": st.session_state.settings["type"], "documents": []}

    # å¤„ç†æ–‡ä»¶ä¸Šä¼ 
    if uploaded_file:
        state["type"] = "file"
        if not st.session_state.settings["uploaded"]:
            # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
            file_path = upload_pdf(uploaded_file)
            # æ·»åŠ æ–‡æ¡£åˆ°è¯·æ±‚
            state["documents"].append(Document(page_content=file_path))
            st.session_state.settings["uploaded"] = True

    # è·å–AIå›ç­”å¹¶ä»¥æµå¼æ–¹å¼æ˜¾ç¤º
    answer = st.chat_message("assistant").write_stream(stream_graph_updates(st.session_state.graph, state, st.session_state.config))

    # å°†å¯¹è¯ä¿å­˜åˆ°å†å²è®°å½•
    st.session_state.history.append({"role": "user", "content": question})
    st.session_state.history.append({"role": "assistant", "content": answer})
</code></pre>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.509656039005787" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-04-06 16:01">2025-04-06 16:00</span>&nbsp;
<a href="https://www.cnblogs.com/zhanggaoxing">å¼ é«˜å…´</a>&nbsp;
é˜…è¯»(<span id="post_view_count">48</span>)&nbsp;
è¯„è®º(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18811224" rel="nofollow">ç¼–è¾‘</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18811224);return false;">æ”¶è—</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18811224', targetLink: 'https://www.cnblogs.com/zhanggaoxing/p/18811224', title: 'å¼ é«˜å…´çš„å¤§æ¨¡å‹å¼€å‘å®æˆ˜ï¼šï¼ˆå››ï¼‰ä½¿ç”¨ LangGraph å®ç°å¤šæ™ºèƒ½ä½“åº”ç”¨' })">ä¸¾æŠ¥</a>
</div>
        