
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/zhanggaoxing/p/18831270" title="发布于 2025-04-17 17:23">
    <span role="heading" aria-level="2">张高兴的大模型开发实战：（五）使用 LLaMA Factory 微调与量化模型并部署至 Ollama</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p></p><div class="toc"><div class="toc-container-header">目录</div><ul><li><a href="#环境搭建与配置" rel="noopener nofollow">环境搭建与配置</a></li><li><a href="#数据集准备" rel="noopener nofollow">数据集准备</a></li><li><a href="#webui-配置微调参数" rel="noopener nofollow">WebUI 配置微调参数</a></li><li><a href="#模型导出与量化" rel="noopener nofollow">模型导出与量化</a></li><li><a href="#导入-ollama" rel="noopener nofollow">导入 Ollama</a></li></ul></div><p></p>
<p>LLaMA Factory 是一个开源的全栈大模型微调框架，简化和加速大型语言模型的训练、微调和部署流程。它支持从预训练到指令微调、强化学习、多模态训练等全流程操作，并提供灵活的配置选项和高效的资源管理能力，适合开发者快速定制化模型以适应特定应用场景。下面通过一个简单的示例来展示如何使用 LLaMA Factory 进行模型微调并部署至 Ollama。</p>
<h2 id="环境搭建与配置">环境搭建与配置</h2>
<p>克隆 LLaMA Factory 的 Git 仓库（<a href="https://github.com/hiyouga/LLaMA-Factory%EF%BC%89%EF%BC%8C%E5%88%9B%E5%BB%BA" target="_blank" rel="noopener nofollow">https://github.com/hiyouga/LLaMA-Factory），创建</a> Python 虚拟环境并安装依赖。</p>
<pre><code class="language-bash">git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e ".[torch,metrics,gptq]"
</code></pre>
<p>安装完成后，在 Python 终端执行以下代码，检查 PyTorch 是否为 GPU 版本，如果不是则需要手动安装。</p>
<pre><code class="language-python">import torch
print(torch.__version__)    # '2.6.0+cu126'
print(torch.cuda.is_available())    # True
</code></pre>
<p>在命令行中使用以下命令运行 LLaMA Factory。</p>
<pre><code class="language-bash">llamafactory-cli webui
</code></pre>
<p><img src="https://img2024.cnblogs.com/blog/1085877/202504/1085877-20250417172156612-1499587307.png" alt="" loading="lazy"></p>
<h2 id="数据集准备">数据集准备</h2>
<p>微调数据集使用“弱智吧数据集”（<a href="https://github.com/FunnySaltyFish/Better-Ruozhiba%EF%BC%89%EF%BC%8C%E4%BB%8E%E7%99%BE%E5%BA%A6%E5%BC%B1%E6%99%BA%E5%90%A7%E4%B8%8A%E6%94%B6%E9%9B%86%E7%9A%84%E4%B8%80%E7%B3%BB%E5%88%97%E5%B8%96%E5%AD%90%EF%BC%8C%E6%97%A8%E5%9C%A8%E5%90%AF%E5%8F%91%E4%BA%BA%E4%BB%AC%E5%A8%B1%E4%B9%90%E6%80%A7%E4%BD%BF%E7%94%A8" target="_blank" rel="noopener nofollow">https://github.com/FunnySaltyFish/Better-Ruozhiba），从百度弱智吧上收集的一系列帖子，旨在启发人们娱乐性使用</a> ChatGPT 等 LLM 时的思路。微调模型使用阿里的 Qwen2.5:7B 模型。</p>
<pre><code class="language-json">// 数据集示例
[
    {
        "instruction": "只剩一个心脏了还能活吗？",
        "output": "能，人本来就只有一个心脏。"
    },
    {
        "instruction": "爸爸再婚，我是不是就有了个新娘？",
        "output": "不是的，你有了一个继母。\"新娘\"是指新婚的女方，而你爸爸再婚，他的新婚妻子对你来说是继母。"
    }
]
</code></pre>
<p>数据集和模型可以在魔塔社区（<a href="https://www.modelscope.cn" target="_blank" rel="noopener nofollow">https://www.modelscope.cn</a>）上搜索并下载，下载可以使用魔塔社区提供的 SDK 或者 Git 命令行下载。</p>
<p><img src="https://img2024.cnblogs.com/blog/1085877/202504/1085877-20250417172206250-89184151.png" alt="" loading="lazy"></p>
<p>下面的命令可以直接下载弱智吧数据集和 Qwen2.5:7B 模型。</p>
<pre><code class="language-bash">git clone https://www.modelscope.cn/datasets/AI-ModelScope/Better-Ruozhiba.git    # 弱智吧数据集
git clone https://www.modelscope.cn/Qwen/Qwen2.5-7B-Instruct.git    # Qwen2.5:7B 模型
</code></pre>
<p>下载好的数据集还需要在 LLaMA Factory 中进行配置。LLaMA Factory 支持 Alpaca 和 ShareGPT 两种数据格式，分别适用于指令监督微调和多轮对话任务。</p>
<ul>
<li><strong>Alpaca 格式</strong>：适用于单轮任务，如问答、文本生成、摘要、翻译等。结构简洁，任务导向清晰，适合低成本的指令微调。<pre><code class="language-json">{
    "instruction": "计算这些物品的总费用。",
    "input": "输入：汽车 - $3000，衣服 - $100，书 - $20。",
    "output": "汽车、衣服和书的总费用为 $3000 + $100 + $20 = $3120。"
}
</code></pre>
</li>
<li><strong>ShareGPT 格式</strong>：适用于多轮对话、聊天机器人等任务。结构复杂，包含多轮对话上下文，适合高质量的对话生成和人机交互任务。<pre><code class="language-json">[
    {
        "instruction": "今天的天气怎么样？",
        "input": "",
        "output": "今天的天气不错，是晴天。",
        "history": [
            [
                "今天会下雨吗？",
                "今天不会下雨，是个好天气。"
            ],
            [
                "今天适合出去玩吗？",
                "非常适合，空气质量很好。"
            ]
        ]
    }
]
</code></pre>
</li>
</ul>
<p>数据集中的字段含义如下：</p>
<ul>
<li>instruction（必填）：明确的任务指令，模型需要根据该指令生成输出。</li>
<li>input（可选）：与任务相关的背景信息或上下文。</li>
<li>output（必填）：模型需要生成的正确回答。</li>
<li>system（可选）：系统提示词，用于定义任务的上下文。</li>
<li>history（可选）：历史对话记录，用于多轮对话任务。</li>
</ul>
<p>将下载好的 JSON 数据集放入 <code>LLaMA-Factory/data</code> 目录下，并在 <code>LLaMA-Factory/data/data_info.json</code> 中注册数据集。</p>
<p><img src="https://img2024.cnblogs.com/blog/1085877/202504/1085877-20250417172214850-1003098371.png" alt="" loading="lazy"></p>
<h2 id="webui-配置微调参数">WebUI 配置微调参数</h2>
<p>访问 <code>http://localhost:7860/</code> ，进入 LLaMA Factory 的 WebUI 界面。WebUI 主要分为四个界面：<strong>训练（Train）、评估与预测（Evaluate &amp; Predict）、对话（Chat）、导出（Export）。</strong></p>
<p>先设置页面上半部分的内容。</p>
<p><img src="https://img2024.cnblogs.com/blog/1085877/202504/1085877-20250417172221564-1176168281.png" alt="" loading="lazy"></p>
<p>模型名称选择为待训练的模型名称，这里设置为 <code>Qwen2.5-7B-Instruct</code>。模型路径设置为上面下载的模型路径，例如在 <code>LLaMA-Factory</code> 目录下新建一个 <code>models</code> 文件夹，将下载的模型移动到此文件夹内，可设置路径为 <code>models/Qwen2.5-7B-Instruct</code>。微调方法支持 lora/freeze/full 方法，这里选择 <code>lora</code> 方法，其他方法对计算机配置要求较高，对个人电脑来说一般不适用。</p>
<ul>
<li>LoRA（Low-Rank Adaptation）：通过在模型的某些层中添加低秩矩阵来实现微调。</li>
<li>全量微调（Full Fine-Tuning）：对模型的所有参数进行微调。</li>
<li>冻结微调（Freeze Fine-Tuning）：冻结模型的某些层或全部层，仅微调特定的参数。</li>
</ul>
<p>下表描述了在训练或推理不同规模的大模型（如 7B、13B 参数模型）时，所需硬件的显存需求。<strong>例如使用 LoRA 微调 Qwen2.5:7B 模型时，显存需求为 16GB。</strong></p>
<table>
<thead>
<tr>
<th>方法</th>
<th>精度</th>
<th>7B</th>
<th>14B</th>
<th>30B</th>
<th>70B</th>
<th><code>x</code>B</th>
</tr>
</thead>
<tbody>
<tr>
<td>Full (<code>bf16</code> or <code>fp16</code>)</td>
<td>32</td>
<td>120GB</td>
<td>240GB</td>
<td>600GB</td>
<td>1200GB</td>
<td><code>18x</code>GB</td>
</tr>
<tr>
<td>Full (<code>pure_bf16</code>)</td>
<td>16</td>
<td>60GB</td>
<td>120GB</td>
<td>300GB</td>
<td>600GB</td>
<td><code>8x</code>GB</td>
</tr>
<tr>
<td>Freeze/LoRA/GaLore/APOLLO/BAdam</td>
<td>16</td>
<td>16GB</td>
<td>32GB</td>
<td>64GB</td>
<td>160GB</td>
<td><code>2x</code>GB</td>
</tr>
<tr>
<td>QLoRA</td>
<td>8</td>
<td>10GB</td>
<td>20GB</td>
<td>40GB</td>
<td>80GB</td>
<td><code>x</code>GB</td>
</tr>
<tr>
<td>QLoRA</td>
<td>4</td>
<td>6GB</td>
<td>12GB</td>
<td>24GB</td>
<td>48GB</td>
<td><code>x/2</code>GB</td>
</tr>
<tr>
<td>QLoRA</td>
<td>2</td>
<td>4GB</td>
<td>8GB</td>
<td>16GB</td>
<td>24GB</td>
<td><code>x/4</code>GB</td>
</tr>
</tbody>
</table>
<p>下面设置 Train 选项卡中的参数。</p>
<p><img src="https://img2024.cnblogs.com/blog/1085877/202504/1085877-20250417172228280-173013320.png" alt="" loading="lazy"></p>
<p>训练阶段设置为 <code>Supervised Fine-Tuning</code>。</p>
<ul>
<li>Supervised Fine-Tuning：监督微调是最常见的微调方法，使用标注好的数据对预训练模型进行进一步训练，以适应特定任务（如分类、问答等）。</li>
<li>Reward Modeling：奖励建模是一种用于优化模型输出质量的方法，通常用于强化学习的上下文中。</li>
<li>PPO（Proximal Policy Optimization）：PPO 是一种基于强化学习的微调方法，用于优化模型的输出策略。</li>
<li>DPO （Direct Preference Optimization）：DPO 是一种基于人类偏好的直接优化方法，用于训练模型以生成更符合人类偏好的输出。</li>
<li>Pre-Training：预训练是指从头开始训练一个大模型，通常使用大量的无监督数据（如文本语料库）。预训练的目标是让模型学习通用的语言知识和模式。</li>
</ul>
<p>数据集选择上文注册的数据集名称，这里设置为 <code>ruozhiba</code>。训练轮次根据数据集大小调整，这里设置为 100。学习率通常设置为 1e-4 或 5e-5。计算类型设置为 <code>bf16</code>，如果你的硬件不支持，可以选择 <code>fp16</code>，基本上 2020 年之后的 GPU 都支持 <code>bf16</code>。</p>
<p><img src="https://img2024.cnblogs.com/blog/1085877/202504/1085877-20250417172234323-2005018850.png" alt="" loading="lazy"></p>
<p>接着对 LoRA 参数进行设置。其中关键的参数是<strong>秩（rank）</strong>，秩的大小直接影响模型的性能和资源消耗。秩越大，引入的可训练参数越多，模型对新数据的适应能力越强，但也增加了计算和内存的需求，可能导致过拟合。秩越小，引入的可训练参数较少，减少了计算和内存的需求，但可能不足以充分适应新数据，影响模型性能。可以从较小的值开始（如8、10、12），逐步增加，观察模型性能的变化。</p>
<p>参数配置好后，点击开始，即可进行训练。训练时可以观察右侧的损失曲线，曲线长时间不下降时，即可考虑退出训练。</p>
<p><img src="https://img2024.cnblogs.com/blog/1085877/202504/1085877-20250417172240395-2086677712.png" alt="" loading="lazy"></p>
<p>模型训练好后，会保存至 LLaMA-Factory 的 <code>saves</code> 文件夹中。</p>
<p><img src="https://img2024.cnblogs.com/blog/1085877/202504/1085877-20250417172245963-846885232.png" alt="" loading="lazy"></p>
<h2 id="模型导出与量化">模型导出与量化</h2>
<p>下面切换至 Export 选项卡，设置导出参数。补全检查点路径与导出目录，点击开始导出。到此为止，模型已经具备了使用能力。</p>
<p><img src="https://img2024.cnblogs.com/blog/1085877/202504/1085877-20250417172252222-1100715971.png" alt="" loading="lazy"></p>
<p>大语言模型的参数通常以高精度浮点数（如32位浮点数，FP32）存储，这导致模型推理需要大量计算资源。量化技术通过将高精度数据类型存储的参数转换为低精度数据类型（如8位整数，INT8）存储，可以在不改变模型参数量和架构的前提下加速推理过程。这种方法使得模型的部署更加经济高效，也更具可行性。</p>
<p>量化前需要先将模型导出后再量化。修改模型路径为导出后的模型路径，导出量化等级一般选择 8 或 4，太低模型会答非所问。</p>
<p><img src="https://img2024.cnblogs.com/blog/1085877/202504/1085877-20250417172258808-461376248.png" alt="" loading="lazy"></p>
<h2 id="导入-ollama">导入 Ollama</h2>
<p>新版的 Ollama 可以直接导入 safetensors 模型，首先需要准备 Modelfile 文件。Modelfile 文件是一个文本文件，包含了模型的基本信息和配置参数。可以在命令行中执行下面的命令，看看 Ollama 中对应的模型是怎么写的。</p>
<pre><code class="language-bash">ollama show --modelfile qwen2.5:7b
</code></pre>
<p><img src="https://img2024.cnblogs.com/blog/1085877/202504/1085877-20250417172305704-195845759.png" alt="" loading="lazy"></p>
<p>当然 LLaMA Factory 导出时也已经生成了 Modelfile 文件，直接使用即可。</p>
<p><img src="https://img2024.cnblogs.com/blog/1085877/202504/1085877-20250417172311231-969381757.png" alt="" loading="lazy"></p>
<p>将命令行切换到导出模型的目录，执行下面的命令，导入模型。</p>
<pre><code class="language-bash">ollama create qwen2.5-ruozhi:7b -f Modelfile
</code></pre>
<p>最后运行微调前和微调后的模型，比较一下效果</p>
<p><img src="https://img2024.cnblogs.com/blog/1085877/202504/1085877-20250417172317350-850846603.png" alt="" loading="lazy"></p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.03712885146990741" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-04-17 17:24">2025-04-17 17:23</span>&nbsp;
<a href="https://www.cnblogs.com/zhanggaoxing">张高兴</a>&nbsp;
阅读(<span id="post_view_count">0</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18831270);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18831270', targetLink: 'https://www.cnblogs.com/zhanggaoxing/p/18831270', title: '张高兴的大模型开发实战：（五）使用 LLaMA Factory 微调与量化模型并部署至 Ollama' })">举报</a>
</div>
        