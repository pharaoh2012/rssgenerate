
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/orion-orion/p/18688763" title="发布于 2025-01-23 23:12">
    <span role="heading" aria-level="2">贝叶斯机器学习：最大熵及高斯分布</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                    <div id="cnblogs_post_description" style="display: none">
        <img src="https://img2024.cnblogs.com/blog/1784958/202501/1784958-20250124005017532-98583387.png" alt="贝叶斯机器学习：最大熵及高斯分布" class="desc_img">
        高斯分布，也被称为正态分布，广泛应用于连续型随机变量分布的模型中。高斯分布可以从多个不同的角度来理解。例如，对于一个一元实值向量，使得熵取得最大值的是高斯分布。这个性质对于多元高斯分布也成立。当我们考虑多个随机变量之和的时候，也会产生高斯分布。观察式多元高斯分布的形式，考虑其中在指数位置上出现的二次型(x - mu)^T∑^{-1}(x - mu)。由于协方差矩阵∑是对称矩阵，那么∑^{-1}也是对称矩阵。我们假定∑是正定的，那么∑^{-1}也是正定的。于是，该二次型为x到mu的马⽒距离（Mahalanobis distance）Delta的平方。当∑是单位阵时，就变成了欧氏距离。
    </div>
<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p>高斯分布<sup>[1]</sup>，也被称为正态分布，广泛应用于连续型随机变量分布的模型中。对于一元变量<span class="math inline">\(x\)</span>的情形。高斯分布可以写成下列的形式：</p>
<p></p><div class="math display">\[\mathcal{N}(x\mid \mathcal{\mu}, \sigma^2) = \frac{1}{(2\pi\sigma^2)^{\frac{1}{2}}}\exp\left\{-\frac{1}{2\sigma^2}(x - \mu)^2\right\}
\]</div><p></p><p>其中<span class="math inline">\(\mu\)</span>是均值，<span class="math inline">\(\sigma^2\)</span>是方差。对于<span class="math inline">\(D\)</span>维向量<span class="math inline">\(\boldsymbol{x}\)</span>，多元高斯分布的形式为：</p>
<p></p><div class="math display">\[\mathcal{N}(\boldsymbol{x}\mid \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{(\mathrm{det}\boldsymbol{\Sigma})^{\frac{1}{2}}}\exp\left\{-\frac{1}{2}(\boldsymbol{x} - \boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x} - \boldsymbol{\mu})\right\}\tag{1}
\]</div><p></p><p>其中，<span class="math inline">\(\boldsymbol{\mu}\)</span>是一个<span class="math inline">\(D\)</span>维均值向量，<span class="math inline">\(\boldsymbol{\Sigma}\)</span>是一个<span class="math inline">\(D\times D\)</span>的<strong>协方差矩阵（covariance matrix）</strong>，<span class="math inline">\(\mathrm{det}\boldsymbol{\Sigma}\)</span>是<span class="math inline">\(\boldsymbol{\Sigma}\)</span>的行列式。</p>
<h1 id="1-高斯分布的物理意义">1 高斯分布的物理意义</h1>
<h2 id="11-做为最大熵分布的高斯分布">1.1 做为最大熵分布的高斯分布</h2>
<p>高斯分布可以从多个不同的角度来理解。例如，对于一个一元实值向量，使得熵取得最大值的是高斯分布。这个性质对于多元高斯分布也成立。</p>
<p><strong>熵（entropy）</strong> 的概念最早起源于物理学，是在热力学平衡的背景中介绍的。后来，熵称为描述统计力学中的无序程度的度量。在统计力学中，<strong>玻尔兹曼熵（Boltzmann's entropy）</strong><sup>[2][3]</sup>的定义为：</p>
<p></p><div class="math display">\[S \equiv k\ln\Omega
\]</div><p></p><p>这里<span class="math inline">\(\Omega\)</span>为系统宏观态的重数，<span class="math inline">\(k\)</span>为玻尔兹曼常量。</p>
<blockquote>
<p><strong>注</strong> 关于统计力学的术语我们用一个例子<sup>[4]</sup>来简要做一下介绍。考虑将3个不同的球放进3个不同的箱子中，全部的<span class="math inline">\(3^3 = 27\)</span>种可能放法如下图所示：</p>
<p align="center">
<img src="https://images.cnblogs.com/cnblogs_com/blogs/538207/galleries/2106514/o_250118021305_3%E4%B8%AA%E7%90%83%E6%94%BE%E5%85%A53%E4%B8%AA%E7%9B%92%E4%B8%AD%E5%85%A8%E9%83%A8%E5%8F%AF%E8%83%BD%E6%94%BE%E6%B3%95.png" width="700" height="240" alt="" align="center">        
</p>
<p>这27种不同结果中的每一种都称为<strong>微观态（microstate）</strong>。通常，在统计力学中，为了知道系统的微观态，我们必须清楚每个粒子的状态，在我们这个例子中是每个球划分到箱子中去的状态。如果更一般地指定状态——例如仅仅说各个箱子里有多少个球，我们称它为<strong>宏观态（macrostate）</strong>。当然，如果知道一个系统的微观态（比如<span class="math inline">\(\{ab\space\mid\space\space c\mid\space-\space\}\)</span>），那么我们肯定也能知道它的宏观态（比如箱子中的球个数分别为2、1、0）。但反过来却不行：知道箱子中的球个数分别为2、1、0并没有告诉我们每个球的状态，因为有3个微观态都对应于这个宏观态。对应于给定宏观态的微观态数量称为该宏观态的<strong>重数（multiplicity）</strong>，在这种情况下为3。<br>
更一般地，考虑将<span class="math inline">\(N\)</span>个球放进<span class="math inline">\(M\)</span>个箱子里，各个箱子里有<span class="math inline">\(n_1, n_2,\cdots, n_M\)</span>个球时宏观态的重数为我们在博客<a href="https://www.cnblogs.com/orion-orion/p/18519155" target="_blank">《概率论沉思录：初等抽样论》</a>中提到的多项式系数：</p>
<p></p><div class="math display">\[\Omega(n_1, n_2, \cdots, n_M) = \frac{N!}{\prod_{i}^Mn_i!}
\]</div><p></p><p>其中<span class="math inline">\(\sum_{i=1}^Mn_i = N\)</span>。</p>
</blockquote>
<p>对于将<span class="math inline">\(N\)</span>个不同的球放进<span class="math inline">\(M\)</span>个不同的箱子所形成的这样一个系统，各个箱子里有<span class="math inline">\(n_1, n_2,\cdots, n_M\)</span>个球时系统的熵为</p>
<p></p><div class="math display">\[S = k\ln\frac{N!}{\prod_{i}^Mn_i!} = k\ln N! - k\sum_{i=1}^M\ln n_i!
\]</div><p></p><p>现在我们在此基础上乘以一个缩放参数<span class="math inline">\(\frac{1}{N}\)</span>，忽略掉常数<span class="math inline">\(k\)</span>，并考虑<span class="math inline">\(N\gg 1\)</span>，根据<strong>斯特林近似（Stirlings approximation）</strong> 我们有<span class="math inline">\(\ln N!\approx N\ln N - N\)</span>，于是</p>
<p></p><div class="math display">\[\begin{aligned}
    \frac{1}{N}S &amp;\approx \frac{1}{N}\left(N\ln N - N\right) - \frac{1}{N}\sum_{i=1}^M\left[n_i\ln n_i - n_i\right]\\
    &amp;= \ln N - \sum_{i=1}^M \frac{n_i}{N}\ln n_i\\
    &amp;= - \sum_{i=1}^M \left(\frac{n_i}{N}\right)\ln \left(\frac{n_i}{N}\right)
\end{aligned}
\]</div><p></p><blockquote>
<p><strong>注</strong> 斯特林近似为：</p>
<p></p><div class="math display">\[    N!\approx \sqrt{2\pi N}\left(\frac{N}{e}\right)^N
\]</div><p></p><p>当<span class="math inline">\(N \gg 1\)</span>时，这个公式十分准确。该公式可以采用下列的直观方式进行理解。<span class="math inline">\(N!\)</span>是从<span class="math inline">\(1\)</span>到<span class="math inline">\(N\)</span>这<span class="math inline">\(N\)</span>个因子的乘积，一个十分粗略的估计是把每个因子都替换成<span class="math inline">\(N\)</span>，这就是<span class="math inline">\(N!\approx N^N\)</span>。这是一个过高的估计，因为几乎所有的因子都比<span class="math inline">\(N\)</span>小，平均下来每个因子都大了大约<span class="math inline">\(e\)</span>倍，也就是说</p>
<p></p><div class="math display">\[    N!\approx (\frac{N}{e})^N
\]</div><p></p><p>这仍比<span class="math inline">\(N!\)</span>差了一个大数字的因子，大约是<span class="math inline">\(\sqrt{2\pi N}\)</span>。但若<span class="math inline">\(N\)</span>是一个大数字，那么<span class="math inline">\(N!\)</span>会是一个非常大的数字，所以这个（大数字）因子就可以被省略了。如果我们只关心<span class="math inline">\(N!\)</span>的对数，通常上式就已经足够准确：</p>
<p></p><div class="math display">\[\ln N! \approx N\ln N - N
\]</div><p></p></blockquote>
<p>取极限<span class="math inline">\(N\rightarrow \infty\)</span>，并保持比值<span class="math inline">\(\frac{n_i}{N}\)</span>固定，我们可以得到</p>
<p></p><div class="math display">\[-\lim_{N\rightarrow \infty}\sum_{i=1}^M \left(\frac{n_i}{N}\right)\ln \left(\frac{n_i}{N}\right) = -\sum_{i=1}^M p_i\ln p_i
\]</div><p></p><p>这里<span class="math inline">\(p_i = \lim_{N\rightarrow \infty}(\frac{n_i}{N})\)</span>为给定系统宏观态时，一个球被分配到第<span class="math inline">\(i\)</span>个箱子的概率。对于一个球，它被分配到的箱子可以表述成一个离散随机变量<span class="math inline">\(X\)</span>（<span class="math inline">\(X\)</span>一共有<span class="math inline">\(M\)</span>个状态），且<span class="math inline">\(p(X = x_i) = p_i\space (i = 1, \cdots, M)\)</span>。这样，随机变量<span class="math inline">\(X\)</span>的熵就为</p>
<p></p><div class="math display">\[H[p] = -\sum_{i=1}^M p(x_i)\ln p(x_i)
\]</div><p></p><p>如果分布<span class="math inline">\(p(x_i)\)</span>在几个值周围有尖锐的峰值，熵就会相对较低。如果分布<span class="math inline">\(p(x_i)\)</span>相对平衡地跨过许多值，那么熵就会相对较高。下图展示了两个概率分布在30个箱子上的直方图：</p>
<p align="center">
<img src="https://images.cnblogs.com/cnblogs_com/blogs/538207/galleries/2015781/o_250112145113_%E4%B8%A4%E4%B8%AA%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83%E5%9C%A830%E4%B8%AA%E7%AE%B1%E5%AD%90%E4%B8%8A%E7%9A%84%E7%9B%B4%E6%96%B9%E5%9B%BE.png" width="700" height="260" alt="" align="center">        
</p>
<p>可以看到，熵值越大，<span class="math inline">\(H\)</span>越宽（最大的熵值产生于均匀分布，此时的熵值为<span class="math inline">\(H = -\ln(\frac{1}{30})\approx 3.40\)</span>）。</p>
<p>由于<span class="math inline">\(0\leqslant p_i\leqslant 1\)</span>，因此熵是非负的（由于当<span class="math inline">\(x\rightarrow 0\)</span>时，<span class="math inline">\(x\ln x\rightarrow 0\)</span>，我们约定<span class="math inline">\(0\ln 0 = 0\)</span>）。当<span class="math inline">\(p_i = 1\)</span>且所有其他的<span class="math inline">\(p_{j\neq i}=0\)</span>时，熵取得最小值<span class="math inline">\(0\)</span>。在<span class="math inline">\(\sum_i p(x_i) = 1\)</span>（概率归一化）的约束下，使用拉格朗日乘数法可以找到熵的最大值。因此，我们要最大化</p>
<p></p><div class="math display">\[\tilde{H} = -\sum_{i=1}^M p(x_i)\ln p(x_i) + \lambda\left(\sum_{i=1}^M p(x_i) - 1\right)
\]</div><p></p><p>令<span class="math inline">\(\frac{\mathrm{d}(\tilde{H})}{\mathrm{d}p(x_i)}=0\)</span>，解得<span class="math inline">\(p(x_i) = e^{\lambda - 1}\)</span>，然后代回约束条件<span class="math inline">\(\sum_i p(x_i)\)</span>，得到<span class="math inline">\(\lambda = 1 - \ln M\)</span>，于是最优解为<span class="math inline">\(p(x_i) = \frac{1}{M}\)</span>（此时所有的<span class="math inline">\(p(x_i)\)</span>都相等），此时熵取得最大值<span class="math inline">\(\ln M\)</span>。</p>
<p>我们可以把熵的定义扩展到连续随机变量<span class="math inline">\(X\)</span>的概率分布<span class="math inline">\(p(x)\)</span>，方法如下。首先把连续随机变量<span class="math inline">\(X\)</span>的取值范围切分成宽度为<span class="math inline">\(\Delta\)</span>的小段区间。根据<strong>均值定理（mean value theorem）</strong><sup>[5]</sup>，对于每个这样的小段区间<span class="math inline">\(\Delta\)</span>，一定在区间中存在一个值<span class="math inline">\(x_i\)</span>使得</p>
<p></p><div class="math display">\[\int_{i\Delta}^{(i + 1)\Delta}p(x)\mathrm{d}x = p(x_i)\Delta
\]</div><p></p><p>现在我们可以这样量化连续随机变量<span class="math inline">\(X\)</span>：只要<span class="math inline">\(X\)</span>落在第<span class="math inline">\(i\)</span>个区间中，我们就把<span class="math inline">\(X\)</span>赋值为<span class="math inline">\(x_i\)</span>。因此观察到值<span class="math inline">\(x_i\)</span>的概率为<span class="math inline">\(p(x_i)\Delta\)</span>。这样就变成了离散的分布，这种情况下熵的形式为：</p>
<p></p><div class="math display">\[H_{\Delta} = -\sum_{i=1}\left(p(x_i)\Delta\right) \ln \left(p(x_i)\Delta\right) = -\sum_{i=1}p(x_i)\Delta \ln p(x_i) - \ln \Delta
\]</div><p></p><p>推导时我们使用了<span class="math inline">\(\sum_ip(x_i)\Delta\)</span>，这是因为<span class="math inline">\(\sum_{i}p(x_i)\Delta = \sum_i\int_{i\Delta}^{(i + 1)\Delta}p(x)\mathrm{d}x = 1\)</span>。我们现在省略上式右侧的第二项<span class="math inline">\(-\ln \Delta\)</span>，然后考虑极限<span class="math inline">\(\Delta\rightarrow 0\)</span>，此时有：</p>
<p></p><div class="math display">\[\lim_{\Delta\rightarrow 0}\left\{-\sum_{i=1}p(x_i)\Delta \ln p(x_i)\right\} = -\int p(x)\ln p(x) \mathrm{d}x
\]</div><p></p><p>其中，右侧的量被称为<strong>微分熵（differential entropy）</strong>。我们看到，熵的离散形式与连续形式的差是<span class="math inline">\(\ln\Delta\)</span>，这在极限<span class="math inline">\(\Delta\rightarrow 0\)</span>的情形下发散。这反映出一个事实：具体化一个连续随机变量需要大量的比特位。和离散情形类似，我们将微分熵记为关于概率密度函数<span class="math inline">\(p(x)\)</span>的函数：</p>
<p></p><div class="math display">\[H[p] = -\int p(x)\ln p(x) \mathrm{d}x
\]</div><p></p><p>在离散分布的情况下，我们看到最大熵对应于变量所有可能状态的均匀分布。现在让我们考虑连续随机变量的最大熵。为了让这个最大值有一个合理的定义，除了保留归一化的约束之外，还有必要限制<span class="math inline">\(p(x)\)</span>的均值（一阶矩）和方差（二阶矩）。之所以需要限制其方差，是因为当方差增大时，熵也会无限制地增加，因此除非我们给定固定的方差<span class="math inline">\(\sigma^2\)</span>，否则寻找哪一个分布有最大熵这个问题是没有意义的。之所以需要限制其均值，是因为在不改变熵的条件下一个分布可以被随意地改变，因此我们需要再加一个均值为<span class="math inline">\(\mu\)</span>的约束以获得一个唯一的解<sup>[6]</sup>。综上，我们最大化微分熵的时候要施加下面三个约束：</p>
<p></p><div class="math display">\[\begin{aligned}
    \int p(x) \mathrm{d}x &amp;= 1\\
    \int xp(x) \mathrm{d}x &amp;= \mu\\
    \int (x - \mu)^2p(x) \mathrm{d}x &amp;= \sigma^2
\end{aligned}
\]</div><p></p><p>那么这个问题的拉格朗日泛函如下：</p>
<p></p><div class="math display">\[J(p) = H[p] + \lambda_1 (\int p(x) \mathrm{d}x - 1) + \lambda_2 (\int xp(x) \mathrm{d}x - \mu) + \lambda_3 (\int (x - \mu)^2p(x) \mathrm{d}x - \sigma^2)
\]</div><p></p><p>根据变分法，可以得到泛函导数为：</p>
<p></p><div class="math display">\[\frac{\delta J}{\delta p(x)} = -(\ln p(x) + 1) + \lambda_1 + \lambda_2 x + \lambda_3(x - \mu)^2
\]</div><p></p><p>令<span class="math inline">\(\frac{\delta J}{\delta p(x)} = 0\)</span>（对<span class="math inline">\(\forall x\)</span>），解得<span class="math inline">\(p(x) = \exp\{-1 + \lambda_1 + \lambda_2x + \lambda_3(x - \mu)^2\}\)</span>。</p>
<blockquote>
<p><strong>注</strong> 函数<span class="math inline">\(f\)</span>的函数被称为<strong>泛函（functional）</strong><span class="math inline">\(J(f)\)</span>。正如许多情况下对一个函数求关于以向量的各元素为变量的偏导数一样，我们可以使用<strong>泛函导数（functional derivative）</strong>，即在任意特定的<span class="math inline">\(x\)</span>值，对一个泛函<span class="math inline">\(J(f)\)</span>求关于函数<span class="math inline">\(f(x)\)</span>的导数，这也被称为<strong>变分导数（variational derivative）</strong>。泛函<span class="math inline">\(J\)</span>的关于函数<span class="math inline">\(f\)</span>在点<span class="math inline">\(x\)</span>处的泛函导数被记作<span class="math inline">\(\frac{\delta J}{\delta f(x)}\)</span>。</p>
<p>对于可微分函数<span class="math inline">\(f(x)\)</span>以及带有连续导数的可微分函数<span class="math inline">\(g(y, x)\)</span>（其中<span class="math inline">\(y=f(x)\)</span>），有下列结论：</p>
<p></p><div class="math display">\[\frac{\delta}{\delta f(x)} \int g(f(x), x)\mathrm{d}x = \frac{\partial}{\partial y} g(f(x), x)
\]</div><p></p><p>为了使上述等式更加直观，我们可以把<span class="math inline">\(f(x)\)</span>看做是一个有着无穷不可数多个元素的向量。在这里，这种关系式中描述的对于特定的<span class="math inline">\(x\)</span>，关于<span class="math inline">\(f(x)\)</span>的泛函导数可以类比为对于特定的下标<span class="math inline">\(i\)</span>，关于向量<span class="math inline">\(\boldsymbol{\theta}\in \mathbb{R}^n\)</span>的第<span class="math inline">\(i\)</span>个元素的偏导数：</p>
<p></p><div class="math display">\[\frac{\partial}{\partial \theta_i}\sum_j g(\theta_j, j) = \frac{\partial}{\partial \theta_i} g(\theta_i, i)
\]</div><p></p><p>为了关于一个向量优化某个函数，我们可以求出这个函数关于这个向量的梯度，然后找到使这个梯度中每一个元素都为0的点。类似地，我们可以通过寻找一个函数使得泛函导数在每个点上都等于0，从而来优化一个泛函。</p>
</blockquote>
<p>为了满足所有的约束，我们可以令<span class="math inline">\(\lambda_1 = 1 - \ln \sigma \sqrt{2\pi}\)</span>，<span class="math inline">\(\lambda_2 = 0\)</span>，<span class="math inline">\(\lambda_3 = - \frac{1}{2\sigma^2}\)</span>，从而得到</p>
<p></p><div class="math display">\[p(x) = \frac{1}{(2\pi \sigma^2)^{\frac{1}{2}}}\exp\left\{ - \frac{(x - \mu)^2}{2\sigma^2}\right\}
\]</div><p></p><p>因此最大化微分熵的分布是高斯分布<span class="math inline">\(\mathcal{N}(x\mid \mathcal{\mu}, \sigma^2)\)</span>。这也是当我们不知道真实的分布时，往往使用高斯分布的一个原因。因为高斯分布拥有最大的熵，我们通过这个假定来保证了最小可能量的结构。</p>
<blockquote>
<p><strong>注</strong> 若<span class="math inline">\(X\in [a, b]\)</span>，无其它约束条件，则此时最大熵分布就是该区间上的均匀分布，这里可以和<span class="math inline">\(X\)</span>为离散随机变量的情形联系起来。</p>
</blockquote>
<p>如果我们求高斯分布的微分熵，我们会得到：</p>
<p></p><div class="math display">\[\begin{aligned}
    H(p) &amp;= -\int p(x) \ln p(x) \mathrm{d}x\\
    &amp;= -\int \frac{1}{(2\pi \sigma^2)^{\frac{1}{2}}}\exp\left\{ - \frac{(x - \mu)^2}{2\sigma^2}\right\}\left[\ln \frac{1}{(2\pi \sigma^2)^{\frac{1}{2}}} - \frac{(x - \mu)^2}{2\sigma^2}\right]\\
    &amp;= -\ln \frac{1}{(2\pi \sigma^2)^{\frac{1}{2}}} + \frac{1}{2\sigma^2} \int \underbrace{\frac{1}{(2\pi \sigma^2)^{\frac{1}{2}}} \exp\left\{ - \frac{(x - \mu)^2}{2\sigma^2}\right\} (x - \mu)^2 \mathrm{d}x}_{\sigma^2}\\
    &amp;= \frac{1}{2}\left\{\ln (2\pi\sigma^2) + 1\right\}
\end{aligned}
\]</div><p></p><p>我们可以看到熵随着分布宽度（即<span class="math inline">\(\sigma^2\)</span>）的增加而增加。这个结果也表明，与离散熵不同，微分熵可以为负，因为对于上式而言，当<span class="math inline">\(\sigma^2 &lt; \frac{1}{2\pi e}\)</span>时，<span class="math inline">\(H(p) &lt; 0\)</span>。</p>
<h2 id="12-做为随机变量和分布的高斯分布">1.2 做为随机变量和分布的高斯分布</h2>
<p>当我们考虑多个随机变量之和的时候，也会产生高斯分布。根据林德伯格所证明的<strong>中心极限定理（central limit theorem）</strong>（为我们在博客<a href="https://www.cnblogs.com/orion-orion/p/18621496" target="_blank">《概率论沉思录：初等假设检验》</a>中提到过的伯努利试验的棣莫弗-拉普拉斯极限定理的推广），设<span class="math inline">\(\{X_k\}_{k=1}^N\)</span>是相互独立且具有共同分布的随机变量序列，假定<span class="math inline">\(\mu=\mathbb{E}[X_k]\)</span>和<span class="math inline">\(\sigma^2 = \mathrm{Var}[X_k]\)</span>都存在，并令<span class="math inline">\(S_N = X_1 + \cdots + X_N\)</span>，则当<span class="math inline">\(N\rightarrow \infty\)</span>时，有<span class="math inline">\(S_N \rightarrow \mathcal{N}(N\mu, \sigma\sqrt{N})\)</span>（依分布）。</p>
<p>例如，考虑<span class="math inline">\(N\)</span>个随机变量<span class="math inline">\(X_1, \cdots, X_N\)</span>，每一个都是区间<span class="math inline">\([0, 1]\)</span>上的均匀分布，然后考虑<span class="math inline">\(N\)</span>个随机变量的均值<span class="math inline">\(\frac{S_N}{N} = \frac{1}{N}(X_1 + \cdots + X_N)\)</span>的分布。对于大的<span class="math inline">\(N\)</span>，这个分布趋向于高斯分布，如下图所示：</p>
<p align="center">
<img src="https://images.cnblogs.com/cnblogs_com/blogs/538207/galleries/2015781/o_5c57ee58.png" width="700" height="160" alt="" align="center">        
</p>
<p>在实际应用中，随着<span class="math inline">\(N\)</span>的增加，分布会很迅速收敛为高斯分布。</p>
<h1 id="2-高斯分布的性质">2 高斯分布的性质</h1>
<h2 id="21-高斯分布的几何性质">2.1 高斯分布的几何性质</h2>
<p>观察式<span class="math inline">\((1)\)</span>中的多元高斯分布的形式，考虑其中在指数位置上出现的二次型</p>
<p></p><div class="math display">\[(\boldsymbol{x} - \boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x} - \boldsymbol{\mu})\tag{2}
\]</div><p></p><p>由于协方差矩阵<span class="math inline">\(\mathbf{\Sigma}\)</span>是对称矩阵，那么<span class="math inline">\(\mathbf{\Sigma}^{-1}\)</span>也是对称矩阵（注意<span class="math inline">\(\mathbf{\Sigma}^{-1}\)</span>的第<span class="math inline">\(j, i\)</span>个元素为<span class="math inline">\((-1)^{i + j}\left[\mathrm{det}\mathbf{\Sigma}_{ij} / \mathrm{det}\mathbf{\Sigma}\right]\)</span>，其中<span class="math inline">\(\mathbf{\Sigma}_{ij}\)</span>是从<span class="math inline">\(\mathbf{\Sigma}\)</span>中除去第<span class="math inline">\(i\)</span>行和第<span class="math inline">\(j\)</span>列后得到的矩阵，而对于对称矩阵<span class="math inline">\(\mathbf{\Sigma}\)</span>有<span class="math inline">\(\mathrm{det}\mathbf{\Sigma}_{ij} = \mathrm{det}\mathbf{\Sigma}_{ji}\)</span>）。我们假定<span class="math inline">\(\mathbf{\Sigma}\)</span>是正定的，那么<span class="math inline">\(\mathbf{\Sigma}^{-1}\)</span>也是正定的（后面我们会证明）。于是，式<span class="math inline">\((2)\)</span>为<span class="math inline">\(\boldsymbol{x}\)</span>到<span class="math inline">\(\boldsymbol{\mu}\)</span>的<strong>马⽒距离（Mahalanobis distance）</strong><span class="math inline">\(\Delta\)</span>的平方。当<span class="math inline">\(\boldsymbol{\Sigma}\)</span>是单位阵时，就变成了欧氏距离。</p>
<blockquote>
<p><strong>注</strong> 给定<span class="math inline">\(D\times D\)</span>的对称正定矩阵<span class="math inline">\(\mathbf{A}\)</span>，则从点<span class="math inline">\(\boldsymbol{x}\)</span>到原点的马氏距离<span class="math inline">\(\Delta\)</span>由正定二次型<span class="math inline">\(0 &lt; \Delta^2 = \boldsymbol{x}^T\mathbf{A}\boldsymbol{x}\)</span>（<span class="math inline">\(\boldsymbol{x} \neq \boldsymbol{0}\)</span>）确定；反过来，一个正定二次型<span class="math inline">\(\boldsymbol{x}^T\mathbf{A}\boldsymbol{x} &gt; 0\)</span>（<span class="math inline">\(\boldsymbol{x} \neq \boldsymbol{0}\)</span>）可以解释为点<span class="math inline">\(\boldsymbol{x}\)</span>到原点的马氏距离<span class="math inline">\(\Delta\)</span>的平方<sup>[7]</sup>。此外，从点<span class="math inline">\(\boldsymbol{x}\)</span>到任意一固定点<span class="math inline">\(\boldsymbol{\mu}\)</span>的马氏距离<span class="math inline">\(\Delta\)</span>的平方由公式<span class="math inline">\((\boldsymbol{x} - \boldsymbol{\mu})^T\mathbf{A}(\boldsymbol{x} - \boldsymbol{\mu})\)</span>给出。</p>
<p>马氏距离可以理解为在新的基底（也即<span class="math inline">\(\mathbf{A}\)</span>的规范正交特征向量组成的基底）下两点之间的距离。例如，设<span class="math inline">\(D=2\)</span>，则到原点的马氏距离为常数<span class="math inline">\(c\)</span>的点<span class="math inline">\(\boldsymbol{x} = (x_1, x_2)^T\)</span>满足<span class="math inline">\(\boldsymbol{x}^T\mathbf{A}\boldsymbol{x} = c^2\)</span>。对<span class="math inline">\(\mathbf{A}\)</span>进行谱分解得：</p>
<p></p><div class="math display">\[\mathbf{A} = \lambda_1 \boldsymbol{e}_1\boldsymbol{e}_1^T + \lambda_2\boldsymbol{e}_2\boldsymbol{e}_2^T
\]</div><p></p><p>所以</p>
<p></p><div class="math display">\[\begin{aligned}
    \boldsymbol{x}^T\mathbf{A}\boldsymbol{x} &amp;= \boldsymbol{x}^T(\lambda_1 \boldsymbol{e}_1\boldsymbol{e}_1^T + \lambda_2\boldsymbol{e}_2\boldsymbol{e}_2^T)\boldsymbol{x}\\
    &amp; = \lambda_1 (\boldsymbol{e}_1^T\boldsymbol{x})^2 + \lambda_2 (\boldsymbol{e}_2^T\boldsymbol{x})^2
\end{aligned}
\]</div><p></p><p>令<span class="math inline">\(y_1 = \boldsymbol{e}_1^T\boldsymbol{x}, y_2 = \boldsymbol{e}_2^T\boldsymbol{x}\)</span>，则<span class="math inline">\(\boldsymbol{y} = (y_1, y_2)^T\)</span>可视为点<span class="math inline">\(\boldsymbol{x}=(x_1, x_2)^T\)</span>在新的基底<span class="math inline">\(\boldsymbol{e}_1, \boldsymbol{e}_2\)</span>下的坐标，基变换关系为<span class="math inline">\(\left(\begin{matrix}
    y_1\\
    y_2
\end{matrix}\right) = \left(\begin{array}{c:c}\boldsymbol{e}_1 &amp; \boldsymbol{e}_2\end{array}\right)^T\left(\begin{matrix}
    x_1\\
    x_2
\end{matrix}\right) = \left(\begin{matrix}
    \boldsymbol{e}_1^T\\
    \hdashline
    \boldsymbol{e}_2^T
\end{matrix}\right)\left(\begin{matrix}
    x_1\\
    x_2
\end{matrix}\right)\)</span>。此时，<span class="math inline">\(\boldsymbol{y}=(y_1, y_2)^T\)</span>在以原点为中心的椭圆上，其方程为：</p>
<p></p><div class="math display">\[    \begin{aligned}
        \lambda_1 y_1^2 + \lambda_2 y_2^2 = c^2
    \end{aligned}
\]</div><p></p><p>该椭圆的轴即为做为新基底的规范正交特征向量<span class="math inline">\(\boldsymbol{e}_1, \boldsymbol{e}_2\)</span>。我们可以发现将<span class="math inline">\(\boldsymbol{x} = c\lambda_1^{-1/2}\boldsymbol{e}_1\)</span>代入椭圆方程得到<span class="math inline">\(\boldsymbol{x}^T\mathbf{A}\boldsymbol{x} = \lambda_1 (c\lambda_1^{-1/2}\boldsymbol{e}_1^T\boldsymbol{e}_1)^2=c^2\)</span>，可见椭圆沿<span class="math inline">\(\boldsymbol{e}_1\)</span>方向的半轴长为<span class="math inline">\(c\lambda_1^{-1/2}\)</span>；同理，<span class="math inline">\(\boldsymbol{x} = c\lambda_2^{-1/2}\boldsymbol{e}_2\)</span>也给出了椭圆沿<span class="math inline">\(\boldsymbol{e}_2\)</span>方向的半轴长<span class="math inline">\(c\lambda_2^{-1/2}\)</span>。到原点的马氏距离为常数<span class="math inline">\(c\)</span>的点的位置如下图所示（<span class="math inline">\(D = 2, \lambda_1 &lt; \lambda_2\)</span>）：</p>
<p align="center">
<img src="https://images.cnblogs.com/cnblogs_com/blogs/538207/galleries/2015781/o_250121121221_%E5%88%B0%E5%8E%9F%E7%82%B9%E7%9A%84%E9%A9%AC%E6%B0%8F%E8%B7%9D%E7%A6%BB%E4%B8%BA%E5%B8%B8%E6%95%B0c%E7%9A%84%E7%82%B9%E7%9A%84%E4%BD%8D%E7%BD%AE.png" width="420" height="280" alt="" align="center">        
</p>
<p>如果<span class="math inline">\(D &gt; 2\)</span>，到原点的马氏距离为常数<span class="math inline">\(c = \sqrt{\boldsymbol{x}^T\mathbf{A}\boldsymbol{x}}\)</span>的点<span class="math inline">\(\boldsymbol{x} = (x_1, \cdots, x_D)\)</span>在椭球面<span class="math inline">\(\lambda_1 (\boldsymbol{e_1}^T\boldsymbol{x})^2 + \cdots + \lambda_D (\boldsymbol{e}_D^T\boldsymbol{x}^T)^2 = c^2\)</span>上，其轴由<span class="math inline">\(\mathbf{A}\)</span>的规范正交特征向量给出。沿<span class="math inline">\(\boldsymbol{e}_i\)</span>方向的半轴长为<span class="math inline">\(\frac{c}{\sqrt{\lambda_i}}, i = 1, 2, \cdots, p\)</span>，其中<span class="math inline">\(\lambda_1, \cdots, \lambda_D\)</span>是<span class="math inline">\(\mathbf{A}\)</span>的特征值。</p>
<p>下图直观地说明了马氏距离相比欧氏距离具有优越性的情况：</p>
<p align="center">
<img src="https://images.cnblogs.com/cnblogs_com/blogs/538207/galleries/2015781/o_250121123855_%E7%9B%B8%E5%AF%B9%E4%BA%8E%E7%82%B9P%E5%92%8C%E5%8E%9F%E7%82%B9%E7%9A%84%E4%B8%80%E7%BB%84%E7%82%B9.png" width="420" height="280" alt="" align="center">        
</p>
<p>该图描述了重心（样本均值）在点<span class="math inline">\(Q\)</span>的一组点。<span class="math inline">\(P\)</span>到<span class="math inline">\(Q\)</span>的欧氏距离大于<span class="math inline">\(O\)</span>到<span class="math inline">\(Q\)</span>的欧氏距离。然而，<span class="math inline">\(P\)</span>点却比原点<span class="math inline">\(O\)</span>更像是属于这一组点内的点。如果我们用马氏距离来度量距离，则<span class="math inline">\(P\)</span>距离<span class="math inline">\(Q\)</span>就会比<span class="math inline">\(O\)</span>距离<span class="math inline">\(Q\)</span>要近了。</p>
<p>至于马氏距离中的矩阵<span class="math inline">\(\mathbf{A}\)</span>如何确定则和样本各维度的标准差有关。马氏距离相当于将样本各维度除以样本标准差之后（也即“标准化”之后），再采用欧氏距离的公式进行计算，感兴趣的读者可以阅读《Applied Multivariate Statistical Analysis》1.5节。</p>
</blockquote>
<p>如果令多元高斯分布中的马氏距离的平方<span class="math inline">\(\Delta^2 = (\boldsymbol{x} - \boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x} - \boldsymbol{\mu})\)</span>为常数<span class="math inline">\(c^2\)</span>，那么多元高斯分布的概率密度也为常数，这也就意味着到均值<span class="math inline">\(\boldsymbol{\mu}\)</span>的马氏距离相等的点拥有相同的赋概。我们前面提到过，这些点在一个椭球面上，我们将其称为<strong>轮廓线（contours）</strong>：</p>
<p></p><div class="math display">\[\begin{aligned}
    \quad\text{常数概率密度轮廓线} &amp;= \{\boldsymbol{x}\mid (\boldsymbol{x} - \boldsymbol{\mu})^T \mathbf{\Sigma}^{-1} (\boldsymbol{x} - \boldsymbol{\mu}) = c^2\}\\
    &amp;= \text{中心在}\boldsymbol{\mu}的椭球面
\end{aligned}
\]</div><p></p><p>为了确定椭球面的轴方向和半轴长，我们对协方差矩阵的逆矩阵<span class="math inline">\(\mathbf{\Sigma}^{-1}\)</span>进行谱分解：</p>
<p></p><div class="math display">\[\mathbf{\Sigma}^{-1} = \sum_{i=1}^D \frac{1}{\lambda_i}\boldsymbol{e}_i\boldsymbol{e}_i^T\tag{3}
\]</div><p></p><p>其中<span class="math inline">\(\lambda_1, \cdots, \lambda_D\)</span>为<span class="math inline">\(\mathbf{\Sigma}\)</span>的特征值，<span class="math inline">\(\boldsymbol{e}_1, \cdots, \boldsymbol{e}_D\)</span>为与之相伴的规范正交特征向量。</p>
<blockquote>
<p><strong>注</strong> 关于<span class="math inline">\(\mathbf{\Sigma}^{-1}\)</span>，我们有下列结论：<br>
<span class="math inline">\(\quad\)</span> <strong>结论</strong> 若<span class="math inline">\(\mathbf{\Sigma}\)</span>是正定的，其逆为<span class="math inline">\(\mathbf{\Sigma}^{-1}\)</span>，则$$<br>
\mathbf{\Sigma}\boldsymbol{e} = \lambda \boldsymbol{e}\Rightarrow\mathbf{\Sigma}^{-1}\boldsymbol{e} = \left(\frac{1}{\lambda}\right)\boldsymbol{e}$$</p>
<p>所以<span class="math inline">\(\mathbf{\Sigma}\)</span>的特征值-特征向量对<span class="math inline">\((\lambda, \boldsymbol{e})\)</span>对应于<span class="math inline">\(\mathbf{\Sigma}^{-1}\)</span>的特征值-特征向量对<span class="math inline">\((\frac{1}{\lambda}, \boldsymbol{e})\)</span>。且<span class="math inline">\(\mathbf{\Sigma}^{-1}\)</span>也是正定的。</p>
<p><span class="math inline">\(\quad\)</span> <strong>证明</strong> 对于正定的<span class="math inline">\(\mathbf{\Sigma}\)</span>以及一个特征向量<span class="math inline">\(\boldsymbol{e}\neq \boldsymbol{0}\)</span>，我们有<span class="math inline">\(0 &lt; \boldsymbol{e}^T\mathbf{\Sigma}\boldsymbol{e} = \boldsymbol{e}^T(\mathbf{\Sigma}\boldsymbol{e}) = \boldsymbol{e}^T(\lambda\boldsymbol{e}) = \lambda\boldsymbol{e}^T\boldsymbol{e} = \lambda\)</span>。而且<span class="math inline">\(\boldsymbol{e} = \mathbf{\Sigma}^{-1}(\mathbf{\Sigma}\boldsymbol{e}) = \mathbf{\Sigma}^{-1}(\lambda\boldsymbol{e}) = \lambda\mathbf{\Sigma}^{-1}\boldsymbol{e}\)</span>，用<span class="math inline">\(\lambda &gt; 0\)</span>除，得到<span class="math inline">\(\mathbf{\Sigma}^{-1}\boldsymbol{e} = \left(\frac{1}{\lambda}\right)\boldsymbol{e}\)</span>。于是，<span class="math inline">\((\frac{1}{\lambda}, \boldsymbol{e})\)</span>是<span class="math inline">\(\mathbf{\Sigma}^{-1}\)</span>的一对特征值-特征向量。因此，我们对任意<span class="math inline">\(D\)</span>维向量<span class="math inline">\(\boldsymbol{x}\neq \boldsymbol{0}\)</span>有</p>
<p></p><div class="math display">\[\begin{aligned}
    \boldsymbol{x}^T\mathbf{\Sigma}^{-1}\boldsymbol{x} &amp;= \boldsymbol{x}^T\left(\sum_{i=1}^D \frac{1}{\lambda_i}\boldsymbol{e}_i\boldsymbol{e}_i^T\right)\boldsymbol{x}\\
    &amp;= \sum_{i=1}^D (\frac{1}{\lambda_i}) (\boldsymbol{e}_i^T\boldsymbol{x})^2 &gt; 0
\end{aligned}
\]</div><p></p><p>由此得出<span class="math inline">\(\mathbf{\Sigma}^{-1}\)</span>是正定的。</p>
</blockquote>
<p>将式<span class="math inline">\((3)\)</span>的<span class="math inline">\(\mathbf{\Sigma}^{-1}\)</span>的谱分解结果代入式<span class="math inline">\((2)\)</span>所示的二次型中，有</p>
<p></p><div class="math display">\[\begin{aligned}
    c^2 &amp;= (\boldsymbol{x} - \boldsymbol{\mu})^T \mathbf{\Sigma}^{-1} (\boldsymbol{x} - \boldsymbol{\mu})\\
    &amp;= (\boldsymbol{x} - \boldsymbol{\mu})^T \left(\sum_{i=1}^D \frac{1}{\lambda_i}\boldsymbol{e}_i\boldsymbol{e}_i^T\right) (\boldsymbol{x} - \boldsymbol{\mu})\\
    &amp;= \sum_{i=1}^D \frac{\left(\boldsymbol{e}_i^T(\boldsymbol{x} - \boldsymbol{\mu})\right)^2}{\lambda_i}\\
\end{aligned}
\]</div><p></p><p>令<span class="math inline">\(y_i = \boldsymbol{e}_i^T(\boldsymbol{x} - \boldsymbol{\mu})\)</span>，则<span class="math inline">\(\boldsymbol{y}\)</span>可视为点<span class="math inline">\(\boldsymbol{x} - \boldsymbol{\mu}\)</span>在新的基底<span class="math inline">\(\{\boldsymbol{e}_i\}\)</span>下的坐标，基变换关系为<span class="math inline">\(\left(\begin{matrix}
    y_1\\
    \vdots\\
    y_D
\end{matrix}\right) = \left(\begin{array}{c:c:c}\boldsymbol{e}_1&amp; \cdots &amp;\boldsymbol{e}_D\end{array}\right)^T(\boldsymbol{x} - \boldsymbol{\mu}) = \left(\begin{matrix}
    \boldsymbol{e}_1^T\\
    \hdashline
    \vdots\\
    \hdashline
    \boldsymbol{e}_D^T
\end{matrix}\right)(\boldsymbol{x} - \boldsymbol{\mu})\)</span>。此时，<span class="math inline">\(\boldsymbol{y}=(y_1, \cdots, y_D)^T\)</span>在以<span class="math inline">\(\boldsymbol{\mu}\)</span>为中心的椭球面上，其方程为：</p>
<p></p><div class="math display">\[    \begin{aligned}
        \sum_{i=1}^D \frac{y_i^2}{\lambda_i} = c^2
    \end{aligned}
\]</div><p></p><p>该椭球面的轴即为做为新基底的规范正交特征向量<span class="math inline">\(\boldsymbol{e}_1, \cdots, \boldsymbol{e}_D\)</span>。我们可以发现将<span class="math inline">\(\boldsymbol{x}_i = \boldsymbol{\mu} + c\lambda_i^{1/2}\boldsymbol{e}_i\)</span>代入椭圆方程得到<span class="math inline">\((\boldsymbol{x} - \boldsymbol{\mu})^T \mathbf{\Sigma}^{-1} (\boldsymbol{x} - \boldsymbol{\mu}) = \frac{\left(c\lambda_i^{1/2}\boldsymbol{e}_i^T\boldsymbol{e}_i\right)^2}{\lambda_i} = c^2\)</span>，可见椭圆沿<span class="math inline">\(\boldsymbol{e}_i\)</span>方向的半轴长为<span class="math inline">\(c\lambda_i^{1/2}\)</span>（<span class="math inline">\(i = 1, \cdots, D\)</span>）。在<span class="math inline">\(D=2\)</span>的情况下，该椭球面为中心为<span class="math inline">\(\boldsymbol{\mu}\)</span>的椭圆，如下图所示（<span class="math inline">\(\lambda_2 &lt; \lambda_1\)</span>）：</p>
<p align="center">
<img src="https://images.cnblogs.com/cnblogs_com/blogs/538207/galleries/2015781/o_250122103330_%E5%9C%A8D%3D2%E7%9A%84%E6%83%85%E5%86%B5%E4%B8%8B%EF%BC%8C%E6%A4%AD%E7%90%83%E9%9D%A2%E4%B8%BA%E4%B8%AD%E5%BF%83%E4%B8%BAmu%E7%9A%84%E6%A4%AD%E5%9C%86.png" width="460" height="280" alt="" align="center">        
</p>
<p>现在我们考虑多元高斯分布是否是归一化的。我们之前使用了变量替换<span class="math inline">\(\boldsymbol{y} = T(\boldsymbol{x}) = \left(\begin{matrix}
    \boldsymbol{e}_1^T\\
    \hdashline
    \vdots\\
    \hdashline
    \boldsymbol{e}_D^T
\end{matrix}\right)(\boldsymbol{x} - \boldsymbol{\mu})\)</span>，于是在新的基底下的积分公式可以经由变量替换<sup>[8][9]</sup>表示为</p>
<p></p><div class="math display">\[\int \mathcal{N}(\boldsymbol{y}\mid \boldsymbol{0}, \boldsymbol{\Sigma})\mathrm{d}\boldsymbol{y} = \int \mathcal{N}\left(T(\boldsymbol{x})\mid \boldsymbol{0}, \boldsymbol{\Sigma}\right)|\mathrm{det}\mathbf{J}|\mathrm{d}\boldsymbol{x}
\]</div><p></p><p>这里<span class="math inline">\(\mathbf{J}\)</span>为仿射变换<span class="math inline">\(T\)</span>的Jacobian矩阵：</p>
<p></p><div class="math display">\[\mathbf{J} = \left(\begin{matrix}
    &amp;\frac{\partial T_1(\boldsymbol{x})}{\partial x_1} &amp;\cdots &amp;\frac{\partial T_1(\boldsymbol{x})}{\partial x_D}\\
    &amp;\vdots &amp; &amp;\vdots\\
    &amp;\frac{\partial T_D(\boldsymbol{x})}{\partial x_1} &amp;\cdots &amp;\frac{\partial T_D(\boldsymbol{x})}{\partial x_D}
\end{matrix}\right) = \left(\begin{matrix}
    &amp;e_{11} &amp;\cdots &amp;e_{1D}\\
    &amp;\vdots &amp; &amp;\vdots\\
    &amp;e_{D1} &amp;\cdots &amp;e_{DD}
\end{matrix}\right) = \left(\begin{matrix}
    \boldsymbol{e}_1^T\\
    \hdashline
    \vdots\\
    \hdashline
    \boldsymbol{e}_D^T
\end{matrix}\right)
\]</div><p></p><p>设<span class="math inline">\(\mathbf{U} = \left(\begin{array}{c:c:c}\boldsymbol{e}_1&amp; \cdots &amp;\boldsymbol{e}_D\end{array}\right)\)</span>（此时<span class="math inline">\(\mathbf{J} = \left(\begin{matrix}
    \boldsymbol{e}_1^T\\
    \hdashline
    \vdots\\
    \hdashline
    \boldsymbol{e}_D^T
\end{matrix}\right) = \mathbf{U}^T\)</span>），由于新的基底<span class="math inline">\(\boldsymbol{e}_1, \cdots, \boldsymbol{e}_D\)</span>是规范正交的，因此<span class="math inline">\(\mathbf{U}\)</span>是正交矩阵，此时我们有：</p>
<p></p><div class="math display">\[|\mathrm{det}\mathbf{J}| = |\mathrm{det}\mathbf{U}^T| = \sqrt{\left(\mathrm{det}\mathbf{U}^T\right)^2} = \sqrt{\mathrm{det}\mathbf{U}^T\mathrm{det}\mathbf{U}} = \sqrt{\mathrm{det}\left(\mathbf{U}^T\mathbf{U}\right)} = \sqrt{\mathrm{det}\mathbf{I}} = 1
\]</div><p></p><p>而我们发现又多元高斯分布<span class="math inline">\(\mathcal{N}\left(\boldsymbol{y}\mid \boldsymbol{0}, \boldsymbol{\Sigma}\right)\)</span>可以分解成<span class="math inline">\(D\)</span>个独立一元高斯分布<span class="math inline">\(\mathcal{N}(y_i\mid 0, \lambda_i)\)</span>的乘积：</p>
<p></p><div class="math display">\[\begin{aligned}
    \mathcal{N}\left(\boldsymbol{y}\mid \boldsymbol{\mu}, \boldsymbol{\Sigma}\right) &amp;= \frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{(\mathrm{det}\boldsymbol{\Sigma})^{\frac{1}{2}}}\exp\left\{-\frac{1}{2}\sum_{i=1}^D \frac{y_i^2}{\lambda_i}\right\}\\
    &amp;= \prod_{i=1}^D\frac{1}{(2\pi\lambda_i)^{\frac{1}{2}}}\exp\left\{-\frac{y_i^2}{2\lambda_i}\right\}\\
    &amp;= \prod_{i=1}^D\mathcal{N}(y_i\mid 0, \lambda_i)
\end{aligned}
\]</div><p></p><p>（其中我们用到了结论<span class="math inline">\(\mathrm{det}\mathbf{\Sigma} = \prod_i\lambda_i\)</span>）因此</p>
<p></p><div class="math display">\[\int \mathcal{N}(\boldsymbol{y}\mid \boldsymbol{0}, \boldsymbol{\Sigma})\mathrm{d}\boldsymbol{y} = \prod_{i=1}^D \int \mathcal{N}(y_i\mid 0, \lambda_i)\mathrm{d}y_i = 1
\]</div><p></p><p>于是我们有</p>
<p></p><div class="math display">\[\int \mathcal{N}\left(\boldsymbol{x}\mid \boldsymbol{\mu}, \boldsymbol{\Sigma}\right)\mathrm{d}\boldsymbol{x} = \int \mathcal{N}\left(T(\boldsymbol{x})\mid \boldsymbol{0}, \boldsymbol{\Sigma}\right)\mathrm{d}\boldsymbol{x} = \int \mathcal{N}(\boldsymbol{y}\mid \boldsymbol{0}, \boldsymbol{\Sigma})\mathrm{d}\boldsymbol{y} = 1
\]</div><p></p><p>至此我们证明了多元高斯分布是归一化的。</p>
<h2 id="22-高斯分布的一阶矩和二阶矩">2.2 高斯分布的一阶矩和二阶矩</h2>
<p>现在我们考察多元高斯分布的一阶矩和二阶矩，这可以提供参数<span class="math inline">\(\boldsymbol{\mu}\)</span>和<span class="math inline">\(\mathbf{\Sigma}\)</span>的描述。多元高斯分布下<span class="math inline">\(\boldsymbol{x}\)</span>的期望为：</p>
<p></p><div class="math display">\[\begin{aligned}
    \mathbb{E}[\boldsymbol{x}] &amp;= \frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{(\mathrm{det}\boldsymbol{\Sigma})^{\frac{1}{2}}} \int \exp\left\{-\frac{1}{2}(\boldsymbol{x} - \boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x} - \boldsymbol{\mu})\right\} \boldsymbol{x} \mathrm{d}\boldsymbol{x}\\
    &amp;= \frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{(\mathrm{det}\boldsymbol{\Sigma})^{\frac{1}{2}}} \int \exp\left\{-\frac{1}{2}\boldsymbol{z}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{z}\right\} (\boldsymbol{z} + \boldsymbol{\mu}) \mathrm{d}\boldsymbol{z}\\
    &amp;= \frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{(\mathrm{det}\boldsymbol{\Sigma})^{\frac{1}{2}}} \int \exp\left\{-\frac{1}{2}\boldsymbol{z}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{z}\right\} \boldsymbol{z} \mathrm{d}\boldsymbol{z} + \boldsymbol{\mu}
\end{aligned}
\]</div><p></p><p>其中在第二个等式中我们使用了<span class="math inline">\(\boldsymbol{z} = \boldsymbol{x} - \boldsymbol{\mu}\)</span>进行了变量替换，在第三个等式中我们利用了<span class="math inline">\(\int \mathcal{N}(\boldsymbol{z}\mid \boldsymbol{0}, \mathbf{\Sigma})\mathrm{d}\boldsymbol{z} = 1\)</span>。现在我们来考虑第一个积分项，我们注意到指数位置是<span class="math inline">\(\boldsymbol{z}\)</span>的偶函数，而<span class="math inline">\(\boldsymbol{z}\)</span>是奇函数，因此被积函数是奇函数<sup>[10]</sup>，又由于积分区域关于原点对称，因此第一个积分项为0。于是我们有</p>
<p></p><div class="math display">\[\mathbb{E}[\boldsymbol{x}] = \boldsymbol{\mu}
\]</div><p></p><p>因此我们把<span class="math inline">\(\boldsymbol{\mu}\)</span>称为多元高斯分布的均值。</p>
<p>现在我们考虑多元高斯分布的二阶矩。在一元变量的情形下，二阶矩由<span class="math inline">\(\mathbb{E}[x^2]\)</span>给出。对于多元高斯分布，有<span class="math inline">\(D^2\)</span>个由<span class="math inline">\(\mathbb{E}[x_ix_j]\)</span>给出的二阶矩，可以聚集在一起组成矩阵<span class="math inline">\(\mathbb{E}[\boldsymbol{x}\boldsymbol{x}^T]\)</span>。这个矩阵可以表示为：</p>
<p></p><div class="math display">\[    \begin{aligned}
    \mathbb{E}[\boldsymbol{x}\boldsymbol{x}^T] &amp;= \frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{(\mathrm{det}\boldsymbol{\Sigma})^{\frac{1}{2}}} \int \exp\left\{-\frac{1}{2}(\boldsymbol{x} - \boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x} - \boldsymbol{\mu})\right\} \boldsymbol{x}\boldsymbol{x}^T \mathrm{d}\boldsymbol{x}\\
    &amp;= \frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{(\mathrm{det}\boldsymbol{\Sigma})^{\frac{1}{2}}} \int \exp\left\{-\frac{1}{2}\boldsymbol{z}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{z}\right\} (\boldsymbol{z} + \boldsymbol{\mu})(\boldsymbol{z} + \boldsymbol{\mu})^T \mathrm{d}\boldsymbol{z}\\
\end{aligned}
\]</div><p></p><p>其中在第二个等式中我们再次使用了<span class="math inline">\(\boldsymbol{z} = \boldsymbol{x} - \boldsymbol{\mu}\)</span>来进行变量替换，涉及到<span class="math inline">\(\boldsymbol{z}\boldsymbol{\mu}^T\)</span>和<span class="math inline">\(\boldsymbol{\mu}\boldsymbol{z}^T\)</span>的交叉项将再次变为0，而<span class="math inline">\(\boldsymbol{\mu}\boldsymbol{\mu}^T\)</span>也可以拿出。于是我们有</p>
<p></p><div class="math display">\[\mathbb{E}[\boldsymbol{x}\boldsymbol{x}^T] = \frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{(\mathrm{det}\boldsymbol{\Sigma})^{\frac{1}{2}}} \int \exp\left\{-\frac{1}{2}\boldsymbol{z}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{z}\right\} \boldsymbol{z}\boldsymbol{z}^T \mathrm{d}\boldsymbol{z} + \boldsymbol{\mu}\boldsymbol{\mu}^T
\]</div><p></p><p>接下来考虑第一个积分项，我们采用和之前类似的做法，对<span class="math inline">\(\mathbf{\Sigma}^{-1}\)</span>进行谱分解得<span class="math inline">\(\mathbf{\Sigma}^{-1} = \sum_{i} \frac{1}{\lambda_i}\boldsymbol{e}_i\boldsymbol{e}_i^T\)</span>，并使用变量替换令<span class="math inline">\(y_i = \boldsymbol{e}_i^T\boldsymbol{z}\)</span>（基变换关系为<span class="math inline">\(\left(\begin{matrix}
    y_1\\
    \vdots\\
    y_D
\end{matrix}\right) = \left(\begin{matrix}
    \boldsymbol{e}_1^T\\
    \hdashline
    \vdots\\
    \hdashline
    \boldsymbol{e}_D^T
\end{matrix}\right)\boldsymbol{z}\)</span>，因此<span class="math inline">\(\boldsymbol{z} = \left(\begin{array}{c:c:c}\boldsymbol{e}_1&amp; \cdots &amp;\boldsymbol{e}_D\end{array}\right)\boldsymbol{y} = \sum_iy_i\boldsymbol{e}_i\)</span>），于是有</p>
<p></p><div class="math display">\[\begin{aligned}
    &amp;\frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{(\mathrm{det}\boldsymbol{\Sigma})^{\frac{1}{2}}} \int \exp\left\{-\frac{1}{2}\boldsymbol{z}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{z}\right\} \boldsymbol{z}\boldsymbol{z}^T \mathrm{d}\boldsymbol{z}\\
    &amp;= \frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{(\mathrm{det}\boldsymbol{\Sigma})^{\frac{1}{2}}} \sum_{i=1}^D\sum_{j=1}^D\boldsymbol{e}_i\boldsymbol{e}_j^T\int \exp\left\{-\frac{1}{2}\sum_{k=1}^D \frac{y_k^2}{\lambda_k}\right\} y_iy_j \mathrm{d}\boldsymbol{y}\\
    &amp;= \frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{(\mathrm{det}\boldsymbol{\Sigma})^{\frac{1}{2}}} \sum_{i=1}^D\boldsymbol{e}_i\boldsymbol{e}_i^T\int \exp\left\{-\frac{1}{2}\sum_{k=1}^D \frac{y_k^2}{\lambda_k}\right\} y_i^2 \mathrm{d}\boldsymbol{y}\\
    &amp;= \sum_{i=1}^D\boldsymbol{e}_i\boldsymbol{e}_i^T\lambda_i\\
    &amp;= \mathbf{\Sigma}
\end{aligned}
\]</div><p></p><p>其中第二个等式是由于当<span class="math inline">\(i\neq j\)</span>时，被积函数是奇函数，导致积分项为0；第三个等式是由于<span class="math inline">\(\mathbb{E}[y_i^2] = \int \mathcal{N}(y_i\mid 0, \lambda_i)y_i^2\mathrm{d}y_i=\lambda_i + 0^2 = \lambda_i\)</span>；最后一个等式是根据<span class="math inline">\(\mathbf{\Sigma}\)</span>的谱分解得到。</p>
<p>这样，我们就得到了</p>
<p></p><div class="math display">\[\mathbb{E}[\boldsymbol{x}\boldsymbol{x}^T] = \mathbf{\Sigma} + \boldsymbol{\mu}\boldsymbol{\mu}^T
\]</div><p></p><p>对于一元随机变量的方差，为了定义方差，我们在取二阶矩之前会减掉均值。类似地，对于多元变量的情形，把均值减掉同样很方便。这给出了随机变量<span class="math inline">\(\boldsymbol{x}\)</span>的<strong>协方差（covariance）</strong> 的定义：</p>
<p></p><div class="math display">\[\mathrm{Cov}[\boldsymbol{x}] = \mathbb{E}\left[(\boldsymbol{x} - \mathbb{E}[\boldsymbol{x}])(\boldsymbol{x} - \mathbb{E}[\boldsymbol{x}])^T\right]
\]</div><p></p><p>对于多元高斯分布这一特例，我们有</p>
<p></p><div class="math display">\[\begin{aligned}
    \mathrm{Cov}[\boldsymbol{x}] &amp;= \mathbb{E}\left[\boldsymbol{x}\boldsymbol{x}^T - \boldsymbol{x}\boldsymbol{\mu}^T - \boldsymbol{\mu}\boldsymbol{x}^T + \boldsymbol{\mu}\boldsymbol{\mu}^T\right]\\
    &amp;= \mathbb{E}[\boldsymbol{x}\boldsymbol{x}^T] - \boldsymbol{\mu}\boldsymbol{\mu}^T\\
    &amp;= \mathbf{\Sigma}
\end{aligned}
\]</div><p></p><p>由于参数<span class="math inline">\(\mathbf{\Sigma}\)</span>控制了多元高斯分布下<span class="math inline">\(\boldsymbol{x}\)</span>的协方差，因此它被称为协方差矩阵。</p>
<p>虽然式<span class="math inline">\((1)\)</span>定义的高斯分布<span class="math inline">\(\mathcal{N}(\boldsymbol{x}\mid \boldsymbol{\mu}, \boldsymbol{\Sigma})\)</span>被广泛用作概率密度模型，但是它有着一些巨大的局限性。考虑分布中自由参数的数量。一个通常的对称协方差矩阵<span class="math inline">\(\boldsymbol{\Sigma}\)</span>有<span class="math inline">\(1 + 2 + \cdots + D = \frac{D(D + 1)}{2}\)</span>个独立参数，<span class="math inline">\(\boldsymbol{\mu}\)</span>中有另外<span class="math inline">\(D\)</span>个独立参数，因此总计有<span class="math inline">\(\frac{D(D + 1)}{2} + D = \frac{D(D + 3)}{2}\)</span>个独立参数。对于大的<span class="math inline">\(D\)</span>值，参数的总数随着<span class="math inline">\(D\)</span>以平方的方式增长，导致对大的矩阵进行操作（如求逆）的计算变得不可行。解决这个问题的一种方式是使用协方差矩阵的限制形式。如果我们考虑对角的协方差矩阵，即<span class="math inline">\(\mathbf{\Sigma} = \mathrm{diag}(\sigma_i^2)\)</span>，那么在概率密度模型中，我们就有总数<span class="math inline">\(2D\)</span>个独立参数。此时常数概率密度轮廓线为与轴对齐的椭球。我们可以进一步地把协方差矩阵限制成正比于单位矩阵，也即<span class="math inline">\(\mathbf{\Sigma} = \sigma^2 \mathbf{I}\)</span>，此时它被称为各向同性（isotropic）的协方差。这使得模型有<span class="math inline">\(D + 1\)</span>个独立的参数，并且常数概率密度轮廓线为球面。下图展示了通常的协方差矩阵、对角的协方差矩阵以及各向同性协方差矩阵的概率密度轮廓线（<span class="math inline">\(D=2\)</span>）：</p>
<p align="center">
<img src="https://images.cnblogs.com/cnblogs_com/blogs/538207/galleries/2015781/o_250123143320_%E9%80%9A%E5%B8%B8%E7%9A%84%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5%E3%80%81%E5%AF%B9%E8%A7%92%E7%9A%84%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5%E4%BB%A5%E5%8F%8A%E5%90%84%E5%90%91%E5%90%8C%E6%80%A7%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5%E7%9A%84%E6%A6%82%E7%8E%87%E5%AF%86%E5%BA%A6%E8%BD%AE%E5%BB%93%E7%BA%BF%EF%BC%88D%3D2%EF%BC%89.png" width="600" height="160" alt="" align="center">        
</p>
<p>高斯分布的另一个局限性是它本质上是单峰的（即只有一个最大值），因此不能很好地近似多峰分布。不过，相当多的多峰分布可以使用混合高斯分布来描述（参见博客<a href="https://www.cnblogs.com/orion-orion/p/15984132.html" target="_blank">《统计学习：EM算法及其在高斯混合模型(GMM)中的应用》</a>）。</p>
<h1 id="参考">参考</h1>
<ul>
<li>[1] Bishop C M, Nasrabadi N M. Pattern recognition and machine learning[M]. New York: springer, 2006.</li>
<li>[2] Schroeder D V. An introduction to thermal physics[M]. Oxford University Press, 2020.</li>
<li>[3] <a href="https://zh.wikipedia.org/wiki/%E7%86%B5_(%E7%BB%9F%E8%AE%A1%E7%89%A9%E7%90%86%E5%AD%A6)" target="_blank" rel="noopener nofollow">《维基百科：熵 (统计物理学)》</a></li>
<li>[4] Feller W. An introduction to probability theory and its applications, Volume 1[M]. John Wiley &amp; Sons, 1991.</li>
<li>[5] Weisstein E W. CRC concise encyclopedia of mathematics[M]. Chapman and Hall/CRC, 2002.</li>
<li>[6] Bengio Y, Goodfellow I, Courville A. Deep learning[M]. Cambridge, MA, USA: MIT press, 2017.</li>
<li>[7] Johnson R A, Wichern D W. Applied multivariate statistical analysis[J]. 2002.</li>
<li>[8] Rudin W. Principles of mathematical analysis[M]. New York: McGraw-hill, 1964.</li>
<li>[9] Axler S. Linear algebra done right[M]. springer publication, 2015.</li>
<li>[10] <a href="https://zh.wikipedia.org/wiki/%E5%A5%87%E5%87%BD%E6%95%B8%E8%88%87%E5%81%B6%E5%87%BD%E6%95%B8" target="_blank" rel="noopener nofollow">维基百科：《奇函数与偶函数》</a>)</li>
</ul>

</div>
<div id="MySignature" role="contentinfo">
    数学是符号的艺术，音乐是上界的语言。
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="1.643412481162037" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-01-24 00:50">2025-01-23 23:12</span>&nbsp;
<a href="https://www.cnblogs.com/orion-orion">orion-orion</a>&nbsp;
阅读(<span id="post_view_count">223</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18688763" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18688763);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18688763', targetLink: 'https://www.cnblogs.com/orion-orion/p/18688763', title: '贝叶斯机器学习：最大熵及高斯分布' })">举报</a>
</div>
        