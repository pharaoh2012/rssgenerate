
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/jellyai/p/18721369" title="发布于 2025-02-18 10:17">
    <span role="heading" aria-level="2">如何训练LLM“思考”（像o1和DeepSeek-R1一样, 高级推理模型解析</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p>2024年9月，OpenAI发布了它的o1模型，该模型基于大规模强化学习训练，赋予了它“高级推理”能力。不幸的是，他们是如何做到这一点的细节从未被公开披露。然而，今天，DeepSeek（一个AI研究实验室）成功复现了这种推理行为，并公开了他们方法的完整技术细节。在这篇文章中，我将讨论这一创新背后的关键思想，并描述它们在底层是如何运作的。</p>
<p><img src="https://img2024.cnblogs.com/blog/3524016/202502/3524016-20250218101505027-1221620094.png" alt="" loading="lazy"></p>
<pre><code>                                                一台会思考的笔记本电脑
</code></pre>
<p>OpenAI的o1模型标志着训练大语言模型（LLM）的一个新范式。它引入了所谓的“思考”标记（thinking tokens），这些标记相当于一个临时记事本，使模型能够思考问题和用户查询。</p>
<p>o1的主要发现是，随着测试时计算量（test-time compute）的增加，性能会得到提升。这其实只是一个花哨的说法，意思是：模型生成的标记越多，回答就越好。下面这张图是从OpenAI的博客中复现的，它很好地展示了这一点。</p>
<p><img src="https://img2024.cnblogs.com/blog/3524016/202502/3524016-20250218101538009-448486878.png" alt="" loading="lazy"></p>
<pre><code>                                              AIME准确率随训练计算量和测试计算量的变化（重新绘制自[1]）
</code></pre>
<p>在上面的图表中，纵轴是模型在AIME（数学问题）上的表现，横轴是不同的计算时间。左侧的图展示的是众所周知的神经网络扩展定律，这一发现在2023年引爆了LLM热潮。换句话说，模型训练得越久（即训练计算量，train-time compute 越大），其性能就越好。</p>
<p>然而，在右侧，我们看到了另一种新的扩展定律。这里，模型生成的标记越多（即测试计算量，test-time compute 越大），其性能也越好。</p>
<p>“思考”标记</p>
<p>o1的一个关键特性就是它的所谓“思考”标记。这些是在后训练（post-training）阶段引入的特殊标记，它们限定了模型的链式思维（CoT）推理（即思考问题的过程）。这些特殊标记之所以重要，有两个原因。</p>
<p>第一，它们清晰地划分了模型“思考”的起点和终点，这样在UI界面上呈现时可以轻松解析。第二，它生成了一个人类可读的输出，让我们可以理解模型是如何“思考”这个问题的。</p>
<p>尽管OpenAI透露他们使用了强化学习来实现这一能力，但他们并未分享具体的实现细节。然而，今天，多亏了DeepSeek最近的一篇论文，我们大致能猜到是怎么回事了。</p>
<p>DeepSeek的论文</p>
<p>2025年1月，DeepSeek发布了《DeepSeek-R1：通过强化学习激励LLM的推理能力》[2]。虽然这篇论文在学术界引发了不小的轰动，但它的核心贡献是揭示了o1背后的秘密。</p>
<p>它引入了两个模型：DeepSeek-R1-Zero 和 DeepSeek-R1。前者完全基于强化学习（RL）训练，而后者则结合了监督微调（SFT）和RL。</p>
<p>虽然各大新闻标题（包括论文标题）都在谈DeepSeek-R1，但其实前者（R1-Zero）同样重要，原因有两个：第一，它为R1生成了训练数据；第二，它展现出了惊人的涌现推理能力，而这些能力并不是直接教给它的。</p>
<p>换句话说，R1-Zero 仅凭强化学习 就自己发现了链式思维（CoT）和测试时计算扩展规律！我们来看看它是如何做到的。</p>
<p>DeepSeek-R1-Zero（纯RL）</p>
<p>强化学习（RL）是一种机器学习方法，它不像传统的监督学习那样依赖明确的训练示例，而是通过试错的方式让模型自己学习[3]。它的运作方式是给模型传递一个奖励信号，这个信号与模型的参数并没有直接的函数关系。</p>
<p>这和我们在现实世界中的学习方式很相似。例如，如果我去申请一份工作但没有收到回复，我需要自己琢磨出哪里出了问题，以及如何改进。这与监督学习不同，在监督学习中，相当于招聘官直接告诉我我做错了什么，以及如何改正。</p>
<p>虽然用RL来训练R1-Zero涉及很多技术细节，但这里我想重点介绍三个关键点：提示模板（Prompt Template）、奖励信号（Reward Signal）、以及GRPO（Group Relative Policy Optimization）。</p>
<p>1）提示模板（Prompt Template）</p>
<p>用于训练的模板如下，其中 {prompt} 被替换为数据集中（推测）包含的复杂数学、编程和逻辑问题。请注意，这里通过简单的提示加入了 <answer> 和 <think> 标签。</think></answer></p>
<p>A conversation between User and Assistant. The user asks a question, and the</p>
<p>Assistant solves it.The assistant first thinks about the reasoning process in</p>
<p>the mind and then provides the user with the answer. The reasoning process and</p>
<p>answer are enclosed within <think> </think> and <answer> </answer> tags,</p>
<p>respectively, i.e., <think> reasoning process here </think></p>
<p><answer> answer here </answer>. User: {prompt}. Assistant:</p>
<p>有一点特别值得注意，那就是极简且松散的提示策略。DeepSeek 之所以选择这种方式，是为了避免对模型的回答产生偏向，并观察它在强化学习（RL）过程中如何自然演化。</p>
<p>2）奖励信号（Reward Signal）</p>
<p>RL 的奖励由两个部分组成：准确度奖励和格式奖励。由于训练数据集包含带有明确正确答案的问题，因此使用了一种基于规则的策略来评估回答的准确性。同样，格式奖励也是基于规则的，用于确保推理标记被正确地生成在 <think> 标签之间。</think></p>
<p>论文作者特别指出，他们没有使用神经网络奖励模型（即奖励不是由神经网络计算的），因为这种方法可能会导致奖励欺骗（reward hacking）。换句话说，LLM 可能会学会欺骗奖励模型，以最大化奖励，同时实际推理能力却下降。</p>
<p>这就像人类一样，一旦我们找到某种可以钻空子的激励机制，我们往往会利用它来最大化个人利益，而忽略激励机制的原始意图。这也突出了设计良好奖励机制的难度——无论是对人还是对计算机来说都一样。</p>
<p>3）GRPO（Group Relative Policy Optimization）</p>
<p>最后一个关键点是，如何将奖励信号转化为模型参数的更新。这个部分非常技术向，如果你只是想了解整体概念，可以直接跳过。</p>
<p>GRPO 是一种强化学习方法，它结合多个回答样本来更新模型参数。为了保证训练的稳定性，作者在损失函数（Loss Function）中引入了梯度裁剪（Clipping）和 KL 散度正则化（KL-divergence Regularization）。</p>
<p>• 梯度裁剪（Clipping） 确保优化步骤不会太大，防止训练过程中的剧烈波动。</p>
<p>• KL 散度正则化 确保模型预测不会发生过大的变化，以保证训练稳定性。</p>
<p>下面是完整的损失函数，并附带了一些（希望对你有帮助的）注释。</p>
<p><img src="https://img2024.cnblogs.com/blog/3524016/202502/3524016-20250218101630880-1717449075.png" alt="" loading="lazy"></p>
<pre><code>                            GRPO 损失函数（带注释）,图片由作者提供
</code></pre>
<p>结果（涌现能力）</p>
<p>R1-Zero 最令人惊讶的结果是，尽管它只接受了极少的指导，它仍然发展出了高效的推理策略，而这些策略是我们能理解的。</p>
<p>例如，它通过 RL 自发地 学会了在测试时利用计算资源（test-time compute）来提升回答质量（还记得 o1 提出的这个发现吧？）。这一点可以从下图（来自 R1 论文 [2]）中看出。</p>
<p><img src="https://img2024.cnblogs.com/blog/3524016/202502/3524016-20250218101713720-1179025071.png" alt="" loading="lazy"></p>
<pre><code>                        随着额外训练，平均回答长度增加（并且表现更好）,截图来自 DeepSeek R1 论文 [2]
</code></pre>
<p>另一个显著的能力是，R1-Zero 能够回顾自己的回答，并且拥有一种令人惊叹的类似人类的内在独白。下面是 R1-Zero 的一个示例。请注意，它在给出最终答案之前，使用了一个超长的链式思维（CoT）推理过程，并且增加了额外的验证步骤。（由于回答太长，我省略了一些部分。）</p>
<p>User prompt: If it takes me 1.5 hours to write a codebase and my intern 8</p>
<p>hours. How long will it take us both to write 3 codebases?</p>
<p><think> To solve for how long it will take both you and your intern to write</think></p>
<p>three codebases together, we need to first find out how many codebases each of</p>
<p>you can write per hour individually.</p>
<p>(long CoT) ...</p>
<p>So, it will take approximately 3.7895 hours for you and your intern to write</p>
<p>three codebases together.</p>
<p>To make sure everything makes sense, let us round the answer to a reasonable</p>
<p>number of decimal places (e.g., round to two decimal places):</p>
<p>t ≈ 3.79 hours</p>
<p>However, if you need a more "human-friendly" answer, you could convert the</p>
<p>decimal part into minutes. Since 0.7895 of an hour is:</p>
<p>0.7895 × 60 minutes ≈ 47.37 minutes</p>
<p>So, 3.7895 hours is approximately 3 hours and 47.37 minutes (or roughly 3</p>
<p>hours and 47 minutes if rounding to the nearest minute).</p>
<h2 id="verification-optional-step-for-verification">Verification (optional step for verification)</h2>
<p>To verify, let us check how many codebases you can individually write in t =</p>
<p>72/19 hours:</p>
<p>Your work in 72/19 hours</p>
<p>= (2/3 codebases per hour) × (72/19 hours)</p>
<p>= (2 × 72) / (3 × 19)</p>
<p>= 144 / 57</p>
<p>= 48/19 codebases.</p>
<p>(verification process) ...</p>
<p>However, a more "exact" answer should be kept in fractional form unless</p>
<p>specified otherwise. Thus, the answer is: 72/19hours which is approximately</p>
<p>$3.7895$ hours or about $3.79$ hours when rounded to two decimal places).</p>

<p>72/19 hours (or approximately 3.79 hours).</p>
<p>R1-Zero 的问题</p>
<p>尽管 R1-Zero 的“思考”标记（thinking tokens）提供了一个人类可读的窗口，让我们得以窥探模型的“思维过程”，但论文作者也指出了一些问题。具体来说，模型学到的链式思维（CoT）有时会存在可读性问题，并且会混杂不同的语言。这表明，它的推理过程可能逐渐偏离了人类易于理解的范畴。</p>
<p>DeepSeek-R1（SFT + RL）</p>
<p>为了缓解 R1-Zero 在可解释性上的问题，作者们探索了一种多步骤训练策略，结合了监督微调（SFT）和强化学习（RL）。这个策略最终产生了 DeepSeek-R1——一个表现更优、目前受到更多关注的模型。整个训练流程可以分为 4 个步骤。</p>
<p>步骤 1：基于推理数据的 SFT</p>
<p>为了让模型在推理学习上少走弯路，作者首先使用 SFT（监督微调） 进行训练。这个阶段包含 数千个长链式思维（CoT）示例，数据来源包括：</p>
<p>• Few-shot 提示（即提供示例，展示如何思考问题）</p>
<p>• 直接提示模型进行反思（reflection）和验证（verification）</p>
<p>• 从 R1-Zero 生成的合成数据进行优化[2]</p>
<p>这有两个主要优势：</p>
<ol>
<li>
<p>模型可以明确学习到理想的回答格式</p>
</li>
<li>
<p>通过观察精心挑选的推理示例，最终模型的推理能力会更强</p>
</li>
</ol>
<p>步骤 2：R1-Zero 风格的 RL（+ 语言一致性奖励）</p>
<p>接下来，在 SFT 之后，研究人员对模型应用了一轮 RL 训练。这部分和 R1-Zero 的 RL 训练方式完全一致，但增加了一个额外的语言一致性奖励（Language Consistency Reward）。</p>
<p>这个额外奖励的原因是，R1-Zero 有时会混杂不同语言，导致它的回答变得难以阅读。因此，通过调整奖励信号，引导模型生成更加一致的语言输出。</p>
<p>步骤 3：混合数据 SFT</p>
<p>到了这个阶段，模型在推理任务上的表现应该已经不亚于（甚至超过）R1-Zero。但是，这个“中间版本”的模型有一个问题——它太喜欢思考了。</p>
<p>换句话说，它会对任何输入进行推理，比如有人对它说：“你好”，它可能会开始进行一整套逻辑分析……这对事实问答（Q&amp;A）、翻译、创意写作等任务来说是不必要的。</p>
<p>为了解决这个问题，研究人员进行了第二轮 SFT 训练，这次训练的数据由两部分组成：</p>
<p>• 推理数据（60 万条） → 由 步骤 2 训练出的模型 生成，并且包含由 LLM 评判员（LLM judge） 对比预测答案和真实答案的示例。</p>
<p>• 非推理数据（20 万条） → 主要来源有两个：</p>
<p>o 训练 DeepSeek-V3（基础模型）时使用的 SFT 数据集。</p>
<p>o DeepSeek-V3 生成的合成数据。</p>
<p>另外，这里特意加入了一些“非链式思维”示例，确保模型不会在所有回答里都加入“思考”标记（thinking tokens）。</p>
<p>步骤 4：RL + RLHF</p>
<p>最后，研究人员又进行了一轮 RL 训练，其中包括：</p>
<p>• R1-Zero 风格的推理强化学习（继续增强推理能力）</p>
<p>• 基于人类反馈的强化学习（RLHF）（提升模型的有用性和安全性）</p>
<p>经过这整个训练流程，最终得到了 DeepSeek-R1 —— 一个既擅长推理任务，又能像 AI 助手一样正常聊天的模型。</p>
<p>如何获取 R1-Zero 和 R1</p>
<p>DeepSeek 的另一大贡献是，他们公开了这两个模型（以及许多经过蒸馏的版本）的权重。这意味着，无论是通过云端推理服务，还是本地运行，我们都有多种方式可以使用这些模型。</p>
<p>目前这些模型可以在以下平台上找到：</p>
<p>✅ DeepSeek（DeepSeek-V3 和 DeepSeek-R1）</p>
<p>✅ Together（DeepSeek-V3、DeepSeek-R1 以及蒸馏版本）</p>
<p>✅ Hyperbolic（DeepSeek-V3、DeepSeek-R1-Zero 和 DeepSeek-R1）</p>
<p>✅ Ollama（本地）（DeepSeek-V3、DeepSeek-R1 和蒸馏版本）</p>
<p>✅ Hugging Face（本地）（包含所有上述模型）</p>
<p>总结</p>
<p>OpenAI 发布的 o1 引入了一个新的 LLM 优化维度：测试时计算（test-time compute）。虽然 OpenAI 没有公开 他们的具体方法，但 5 个月后，DeepSeek 成功复现 了这种推理能力，并完整公开了 他们的方法细节。</p>
<p>尽管当前的推理模型仍然存在一些局限，但这仍然是一个非常有前途的研究方向。因为它证明了：强化学习（即使没有人类干预） 也能训练出能够自主学习的模型。</p>
<p>这可能突破了当前 LLM 模型的隐性局限——也就是，它们只能回忆和重组 互联网上已有的知识（即人类已经掌握的东西）。</p>
<p>而这种新的 RL 训练方法的潜力在于，未来的模型可能会自主超越人类认知，推动科学和技术突破，甚至提前几十年发现我们原本需要数十年才能理解的知识。</p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.018463223724537035" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-02-18 10:18">2025-02-18 10:17</span>&nbsp;
<a href="https://www.cnblogs.com/jellyai">果冻人工智能</a>&nbsp;
阅读(<span id="post_view_count">44</span>)&nbsp;
评论(<span id="post_comment_count">1</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18721369" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18721369);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18721369', targetLink: 'https://www.cnblogs.com/jellyai/p/18721369', title: '如何训练LLM“思考”（像o1和DeepSeek-R1一样, 高级推理模型解析' })">举报</a>
</div>
        