
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/huggingface/p/18741971" title="发布于 2025-02-27 20:54">
    <span role="heading" aria-level="2">让 LLM 来评判 | 设计你自己的评估 prompt</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h1 id="设计你自己的评估-prompt">设计你自己的评估 prompt</h1>
<blockquote>
<p>这是 <strong>让 LLM 来评判</strong> 系列文章的第三篇，敬请关注系列文章:</p>
<ul>
<li>基础概念</li>
<li>选择 LLM 评估模型</li>
<li>设计你自己的评估 prompt</li>
<li>评估你的评估结果</li>
<li>奖励模型相关内容</li>
<li>技巧与提示</li>
</ul>
</blockquote>
<h2 id="通用-prompt-设计建议">通用 prompt 设计建议</h2>
<p>我总结的互联网上通用 prompt 的通用设计原则如下:</p>
<ul>
<li>任务描述清晰:
<ul>
<li><code>Your task is to do X (你的任务是 X)</code>.</li>
<li><code>You will be provided with Y (你拿到的信息是 Y)</code>.</li>
</ul>
</li>
<li>评估标准精细，评分细则详尽 (如有必要)：
<ul>
<li><code>You should evaluate property Z on a scale of 1 - 5, where 1 means ... (根据属性 Z 的表现进行评分，评分范围为 1 - 5，其中 1 分表示 ...)</code></li>
<li><code>You should evaluate if property Z is present in the sample Y. Property Z is present if ... (请指出样本 Y 中是否具备属性 Z，如果具备，那么 ...)</code></li>
</ul>
</li>
<li>加入一些 “推理” 评估步骤
<ul>
<li><code>To judge this task, you must first make sure to read sample Y carefully to identify ..., then ... (评估此任务之前，请先仔细阅读样本 Y，识别出 ...，然后再 ...)</code></li>
</ul>
</li>
<li>输出格式明确 (添加特定字段可以提升一致性)
<ul>
<li><code>Your answer should be provided in JSON, with the following format {"Score": Your score, "Reasoning": The reasoning which led you to this score} (以 JSON 格式回答，格式为 {"Score": 评分, "Reasoning": 评分推理过程})</code></li>
</ul>
</li>
</ul>
<p>Prompt 书写灵感可以参考 <a href="https://github.com/huggingface/lighteval/blob/main/src/lighteval/tasks/extended/mix_eval/judge_prompts.pyy" target="_blank" rel="noopener nofollow">MixEval</a> 或 <a href="https://github.com/huggingface/lighteval/blob/main/src/lighteval/tasks/extended/mt_bench/judge_prompt_templates.py" target="_blank" rel="noopener nofollow">MTBench</a> 的 prompt 模板。</p>
<p>其他要点:</p>
<ul>
<li>成对比较比对输出评分<a href="https://arxiv.org/abs/2403.16950" target="_blank" rel="noopener nofollow">更能反映人类的偏好</a>，且通常更稳健</li>
<li>如果任务确实需要对输出评分为具体的值，建议使用整数，并详细解释 <a href="https://x.com/seungonekim/status/1749289437165769177" target="_blank" rel="noopener nofollow">每个分值的代表含义</a>，或添加说明 prompt <code>如 provide 1 point for this characteristic of the answer, 1 additional point if ... (回答具备某项特性得 1 分，如果 ... 再加 1 分)</code> 等</li>
<li>尽量每评估一项能力就使用专门评分 prompt，会得到更好而鲁棒的结果</li>
</ul>
<h2 id="提升评估准确性">提升评估准确性</h2>
<p>可以通过以下方式或技术来提升评估准确性 (有可能会增加成本):</p>
<ul>
<li><strong>Few-shot 示例</strong>：提供少量示例可以帮助模型理解和推理，但也会增加上下文长度。</li>
<li><strong>引用参考</strong>：提供参考内容可以提高模型输出的准确性。</li>
<li>*<strong>思维链 (CoT)</strong>：要求模型 <strong>在评分之前</strong> 给出推理过程，可以 <a href="https://arxiv.org/abs/2212.08073" target="_blank" rel="noopener nofollow">提高准确性</a> (参考这篇 <a href="https://x.com/seungonekim/status/1749289437165769177" target="_blank" rel="noopener nofollow">帖子</a>)。</li>
<li><strong>多轮分析</strong>：可以更好地 <a href="https://arxiv.org/abs/2305.13281" target="_blank" rel="noopener nofollow">检测事实性错误</a></li>
<li><strong>陪审团机制</strong>：汇总多个评价模型的结果 <a href="https://arxiv.org/abs/2404.18796" target="_blank" rel="noopener nofollow">比单一模型的结果更好</a>。
<ul>
<li>使用多个小模型替代一个大模型可以大幅降低成本。</li>
<li>也可以使用一个模型的多个温度参数来进行多次实验。</li>
</ul>
</li>
<li>社区意外发现，prompt 引入奖励机制 (<code>例如：回答正确将得到一只小猫</code>) 可以提高回答正确性。这个方法的效果视场景而异，你可以根据需求灵活调整。</li>
</ul>
<p>注：如要减少模型偏见，可以参考社会学中的问卷设计，然后根据使用场景来书写 prompt。如想使用模型来替代人工评估，可以设计类似的评价指标：如计算标注员一致性，使用正确的问卷方法来减少偏见等。</p>
<p>不过在实际应用中，大多数人并不需要完全可复现且高质量无偏的评估，快速且略显粗糙的 prompt 就能满足需求。(只要知悉使用后果，这种情况也是能接受的)。</p>
<hr>
<blockquote>
<p>英文原文: <a href="https://raw.githubusercontent.com/huggingface/evaluation-guidebook/refs/heads/main/translations/zh/contents/model-as-a-judge/designing-your-evaluation-prompt.md" target="_blank" rel="noopener nofollow">https://raw.githubusercontent.com/huggingface/evaluation-guidebook/refs/heads/main/translations/zh/contents/model-as-a-judge/designing-your-evaluation-prompt.md</a></p>
<p>原文作者: clefourrier</p>
<p>译者: SuSung-boy</p>
<p>审校: adeenayakup</p>
</blockquote>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.5822014543541667" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-02-27 20:55">2025-02-27 20:54</span>&nbsp;
<a href="https://www.cnblogs.com/huggingface">HuggingFace</a>&nbsp;
阅读(<span id="post_view_count">45</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18741971" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18741971);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18741971', targetLink: 'https://www.cnblogs.com/huggingface/p/18741971', title: '让 LLM 来评判 | 设计你自己的评估 prompt' })">举报</a>
</div>
        