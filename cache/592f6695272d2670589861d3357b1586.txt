
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/jellyai/p/18631260" title="发布于 2024-12-25 19:03">
    <span role="heading" aria-level="2">为什么 Llama 3.3 70B 比 GPT-4o 和 Claude 3.5 Sonnet 更优秀</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p><img src="https://img2024.cnblogs.com/blog/3524016/202412/3524016-20241225185859047-1333098510.png" alt="" loading="lazy"></p>
<p>过去七天的 AI 新闻如狂风暴雨般涌来，AI 世界发生了许多重大变化。在这篇文章中，我们将深入探讨来自 Llama 3.3 70B、GPT-4o 和 Claude 3.5 Sonnet 等主要参与者的最新 AI 动态。</p>
<p>12 月 7 日，Meta 将发布其年度最后一个 AI 模型。而就在昨天（12 月 6 日），Meta 发布了拥有 700 亿参数的 Llama 3.3。尽管参数数量远低于 4050 亿的 Llama 3.1，其性能却不相上下。</p>
<p>Meta 强调，Llama 3.3 模型更高效、成本更低，可运行在标准工作站上，不仅降低运营成本，还能提供高质量的文本 AI 解决方案。</p>
<p>Llama 3.3 优化了多语言支持，支持八种语言：英语、德语、法语、意大利语、葡萄牙语、印地语、西班牙语和泰语。</p>
<p>该模型拥有 128K 的上下文长度，并支持多种工具格式。它可以与外部工具和服务集成，扩展模型的功能。</p>
<p><img src="https://img2024.cnblogs.com/blog/3524016/202412/3524016-20241225185929716-843578704.png" alt="" loading="lazy"></p>
<p>在这份逐步指南中，我们将介绍 Llama 3.3 的特点、如何本地使用 Llama 3.3，以及为什么它比 GPT-4o 和 Claude 3.5 Sonnet 更强大。</p>
<p>我强烈建议大家观看本文附带的视频，这将彻底改变你的聊天机器人体验，帮助你领略 Llama 3.3 的强大威力！</p>
<p><strong>什么是 Llama 3.3</strong></p>
<p>Llama 3.3 是 Meta AI 推出的一个多语言大规模预训练语言模型，拥有 700 亿参数。其性能可媲美 4050 亿参数的 Llama 3.1，并针对多语言对话进行了优化，支持英语、德语、法语、意大利语、葡萄牙语、印地语、西班牙语和泰语。</p>
<p>Llama 3.3 拥有更长的上下文窗口、多语言输入输出能力，并能集成第三方工具以扩展功能，非常适合商业和研究用途。</p>
<p><strong>Llama 3.3 的关键特性</strong></p>
<p>• 效率与成本：Llama 3.3 更高效、成本更低，可运行在标准工作站上，降低运营成本，同时提供高质量文本 AI 解决方案。</p>
<p>• 多语言支持：支持英语、德语、法语、意大利语、葡萄牙语、印地语、西班牙语和泰语，能够处理这些语言的输入输出。</p>
<p>• 长上下文窗口：支持 128K 上下文长度。</p>
<p>• 集成第三方工具：可与第三方工具和服务集成，扩展功能和应用场景。</p>
<p>Llama 3.3 与 Llama 3.2 的比较</p>
<p>与 Llama 3.2 相比，Llama 3.3 在文本任务上取得了显著性能提升。Llama 3.2 专注于轻量化模型（1B 和 3B 参数），适合边缘设备部署，以及多模态模型（11B Vision 和 90B Vision）。而 Llama 3.3 则缩小到 700 亿参数，聚焦于提高文本生成的质量。</p>
<p><strong>Llama 3.3 的硬件要求</strong></p>
<p>Llama 3.3 使用 Meta 定制开发的训练库、定制 GPU 集群和生产基础设施进行预训练。微调、标注和评估均在相同的生产基础设施上完成。总计耗费 3930 万 GPU 或 700 万 GPU 小时的训练资源，使用 H100–80GB (TDP 700W) 硬件。</p>
<p><strong>如何在本地使用 Llama 3.3</strong></p>
<p>Llama 3.3 使用与 Llama 3.1 相同的提示格式，因此 Llama 3.1 的提示可直接用于 Llama 3.3，这是迁移用户的一大优势。Llama 3.3 支持与 Llama 3.1 相同的代码解释器和工具调用功能，使模型能够利用外部工具和信息。</p>
<p>使用 Ollama 是运行本地语言模型最快的方法。推荐尝试 Llama 3.3 70B，它体积小巧，在大多数硬件上表现出色。</p>
<ol>
<li>
<p>先下载 Ollama</p>
</li>
<li>
<p>打开终端，运行 ollama run llama3.3</p>
</li>
</ol>
<p>​​​​<img src="https://img2024.cnblogs.com/blog/3524016/202412/3524016-20241225190059611-871526981.png" alt="" loading="lazy"></p>
<p>Llama 3.3 70B 与Claude 3.5 Sonnet 和 GPT-4o的对比</p>
<p>我们将对 Claude 3.5 Sonnet 和 GPT-4o 的性能进行比较，验证 Llama 3.3 70B 的表现。此次验证内容包括以下项目：</p>
<p><strong>编程能力</strong></p>
<p>我们使用了一些基于真实场景的复杂 Python 问题。以下是任务示例：</p>
<p>You are given an array nums consisting of positive integers.</p>
<p>Starting with score = 0, apply the following algorithm:</p>
<p>Choose the smallest integer of the array that is not marked. If there is a tie, choose the one with the smallest index.</p>
<p>Add the value of the chosen integer to score.</p>
<p>Mark the chosen element and its two adjacent elements if they exist.</p>
<p>Repeat until all the array elements are marked.</p>
<p>Return the score you get after applying the above algorithm.</p>
<p>Example 1:</p>
<p>Input: nums = [2,1,3,4,5,2]</p>
<p>Output: 7</p>
<p>Explanation: We mark the elements as follows:</p>
<ul>
<li>
<p>1 is the smallest unmarked element, so we mark it and its two adjacent elements: [2,1,3,4,5,2].</p>
</li>
<li>
<p>2 is the smallest unmarked element, so we mark it and its left adjacent element: [2,1,3,4,5,2].</p>
</li>
<li>
<p>4 is the only remaining unmarked element, so we mark it: [2,1,3,4,5,2].</p>
</li>
</ul>
<p>Our score is 1 + 2 + 4 = 7.</p>
<p>LLama 3.3</p>
<p>def find_score(nums):</p>
<p>"""</p>
<p>Calculate the score by applying the given algorithm.</p>
<p>Args:</p>
<p>nums (list): A list of positive integers.</p>
<p>Returns:</p>
<p>int: The calculated score.</p>
<p>"""</p>
<p>score = 0</p>
<p>marked = [False] * len(nums)</p>
<p>while False in marked:</p>
<p>min_val = float('inf')</p>
<p>min_idx = -1</p>
<h1 id="find-the-smallest-unmarked-integer">Find the smallest unmarked integer</h1>
<p>for i, num in enumerate(nums):</p>
<p>if not marked[i] and num &lt; min_val:</p>
<p>min_val = num</p>
<p>min_idx = i</p>
<h1 id="add-the-value-of-the-chosen-integer-to-score">Add the value of the chosen integer to score</h1>
<p>score += min_val</p>
<h1 id="mark-the-chosen-element-and-its-two-adjacent-elements-if-they-exist">Mark the chosen element and its two adjacent elements if they exist</h1>
<p>for i in range(max(0, min_idx - 1), min(len(nums), min_idx + 2)):</p>
<p>marked[i] = True</p>
<p>return score</p>
<h1 id="example-usage">Example usage</h1>
<p>nums = [2, 1, 3, 4, 5, 2]</p>
<p>print(find_score(nums)) # Output: 7</p>
<p>GPT-4o</p>
<p>def get_score(nums):</p>
<p>n = len(nums)</p>
<p>marked = [False] * n # Array to track marked elements</p>
<p>score = 0</p>
<p>while True:</p>
<h1 id="find-the-smallest-unmarked-element">Find the smallest unmarked element</h1>
<p>smallest_value = float('inf')</p>
<p>smallest_index = -1</p>
<p>for i in range(n):</p>
<p>if not marked[i] and nums[i] &lt; smallest_value:</p>
<p>smallest_value = nums[i]</p>
<p>smallest_index = i</p>
<p>if smallest_index == -1: # If no unmarked elements are left, break</p>
<p>break</p>
<h1 id="add-the-value-of-the-chosen-element-to-the-score">Add the value of the chosen element to the score</h1>
<p>score += nums[smallest_index]</p>
<h1 id="mark-the-chosen-element-and-its-two-adjacent-elements">Mark the chosen element and its two adjacent elements</h1>
<p>marked[smallest_index] = True</p>
<p>if smallest_index &gt; 0:</p>
<p>marked[smallest_index - 1] = True</p>
<p>if smallest_index &lt; n - 1:</p>
<p>marked[smallest_index + 1] = True</p>
<p>return score</p>
<h1 id="example-usage-1">Example usage</h1>
<p>nums = [2, 1, 3, 4, 5, 2]</p>
<p>print(get_score(nums))</p>
<p>Claude 3.5 Sonnet</p>
<p>def markElements(nums):</p>
<h1 id="create-a-copy-of-the-input-array-to-track-marked-elements">Create a copy of the input array to track marked elements</h1>
<p>marked = [False] * len(nums)</p>
<p>score = 0</p>
<p>while not all(marked):</p>
<h1 id="find-the-smallest-unmarked-element-1">Find the smallest unmarked element</h1>
<p>min_val = float('inf')</p>
<p>min_index = -1</p>
<h1 id="find-the-smallest-unmarked-element-with-the-smallest-index">Find the smallest unmarked element with the smallest index</h1>
<p>for i in range(len(nums)):</p>
<p>if not marked[i] and nums[i] &lt; min_val:</p>
<p>min_val = nums[i]</p>
<p>min_index = i</p>
<h1 id="add-the-chosen-elements-value-to-score">Add the chosen element's value to score</h1>
<p>score += nums[min_index]</p>
<h1 id="mark-the-chosen-element">Mark the chosen element</h1>
<p>marked[min_index] = True</p>
<h1 id="mark-left-adjacent-element-if-it-exists">Mark left adjacent element if it exists</h1>
<p>if min_index &gt; 0:</p>
<p>marked[min_index - 1] = True</p>
<h1 id="mark-right-adjacent-element-if-it-exists">Mark right adjacent element if it exists</h1>
<p>if min_index &lt; len(nums) - 1:</p>
<p>marked[min_index + 1] = True</p>
<p>return score</p>
<h1 id="test-cases">Test cases</h1>
<p>test_cases = [</p>
<p>[2,1,3,4,5,2], # Example from the problem statement</p>
<p>[1,2,3,4,5], # Consecutive increasing numbers</p>
<p>[5,4,3,2,1] # Consecutive decreasing numbers</p>
<p>]</p>
<p>for nums in test_cases:</p>
<p>print(f"Input: {nums}")</p>
<p>print(f"Output: {markElements(nums)}\n")</p>
<p>所有三个解决方案在我使用 Visual Studio Code 运行代码时都正确无误。然而，Llama 3.3 的代码虽然清晰，但使用了 while False in marked 循环，这比 while not all(marked) 更不直观。此外，它在寻找最小未标记元素时重复了逻辑，且未使用辅助函数，使得维护性较差。</p>
<p>GPT-4o 的代码最易读，使用了简单明了的 while True 循环和明确的中断条件，注释简洁，逻辑清晰，避免了冗余，成为在可读性和维护性方面的一个优秀选择。</p>
<p>Claude 3.5 Sonnet 的代码也很清晰，并使用了 not all(marked) 作为循环条件，但其风格较为冗长，并且附加的测试用例虽然有帮助，却使代码的重点不如 GPT-4o 的版本集中。</p>
<p>总体而言，GPT-4o 的版本是最佳选择，因为它结构清晰，注释得当，设计直截了当，在可读性、功能性和维护性之间实现了良好的平衡。</p>
<p><strong>数学能力</strong></p>
<p>接下来，我比较和验证数学能力，我们让参与者回答概率问题。很多人害怕进入数据科学领域，因为他们对高中和大学的代数、矩阵和组合数学感到恐惧。</p>
<p>Find the probability of getting exactly two tails in 5-coin flips?</p>
<p><strong>LLama3.3</strong></p>
<p>Step 1: Define the sample space</p>
<p>The sample space consists of all possible outcomes of flipping 5 coins. Each coin can land in one of two ways: Heads (H) or Tails (T). So, the sample space has 2^5 = 32 possible outcomes.</p>
<p>Step 2: Identify the favorable outcomes</p>
<p>We want to find the probability of getting exactly 2 tails. This means we need to count the number of outcomes with 2 tails and 3 heads. We can use the binomial coefficient to calculate this:</p>
<p>Number of ways to choose 2 tails out of 5 flips = C(5, 2) = 5 / (2 × 3!) = 10</p>
<p>So, there are 10 favorable outcomes with exactly 2 tails.</p>
<p>Step 3: Calculate the probability</p>
<p>The probability of getting exactly 2 tails is the number of favorable outcomes divided by the total number of possible outcomes:</p>
<p>P(exactly 2 tails) = Number of favorable outcomes / Total number of outcomes</p>
<p>= 10 / 32</p>
<p>= 5/16</p>
<p>Answer</p>
<p>The probability of getting exactly 2 tails in 5-coin flips is 5/16 or approximately 0.3125 (31.25%).</p>
<p>In mathematical notation, this can be represented as:</p>
<p>P(X = 2) = (5 choose 2) × (1/2)^2 × (1/2)^3 = 10/32 = 5/16</p>
<p>where X is the random variable representing the number of tails, and (5 choose 2) is the binomial coefficient representing the number of ways to choose 2 tails out of 5 flips.</p>
<p><strong>GPT-4o</strong></p>
<p><img src="https://img2024.cnblogs.com/blog/3524016/202412/3524016-20241225190209945-1935479957.png" alt="" loading="lazy"></p>
<p>Claude 3.5 Sonnet</p>
<p>I'll solve this using the binomial probability formula. Here's a step-by-step mathematical approach:</p>
<p>First, let's define our parameters:</p>
<p>Total number of coin flips (n) = 5</p>
<p>Number of desired tails (k) = 2</p>
<p>Probability of getting a tail in a single flip (p) = 1/2</p>
<p>Probability of getting a head in a single flip (q) = 1/2</p>
<p>The binomial probability formula is:</p>
<p>P(X = k) = C(n,k) * p^k * q^(n-k)</p>
<p>Let's break this down:</p>
<p>C(n,k) is the combination formula: 5! / (2! * (5-2)!) = 5! / (2! * 3!)</p>
<p>This calculates how many ways we can choose 2 tails out of 5 flips</p>
<p>Calculating:</p>
<p>C(5,2) = 5! / (2! * 3!) = (5 * 4) / (2 * 1) = 10</p>
<p>(1/2)^2 * (1/2)^(5-2) = (1/2)^2 * (1/2)^3</p>
<p>(1/2)^5 = 1/32</p>
<p>Final calculation:</p>
<p>10 * (1/2)^5 = 10/32 = 5/16</p>
<p>Therefore, the probability of getting exactly two tails in 5 coin flips is 5/16 or approxi</p>
<p>所有三个模型都正确得出了概率为 5/16 的结论。</p>
<p><strong>Llama 3.3 的方法不够直观，缺乏简化步骤。</strong></p>
<p>Claude 3.5 Sonnet 的表达清晰，但增加了一些额外的细节，例如对正面和反面的概率计算，这对于此问题可能并非必要。</p>
<p>GPT-4o 的表现最出色，因为它采用了清晰且结构化的基于公式的方法，直接应用二项式系数并对公式进行了简化，表述清楚易懂，避免了不必要的复杂性。</p>
<p><strong>结论：</strong></p>
<p>Llama 3.3 是 Llama 系列中功能最强大的版本，开源且高度实用。通过利用其开源特性，该模型可以根据每个公司的需求进行改进。</p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.05721481286574074" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2024-12-25 19:03">2024-12-25 19:03</span>&nbsp;
<a href="https://www.cnblogs.com/jellyai">果冻人工智能</a>&nbsp;
阅读(<span id="post_view_count">0</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18631260" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18631260);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18631260', targetLink: 'https://www.cnblogs.com/jellyai/p/18631260', title: '为什么 Llama 3.3 70B 比 GPT-4o 和 Claude 3.5 Sonnet 更优秀' })">举报</a>
</div>
        