
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/rossiXYZ/p/18823734" title="å‘å¸ƒäº 2025-04-14 20:06">
    <span role="heading" aria-level="2">æ¢ç§˜Transformerç³»åˆ—ä¹‹ï¼ˆ27ï¼‰--- MQA &amp; GQA</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                    <div id="cnblogs_post_description" style="display: none">
        
        ä»é›¶å¼€å§‹è§£æTransformerï¼Œç›®æ ‡æ˜¯ï¼š(1) è§£æTransformerå¦‚ä½•è¿ä½œï¼Œä»¥åŠä¸ºä½•å¦‚æ­¤è¿ä½œï¼Œè®©æ–°åŒå­¦å¯ä»¥å…¥é—¨ï¼›(2) åŠ›äº‰èå…¥ä¸€äº›æ¯”è¾ƒæ–°çš„æˆ–è€…æœ‰ç‰¹è‰²çš„è®ºæ–‡æˆ–è€…ç†å¿µï¼Œè®©è€é¸Ÿä¹Ÿå¯ä»¥æœ‰æ‰€æ”¶è·ã€‚
    </div>
<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h1 id="æ¢ç§˜transformerç³»åˆ—ä¹‹27----mqa--gqa">æ¢ç§˜Transformerç³»åˆ—ä¹‹ï¼ˆ27ï¼‰--- MQA &amp; GQA</h1>
<p></p><div class="toc"><div class="toc-container-header">ç›®å½•</div><ul><li><a href="#æ¢ç§˜transformerç³»åˆ—ä¹‹27----mqa--gqa" rel="noopener nofollow">æ¢ç§˜Transformerç³»åˆ—ä¹‹ï¼ˆ27ï¼‰--- MQA &amp; GQA</a><ul><li><a href="#0x00-æ¦‚è¿°" rel="noopener nofollow">0x00 æ¦‚è¿°</a></li><li><a href="#0x01-mha" rel="noopener nofollow">0x01 MHA</a><ul><li><a href="#11-æ¦‚å¿µ" rel="noopener nofollow">1.1 æ¦‚å¿µ</a></li><li><a href="#12-å®ç°" rel="noopener nofollow">1.2 å®ç°</a><ul><li><a href="#121-å“ˆä½›" rel="noopener nofollow">1.2.1 å“ˆä½›</a></li><li><a href="#122-llm-foundry" rel="noopener nofollow">1.2.2 llm-foundry</a></li></ul></li><li><a href="#13-èµ„æºå ç”¨" rel="noopener nofollow">1.3 èµ„æºå ç”¨</a></li></ul></li><li><a href="#0x02-mqa" rel="noopener nofollow">0x02 MQA</a><ul><li><a href="#21-æ¦‚å¿µ" rel="noopener nofollow">2.1 æ¦‚å¿µ</a></li><li><a href="#22-å®ç°" rel="noopener nofollow">2.2 å®ç°</a><ul><li><a href="#121-ç²¾ç®€ç‰ˆ" rel="noopener nofollow">1.2.1 ç²¾ç®€ç‰ˆ</a></li><li><a href="#122-å®Œæ•´ç‰ˆ" rel="noopener nofollow">1.2.2 å®Œæ•´ç‰ˆ</a></li></ul></li><li><a href="#23-æ•ˆæœ" rel="noopener nofollow">2.3 æ•ˆæœ</a><ul><li><a href="#231-å†…å­˜" rel="noopener nofollow">2.3.1 å†…å­˜</a></li><li><a href="#232-é€Ÿåº¦" rel="noopener nofollow">2.3.2 é€Ÿåº¦</a></li><li><a href="#233-è¡¨å¾èƒ½åŠ›" rel="noopener nofollow">2.3.3 è¡¨å¾èƒ½åŠ›</a></li><li><a href="#233-é€šä¿¡" rel="noopener nofollow">2.3.3 é€šä¿¡</a></li></ul></li></ul></li><li><a href="#0x03-gqa" rel="noopener nofollow">0x03 GQA</a><ul><li><a href="#31-æ¦‚å¿µ" rel="noopener nofollow">3.1 æ¦‚å¿µ</a></li><li><a href="#32-æ¶æ„æ¯”å¯¹" rel="noopener nofollow">3.2 æ¶æ„æ¯”å¯¹</a></li><li><a href="#33-å®ç°" rel="noopener nofollow">3.3 å®ç°</a><ul><li><a href="#331-ç²¾ç®€ç‰ˆ" rel="noopener nofollow">3.3.1 ç²¾ç®€ç‰ˆ</a></li><li><a href="#332-å®Œæ•´ç‰ˆ" rel="noopener nofollow">3.3.2 å®Œæ•´ç‰ˆ</a></li></ul></li><li><a href="#34-æ•ˆæœ" rel="noopener nofollow">3.4 æ•ˆæœ</a><ul><li><a href="#341-å†…å­˜" rel="noopener nofollow">3.4.1 å†…å­˜</a></li><li><a href="#342-é€Ÿåº¦" rel="noopener nofollow">3.4.2 é€Ÿåº¦</a></li><li><a href="#343-è¡¨å¾èƒ½åŠ›" rel="noopener nofollow">3.4.3 è¡¨å¾èƒ½åŠ›</a></li></ul></li><li><a href="#35-è½¬æ¢" rel="noopener nofollow">3.5 è½¬æ¢</a><ul><li><a href="#351-å¹³å‡æ± åŒ–" rel="noopener nofollow">3.5.1 å¹³å‡æ± åŒ–</a></li><li><a href="#352-åŸºäºæ©ç " rel="noopener nofollow">3.5.2 åŸºäºæ©ç </a><ul><li><a href="#ç½‘ç»œè½¬æ¢" rel="noopener nofollow">ç½‘ç»œè½¬æ¢</a></li><li><a href="#æ‰¾åˆ°æ›´å¥½çš„åˆ†ç»„æ–¹æ³•" rel="noopener nofollow">æ‰¾åˆ°æ›´å¥½çš„åˆ†ç»„æ–¹æ³•</a></li><li><a href="#å‰ªæè®­ç»ƒ" rel="noopener nofollow">å‰ªæè®­ç»ƒ</a></li></ul></li></ul></li><li><a href="#36-ä¼˜åŒ–" rel="noopener nofollow">3.6 ä¼˜åŒ–</a></li></ul></li><li><a href="#0xff-å‚è€ƒ" rel="noopener nofollow">0xFF å‚è€ƒ</a></li></ul></li></ul></div><p></p>
<h2 id="0x00-æ¦‚è¿°">0x00 æ¦‚è¿°</h2>
<p>åœ¨å‰æ–‡â€œä¼˜åŒ–KV Cache"ä¸­æˆ‘ä»¬æåˆ°è¿‡ï¼Œåœ¨â€å‡å°‘æ³¨æ„åŠ›å¤´çš„æ•°é‡â€œè¿™ä¸ªç»´åº¦ä¸Šï¼Œç›®å‰ä¸»è¦çš„ç›¸å…³å·¥ä½œæœ‰ MQAå’ŒGQAã€‚MQA å’Œ GQA æ˜¯åœ¨ç¼“å­˜å¤šå°‘æ•°é‡KVçš„æ€è·¯ä¸Šè¿›è¡Œä¼˜åŒ–ï¼šç›´è§‰æ˜¯å¦‚æœç¼“å­˜çš„KVä¸ªæ•°å°‘ä¸€äº›ï¼Œæ˜¾å­˜å°±å ç”¨å°‘ä¸€äº›ï¼Œå¤§æ¨¡å‹èƒ½åŠ›çš„é™ä½å¯ä»¥é€šè¿‡è¿›ä¸€æ­¥çš„è®­ç»ƒæˆ–è€…å¢åŠ FFN/GLUçš„è§„æ¨¡æ¥å¼¥è¡¥ã€‚</p>
<p>å› ä¸ºMQAå’ŒGQAæ˜¯åŸºäºMHAè¿›è¡Œæ”¹è¿›ï¼Œæ‰€ä»¥æˆ‘ä»¬ç”¨ä¸‹å›¾å±•ç¤ºäº†ä¸‰è€…çš„åŒºåˆ«ã€‚å¯ä»¥çœ‹åˆ°ï¼Œé€šè¿‡ç¼©å‡æ³¨æ„åŠ›å¤´æ•°ç›®ï¼ŒMQA/GQAä¼šé™ä½KV Cacheå­˜å‚¨ï¼Œè®©ä¸åŒçš„æ³¨æ„åŠ›å¤´æˆ–è€…åŒä¸€ç»„çš„æ³¨æ„åŠ›å¤´å…±äº«ä¸€ä¸ªKå’ŒVçš„é›†åˆï¼Œå› ä¸ºåªå•ç‹¬ä¿ç•™äº†ä¸€ä»½ï¼ˆæˆ–è€…å‡ ä»½ï¼‰æŸ¥è¯¢å‚æ•°ã€‚å› æ­¤Kå’ŒVçš„çŸ©é˜µä»…æœ‰ä¸€ä»½ï¼ˆæˆ–è€…å‡ ä»½ï¼‰ï¼Œè¿™å¤§å¹…åº¦å‡å°‘äº†æ˜¾å­˜å ç”¨ï¼Œä½¿å…¶æ›´é«˜æ•ˆã€‚å¦å¤–ï¼Œä¼ ç»Ÿçš„åŸºäºMHAçš„Attentionç®—å­è¿‡äºå¡è®¿å­˜å¸¦å®½ï¼ŒMQAå’ŒGQAï¼Œä¹ƒè‡³åç»­çš„MLAéƒ½å¯ä»¥æè®¡ç®—è®¿å­˜æ¯”ï¼Œè¿™æ ·ä¹Ÿæ˜¯å¯¹æ€§èƒ½çš„æå¤§æå‡ã€‚</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413202825378-692989445.jpg" alt="" loading="lazy"></p>
<hr>
<p>æ³¨ï¼š</p>
<ul>
<li>å…¨éƒ¨æ–‡ç« åˆ—è¡¨åœ¨è¿™é‡Œï¼Œä¼°è®¡æœ€ç»ˆåœ¨35ç¯‡å·¦å³ï¼Œåç»­æ¯å‘ä¸€ç¯‡æ–‡ç« ï¼Œä¼šä¿®æ”¹æ­¤æ–‡ç« åˆ—è¡¨ã€‚cnblogs <a href="https://www.cnblogs.com/rossiXYZ/p/18785601" target="_blank">æ¢ç§˜Transformerç³»åˆ—ä¹‹æ–‡ç« åˆ—è¡¨</a></li>
<li>æœ¬ç³»åˆ—æ˜¯å¯¹è®ºæ–‡ã€åšå®¢å’Œä»£ç çš„å­¦ä¹ å’Œè§£è¯»ï¼Œå€Ÿé‰´äº†å¾ˆå¤šç½‘ä¸Šæœ‹å‹çš„æ–‡ç« ï¼Œåœ¨æ­¤è¡¨ç¤ºæ„Ÿè°¢ï¼Œå¹¶ä¸”ä¼šåœ¨å‚è€ƒä¸­åˆ—å‡ºã€‚å› ä¸ºæœ¬ç³»åˆ—å‚è€ƒæ–‡ç« å¤ªå¤šï¼Œå¯èƒ½æœ‰æ¼ç»™å‡ºå¤„çš„ç°è±¡ã€‚å¦‚æœåŸä½œè€…å‘ç°ï¼Œè¿˜è¯·æŒ‡å‡ºï¼Œæˆ‘åœ¨å‚è€ƒæ–‡çŒ®ä¸­è¿›è¡Œå¢è¡¥ã€‚</li>
</ul>
<hr>
<h2 id="0x01-mha">0x01 MHA</h2>
<p>å› ä¸ºMQAï¼ŒGQAæ˜¯åŸºäºMHAè¿›è¡Œä¿®æ”¹ï¼Œæ‰€ä»¥æˆ‘ä»¬æœ‰å¿…è¦å…ˆå›é¡¾ä¸‹MHAã€‚</p>
<h3 id="11-æ¦‚å¿µ">1.1 æ¦‚å¿µ</h3>
<p>MHAï¼ˆå³å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼‰åœ¨2017å¹´å°±éšç€TransformeråŸå§‹è®ºæ–‡"Attention Is All You Need"ä¸€èµ·æå‡ºï¼Œå…¶ä¸»è¦å·¥ä½œæ˜¯ï¼šæŠŠåŸæ¥ä¸€ä¸ªæ³¨æ„åŠ›è®¡ç®—æ‹†æˆå¤šä¸ªå°ä»½çš„æ³¨æ„åŠ›å¤´ï¼Œå³æŠŠQã€Kã€Våˆ†åˆ«æ‹†åˆ†æˆå¤šä»½ï¼Œæ¯ä¸ªæ³¨æ„åŠ›å¤´ä½¿ç”¨ç‹¬ç«‹çš„Qã€Kã€Vè¿›è¡Œè®¡ç®—ã€‚è€Œå¤šä¸ªå¤´å¯ä»¥å¹¶è¡Œè®¡ç®—ï¼Œåˆ†åˆ«å¾—å‡ºç»“æœï¼Œæœ€åå†åˆå›åŸæ¥çš„ç»´åº¦ã€‚</p>
<p>æˆ‘ä»¬é€šè¿‡ä¸‹å›¾æ¥çœ‹çœ‹MHAçš„æµç¨‹ï¼Œè¿™é‡Œè®¾ ğ‘‘ è¡¨ç¤ºè¯åµŒå…¥çš„ç»´åº¦ï¼Œ <span class="math inline">\(ğ‘›_â„\)</span> è¡¨ç¤ºæ³¨æ„åŠ›å¤´çš„æ•°é‡ï¼Œ <span class="math inline">\(ğ‘‘_â„\)</span> è¡¨ç¤ºæ¯ä¸€ä¸ªå¤´çš„ç»´åº¦ï¼Œ <span class="math inline">\(â„_ğ‘¡\inğ‘…^ğ‘‘\)</span> è¡¨ç¤ºç¬¬ ğ‘¡ ä¸ªtokenåœ¨ä¸€ä¸ªæ³¨æ„åŠ›å±‚çš„è¾“å…¥ï¼Œ <span class="math inline">\(ğ‘Š^ğ‘‚âˆˆğ‘…^{ğ‘‘Ã—ğ‘‘_â„ğ‘›_â„}\)</span> è¡¨ç¤ºè¾“å‡ºæ˜ å°„çŸ©é˜µã€‚åˆ™MHAå¯ä»¥åˆ†ä¸ºä»¥ä¸‹å››æ­¥ï¼š</p>
<ol>
<li>é€šè¿‡3ä¸ªå‚æ•°çŸ©é˜µ <span class="math inline">\(ğ‘Š^ğ‘„,ğ‘Š^ğ¾,ğ‘Š^ğ‘‰âˆˆğ‘…^{ğ‘‘_â„ğ‘›_h\times d}\)</span> å°±å¯ä»¥å¾—åˆ° <span class="math inline">\(ğ‘_ğ‘¡,ğ‘˜_ğ‘¡,ğ‘£_ğ‘¡âˆˆğ‘…^{ğ‘‘_â„ğ‘›_h}\)</span> ã€‚</li>
<li><span class="math inline">\(ğ‘_ğ‘¡,ğ‘˜_ğ‘¡,ğ‘£_ğ‘¡\)</span> ä¼šåˆ†å‰²æˆ <span class="math inline">\(ğ‘›_â„\)</span> ä¸ªå‘é‡ï¼Œ<span class="math inline">\(ğ‘_{ğ‘¡,ğ‘–},ğ‘˜_{ğ‘¡,ğ‘–},ğ‘£_{ğ‘¡,ğ‘–}âˆˆğ‘…^{ğ‘‘_â„}\)</span> åˆ†åˆ«è¡¨ç¤ºQã€Kå’ŒVçš„ç¬¬ ğ‘– ä¸ªå‘é‡ï¼Œè¿™äº›æ‹†åˆ†åçš„å‘é‡æˆ‘ä»¬åç»­ç§°ä¹‹ä¸ºQå¤´ï¼ŒKå¤´å’ŒVå¤´ã€‚</li>
<li>æ¯ä¸ªæ³¨æ„åŠ›å¤´ä¼šåˆ©ç”¨è‡ªå·±è·å¾—çš„Qã€Kã€Vå‘é‡è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ã€‚</li>
<li>åˆ©ç”¨<span class="math inline">\(W^O\)</span>å¯¹å¤šå¤´æ³¨æ„åŠ›è®¡ç®—ç»“æœè¿›è¡Œåˆå¹¶ã€‚</li>
</ol>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413202836695-310488105.jpg" alt="" loading="lazy"></p>
<h3 id="12-å®ç°">1.2 å®ç°</h3>
<h4 id="121-å“ˆä½›">1.2.1 å“ˆä½›</h4>
<p>æˆ‘ä»¬å›é¡¾ä¸‹â€œThe Annotated Transformerâ€ä¸­MHAä»£ç çš„å®ç°</p>
<pre><code class="language-python">def attention(query, key, value, mask=None, dropout=None):
    "Compute 'Scaled Dot Product Attention'"
    d_k = query.size(-1)
    scores = torch.matmul(query, key.transpose(-2, -1)) \
             / math.sqrt(d_k)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    p_attn = F.softmax(scores, dim = -1)
    if dropout is not None:
        p_attn = dropout(p_attn)
    return torch.matmul(p_attn, value), p_attn

class MultiHeadedAttention(nn.Module):
    def __init__(self, h, d_model, dropout=0.1):
        '''
        h: head number
        '''
        super(MultiHeadedAttention, self).__init__()
        assert d_model % h == 0
        # We assume d_v always equals d
        self.d = d_model // h
        self.h = h
        self.linears = clones(nn.Linear(d_model, d_model), 4)
        self.attn = None
        self.dropout = nn.Dropout(p=dropout)
        
    def forward(self, query, key, value, mask=None):
        if mask is not None:
            # Same mask applied to all h heads.
            mask = mask.unsqueeze(1)
        nbatches = query.size(0)
        
        # 1) Do all the linear projections in batch from d_model =&gt; h x d 
        query, key, value = \
            [l(x).view(nbatches, -1, self.h, self.d).transpose(1, 2)
             for l, x in zip(self.linears, (query, key, value))]
        
        # 2) Apply attention on all the projected vectors in batch. 
        x, self.attn = attention(query, key, value, mask=mask, 
                                 dropout=self.dropout)
        
        # 3) "Concat" using a view and apply a final linear. 
        x = x.transpose(1, 2).contiguous() \
             .view(nbatches, -1, self.h * self.d)
        return self.linears[-1](x)
</code></pre>
<h4 id="122-llm-foundry">1.2.2 llm-foundry</h4>
<p>ä½œä¸ºå¯¹æ¯”ï¼Œæˆ‘ä»¬çœ‹çœ‹å·¥ä¸šç•Œçš„äº§å“ã€‚</p>
<p><a href="https://github.com/mosaicml/llm-foundry/blob/9c89ab263e72fb9610f28c8ab9cde5d2205b6bff/llmfoundry/models/layers/attention.py" target="_blank" rel="noopener nofollow">https://github.com/mosaicml/llm-foundry/blob/9c89ab263e72fb9610f28c8ab9cde5d2205b6bff/llmfoundry/models/layers/attention.py</a></p>
<pre><code class="language-python">class MultiheadAttention(nn.Module):
    """Multi-head self attention.

    Using torch or triton attention implemetation enables user to also use
    additive bias.
    """

    def __init__(
        self,
        d_model: int,
        n_heads: int,
        attn_impl: str = 'triton',
        clip_qkv: Optional[float] = None,
        qk_ln: bool = False,
        softmax_scale: Optional[float] = None,
        attn_pdrop: float = 0.0,
        low_precision_layernorm: bool = False,
        verbose: int = 0,
        device: Optional[str] = None,
    ):
        super().__init__()

        self.attn_impl = attn_impl
        self.clip_qkv = clip_qkv
        self.qk_ln = qk_ln

        self.d_model = d_model
        self.n_heads = n_heads
        self.softmax_scale = softmax_scale
        if self.softmax_scale is None:
            self.softmax_scale = 1 / math.sqrt(self.d_model / self.n_heads)
        self.attn_dropout_p = attn_pdrop

        self.Wqkv = nn.Linear(self.d_model, 3 * self.d_model, device=device)
        # for param init fn; enables shape based init of fused layers
        fuse_splits = (d_model, 2 * d_model)
        self.Wqkv._fused = (0, fuse_splits)  # type: ignore

        if self.qk_ln:
            layernorm_class = LPLayerNorm if low_precision_layernorm else nn.LayerNorm
            self.q_ln = layernorm_class(self.d_model, device=device)
            self.k_ln = layernorm_class(self.d_model, device=device)

        if self.attn_impl == 'flash':
            self.attn_fn = flash_attn_fn
        elif self.attn_impl == 'triton':
            self.attn_fn = triton_flash_attn_fn
        elif self.attn_impl == 'torch':
            self.attn_fn = scaled_multihead_dot_product_attention
        else:
            raise ValueError(f'{attn_impl=} is an invalid setting.')

        self.out_proj = nn.Linear(self.d_model, self.d_model, device=device)
        self.out_proj._is_residual = True  # type: ignore

    def forward(
        self,
        x,
        past_key_value=None,
        attn_bias=None,
        attention_mask=None,
        is_causal=True,
        needs_weights=False,
    ):
        qkv = self.Wqkv(x)

        if self.clip_qkv:
            qkv.clamp_(min=-self.clip_qkv, max=self.clip_qkv)

        query, key, value = qkv.chunk(3, dim=2)

        key_padding_mask = attention_mask

        if self.qk_ln:
            # Applying layernorm to qk
            dtype = query.dtype
            query = self.q_ln(query).to(dtype)
            key = self.k_ln(key).to(dtype)

        context, attn_weights, past_key_value = self.attn_fn(
            query,
            key,
            value,
            self.n_heads,
            past_key_value=past_key_value,
            softmax_scale=self.softmax_scale,
            attn_bias=attn_bias,
            key_padding_mask=key_padding_mask,
            is_causal=is_causal,
            dropout_p=self.attn_dropout_p,
            training=self.training,
            needs_weights=needs_weights,
        )

        return self.out_proj(context), attn_weights, past_key_value
</code></pre>
<p>scaled_multihead_dot_product_attention()ä»£ç å¦‚ä¸‹ã€‚</p>
<pre><code class="language-python">def scaled_multihead_dot_product_attention(
    query,
    key,
    value,
    n_heads,
    past_key_value=None,
    softmax_scale=None,
    attn_bias=None,
    key_padding_mask=None,
    is_causal=False,
    dropout_p=0.0,
    training=False,
    needs_weights=False,
    multiquery=False,
):
    q = rearrange(query, 'b s (h d) -&gt; b h s d', h=n_heads)
    kv_n_heads = 1 if multiquery else n_heads
    k = rearrange(key, 'b s (h d) -&gt; b h d s', h=kv_n_heads)
    v = rearrange(value, 'b s (h d) -&gt; b h s d', h=kv_n_heads)

    if past_key_value is not None:
        if len(past_key_value) != 0:
            k = torch.cat([past_key_value[0], k], dim=3)
            v = torch.cat([past_key_value[1], v], dim=2)
        past_key_value = (k, v)

    b, _, s_q, d = q.shape
    s_k = k.size(-1)

    if softmax_scale is None:
        softmax_scale = 1 / math.sqrt(d)

    attn_weight = q.matmul(k) * softmax_scale

    if attn_bias is not None:
        _s_q = max(0, attn_bias.size(2) - s_q)
        _s_k = max(0, attn_bias.size(3) - s_k)
        attn_bias = attn_bias[:, :, _s_q:, _s_k:]
        attn_weight = attn_weight + attn_bias

    min_val = torch.finfo(q.dtype).min

    if key_padding_mask is not None:
        attn_weight = attn_weight.masked_fill(
            ~key_padding_mask.view((b, 1, 1, s_k)), min_val)

    if is_causal and (not q.size(2) == 1):
        s = max(s_q, s_k)
        causal_mask = attn_weight.new_ones(s, s, dtype=torch.float16)
        causal_mask = causal_mask.tril()
        causal_mask = causal_mask.to(torch.bool)
        causal_mask = ~causal_mask
        causal_mask = causal_mask[-s_q:, -s_k:]
        attn_weight = attn_weight.masked_fill(causal_mask.view(1, 1, s_q, s_k),
                                              min_val)

    attn_weight = torch.softmax(attn_weight, dim=-1)

    if dropout_p:
        attn_weight = torch.nn.functional.dropout(attn_weight,
                                                  p=dropout_p,
                                                  training=training,
                                                  inplace=True)

    out = attn_weight.matmul(v)
    out = rearrange(out, 'b h s d -&gt; b s (h d)')

    if needs_weights:
        return out, attn_weight, past_key_value
    return out, None, past_key_value
</code></pre>
<h3 id="13-èµ„æºå ç”¨">1.3 èµ„æºå ç”¨</h3>
<p>å¦‚æœæ¨¡å‹ç»“æ„æ˜¯MHAï¼Œåœ¨æ¨ç†æ—¶ï¼ŒKV Cacheå¯¹äºæ¯ä¸ªtokenéœ€è¦ç¼“å­˜çš„å‚æ•°æœ‰ <span class="math inline">\(2ğ‘›_â„ğ‘‘_â„ğ‘™\)</span>ï¼ˆğ‘™ è¡¨ç¤ºç½‘ç»œå±‚æ•°ï¼‰ã€‚å½“æ¨¡å‹å±‚æ•°åŠ æ·±å’Œå¤´æ•°å˜å¤šåï¼Œæ³¨æ„åŠ›è®¡ç®—æ‰€æ¶‰åŠçš„ç®—åŠ›ã€IOå’Œå†…å­˜éƒ½ä¼šå¿«é€Ÿå¢åŠ ã€‚ä½†æ˜¯å¯¹è¿™äº›èµ„æºå´åˆ©ç”¨å¾—ä¸å¥½ã€‚</p>
<p>å°±ä¸‹å›¾è€Œè¨€ï¼Œd è¡¨ç¤º hidden sizeï¼Œh è¡¨ç¤º Head ä¸ªæ•°ï¼Œl è¡¨ç¤ºå½“å‰è¾“å…¥åºåˆ—ä¸€å…±æœ‰ l ä¸ª Tokenã€‚</p>
<ul>
<li>
<p>å½“ Batch Size ä¸º 1 æ—¶ï¼Œå›¾ä¸­çº¢è‰²ã€ç»¿è‰²ã€è“è‰²è™šçº¿åœˆå¤„çš„ä¹˜æ³•å…¨éƒ¨ä¸ºçŸ©é˜µä¹˜å‘é‡ï¼Œæ˜¯æ˜æ˜¾çš„ Memory Boundï¼Œç®—æœ¯å¼ºåº¦ä¸åˆ° 1ã€‚</p>
</li>
<li>
<p>å½“ Batch Size å¤§äº 1 æ—¶ï¼ˆæ¯”å¦‚ Continuous Batchingï¼‰ï¼š</p>
</li>
<li>
<ul>
<li>çº¢è‰²å’Œè“è‰²éƒ¨åˆ†ï¼šçº¿æ€§å±‚è®¡ç®—æ˜¯æƒé‡ä¹˜ä»¥æ¿€æ´»ï¼Œä¸åŒè¯·æ±‚ä¹‹é—´å¯ä»¥å…±äº«æƒé‡ï¼Œå› æ­¤æ˜¯çŸ©é˜µä¹˜çŸ©é˜µï¼Œå¹¶ä¸” Batch Size è¶Šå¤§ï¼Œç®—æœ¯å¼ºåº¦è¶Šå¤§ï¼Œè¶Šè¶‹è¿‘äºè®¡ç®—å¯†é›†å‹ï¼ˆFFN å±‚ä¹Ÿç±»ä¼¼ï¼‰ã€‚</li>
<li>ç»¿è‰²éƒ¨åˆ†ï¼šæ³¨æ„åŠ›è®¡ç®—æ˜¯æ¿€æ´»ä¹˜ä»¥æ¿€æ´»ã€‚å› ä¸ºä¸åŒçš„è¯·æ±‚ä¹‹é—´æ²¡æœ‰ä»»ä½•ç›¸å…³æ€§ï¼Œå³ä½¿ Batchingï¼Œæ­¤å¤„ä¹Ÿæ˜¯ Batched çŸ©é˜µä¹˜å‘é‡ï¼Œå¹¶ä¸”å› ä¸ºåºåˆ—é•¿åº¦å¯èƒ½ä¸åŒï¼Œè¿™é‡Œä¸åŒè¯·æ±‚çš„çŸ©é˜µä¹˜å‘é‡æ˜¯ä¸è§„åˆ™çš„ã€‚å³ï¼Œè¿™é‡Œç®—æœ¯å¼ºåº¦å§‹ç»ˆä¸åˆ° 1ï¼Œæ˜¯æ˜æ˜¾çš„ Memory Boundã€‚</li>
</ul>
</li>
<li>
<p>å› æ­¤ï¼Œç»¿è‰²éƒ¨åˆ†éš¾ä»¥ä¼˜åŒ–ï¼Œè¾“å…¥åºåˆ—è¶Šé•¿ï¼Œæ­¤å¤„çš„ç“¶é¢ˆå°±è¶Šå¤§ã€‚</p>
</li>
</ul>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250414200154304-642792608.jpg" alt="" loading="lazy"></p>
<p>ä¸ºäº†ç¼“è§£è¿™äº›èµ„æºå ç”¨ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥æ›´å¥½çš„åˆ©ç”¨èµ„æºï¼Œç›¸ç»§å‡ºç°äº†MQAï¼ˆMulti-Query Attentionï¼‰ å’ŒGQAï¼ˆGrouped-Query Attention ï¼‰ç­‰æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•éƒ½æ˜¯å›´ç»•â€œå¦‚ä½•å‡å°‘èµ„æºå ç”¨ä¸”å°½å¯èƒ½åœ°ä¿è¯æ•ˆæœâ€è¿™ä¸ªä¸»é¢˜å‘å±•è€Œæ¥çš„äº§ç‰©ã€‚</p>
<h2 id="0x02-mqa">0x02 MQA</h2>
<p>ç›®å‰çš„åŸºæœ¬å‡è®¾æ˜¯ï¼Œåœ¨å¤´ç»´åº¦ä¸Šå­˜åœ¨éå¸¸é«˜çš„ç¨€ç–æ€§ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠå¤´çš„æ•°é‡ç¼©å‡åˆ°ç›¸å½“å°çš„æ•°ç›®ã€‚åœ¨è¿™äº›æ³¨æ„åŠ›å¤´ä¸­ï¼Œæœ‰ä¸€äº›å¤´éƒ¨ä¸“é—¨ç”¨äºæ£€ç´¢å’Œé•¿ä¸Šä¸‹æ–‡ç›¸å…³èƒ½åŠ›ï¼Œå› æ­¤åº”è¯¥ä¿ç•™è¿™äº›æ£€ç´¢å¤´å¹¶ä¿®å‰ªå…¶ä»–å¤´ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¤´éƒ¨ä¿®å‰ªé€šå¸¸å‘ç”Ÿåœ¨é¢„å¡«å……ä¹‹åï¼Œè¿™æ„å‘³ç€å®ƒä»¬åªä¼šæ”¹å–„è§£ç ã€å¹¶å‘æ€§å’Œä¸Šä¸‹æ–‡åˆ‡æ¢ï¼Œä½†å¹¶æ²¡æœ‰æ”¹å–„é¢„å¡«å……é˜¶æ®µã€‚</p>
<h3 id="21-æ¦‚å¿µ">2.1 æ¦‚å¿µ</h3>
<p>MQAï¼ˆMulti Queries Attentionï¼‰å‡ºè‡ªè®ºæ–‡ [<a href="https://arxiv.org/pdf/1911.02150.pdf" target="_blank" rel="noopener nofollow">2019] Fast Transformer Decoding: One Write-Head is All You Need</a>ã€‚åœ¨MQAä¸­ï¼Œä¿ç•™queryçš„å¤šå¤´æ€§è´¨ï¼Œæ‰€æœ‰æŸ¥è¯¢å¤´å…±äº«ç›¸åŒçš„å•ä¸€é”®å’Œå€¼å¤´ï¼Œè¿™ç”¨å¯ä»¥å‡å°‘Keyå’ŒValueçŸ©é˜µçš„æ•°é‡ï¼Œä»è€Œé™ä½è®¡ç®—å’Œå­˜å‚¨å¼€é”€ã€‚è¿™ç›¸å½“äºæŠŠä¸åŒHeadçš„æ³¨æ„åŠ›å·®å¼‚ï¼Œå…¨éƒ¨éƒ½æ”¾åœ¨äº†Queryä¸Šï¼Œéœ€è¦æ¨¡å‹ä»…ä»ä¸åŒçš„Query Headsä¸Šå°±èƒ½å¤Ÿå…³æ³¨åˆ°è¾“å…¥hidden statesä¸åŒæ–¹é¢çš„ä¿¡æ¯ã€‚</p>
<p>MQAçš„å…·ä½“ç‰¹ç‚¹å¦‚ä¸‹ã€‚</p>
<ul>
<li>Q ä»ç„¶ä¿æŒåŸæ¥çš„å¤´æ•°ï¼Œå³çº¿æ€§å˜æ¢ä¹‹åï¼Œä¾ç„¶å¯¹Qè¿›è¡Œåˆ‡åˆ†ï¼ˆåƒMHAä¸€æ ·ï¼‰ï¼Œæ¯ä¸ªæ³¨æ„åŠ›å¤´å•ç‹¬ä¿ç•™äº†è‡ªå·±çš„Qå‘é‡ã€‚</li>
<li>K å’Œ V åªæœ‰ä¸€ä¸ªå¤´ï¼Œå…·ä½“æ˜¯åœ¨çº¿æ€§å˜æ¢æ—¶ç›´æ¥æŠŠKå’ŒVçš„ç»´åº¦é™åˆ°äº†<span class="math inline">\(d_{head}\)</span>ï¼Œè€Œä¸æ˜¯åšåˆ‡åˆ†å˜å°ã€‚</li>
<li>æ‰€æœ‰çš„ Q å¤´å…±äº«è¿™ä¸ªK å’Œ V å¤´ï¼Œæˆ–è€…å¯ä»¥è®¤ä¸ºæ˜¯ k, vçŸ©é˜µå‚æ•°å…±äº«ã€‚å®ç°ä¸Šï¼Œå°±æ˜¯æ”¹ä¸€ä¸‹çº¿æ€§å˜æ¢çŸ©é˜µï¼Œç„¶åæŠŠ Kã€V çš„å¤„ç†ä»åˆ‡åˆ†å˜æˆå¤åˆ¶ã€‚</li>
<li>æ‰€æœ‰Qå¤´éƒ½ä½¿ç”¨è¿™ä¸ªç›¸åŒçš„Kå¤´è®¡ç®—å®ƒä»¬çš„æ³¨æ„åŠ›åˆ†æ•°ï¼Œå¹¶ä¸”æ‰€æœ‰å¤´çš„è¾“å‡ºéƒ½ä½¿ç”¨ç›¸åŒçš„Vå¤´è®¡ç®—ï¼ˆä½†æ³¨æ„åŠ›åˆ†æ•°ä¸åŒï¼‰ã€‚</li>
<li>æœ€åå°†æ¯ä¸ªå¤´è®¡ç®—çš„ç»“æœæ‹¼æ¥èµ·æ¥ã€‚</li>
</ul>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413202855494-1240701416.jpg" alt="" loading="lazy"></p>
<h3 id="22-å®ç°">2.2 å®ç°</h3>
<p>æˆ‘ä»¬è¿˜æ˜¯ä»¥llm-foundryä¸ºä¾‹æ¥è¿›è¡Œåˆ†æã€‚</p>
<h4 id="121-ç²¾ç®€ç‰ˆ">1.2.1 ç²¾ç®€ç‰ˆ</h4>
<p>æˆ‘ä»¬å…ˆç»™å‡ºMHAå’ŒMQAçš„ç²¾ç®€ç‰ˆå¯¹æ¯”ã€‚è¿™é‡Œå‡è®¾ x (tensor): (batch, hidden_state, d_model) ï¼Œæ¯”å¦‚ (1,  512, 768) ã€‚å¯ä»¥çœ‹åˆ°ï¼Œä¸¤è€…ä¸»è¦ä¸åŒåœ¨äºï¼š</p>
<ul>
<li>WçŸ©é˜µçš„ç»´åº¦ä¸åŒã€‚</li>
<li>QKVåˆ‡åˆ†æ–¹å¼ä¸åŒã€‚</li>
</ul>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413202905108-376926323.jpg" alt="" loading="lazy"></p>
<p>ä»ä»£ç ä¸­å¯ä»¥çœ‹åˆ°ï¼Œå¯¹äºMQAæ¥è¯´ï¼Œæ‰€æœ‰å¤´ä¹‹é—´å…±äº«ä¸€ä»½ key å’Œ value çš„å‚æ•°ï¼Œä½†æ˜¯å¦‚ä½•å°†è¿™ 1 ä»½å‚æ•°åŒæ—¶è®© 8 ä¸ªå¤´éƒ½ä½¿ç”¨å‘¢ï¼Ÿåœ¨scaled_multihead_dot_product_attention()å‡½æ•°çš„ä»£ç ä¼šä½¿ç”¨çŸ©é˜µä¹˜æ³• matmulæ¥å¹¿æ’­ï¼Œä½¿å¾—æ¯ä¸ªå¤´éƒ½ä¹˜ä»¥è¿™åŒä¸€ä¸ªå¼ é‡ï¼Œä»¥æ­¤æ¥å®ç°å‚æ•°å…±äº«ã€‚</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413202915394-772507533.jpg" alt="" loading="lazy"></p>
<p>MQAçš„æ€»ä½“æµç¨‹å¯ä»¥å‚è§ä¸‹å›¾ã€‚</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413202923601-315033145.jpg" alt="" loading="lazy"></p>
<h4 id="122-å®Œæ•´ç‰ˆ">1.2.2 å®Œæ•´ç‰ˆ</h4>
<p>æˆ‘ä»¬å†ç»™å‡ºå®Œæ•´ç‰ˆæœ¬ä»£ç ã€‚</p>
<pre><code class="language-python">class MultiQueryAttention(nn.Module):
    """Multi-Query self attention.

    Using torch or triton attention implemetation enables user to also use
    additive bias.
    """

    def __init__(
        self,
        d_model: int,
        n_heads: int,
        attn_impl: str = 'triton',
        clip_qkv: Optional[float] = None,
        qk_ln: bool = False,
        softmax_scale: Optional[float] = None,
        attn_pdrop: float = 0.0,
        low_precision_layernorm: bool = False,
        verbose: int = 0,
        device: Optional[str] = None,
    ):
        super().__init__()

        self.attn_impl = attn_impl
        self.clip_qkv = clip_qkv
        self.qk_ln = qk_ln

        self.d_model = d_model
        self.n_heads = n_heads
        self.head_dim = d_model // n_heads
        self.softmax_scale = softmax_scale
        if self.softmax_scale is None:
            self.softmax_scale = 1 / math.sqrt(self.head_dim)
        self.attn_dropout_p = attn_pdrop

        # NOTE: if we ever want to make attn TensorParallel, I'm pretty sure we'll
        # want to split Wqkv into Wq and Wkv where Wq can be TensorParallel but
        # Wkv shouldn't be TensorParallel
        # - vchiley
        self.Wqkv = nn.Linear(
            d_model,
            d_model + 2 * self.head_dim,
            device=device,
        )
        # for param init fn; enables shape based init of fused layers
        fuse_splits = (d_model, d_model + self.head_dim)
        self.Wqkv._fused = (0, fuse_splits)  # type: ignore

        if self.qk_ln:
            layernorm_class = LPLayerNorm if low_precision_layernorm else nn.LayerNorm
            self.q_ln = layernorm_class(d_model, device=device)
            self.k_ln = layernorm_class(self.head_dim, device=device)

        if self.attn_impl == 'flash':
            self.attn_fn = flash_attn_fn
        elif self.attn_impl == 'triton':
            self.attn_fn = triton_flash_attn_fn
        elif self.attn_impl == 'torch':
            self.attn_fn = scaled_multihead_dot_product_attention
        else:
            raise ValueError(f'{attn_impl=} is an invalid setting.')

        self.out_proj = nn.Linear(self.d_model, self.d_model, device=device)
        self.out_proj._is_residual = True  # type: ignore

    def forward(
        self,
        x,
        past_key_value=None,
        attn_bias=None,
        attention_mask=None,
        is_causal=True,
        needs_weights=False,
    ):
        qkv = self.Wqkv(x)

        if self.clip_qkv:
            qkv.clamp_(min=-self.clip_qkv, max=self.clip_qkv)

        query, key, value = qkv.split(
            [self.d_model, self.head_dim, self.head_dim], dim=2)

        key_padding_mask = attention_mask

        if self.qk_ln:
            # Applying layernorm to qk
            dtype = query.dtype
            query = self.q_ln(query).to(dtype)
            key = self.k_ln(key).to(dtype)

        context, attn_weights, past_key_value = self.attn_fn(
            query,
            key,
            value,
            self.n_heads,
            past_key_value=past_key_value,
            softmax_scale=self.softmax_scale,
            attn_bias=attn_bias,
            key_padding_mask=key_padding_mask,
            is_causal=is_causal,
            dropout_p=self.attn_dropout_p,
            training=self.training,
            needs_weights=needs_weights,
            multiquery=True,
        )

        return self.out_proj(context), attn_weights, past_key_value
</code></pre>
<h3 id="23-æ•ˆæœ">2.3 æ•ˆæœ</h3>
<h4 id="231-å†…å­˜">2.3.1 å†…å­˜</h4>
<p>MQAéœ€è¦ç¼“å­˜çš„ Kã€V å€¼ä»æ‰€æœ‰å¤´å˜æˆä¸€ä¸ªå¤´ï¼Œå› æ­¤ç›´æ¥å°†KV Cacheå‡å°‘åˆ°äº†åŸæ¥çš„1/â„ã€‚MHAçš„å•ä¸ªTokenéœ€è¦ä¿å­˜çš„KVæ•°ï¼ˆ <span class="math inline">\(2âˆ—ğ‘™âˆ—ğ‘›_â„\)</span> ï¼‰ï¼Œè€ŒMQAå‡å°‘åˆ°äº†ï¼ˆ 2Ã—ğ‘™ ï¼‰ä¸ªï¼Œå³æ¯ä¸€å±‚å…±äº«ä½¿ç”¨ä¸€ä¸ª ğ‘˜ å‘é‡å’Œä¸€ä¸ª ğ‘£ å‘é‡ã€‚</p>
<h4 id="232-é€Ÿåº¦">2.3.2 é€Ÿåº¦</h4>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413202935498-1452899910.jpg" alt="" loading="lazy"></p>
<p>è®ºæ–‡ä½œè€…åšäº†ä¸€ç³»åˆ—æµ‹è¯•ï¼Œå…·ä½“å‚è§ä¸Šè¡¨ï¼ˆæ•°å€¼æ˜¯å¹³å‡ç”Ÿæˆæ¯ä¸ªtokenæ‰€éœ€è¦çš„æ¯«ç§’æ•°ï¼‰ã€‚éœ€è¦æ³¨æ„çš„å‡ ä¸ªç‚¹æ˜¯ï¼š</p>
<ol>
<li>è®­ç»ƒé€Ÿåº¦å‡ ä¹æ²¡æœ‰å˜åŒ–ã€‚</li>
<li>æ¨ç†æ—¶é—´å’ŒBeam searchæ—¶é—´éƒ½æ˜¾è‘—ç¼©çŸ­ã€‚</li>
<li>æ¨ç†é€Ÿåº¦ä¸­ï¼Œencoderçš„æ¨ç†é€Ÿåº¦åŸºæœ¬ä¸å˜ï¼Œdecoderçš„æ¨ç†å¿«äº†å¾ˆå¤šã€‚</li>
</ol>
<p>è™½ç„¶MQAåªæœ‰ä¸€ç»„KVå¤´ï¼Œä½†å®é™…ä¸ŠMQAæ˜¯è¯»å–è¿™ç»„KVå¤´ä¹‹åï¼Œå¤åˆ¶ç»™æ‰€æœ‰Qå¤´ä½¿ç”¨ï¼Œå› æ­¤æŒ‰ç…§é“ç†æ¥è¯´ï¼ŒMQAåªèƒ½é™ä½æ˜¾å­˜çš„ä½¿ç”¨ï¼Œè¿ç®—é‡å¹¶æ²¡æœ‰å‡å°‘ï¼Œä¸ºå•¥é€Ÿåº¦èƒ½æé«˜è¿™ä¹ˆå¤šï¼Ÿå…¶å®ä¸»è¦æ”¶ç›Šæ˜¯å› ä¸ºé™ä½äº†KV Cacheè€Œå¸¦æ¥è®¡ç®—é‡çš„å‡å°‘ï¼Œå…·ä½“å¦‚ä¸‹ï¼š</p>
<ul>
<li>KV-Cacheç©ºé—´å ç”¨é™ä½ã€‚å› ä¸ºå¤´æ•°é‡çš„å‡å°‘ï¼Œæ‰€ä»¥éœ€è¦å­˜å‚¨åœ¨GPUå†…å­˜ä¸­çš„å¼ é‡ä¹Ÿå‡å°‘äº†ï¼ˆå‡è®¾ä¹‹å‰è¦å­˜å‚¨32ä¸ªå¤´çš„KV Cacheï¼Œç›®å‰åªéœ€è¦å­˜å‚¨1ä¸ªå¤´çš„KV Cacheï¼‰ã€‚èŠ‚çœçš„ç©ºé—´å¯ä»¥ç”¨æ¥å¢åŠ æ‰¹æ¬¡å¤§å°ï¼Œæå‡ååï¼Œä»è€Œæé«˜æ•ˆç‡ï¼ˆè™½ç„¶å•æ¡è¯·æ±‚çš„æ€»æ—¶å»¶ä¼šå¢åŠ ï¼Œä½†æœåŠ¡çš„æ€»ååé‡æ˜¯æ˜æ˜¾å¢åŠ ï¼‰ã€‚</li>
<li>é™ä½å†…å­˜è¯»å–æ¨¡å‹æƒé‡çš„æ—¶é—´å¼€é”€ã€‚å› ä¸ºå¤´æ•°é‡çš„å‡å°‘ï¼Œæ‰€ä»¥å‡å°‘äº†ä»æ˜¾å­˜ä¸­è¯»å–çš„æ•°æ®é‡ï¼Œå‡å°‘äº†è®¡ç®—å•å…ƒçš„ç­‰å¾…æ—¶é—´ï¼Œä»å†…å­˜å¯†é›†å‹è¶‹è¿‘äºè®¡ç®—å¯†é›†å‹ã€‚å¦å¤–ï¼ŒåŒä¸€ä¸ª Request ä¸­çš„ä¸åŒ Head å¯ä»¥å…±äº«ï¼Œè¿™å°±æå‡äº† Qã€K å’Œ V çš„ Attention è®¡ç®—çš„ç®—æœ¯å¼ºåº¦ã€‚</li>
</ul>
<h4 id="233-è¡¨å¾èƒ½åŠ›">2.3.3 è¡¨å¾èƒ½åŠ›</h4>
<p>å› ä¸ºç›®å‰åªæœ‰ä¸€ä¸ªå…±äº«çš„KVå¤´ï¼Œæ‰€ä»¥åŸå…ˆå¤šQKVå¤´å¸¦æ¥çš„æ³¨æ„åŠ›å·®å¼‚éƒ½éœ€è¦ä»…ä»…ä¾é å¤šä¸ªQå¤´å®Œæˆï¼Œè¿™æ ·é™åˆ¶äº†æ¨¡å‹çš„è¡¨å¾èƒ½åŠ›ï¼Œå› æ­¤MQAè™½ç„¶èƒ½å¥½åœ°æ”¯æŒæ¨ç†åŠ é€Ÿï¼Œä½†æ˜¯åœ¨æ•ˆæœä¸Šæ¯”MHAç•¥å·®ã€‚ä¸ºäº†å¼¥è¡¥å…±äº«KVå¸¦æ¥çš„å‚æ•°é‡å‡å°‘ï¼Œäººä»¬å¾€å¾€ä¼šç›¸åº”åœ°å¢å¤§FFN/GLUçš„è§„æ¨¡ï¼Œä»¥æ­¤æ¥ç»´æŒæ¨¡å‹æ€»å‚æ•°é‡çš„ä¸å˜ï¼Œè¿›è€Œå¼¥è¡¥ä¸€éƒ¨åˆ†æ•ˆæœæŸå¤±ã€‚</p>
<p>å¦å¤–éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œç”±äºMQAå’ŒGQAæ”¹å˜äº†æ³¨æ„åŠ›æœºåˆ¶çš„ç»“æ„ï¼Œå› æ­¤æ¨¡å‹é€šå¸¸éœ€è¦ä»è®­ç»ƒå¼€å§‹å°±æ”¯æŒ MQAæˆ–è€…GQA ã€‚å¦‚æœæ¨¡å‹å·²ç»è®­ç»ƒå¥½äº†ï¼Œå°†KV Cacheå¼ºè¡Œæ¢æˆè¿™ä¸¤ä¸ªæ–¹æ³•ï¼Œæ•ˆæœä¼šå¾ˆå·®ï¼Œå› æ­¤éœ€è¦éœ€è¦å€ŸåŠ©å¾®è°ƒæ¥å¼¥è¡¥ã€‚æœ‰ç ”ç©¶è¡¨æ˜éœ€è¦çº¦ 5% çš„åŸå§‹è®­ç»ƒæ•°æ®é‡å°±å¯ä»¥è¾¾åˆ°ä¸é”™çš„æ•ˆæœã€‚</p>
<h4 id="233-é€šä¿¡">2.3.3 é€šä¿¡</h4>
<p>åœ¨å¤šå¡å¹¶è¡Œæƒ…å†µä¸‹ï¼ŒMQAå‡å°‘äº†è®¿å­˜ï¼Œä½†æ˜¯å¢åŠ äº†å¹¶è¡Œé€šä¿¡å¼€é”€ã€‚å› ä¸ºKå’ŒVå¼ é‡åœ¨æ‰€æœ‰å¤´éƒ¨ä¹‹é—´å…±äº«ï¼Œæ¯ä¸ªGPUä¸Šéƒ½éœ€è¦æœ‰è‡ªå·±çš„å¤‡ä»½ã€‚ä¸ä¸‹å›¾(a)ä¸­MHAå¹¶è¡Œç­–ç•¥ç›¸æ¯”ï¼ŒMQAéœ€è¦ä½¿ç”¨all-to-allå¯¹è¿›è¡Œè¾“å…¥è¾“å‡ºæ¿€æ´»å¼ é‡reshardingï¼Œä»è€Œäº§ç”Ÿé¢å¤–çš„é€šä¿¡æˆæœ¬ã€‚å…·ä½“å¦‚ä¸‹å›¾(b)æ‰€ç¤ºã€‚å¦å¤–ï¼Œå› ä¸ºæ¯ä¸ªå¡ä¸Šéƒ½æœ‰å¤‡ä»½ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´MQAçš„å†…å­˜æˆæœ¬èŠ‚çœå°†ä¼šä¸§å¤±ã€‚</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413202947399-47036567.jpg" alt="" loading="lazy"></p>
<h2 id="0x03-gqa">0x03 GQA</h2>
<p>å¯¹äºæ›´å¤§çš„æ¨¡å‹è€Œè¨€ï¼Œå½»åº•å‰¥ç¦»æ‰€æœ‰å¤´è¿‡äºæ¿€è¿›ã€‚ä¾‹å¦‚ï¼Œç›¸æ¯”ä»32å‡å°‘åˆ°1ï¼Œå°†å¤´æ•°ä»64å‡å°‘åˆ°1åœ¨æ¨¡å‹çš„è¡¨å¾èƒ½åŠ›ä¸Šæ˜¯ä¸€ä¸ªæ›´å¤§çš„å‰Šå‡ã€‚è€Œä¸”æ ¹æ®GQAè®ºæ–‡çš„å®éªŒè¯´ï¼ŒMQAè™½ç„¶â€drasticallyâ€œæå‡äº†decoderä¸­çš„æ¨ç†æ€§èƒ½ï¼Œä½†è¿™æ ·åšä¼šå¸¦æ¥ç”Ÿæˆè´¨é‡çš„æ˜¾è‘—ä¸‹é™ä»¥åŠå¯¼è‡´è®­ç»ƒä¸ç¨³å®šã€‚æ‰€ä»¥ä¸ºäº†åœ¨ç‰ºç‰²æ›´å°æ€§èƒ½å‰æä¸‹åŠ é€Ÿï¼ŒGQAåº”è¿è€Œç”Ÿã€‚</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203000275-1984809569.jpg" alt="" loading="lazy"></p>
<p>ä¸Šå›¾æ˜¾ç¤ºäº†ä»2022å¹´åˆ°2024å¹´æœŸé—´è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æ¼”å˜è¶‹åŠ¿ã€‚å¯ä»¥çœ‹å‡ºï¼ŒMHA æ­£åœ¨é€æ­¥æ·˜æ±°ï¼Œå¹¶è¢« GQA æ‰€å–ä»£ã€‚</p>
<h3 id="31-æ¦‚å¿µ">3.1 æ¦‚å¿µ</h3>
<p>GQAï¼ˆGrouped Query Attention/åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›æœºåˆ¶ï¼‰ç”±è®ºæ–‡â€œGQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpointsâ€æå‡ºï¼Œå®ƒé€šè¿‡åˆ†ç»„æŸ¥è¯¢çš„æ–¹å¼æ¥æé«˜ä¿¡æ¯å¤„ç†çš„æ•ˆç‡å’Œæ•ˆæœã€‚GQAçš„æ ¸å¿ƒæ”¹è¿›ç‚¹åœ¨äºï¼šè®© å¤šä¸ª Query å…±äº«å°‘é‡çš„ Key å’Œ Valueï¼Œå‡å°‘è®¡ç®—å¼€é”€ï¼Œå¹¶é€šè¿‡ åˆ†ç»„æœºåˆ¶ï¼ˆGrouping Mechanismï¼‰ è¿›è¡Œæ›´é«˜æ•ˆçš„è®¡ç®—ã€‚</p>
<p>GQAæ˜¯MHAå’ŒMQA ä¹‹é—´çš„æ³›åŒ–ï¼Œæˆ–è€…è¯´æ˜¯ä»‹äºMHAå’ŒMQAä¹‹é—´çš„æŠ˜ä¸­æ–¹æ¡ˆã€‚MHA æœ‰ H ä¸ª queryã€key å’Œ value å¤´ã€‚MQA åœ¨æ‰€æœ‰ query å¤´ä¸­å…±äº«å•ä¸ª key å’Œ value å¤´ã€‚è€ŒGQAä¸å†è®©æ‰€æœ‰æŸ¥è¯¢å¤´å…±äº«ç›¸åŒçš„å”¯ä¸€KVå¤´ï¼Œè€Œæ˜¯å°†æ‰€æœ‰çš„Qå¤´åˆ†æˆgç»„ï¼ŒåŒä¸€ç»„çš„Qå¤´å…±äº«ä¸€ä¸ªKå¤´ï¼ˆKey Headï¼‰å’Œä¸€ä¸ªVå¤´ï¼ˆValue Headï¼‰ã€‚</p>
<p>ä¸‹å›¾ä¸­4ä¸ªQå¤´ï¼ˆQuery Headsï¼‰è¢«åˆ†æˆ2ç»„ï¼Œæ¯ä¸ªç»„åŒ…å«2ä¸ªQå¤´ï¼Œæ¯ç»„åˆå¯¹åº”ä¸€ä¸ªKå¤´ï¼Œä¸€ä¸ªVå¤´ã€‚å›¾ä¸Šæ ‡å·1ä¸ºä¸€ç»„ï¼Œæ ‡å·2ä¸ºå¦å¤–ä¸€ç»„ã€‚</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203151535-621628829.jpg" alt="" loading="lazy"></p>
<p>ä¸‹å›¾æ˜¯GQAçš„å…¬å¼å’Œæµç¨‹ã€‚</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203200605-1893859507.jpg" alt="" loading="lazy"></p>
<p>è‹ç¥åˆ™æŒ‡å‡ºï¼ŒGQAå…¶å®æ˜¯ä¸€ä¸ª<span class="math inline">\(x_i\)</span>çš„ä½ç§©æŠ•å½±ã€‚</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203210935-534637606.jpg" alt="" loading="lazy"></p>
<h3 id="32-æ¶æ„æ¯”å¯¹">3.2 æ¶æ„æ¯”å¯¹</h3>
<p>GQAå·§å¦™åœ°ç»“åˆäº†MHAå’ŒMQAçš„å…ƒç´ ï¼Œåˆ›é€ äº†ä¸€ç§æ›´æœ‰æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶ã€‚GQAæ˜¯åœ¨MHAå’ŒMQAä¹‹é—´è¿›è¡Œæ’å€¼ï¼Œå°†KVå¤´çš„æ•°é‡ä»<span class="math inline">\(n\_heads\)</span>å‡å°‘åˆ°<span class="math inline">\(1&lt;g&lt;n\_heads\)</span>ï¼Œè€Œä¸æ˜¯å°†å¤´æ•°ä»<span class="math inline">\(n\_heads\)</span>å‡å°‘åˆ°1ä¸ªKVå¤´ã€‚è¿™ä¸ªæ–°å‚æ•°gå¯ä»¥è¿™ä¹ˆè¡¨è¾¾ï¼š</p>
<p></p><div class="math display">\[g = \frac{æ³¨æ„åŠ›å¤´æ•°}{KVå¤´æ•°}
\]</div><p></p><p>å¼•å…¥è¿™ä¸ªå‚æ•°gä¹‹åï¼ŒGQAå°±æ„æˆäº†ä¸€ä¸ªç»Ÿä¸€è§†è§’ã€‚åœ¨è¿™ä¸ªè§†è§’ä¸‹ï¼ŒMHAå’ŒMQAéƒ½æ˜¯GQAçš„ç‰¹æ®Šæƒ…å†µï¼ˆåˆ†åˆ«å¯¹åº”äºg=1å’Œ g=<span class="math inline">\(n\_heads\)</span>ï¼‰ã€‚</p>
<ul>
<li>g = 1ï¼šç›¸å½“äºMQAï¼Œå³åœ¨æ‰€æœ‰ N ä¸ªå¤´ä¸­ä½¿ç”¨å…±äº«çš„é”®å’Œå€¼æŠ•å½±ã€‚</li>
<li>g = æ³¨æ„åŠ›å¤´æ•°ï¼šç›¸å½“äºMHAã€‚</li>
</ul>
<p>GQAèƒ½æ›´é¡ºç•…åœ°åœ¨æ¨¡å‹å‡†ç¡®æ€§/KVç¼“å­˜å¤§å°ï¼ˆä¸æ—¶å»¶å’Œååé‡æœ‰å…³ï¼‰ï¼Œå’ŒMHAä»¥åŠMQAè¿™ä¸¤ä¸ªæç«¯ç”¨ä¾‹é—´è¿›è¡Œæƒè¡¡ã€‚æˆ–è€…è¯´ï¼ŒGQAæ¯ä¸ªç»„å†…æ˜¯ä¸€ä¸ªå°å‹çš„MQAï¼Œè€Œç»„é—´æ˜¯ä¼ ç»Ÿçš„MHAã€‚</p>
<p>å¤§å‹æ¨¡å‹çš„MHAä¼šå°†å•ä¸ªé”®å’Œå€¼å¤´å¤åˆ¶åˆ°æ¨¡å‹åˆ†åŒºçš„æ•°é‡ï¼ŒMQAä»£è¡¨äº†å†…å­˜å¸¦å®½å’Œå®¹é‡çš„æ›´å¤§å¹…åº¦çš„å‰Šå‡ï¼Œè€ŒGQA ä½¿æˆ‘ä»¬èƒ½å¤Ÿéšç€æ¨¡å‹å¤§å°çš„å¢åŠ ä¿æŒå¸¦å®½å’Œå®¹é‡çš„ç›¸åŒæ¯”ä¾‹ä¸‹é™ï¼Œå¯ä»¥ä¸ºè¾ƒå¤§çš„æ¨¡å‹æä¾›ç‰¹åˆ«å¥½çš„æƒè¡¡ã€‚GQA æ¶ˆé™¤äº†è¿™ç§åˆ†ç‰‡å¸¦æ¥çš„æµªè´¹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é¢„è®¡ GQA å°†ä¸ºè¾ƒå¤§çš„æ¨¡å‹æä¾›ç‰¹åˆ«å¥½çš„æƒè¡¡ã€‚</p>
<p>ä¸‹å›¾åˆ™ç»™å‡ºäº†ä¸‰è€…æ¶æ„ä¸Šçš„åŒºåˆ«ã€‚</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203221768-1617176553.jpg" alt="" loading="lazy"></p>
<h3 id="33-å®ç°">3.3 å®ç°</h3>
<p>åœ¨ç›®å‰å¤§éƒ¨åˆ†ä¸»æµè®­æ¨æ¡†æ¶æˆ–ç®—æ³•ï¼Œéƒ½å·²ç»æ”¯æŒMQA/GQAï¼Œæ¯”å¦‚FlashAttentionä¸­ï¼Œä¹Ÿæ”¯æŒMQAå’ŒGQAã€‚å¯¹äºMQAå’ŒGQAçš„æƒ…å½¢ï¼ŒFlashAttentioné‡‡ç”¨Indexingçš„æ–¹å¼ï¼Œè€Œä¸æ˜¯ç›´æ¥å¤åˆ¶å¤šä»½KV Headçš„å†…å®¹åˆ°æ˜¾å­˜ç„¶åå†è¿›è¡Œè®¡ç®—ã€‚Indexingï¼Œå³é€šè¿‡ä¼ å…¥KV/KV Headç´¢å¼•åˆ°Kernelä¸­ï¼Œç„¶åè®¡ç®—å†…å­˜åœ°å€ï¼Œç›´æ¥ä»å†…å­˜ä¸­è¯»å–KVã€‚</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203230087-2060532392.jpg" alt="" loading="lazy"></p>
<p>é¡ºå¸¦ä¸€æï¼ŒGQA ä¸åº”ç”¨äºç¼–ç å™¨è‡ªæ³¨æ„åŠ›å±‚ï¼Œç¼–ç å™¨è¡¨ç¤ºæ˜¯å¹¶è¡Œè®¡ç®—çš„ï¼Œå› æ­¤å†…å­˜å¸¦å®½é€šå¸¸ä¸æ˜¯ä¸»è¦ç“¶é¢ˆã€‚</p>
<p>æˆ‘ä»¬ä½¿ç”¨llama3çš„ä»£ç æ¥è¿›è¡Œåˆ†æã€‚é¦–å…ˆç»™å‡ºåˆ©äºå­¦ä¹ çš„ç²¾ç®€ç‰ˆï¼Œç„¶åç»™å‡ºå®Œæ•´ç‰ˆã€‚</p>
<h4 id="331-ç²¾ç®€ç‰ˆ">3.3.1 ç²¾ç®€ç‰ˆ</h4>
<p>ä¸ºäº†æ›´å¥½çš„åˆ†æï¼Œæˆ‘ä»¬ç»™å‡ºç²¾ç®€ç‰ˆä»£ç å¦‚ä¸‹ã€‚</p>
<p>æœ¬æ¥ MHA ä¸­ Query, Key, Value çš„çŸ©é˜µçš„å¤§å°ä¸º (batch_size, n_head, seq_length, hidden_size)ã€‚è€Œ GQA ä¸­ Query çš„å¤§å°ä¿æŒä¸å˜ï¼ŒKey, Value çš„çŸ©é˜µçš„å¤§å°å˜ä¸º (batch_size, n_head / group_size, seq_length, hidden_size)ã€‚å³ï¼Œåœ¨GQAä¸­ï¼Œkeyå’Œvalueéƒ½è¦æ¯”queryå°groupå€ã€‚ä¸ºäº†åœ¨åç»­åšçŸ©é˜µä¹˜æ³•ï¼Œä¸€èˆ¬æœ‰ä¸¤ç§åšæ³•ï¼š</p>
<ul>
<li>
<p>åˆ©äºå¹¿æ’­æœºåˆ¶æŠŠQKVçš„å½¢çŠ¶è¿›è¡Œè°ƒæ•´ï¼Œå³Query : (batch_size, n_head / group_size, group_size, seq_length, hidden_size)ï¼ŒKey :  (batch_size, n_head / group_size, 1,  seq_length, hidden_size)ï¼ŒValue :  (batch_size, n_head / group_size, 1,  seq_length, hidden_size)ã€‚ä½†æ˜¯è¿™æ ·éœ€è¦åšå¹¿æ’­å’Œæœ€ç»ˆåˆå¹¶çš„å¤„ç†ï¼Œè¦å¯¹ MHA çš„ä»£ç è¿›è¡Œå¤šå¤„ä¿®æ”¹ã€‚</p>
</li>
<li>
<p>æŠŠGQAæ‹“å±•åˆ°MHAå†è¿›è¡Œè®¡ç®—ï¼Œå³å…ˆæŠŠ<code>key</code>å’Œ<code>value</code>çš„<code>head</code>åˆ©ç”¨expandæ‰©å±•å¼ é‡åˆ°å’Œ<code>query</code>ç›¸åŒçš„ç»´åº¦ï¼Œç„¶åè¿›è¡Œè®¡ç®—ã€‚</p>
</li>
</ul>
<pre><code class="language-python">class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads
        model_parallel_size = fs_init.get_model_parallel_world_size()
        self.n_local_heads = args.n_heads // model_parallel_size
        self.n_local_kv_heads = self.n_kv_heads // model_parallel_size
        self.n_rep = self.n_local_heads // self.n_local_kv_heads # è®¾å®šç»„æ•°ç›®
        self.head_dim = args.dim // args.n_heads

        # ç”¨self.n_kv_heads * self.head_dimåˆå§‹åŒ–ï¼Œå½“n_kv_headså°äºn_headsæ—¶ï¼Œå‚æ•°é‡å˜å°‘
        self.wq = ColumnParallelLinear(args.dim, args.n_heads * self.head_dim,)
        self.wk = ColumnParallelLinear(args.dim, self.n_kv_heads * self.head_dim,)
        self.wv = ColumnParallelLinear(args.dim, self.n_kv_heads * self.head_dim,)
        self.wo = RowParallelLinear(args.n_heads * self.head_dim, args.dim,)

        self.cache_k = torch.zeros((args.max_batch_size, args.max_seq_len,
                self.n_local_kv_heads, self.head_dim,)).cuda()
        self.cache_v = torch.zeros((args.max_batch_size, args.max_seq_len,
                self.n_local_kv_heads, self.head_dim,)).cuda()

    def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor,
        mask: Optional[torch.Tensor],
    ):
        bsz, seqlen, _ = x.shape
        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)
        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)
        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)
        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)

        self.cache_k = self.cache_k.to(xq)
        self.cache_v = self.cache_v.to(xq)
        self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk
        self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv
   
        keys = self.cache_k[:bsz, : start_pos + seqlen]
        values = self.cache_v[:bsz, : start_pos + seqlen]

        '''
        self.n_rep = q_heads // kv_heads
        queryå¤´æ•°å¤§äºKVçš„å¤´æ•°ï¼Œä¸€å¯¹KVå¯¹åº”å¤šä¸ªqueryï¼Œéœ€è¦æŠŠæ¯ä¸ªKVå¤åˆ¶n_repä»½ï¼Œè¿™æ ·ç¬¬2ä¸ªç»´åº¦å°±å’Œqä¸€æ ·äº†
        å³ï¼Œnum_key_value_headså°±æ˜¯q_heads // kv_heads
        repeat_kvæ–¹æ³•å°†hidden statesä»(batch, num_key_value_heads, seqlen, head_dim) å˜æˆ (batch, num_attention_heads, seqlen, head_dim)ï¼Œç›¸å½“äºæ˜¯å¤åˆ¶äº†self.num_key_value_groupsä»½
        '''            
        # repeat k/v heads if n_kv_heads &lt; n_heads
        keys = repeat_kv(keys, self.n_rep
        )  # (bs, cache_len + seqlen, n_local_heads, head_dim)
        values = repeat_kv(values, self.n_rep
        )  # (bs, cache_len + seqlen, n_local_heads, head_dim)

        xq = xq.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)
        keys = keys.transpose(1, 2)  # (bs, n_local_heads, cache_len + seqlen, head_dim)
        values = values.transpose(1, 2)  # (bs, n_local_heads, cache_len + seqlen, head_dim)
        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)
        if mask is not None:
            scores = scores + mask  # (bs, n_local_heads, seqlen, cache_len + seqlen)     
        scores = F.softmax(scores.float(), dim=-1).type_as(xq)
        output = torch.matmul(scores, values)  # (bs, n_local_heads, seqlen, head_dim)
        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)
        return self.wo(output)
</code></pre>
<p>repeat_kv()å‡½æ•°ä»£ç å¦‚ä¸‹ã€‚ä¸ºä»€ä¹ˆè¦ç”¨expandä¹‹åå†reshapeè€Œä¸èƒ½ç›´æ¥ç”¨tensorè‡ªå¸¦çš„repeatï¼Ÿå› ä¸ºä½¿ç”¨expand()å‡½æ•°å¯ä»¥åœ¨è¿ç®—çš„æ—¶å€™èŠ‚çœå¾ˆå¤šæ˜¾å­˜ã€‚</p>
<ul>
<li><code>expand</code> æ–¹æ³•ç”¨äºå¯¹å¼ é‡è¿›è¡Œæ‰©å±•ï¼Œä½†ä¸å®é™…åˆ†é…æ–°çš„å†…å­˜ã€‚å®ƒè¿”å›çš„å¼ é‡ä¸åŸå§‹å¼ é‡å…±äº«ç›¸åŒçš„æ•°æ®</li>
<li><code>repeat</code> æ–¹æ³•é€šè¿‡å®é™…å¤åˆ¶æ•°æ®æ¥æ‰©å±•å¼ é‡ã€‚å®ƒè¿”å›çš„æ–°å¼ é‡ä¸ä¸åŸå§‹å¼ é‡å…±äº«æ•°æ®ï¼Œæ‰©å±•åçš„å¼ é‡å ç”¨äº†æ›´å¤šçš„å†…å­˜ã€‚</li>
</ul>
<pre><code class="language-python"># å®šä¹‰è¾“å…¥xï¼Œ n_repæ˜¯éœ€è¦é‡å¤çš„æ¬¡æ•°ï¼Œåœ¨è¿™é‡Œä¸€èˆ¬æ˜¯ç»„æ•°
def repeat_kv(x: torch.Tensor, n_rep: int) -&gt; torch.Tensor:
    """torch.repeat_interleave(x, dim=2, repeats=n_rep)"""
    bs, slen, n_kv_heads, head_dim = x.shape
    if n_rep == 1:
        return x
    return (
        # ç¬¬4ç»´è¿›è¡Œæ‰©ç»´ï¼Œæ‰©å±•æˆ5ç»´
        x[:, :, :, None, :] 
         # first we expand x to (bs, seq_len, head, group, head_dim)ï¼Œå³ç¬¬4ç»´ä»1æ‰©å±•ä¸ºn_rep
        .expand(bs, slen, n_kv_heads, n_rep, head_dim)  # è¿›è¡Œå¹¿æ’­ï¼Œkï¼Œvå‘é‡å…±äº«
         # reshape make head -&gt; head * groupï¼Œç¼©æˆ4ç»´ï¼Œå³æŠŠç¬¬3ç»´ä»n_kv_headsæ‰©å±•n_repä»½
         # è¿™æ ·ç¬¬3ä¸ªç»´åº¦å°±å’Œqä¸€æ ·äº†
        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)
    )
</code></pre>
<h4 id="332-å®Œæ•´ç‰ˆ">3.3.2 å®Œæ•´ç‰ˆ</h4>
<p>å®Œæ•´ç‰ˆä»£ç å¦‚ä¸‹ã€‚</p>
<pre><code class="language-python">class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads
        model_parallel_size = fs_init.get_model_parallel_world_size()
        self.n_local_heads = args.n_heads // model_parallel_size
        self.n_local_kv_heads = self.n_kv_heads // model_parallel_size
        self.n_rep = self.n_local_heads // self.n_local_kv_heads
        self.head_dim = args.dim // args.n_heads

        self.wq = ColumnParallelLinear(
            args.dim,
            args.n_heads * self.head_dim,
            bias=False,
            gather_output=False,
            init_method=lambda x: x,
        )
        self.wk = ColumnParallelLinear(
            args.dim,
            self.n_kv_heads * self.head_dim,
            bias=False,
            gather_output=False,
            init_method=lambda x: x,
        )
        self.wv = ColumnParallelLinear(
            args.dim,
            self.n_kv_heads * self.head_dim,
            bias=False,
            gather_output=False,
            init_method=lambda x: x,
        )
        self.wo = RowParallelLinear(
            args.n_heads * self.head_dim,
            args.dim,
            bias=False,
            input_is_parallel=True,
            init_method=lambda x: x,
        )

        self.cache_k = torch.zeros(
            (
                args.max_batch_size,
                args.max_seq_len,
                self.n_local_kv_heads,
                self.head_dim,
            )
        ).cuda()
        self.cache_v = torch.zeros(
            (
                args.max_batch_size,
                args.max_seq_len,
                self.n_local_kv_heads,
                self.head_dim,
            )
        ).cuda()

    def forward(
        self,
        x: torch.Tensor,
        start_pos: int,
        freqs_cis: torch.Tensor,
        mask: Optional[torch.Tensor],
    ):
        bsz, seqlen, _ = x.shape
        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)

        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)
        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)
        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)

        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)

        self.cache_k = self.cache_k.to(xq)
        self.cache_v = self.cache_v.to(xq)

        self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk
        self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv

        keys = self.cache_k[:bsz, : start_pos + seqlen]
        values = self.cache_v[:bsz, : start_pos + seqlen]

        # repeat k/v heads if n_kv_heads &lt; n_heads
        keys = repeat_kv(
            keys, self.n_rep
        )  # (bs, cache_len + seqlen, n_local_heads, head_dim)
        values = repeat_kv(
            values, self.n_rep
        )  # (bs, cache_len + seqlen, n_local_heads, head_dim)

        xq = xq.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)
        keys = keys.transpose(1, 2)  # (bs, n_local_heads, cache_len + seqlen, head_dim)
        values = values.transpose(
            1, 2
        )  # (bs, n_local_heads, cache_len + seqlen, head_dim)
        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)
        if mask is not None:
            scores = scores + mask  # (bs, n_local_heads, seqlen, cache_len + seqlen)
        scores = F.softmax(scores.float(), dim=-1).type_as(xq)
        output = torch.matmul(scores, values)  # (bs, n_local_heads, seqlen, head_dim)
        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)
        return self.wo(output)
</code></pre>
<p>å¦å¤–ï¼Œå¯¹äºMQAå’ŒGQAçš„è§£ç é˜¶æ®µï¼Œä¸€ç§å¸¸ç”¨çš„ä¼˜åŒ–æŠ€å·§æ˜¯æŠŠå…±ç”¨ä¸€ä¸ªKVå¤´çš„æ‰€æœ‰QOå¤´ï¼Œä¸queryçš„è¡Œæ•°èåˆï¼ˆå› ä¸ºä»–ä»¬éœ€è¦è·Ÿç›¸åŒçš„KV-CacheåšAttentionè®¡ç®—ï¼‰ã€‚è¿™æ ·çš„æ•ˆæœæ˜¯å¢åŠ äº†æœ‰æ•ˆçš„è¡Œæ•°ï¼Œå¢åŠ äº†ç®—å­å¯†åº¦ï¼Œè‡ªå›å½’è§£ç é˜¶æ®µè™½ç„¶è¯´æŸ¥è¯¢çš„é•¿åº¦æ˜¯1ï¼Œä½†æ˜¯ç»è¿‡Head Groupèåˆä¹‹åï¼Œæœ‰æ•ˆè¡Œæ•°å¢å¤§åˆ° <span class="math inline">\(H_{QO}/H_{KV}\)</span>ã€‚</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203246431-805086330.jpg" alt="" loading="lazy"></p>
<h3 id="34-æ•ˆæœ">3.4 æ•ˆæœ</h3>
<h4 id="341-å†…å­˜">3.4.1 å†…å­˜</h4>
<p>GQAåœ¨æ¨ç†é˜¶æ®µå¯ä»¥æ˜¾è‘—é™ä½ KV Cache çš„å¤§å°ï¼Œä¸ºæ›´å¤§çš„ Batch Size æä¾›äº†ç©ºé—´ï¼Œå¯ä»¥è¿›ä¸€æ­¥æå‡ååã€‚</p>
<p>åœ¨MHAä¸‹ï¼Œå¯¹äºæ‰€æœ‰è¾“å…¥æ‰¹æ¬¡å’Œåºåˆ—ä¸­çš„æ¯ä¸ªtokenï¼ŒKVç¼“å­˜çš„æ€»å¤§å°å¯ä»¥ç”¨ä»¥ä¸‹å…¬å¼è¡¨ç¤ºï¼š</p>
<p></p><div class="math display">\[2 \times B \times L \times H \times D \times N
\]</div><p></p><ul>
<li>Bä»£è¡¨batch sizeï¼Œ</li>
<li>Lä»£è¡¨æ€»åºåˆ—é•¿åº¦ï¼Œsequence lengthï¼ˆè¾“å…¥åºåˆ—+è¾“å‡ºåºåˆ—ï¼Œæˆ–è€…è¯´æ˜¯æç¤º + å®Œæˆéƒ¨åˆ†ï¼‰ï¼Œ</li>
<li>Hä»£è¡¨number of headï¼Œ</li>
<li>Dä»£è¡¨size of headï¼Œæ¯ä¸ªheadçš„ç»´åº¦ã€‚</li>
<li>Nä»£è¡¨å±‚æ•°</li>
</ul>
<p>åœ¨MQAä¸‹ï¼Œæ¯ä¸ªtokençš„å¯¹åº”ä¸ºï¼š</p>
<p></p><div class="math display">\[2 \times B \times L\times D \times N
\]</div><p></p><p>åœ¨GQAä¸‹ï¼Œæ¯ä¸ªtokençš„å¯¹åº”ä¸ºï¼š</p>
<p></p><div class="math display">\[2 \times B \times L\times G \times D\times N
\]</div><p></p><p>å…·ä½“æ¯”å¯¹ä¹Ÿå¯ä»¥å‚è€ƒä¸‹å›¾ï¼Œå…¶ä¸­ g æ˜¯KVå¤´çš„ç»„æ•°ï¼ˆ<span class="math inline">\(ğ‘›_â„/ğ‘”\)</span>ä¸ªHead å…±äº«ä¸€ä¸ªKVï¼‰ï¼Œh æ˜¯æŸ¥è¯¢çš„å¤´æ•° ï¼Œ<span class="math inline">\(d_k\)</span>æ˜¯å¤´ç»´åº¦ï¼Œl æ˜¯å±‚æ•°ï¼Œs æ˜¯åºåˆ—é•¿åº¦ï¼Œb æ˜¯batch sizeã€‚</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203256198-1237072538.jpg" alt="" loading="lazy"></p>
<p>GQAå’ŒMQAåœ¨GPU ä¸Šçš„å®ç°å¸¦æ¥çš„æ”¶ç›Šæ¥ä¸»è¦è‡ªäºKV cache çš„å‡å°‘ï¼Œä»è€Œèƒ½æ”¾ä¸‹æ›´å¤šçš„tokenã€‚ä½†æ˜¯ï¼ŒGQAå’ŒMQAçš„æ€§èƒ½å®¹æ˜“å—åˆ°å¹¶è¡Œç­–ç•¥çš„å½±å“ã€‚å¦‚æœGQA kernelåœ¨Q headç»´åº¦åšå¹¶è¡Œï¼ˆä¸€ä¸ªQ headæ˜¯ä¸€ä¸ªblockï¼‰ï¼Œåˆ™ä¼šå¯¼è‡´å…±äº«ä¸€ä¸ªKV head çš„block è¢«è°ƒåº¦åœ¨ä¸åŒçš„SMä¸Šï¼Œæ¯ä¸ªSM éƒ½ä¼šå¯¹åŒä¸€ä»½KV head åšé‡å¤åŠ è½½ã€‚åˆ™å†…å­˜å‡å°‘çš„æ”¶ç›Šä¼šå¤§å¤§é™ä½ã€‚å¦å¤–ï¼ŒåŠ è½½ KV æ˜¯MHA å’Œ GQA çš„ç“¶é¢ˆã€‚å› æ­¤éœ€è¦å‡å°‘Q headçš„å¹¶è¡Œåº¦ã€‚</p>
<h4 id="342-é€Ÿåº¦">3.4.2 é€Ÿåº¦</h4>
<p>GQAå¹¶æ²¡æœ‰é™ä½Attentionçš„è®¡ç®—é‡ï¼ˆFLOPsï¼‰ï¼Œå› ä¸ºKeyã€Valueæ˜ å°„çŸ©é˜µä¼šä»¥å¹¿æ’­å˜é‡çš„å½¢å¼æ‹“å±•åˆ°å’ŒMHAå’Œä¸€æ ·ï¼Œå› æ­¤è®¡ç®—é‡ä¸å˜ï¼Œåªæ˜¯Keyã€Valueå‚æ•°å…±äº«ã€‚ä½†æ˜¯ï¼Œå› ä¸ºGQA å°†æŸ¥è¯¢çŸ©é˜µ Q åˆ†æˆå¤šä¸ªç»„ï¼Œæ¯ä¸ªç»„åˆ†åˆ«è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°å’ŒåŠ æƒæ±‚å’Œã€‚è¿™æ ·ä¸€æ¥ï¼Œæ¯ä¸ªæ³¨æ„åŠ›å¤´åªéœ€è¦è®¡ç®—ä¸€éƒ¨åˆ†æŸ¥è¯¢çš„æ³¨æ„åŠ›åˆ†æ•°ï¼Œä»è€Œé™ä½äº†è®¡ç®—å¤æ‚åº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†é•¿åºåˆ—æ—¶ã€‚æ‰€ä»¥ï¼Œè™½ç„¶GQA çš„ QKV è®¡ç®—é‡æ²¡æœ‰å‡å°‘ï¼Œä½†æ˜¯é€Ÿåº¦å¾—åˆ°äº†å¾ˆå¤§æé«˜ï¼Œé€Ÿåº¦æé«˜çš„åŸå› å’ŒMQAç›¸åŒã€‚</p>
<h4 id="343-è¡¨å¾èƒ½åŠ›">3.4.3 è¡¨å¾èƒ½åŠ›</h4>
<p>GQAæ—¢ä¿ç•™äº†å¤šå¤´æ³¨æ„åŠ›çš„ä¸€å®šè¡¨è¾¾èƒ½åŠ›ï¼Œåˆé€šè¿‡å‡å°‘å†…å­˜è®¿é—®å‹åŠ›æ¥åŠ é€Ÿæ¨ç†é€Ÿåº¦ã€‚</p>
<p>è®ºæ–‡â€GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpointsâ€œç ”ç©¶äº†æ¨¡å‹çš„ç²¾åº¦å’Œæ¨ç†æ•ˆç‡ã€‚è®ºæ–‡ä½œè€…é‡‡ç”¨T5æ¨¡å‹ä½œä¸ºç ”ç©¶å¯¹è±¡ï¼Œæ¨¡å‹ç‰ˆæœ¬é‡‡ç”¨T5-Largeå’ŒT5-XXLã€‚ä¸‹å›¾ä¸­ï¼Œæ¨ªè½´ä»£è¡¨å¹³å‡æ¯æ¡æ ·æœ¬çš„æ¨ç†è€—æ—¶ï¼Œè¶Šå¤§ä»£è¡¨å»¶è¿Ÿè¶Šå¤§ï¼Œçºµè½´ä»£è¡¨åœ¨ä¼—å¤šæ•°æ®é›†ä¸Šçš„è¯„ä»·å¾—åˆ†ï¼Œè¶Šå¤§ä»£è¡¨å¾—åˆ†è¶Šé«˜ã€‚</p>
<p>ä¸‹å›¾è¡¨æ˜ï¼ŒMQAç•¥å¾®æŸå¤±äº†æ¨¡å‹ç²¾åº¦ï¼Œä½†æ˜¯ç¡®å®èƒ½å¤Ÿå¤§å¹…é™ä½æ¨ç†å¼€é”€ï¼Œè€Œå¦‚æœé€‰æ‹©äº†åˆé€‚çš„åˆ†ç»„æ•°ï¼ŒGQAèƒ½å¤Ÿä¸¤è€…çš†å¾—ã€‚GQAçš„è¡¨å¾èƒ½åŠ›æ˜¾è‘—é«˜äºMQAï¼Œå‡ ä¹è·ŸMHAä¸€è‡´ï¼ˆGQAè¿˜æ˜¯æœ‰å¯èƒ½å¯¼è‡´ç²¾åº¦çš„æŸå¤±ï¼‰ï¼Œè€Œä¸”æ¨ç†é€Ÿåº¦ä¸ŠGQAè·ŸMQAçš„åŒºåˆ«ä¸å¤§ï¼Œæ¯”èµ·MHAä¾æ—§æœ‰æ˜¾è‘—æå‡ã€‚å…¶ä¸­ï¼ŒGQAçš„åˆ†ç»„æ•°æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œç»„æ•°è¶Šå¤§è¶Šæ¥è¿‘MHAï¼Œæ¨ç†å»¶è¿Ÿè¶Šå¤§ï¼ŒåŒæ—¶æ¨¡å‹ç²¾åº¦ä¹Ÿè¶Šé«˜ã€‚å¦å¤–ï¼Œä¹Ÿå¯ä»¥å¢åŠ æ¨¡å‹æ·±åº¦æ¥ç¼“è§£æ¨¡å‹æ•ˆæœçš„ä¸‹é™ã€‚</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203304079-756174985.jpg" alt="" loading="lazy"></p>
<h3 id="35-è½¬æ¢">3.5 è½¬æ¢</h3>
<p>è™½ç„¶æœ€æ–°çš„æ¨¡å‹åŸºæœ¬éƒ½åœ¨é¢„è®­ç»ƒé˜¶æ®µé»˜è®¤é‡‡ç”¨ GQAï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥æ€è€ƒä¸‹ï¼Œå¦‚ä½•å°†å·²ç»è®­ç»ƒå¥½çš„MHAç»“æ„çš„æ¨¡å‹è½¬æ¢æˆMQAæˆ–è€…GQAï¼Ÿ</p>
<h4 id="351-å¹³å‡æ± åŒ–">3.5.1 å¹³å‡æ± åŒ–</h4>
<p>å¦‚æœæ˜¯ä»å·²æœ‰çš„ multi-head model å¼€å§‹ç»§ç»­è®­ç»ƒ multi-query model (Uptraining)ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹MHAçš„å¤´è¿›è¡Œåˆ†ç»„ï¼Œé€šè¿‡å¯¹è¯¥ç»„ä¸­æ‰€æœ‰åŸå§‹å¤´è¿›è¡Œå¹³å‡æ± åŒ–ï¼ˆmean poolï¼‰æ¥æ„å»ºæ¯ä¸ªç»„çš„é”®å’Œå€¼å¤´ï¼Œç„¶åç»§ç»­è¿›è¡Œé¢„è®­ç»ƒå³å¯ã€‚å®éªŒè¯æ˜mean poolçš„æ˜ å°„æ•ˆæœå¥½äºé€‰åˆ™ç¬¬ä¸€ä¸ªheadæˆ–è€…ä»»æ„åˆå§‹åŒ–ã€‚äººä»¬æŠŠè¿™ä¸ªè®­ç»ƒè¿‡ç¨‹å«åšuptrainingã€‚</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203312168-1095087345.jpg" alt="" loading="lazy"></p>
<p>å…·ä½“å‚è€ƒä»£ç å¦‚ä¸‹ã€‚</p>
<pre><code class="language-python">import torch.nn as nn

n_heads=4
n_kv_heads=2
hidden_size=3
group = n_heads // n_kv_heads
k_proj = nn.Linear(hidden_size, n_heads) 

# mean poolæ“ä½œ
k_proj_4d = k_proj.weight.data.unsqueeze(dim=0).unsqueeze(dim=0)
pool=nn.AvgPool2d(kernel_size=(group,1))
pool_out = pool(k_proj_4d).squeeze(dim=0).squeeze(dim=0)

k_proj_gaq = nn.Linear(hidden_size, n_kv_heads)
k_proj_gaq.weight.data = pool_out
</code></pre>
<h4 id="352-åŸºäºæ©ç ">3.5.2 åŸºäºæ©ç </h4>
<p>è®ºæ–‡â€Align Attention Heads Before Merging Them: An Effective Way for Converting MHA to GQAâ€œæå‡ºäº†ä¸€ç§ä½æˆæœ¬æ–¹æ³•ï¼Œå¯å°† MHA æ¨¡å‹æŒ‰ä»»æ„ KV Head å‹ç¼©æ¯”ä¿®å‰ªä¸º GQA æ¨¡å‹ã€‚è¯¥æ–¹æ³•åŸºäº <span class="math inline">\(L_0\)</span> æ©ç é€æ­¥å‰”é™¤å†—ä½™å‚æ•°ã€‚æ­¤å¤–ï¼Œåœ¨ä¸æ”¹å˜æ¨¡å‹çš„å‰æä¸‹ï¼Œå¯¹æ³¨æ„åŠ›å¤´æ–½åŠ æ­£äº¤å˜æ¢ï¼Œä»¥åœ¨ä¿®å‰ªè®­ç»ƒå‰æå‡ Attention Head é—´çš„ç›¸ä¼¼åº¦ï¼Œä»è€Œè¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚</p>
<p>å…·ä½“æ–¹æ¡ˆåˆ†ä¸ºå¦‚ä¸‹å‡ æ­¥ï¼šç½‘ç»œè½¬æ¢ï¼›è¿›è¡Œåˆ†ç»„ï¼›å‰ªæè®­ç»ƒã€‚</p>
<h5 id="ç½‘ç»œè½¬æ¢">ç½‘ç»œè½¬æ¢</h5>
<p>è¿™ä¸€æ­¥æ˜¯åœ¨å‰ªæè®­ç»ƒä¹‹å‰ï¼Œå¯¹æ¨¡å‹è¿›è¡Œè½¬æ¢ã€‚å…·ä½“çš„è¿‡ç¨‹å¤§æ¦‚ä¸ºï¼š</p>
<ul>
<li>ä½¿ç”¨éƒ¨åˆ† C4 çš„è®­ç»ƒé›†æ¥æ”¶é›†ç›¸åº”çš„ KV Cacheï¼Œè¿™æ ·æ‰èƒ½å¯¹KV Cacheè¿›è¡Œæ›´æœ‰æ•ˆçš„åˆ†æã€‚</li>
<li>åŸºäºä½™å¼¦ç›¸ä¼¼æ€§æˆ–è€…æ¬§æ°è·ç¦»ï¼Œè®¡ç®—æœ€ä¼˜çš„æ­£äº¤çŸ©é˜µã€‚</li>
</ul>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203324257-2099402571.jpg" alt="" loading="lazy"></p>
<ul>
<li>å°†è®¡ç®—å¾—åˆ°çš„æ­£äº¤çŸ©é˜µèåˆåˆ°å¯¹åº”çš„ Qã€Kã€V æŠ•å½±çŸ©é˜µä¸­ï¼Œä¿è¯è®¡ç®—ä¸å˜æ€§ã€‚å› ä¸ºRoPEçš„åŸå› ï¼Œæ‰€ä»¥å¯¹äº Q å’Œ K çš„æŠ•å½±çŸ©é˜µï¼Œåˆ†åˆ«åœ¨å­ç©ºé—´åº”ç”¨æ­£äº¤å˜æ¢ã€‚</li>
</ul>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203333938-279084538.jpg" alt="" loading="lazy"></p>
<p>é€šè¿‡æ­£äº¤å˜æ¢ï¼Œå¯ä»¥ä½¿å¾—åŒä¸€ç»„å†…ä¸åŒ Attention Head åœ¨ç‰¹å¾ç©ºé—´ä¸­æ›´åŠ æ¥è¿‘ï¼Œä»è€Œåœ¨åç»­çš„å‰ªæè®­ç»ƒè¿‡ç¨‹ä¸­æ›´å®¹æ˜“æ‰¾åˆ°åˆé€‚çš„å‚æ•°å…±äº«æ–¹å¼ï¼Œæé«˜æ¨¡å‹çš„å‹ç¼©æ•ˆæœå’Œæ€§èƒ½ã€‚</p>
<h5 id="æ‰¾åˆ°æ›´å¥½çš„åˆ†ç»„æ–¹æ³•">æ‰¾åˆ°æ›´å¥½çš„åˆ†ç»„æ–¹æ³•</h5>
<p>åœ¨è·å–äº†æ¯å¯¹ Attention Head ä¹‹é—´çš„ç›¸ä¼¼åº¦è¯„åˆ†åï¼Œå¯ä¾æ®è¿™äº›è¯„åˆ†å¯¹ Attention Head è¿›è¡Œé‡æ–°åˆ†ç»„ã€‚å•ä¸ªç»„çš„ç›¸ä¼¼åº¦è¯„åˆ†æ˜¯è¯¥ç»„å†…æ¯å¯¹ Attention Head ä¹‹é—´ç›¸ä¼¼åº¦è¯„åˆ†çš„æ€»å’Œï¼Œè€Œæ¯ç§åˆ†ç»„ç»“æœçš„æ€»ç›¸ä¼¼åº¦è¯„åˆ†åˆ™æ˜¯æ‰€æœ‰ç»„ç›¸ä¼¼åº¦è¯„åˆ†çš„ç´¯åŠ ã€‚ç®—æ³•çš„ç›®æ ‡æ˜¯æ‰¾åˆ°å¾—åˆ†æœ€é«˜çš„åˆ†ç»„æ–¹æ³•ã€‚</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203344477-728978103.jpg" alt="" loading="lazy"></p>
<p>åˆç†çš„åˆ†ç»„æ–¹å¼å¯ä»¥ä½¿å¾—åŒä¸€ç»„å†…çš„ Attention Head åœ¨ç‰¹å¾ç©ºé—´ä¸­æ›´åŠ ç›¸ä¼¼ï¼Œä»è€Œåœ¨å‰ªææ—¶æ›´å®¹æ˜“æ‰¾åˆ°åˆé€‚çš„å‚æ•°å…±äº«æ–¹å¼ï¼Œæé«˜æ¨¡å‹çš„å‹ç¼©æ•ˆæœå’Œæ€§èƒ½ã€‚</p>
<h5 id="å‰ªæè®­ç»ƒ">å‰ªæè®­ç»ƒ</h5>
<p>æ­¤æ­¥éª¤ä¼šé€šè¿‡å‰ªæè®­ç»ƒï¼Œé€æ­¥å°†åŸå§‹çš„ KV Head è½¬ç§»åˆ°æ–°çš„ KV Head ä¸Šï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚å¦‚ä¸‹å›¾ æ‰€ç¤ºï¼Œå…·ä½“è¿‡ç¨‹åŒ…æ‹¬ï¼š</p>
<ul>
<li>æ·»åŠ æ–°çš„æŠ•å½±çŸ©é˜µï¼šåœ¨æ¯ç»„å†…ä½¿ç”¨ Mean Pooling  åˆå§‹åŒ–æ–°çš„æŠ•å½±çŸ©é˜µã€‚</li>
<li>åº”ç”¨ <span class="math inline">\(L_0\)</span> æ©ç ï¼šå¼•å…¥ <span class="math inline">\(L_0\)</span> æ©ç æ¥æ§åˆ¶åŸå§‹ KV Head å’Œæ–° KV Head ä¹‹é—´çš„è½¬æ¢ã€‚åˆå§‹æ—¶ï¼Œæ©ç å€¼ä¸º 1ï¼Œè¡¨ç¤ºä½¿ç”¨åŸå§‹ KV Headï¼›åœ¨å‰ªæè¿‡ç¨‹ä¸­ï¼Œé€æ­¥å°†æ©ç å€¼çº¦æŸä¸º 0ï¼ˆè¡¨ç¤ºä½¿ç”¨æ–°çš„ KV Headï¼‰ã€‚</li>
<li>çŸ¥è¯†è’¸é¦ï¼šä½¿ç”¨ KL æŸå¤±å’Œ BiLD æŸå¤±ï¼Œé¼“åŠ±å­¦ç”Ÿæ¨¡å‹ä¸æ•™å¸ˆæ¨¡å‹çš„è¾“å‡ºå¯¹é½ï¼Œä»è€Œä¿æŒæ¨¡å‹æ€§èƒ½ã€‚</li>
</ul>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203351548-1094520492.jpg" alt="" loading="lazy"></p>
<h3 id="36-ä¼˜åŒ–">3.6 ä¼˜åŒ–</h3>
<p>è®ºæ–‡â€œA Survey on Large Language Model Acceleration based on KV Cache Managementâ€ç»™å‡ºäº†MQAã€GQAä»¥åŠå…¶æ”¹è¿›æ–¹æ¡ˆçš„æ€»ç»“ï¼Œå…·ä½“å‚è§ä¸‹å›¾ã€‚</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203400480-1535487143.jpg" alt="" loading="lazy"></p>
<p>å‡ ç§æ”¹è¿›æ–¹æ¡ˆå…·ä½“å¦‚ä¸‹ã€‚</p>
<ul>
<li>åŠ æƒGQAï¼ˆWeighted GQAï¼‰ä¸ºæ¯ä¸ªé”®å’Œå€¼å¤´å¼•å…¥äº†é¢å¤–çš„å¯è®­ç»ƒæƒé‡ï¼Œè¿™äº›æƒé‡å¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„GQAæ¨¡å‹ä¸­ã€‚é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è°ƒæ•´æƒé‡ï¼Œå®ƒå¯ä»¥åœ¨ä¸å¢åŠ é¢å¤–æ¨ç†å¼€é”€çš„æƒ…å†µä¸‹æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ul>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203412355-945813625.jpg" alt="" loading="lazy"></p>
<ul>
<li>AsymGQAé€šè¿‡æå‡ºæ¿€æ´»é€šçŸ¥åˆå¹¶ç­–ç•¥ï¼ˆactivationinformed merging strategyï¼‰æ¥æ‰©å±•GQAã€‚AsymGQAä¸æ˜¯é€šè¿‡ç»Ÿä¸€èšç±»ï¼ˆuniform clusteringï¼‰å¯¹å¤´è¿›è¡Œåˆ†ç»„ï¼Œè€Œæ˜¯æ ¹æ®è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¿€æ´»ç›¸ä¼¼æ€§æ¥åŠ¨æ€ç¡®å®šå¦‚ä½•åˆ†ç»„ï¼Œå¹¶æ„å»ºä¸å¯¹ç§°çš„ç»„ï¼Œä»è€Œå®ç°æ›´å¥½çš„ä¼˜åŒ–å’Œæ³›åŒ–ã€‚</li>
</ul>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203419675-2098432331.jpg" alt="" loading="lazy"></p>
<ul>
<li>QCQAåˆ©ç”¨è¿›åŒ–ï¼ˆevolutionaryï¼‰ç®—æ³•æ¥è¯†åˆ«GQAçš„æœ€ä½³æŸ¥è¯¢å¤´åˆ†ç»„ï¼Œè¯¥ç®—æ³•ç”±ä¸€ä¸ªè®¡ç®—é«˜æ•ˆçš„é€‚åº”åº¦ï¼ˆcomputationally efficient fitnessï¼‰å‡½æ•°æŒ‡å¯¼ï¼Œè¯¥å‡½æ•°åˆ©ç”¨æƒé‡å…±äº«ï¼ˆweight-sharingï¼‰è¯¯å·®å’ŒKVç¼“å­˜æ¥è¯„ä¼°æ–‡æœ¬ç”Ÿæˆè´¨é‡å’Œå†…å­˜å®¹é‡ã€‚</li>
</ul>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203428957-701936971.jpg" alt="" loading="lazy"></p>
<ul>
<li>KDGQAè®¤ä¸ºï¼ŒGQAçš„è®¸å¤šå˜ä½“é‡‡ç”¨å›ºå®šçš„åˆ†ç»„ç­–ç•¥ï¼Œå› æ­¤ç¼ºä¹å¯¹è®­ç»ƒè¿‡ç¨‹ä¸­é”®å€¼äº¤äº’æ¼”å˜çš„åŠ¨æ€é€‚åº”æ€§ã€‚ä»–ä»¬çš„Dynamic Key-Driven GQAé€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨key head normsè‡ªé€‚åº”åœ°åˆ†ç»„æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œä»è€Œäº§ç”Ÿäº†ä¸€ç§çµæ´»çš„ç­–ç•¥æ¥å°†æŸ¥è¯¢å¤´åˆ†ç»„å¹¶æé«˜æ€§èƒ½ã€‚</li>
</ul>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203437938-572317046.jpg" alt="" loading="lazy"></p>
<ul>
<li>GQKVAæå‡ºäº†åˆ†ç»„ç­–ç•¥ï¼Œå¹¶æå‡ºäº†ä¸€ç§é€šç”¨çš„æŸ¥è¯¢ã€é”®å’Œå€¼åˆ†ç»„æœºåˆ¶ã€‚å®ƒé¦–å…ˆä»‹ç»äº†MKVAå’ŒGKVAï¼Œå…¶ä¸­é”®å’Œå€¼è¢«åˆ†ç»„ä»¥å…±äº«åŒä¸€ä¸ªæŸ¥è¯¢ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œè¯¥è®ºæ–‡æå‡ºä½¿ç”¨GQKVAå°†æŸ¥è¯¢å’Œé”®å€¼å¯¹åˆ†å¼€åˆ†ç»„ã€‚é€šå¸¸ï¼ŒæŸ¥è¯¢è¢«åˆ’åˆ†ä¸º<span class="math inline">\(g_q\)</span>ç»„ï¼Œé”®å€¼è¢«åˆ’åˆ†ä¸º<span class="math inline">\(g_{kv}\)</span>ç»„ï¼ŒæŸ¥è¯¢å’Œé”®å€¼å¯¹çš„æ¯ä¸ªç»„åˆéƒ½ä¼šä½¿ç”¨ç‚¹ç§¯æ³¨æ„åŠ›è¿›è¡Œäº¤äº’ã€‚è¿™å¯¼è‡´<span class="math inline">\(g_qÃ—g_{kv}\)</span>äº§ç”Ÿä¸åŒçš„è¾“å‡ºã€‚GQKVAåœ¨æŸ¥è¯¢ã€é”®å’Œå€¼ä¸Šæ¨å¹¿äº†ä¸åŒçš„ç»„ç­–ç•¥ï¼Œå¹¶ä¿æŒäº†è‰¯å¥½çš„è®¡ç®—æ•ˆç‡å’Œä¸MHAç›¸å½“çš„æ€§èƒ½ã€‚ä¸‹å›¾å±•ç¤ºäº†åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­å¯¹æŸ¥è¯¢ã€é”®å’Œå€¼è¿›è¡Œåˆ†ç»„çš„å„ç§ç­–ç•¥ï¼ŒåŒ…æ‹¬Vanilla MHAã€MQAã€GQAã€MKVAã€GKVAå’ŒGQKVAã€‚</li>
</ul>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203445313-1209222049.jpg" alt="" loading="lazy"></p>
<h2 id="0xff-å‚è€ƒ">0xFF å‚è€ƒ</h2>
<p><a href="https://arxiv.org/pdf/2305.13245.pdf" target="_blank" rel="noopener nofollow">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpointsarxiv.org/pdf/2305.13245.pdf</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/634236135" target="_blank" rel="noopener nofollow">ã€LLM åŠ é€ŸæŠ€å·§ã€‘Muti Query Attention å’Œ Attention with Linear Biasï¼ˆé™„æºç ï¼‰</a> <a href="https://www.zhihu.com/people/who-u" target="_blank" rel="noopener nofollow">ä½•æ</a></p>
<p><a href="https://github.com/meta-llama/llama3" target="_blank" rel="noopener nofollow">https://github.com/meta-llama/llama3</a></p>
<p><a href="https://mp.weixin.qq.com/s?__biz=MzUzOTgwNDMzOQ==&amp;mid=2247502844&amp;idx=1&amp;sn=067165341bbfeba775fa4301a9d1095e&amp;chksm=fb47e670f9237047b5c94d0657212d88d53640c54f05784fbce8bba9e086deb11d5ad0f7e84c&amp;mpshare=1&amp;scene=1&amp;srcid=0228ATe44326dTsGNtdpPNl2&amp;sharer_shareinfo=f6c641fcb6b4d0caf449ce78dc907e41&amp;sharer_shareinfo_first=f6c641fcb6b4d0caf449ce78dc907e41#rd" target="_blank" rel="noopener nofollow">2ä¸‡å­—é•¿æ–‡ï¼ä¸€æ–‡äº†è§£Attentionï¼Œä»MHAåˆ°DeepSeek MLAï¼Œå¤§é‡å›¾è§£ï¼Œéå¸¸è¯¦ç»†ï¼</a>  ShuYini [AINLPer](javascript:void(0)ğŸ˜‰</p>
<p><a href="https://zhuanlan.zhihu.com/p/700588653" target="_blank" rel="noopener nofollow">ä»MHAã€MQAã€GQAåˆ°MLA</a>  <a href="https://www.zhihu.com/people/su-jian-lin-22" target="_blank" rel="noopener nofollow">è‹å‰‘æ—</a></p>
<p><a href="https://mp.weixin.qq.com/s?__biz=MzUyOTA5OTcwMg==&amp;mid=2247487324&amp;idx=1&amp;sn=cc79e02b124278f3d07067c355390abc&amp;chksm=fb845df40f104ca547107d0dfaeb7424bc67e3c49d30961eecf5b98673df8c93794121484b21&amp;mpshare=1&amp;scene=1&amp;srcid=0213jOpb0yYHTU7MRuvxFq7x&amp;sharer_shareinfo=669a538ad10f3b46605953dd65cb7500&amp;sharer_shareinfo_first=669a538ad10f3b46605953dd65cb7500#rd" target="_blank" rel="noopener nofollow">é˜¿é‡Œä¸€é¢ä»£ç é¢˜ï¼š"å®ç°ä¸€ä¸‹ GQA"</a>  çœ‹å›¾å­¦ [çœ‹å›¾å­¦](javascript:void(0)ğŸ˜‰</p>
<p><a href="https://mp.weixin.qq.com/s?__biz=Mzk0ODU3MjcxNA==&amp;mid=2247488906&amp;idx=1&amp;sn=e2038e8b907c9b703354481ed0193af9&amp;chksm=c2437164308b699ffe83a81842f17e611351c867b5b51fc3ee58bd6e7628a5b0c7716e52c26e&amp;mpshare=1&amp;scene=1&amp;srcid=0115XLnq4kZjRAdZ8tI4DzYD&amp;sharer_shareinfo=6f5890ca41e9b97d037f34b4c9518848&amp;sharer_shareinfo_first=6f5890ca41e9b97d037f34b4c9518848#rd" target="_blank" rel="noopener nofollow">MHA -&gt; GQAï¼šæå‡ LLM æ¨ç†æ•ˆç‡</a> AIé—²è°ˆ [AIé—²è°ˆ](javascript:void(0)ğŸ˜‰</p>
<p><a href="https://arxiv.org/abs/2412.20677" target="_blank" rel="noopener nofollow"> Align Attention Heads Before Merging Them: An Effective Way for Converting MHA to GQA </a></p>
<p><a href="https://arxiv.org/pdf/2501.01005" target="_blank" rel="noopener nofollow">FLASHINFER: EFFICIENT AND CUSTOMIZABLE ATTENTION ENGINE FOR LLM INFERENCE SERVING</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/25920092499" target="_blank" rel="noopener nofollow">FlashInferä¸­DeepSeek MLAçš„å†…æ ¸è®¾è®¡</a>  <a href="https://www.zhihu.com/people/wuyu-98-91" target="_blank" rel="noopener nofollow">yzh119</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/660715870" target="_blank" rel="noopener nofollow">å¤§æ¨¡å‹å¹¶è¡Œæ¨ç†çš„å¤ªç¥–é•¿æ‹³ï¼šè§£è¯»Jeff Deanç½²åMLSys 23æ°å‡ºè®ºæ–‡</a>  æ–¹ä½³ç‘</p>
<p><a href="https://zhuanlan.zhihu.com/p/708776013" target="_blank" rel="noopener nofollow">ç”±GQAæ€§èƒ½æ•°æ®å¼‚å¸¸å¼•å‘çš„å¯¹MHAï¼ŒGQAï¼ŒMQA åœ¨GPUä¸Šçš„æ„Ÿæ€§åˆ†æ</a> <a href="https://www.zhihu.com/people/fly-zhai" target="_blank" rel="noopener nofollow">ä»£ç æ¬è¿å·¥</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/22590523172" target="_blank" rel="noopener nofollow">MHA-&gt;MQA-&gt;GQA-&gt;MLAçš„æ¼”è¿›ä¹‹è·¯</a>   <a href="https://www.zhihu.com/people/ai-81-85-59" target="_blank" rel="noopener nofollow">å‡å¦‚ç»™æˆ‘ä¸€åªAI</a></p>
<p>Y. Chen, C. Zhang, X. Gao, R. D. Mullins, G. A. Constantinides, and Y. Zhao, â€œOptimised Grouped-Query Attention Mechanism for Transformers,â€ in Workshop on Efficient Systems for Foundation Models II @ ICML2024, Jul. 2024. [Online]. Available: <a href="https://openreview.net/forum?id=13MMghY6Kh" target="_blank" rel="noopener nofollow">https://openreview.net/forum?id=13MMghY6Kh</a></p>
<p>S. S. Chinnakonduru and A. Mohapatra, â€œWeighted Grouped Query Attention in Transformers,â€ Jul. 2024. [Online]. Available: <a href="http://arxiv.org/abs/2407.10855" target="_blank" rel="noopener nofollow">http://arxiv.org/abs/2407.10855</a></p>
<p>V. Joshi, P. Laddha, S. Sinha, O. J. Omer, and S. Subramoney, â€œQCQA: Quality and Capacity-aware grouped Query Attention,â€ Jun. 2024. [Online]. Available: <a href="http://arxiv.org/abs/2406.10247" target="_blank" rel="noopener nofollow">http://arxiv.org/abs/2406.10247</a></p>
<p>Z. Khan, M. Khaquan, O. Tafveez, B. Samiwala, and A. A. Raza, â€œBeyond Uniform Query Distribution: Key-Driven Grouped Query Attention,â€ Aug. 2024. [Online]. Available: <a href="http://arxiv.org/abs/2408.08454" target="_blank" rel="noopener nofollow">http://arxiv.org/abs/2408.08454</a></p>
<p>F. Javadi, W. Ahmed, H. Hajimolahoseini, F. Ataiefard, M. Hassanpour, S. Asani, A. Wen, O. M. Awad, K. Liu, and Y. Liu, â€œGQKVA: Efficient Pre-training of Transformers by Grouping Queries, Keys, and Values,â€ Dec. 2023. [Online]. Available: <a href="http://arxiv.org/abs/2311.03426" target="_blank" rel="noopener nofollow">http://arxiv.org/abs/2311.03426</a></p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.016326014032407406" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-04-14 20:06">2025-04-14 20:06</span>&nbsp;
<a href="https://www.cnblogs.com/rossiXYZ">ç½—è¥¿çš„æ€è€ƒ</a>&nbsp;
é˜…è¯»(<span id="post_view_count">0</span>)&nbsp;
è¯„è®º(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18823734);return false;">æ”¶è—</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18823734', targetLink: 'https://www.cnblogs.com/rossiXYZ/p/18823734', title: 'æ¢ç§˜Transformerç³»åˆ—ä¹‹ï¼ˆ27ï¼‰--- MQA &amp;amp; GQA' })">ä¸¾æŠ¥</a>
</div>
        