
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/lizhongzheng/p/18814913" title="发布于 2025-04-08 16:20">
    <span role="heading" aria-level="2">深度学习--个人总结</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h3 id="学习与反思">学习与反思</h3>
<blockquote>
<p>主要是记录自己遇到的问题以及踩的坑<br>
同时欢迎各位大佬，给我提出意见，我一定会好好吸取。<sup>_</sup></p>
</blockquote>
<h4 id="准确率只有01左右or-很低">准确率只有0.1左右？（or 很低）</h4>
<ul>
<li>有可能是因为权重没有初始化（不一定是必要的）</li>
<li>或者学习率设置的问题，可能设置的太大了，试着调小一些</li>
</ul>
<h4 id="如何区分验证集和测试集">如何区分验证集和测试集？</h4>
<p><img src="https://lzz-1340752507.cos.ap-shanghai.myqcloud.com/lzz/image-20250310184619249.png" alt="image-20250310184619249" loading="lazy"></p>
<ul>
<li>训练集 （训练阶段）</li>
</ul>
<blockquote>
<p>用于构建我们的模型，我们的模型在训练集上进行学习，通常在这个阶段我们可以有多种方法进行训练</p>
</blockquote>
<ul>
<li>验证集 Validation Set（模型挑选阶段）</li>
</ul>
<blockquote>
<p>用于挑选最优模型超参的样本集合：使用验证集可以得到反向传播什么时候结束以及超参怎么设置最合理，同时防止过拟合。主要目的是为了挑选在验证集上表现最好的模型。</p>
</blockquote>
<ul>
<li>测试集 Test Set（验证阶段 评估泛化误差）</li>
</ul>
<blockquote>
<p>在我们挑选好验证集上表现最好的模型之后，用于评估该模型泛化能力的数据集。</p>
</blockquote>
<h4 id="如何训练自己的数据集">如何训练自己的数据集？</h4>
<ol>
<li>首先获取自己的数据，各类数据要相对均衡，以及图片命名尽量不要使用中文</li>
<li>划分训练集和测试集</li>
</ol>
<h4 id="数据集为什么要进行归一化">数据集为什么要进行归一化？</h4>
<p>数据归一化（Normalization）是数据预处理中的一个重要步骤，尤其在机器学习和深度学习中。
它将数据缩放到一个统一的范围（通常是 [0, 1] 或 [-1, 1]），从而提高模型的训练效率和性能。</p>
<p>数据归一化是深度学习中不可或缺的步骤，它通过调整数据的范围和分布，帮助模型更快地收敛，减少数值计算问题，提高泛化能力，并简化模型的设计和训练。</p>
<h4 id="图形预处理">图形预处理</h4>
<ol>
<li>尺寸变化</li>
<li>格式转换（变为Tensor格式）</li>
<li>归一化</li>
</ol>
<h4 id="为什么有时损失函数增加但是精度却在上升">为什么有时损失函数增加但是精度却在上升？</h4>
<p>在训练深度学习模型时，有时会观察到一个看似矛盾的现象：损失函数（Loss）在增加，<br>
但精度（Accuracy）却在上升。这种现象可能由多种原因引起，以下是一些可能的解释：</p><hr><p></p>
<ol>
<li>损失函数和精度的衡量方式不同</li>
</ol>
<ul>
<li><strong>损失函数（Loss）</strong>：衡量的是模型输出与真实标签之间的差异，通常是一个连续的数值。常见的损失函数包括交叉熵损失（Cross-Entropy<br>
Loss）、均方误差（MSE）等。损失函数越小，表示模型的预测越接近真实值。</li>
<li><strong>精度（Accuracy）</strong>：衡量的是模型预测正确的比例，是一个离散的指标。例如，在分类任务中，精度是模型正确分类的样本数占总样本数的比例。<br>
原因：损失函数和精度的优化目标不同。损失函数关注的是预测值与真实值之间的差异，而精度关注的是预测结果是否完全正确。因此，即使损失函数增加，模型的预测结果可能仍然足够接近真实值，从而导致精度上升。</li>
</ul>
<ol start="2">
<li>数据不平衡或类别分布不均</li>
</ol>
<ul>
<li>如果数据集中某些类别占主导地位，模型可能会偏向于预测这些类别，从而导致损失函数增加，但精度仍然较高。</li>
</ul>
<ol start="3">
<li>模型过拟合</li>
</ol>
<ul>
<li>如果模型在训练集上表现良好，但在验证集上损失函数增加但精度上升，可能是因为模型过拟合了训练集中的某些噪声或异常值。</li>
</ul>
<ol start="4">
<li>学习率过高</li>
</ol>
<ul>
<li>如果学习率设置过高，模型的权重更新可能会过大，导致损失函数在某些迭代中增加。然而，如果模型的预测结果仍然接近真实值，精度可能不会受到影响。</li>
</ul>
<ol start="5">
<li>损失函数的选择问题</li>
</ol>
<ul>
<li>不同的损失函数对误差的敏感度不同。例如，交叉熵损失对概率值的微小变化非常敏感，而精度则只关注最终的分类结果。</li>
</ul>
<ol start="6">
<li>
<p>数据预处理或数据增强的影响<br>
-如果在训练过程中使用了数据增强（如随机裁剪、旋转等），模型可能会在某些增强后的数据上表现不佳，导致损失函数增加。但如果这些增强后的数据对精度的影响较小，精度可能仍然较高。</p>
</li>
<li>
<p>模型架构或正则化的影响</p>
</li>
</ol>
<ul>
<li>如果模型架构过于复杂，或者正则化不足，模型可能会在某些样本上表现不佳，导致损失函数增加。但如果这些样本对精度的影响较小，精度可能仍然较高。</li>
</ul>
<ol start="8">
<li>批量大小（Batch Size）的影响</li>
</ol>
<ul>
<li>如果批量大小设置得过大或过小，可能会导致损失函数和精度之间的不一致。</li>
</ul>
<h4 id="随着层数的增加-训练误差和测试误差不降反增">随着层数的增加, 训练误差和测试误差不降反增?</h4>
<p>这是由于"退化问题"导致的, 退化问题最明显的表现就是给网络叠加更多的层之后, 性能却快速下降的情况. 按照道理, 给网络叠加更多的层,<br>
浅层网络的解空间是包含在深层网络的解空间中的, 深层网络的解空间至少存在不差于浅层网络的解, 因为只需要将增加的层变成恒等映射,<br>
其他层的权重原封不动复制浅层网络, 就可以获得和浅层网络同样的性能. 更好地解明明存在, 为什么找不到, 找到的反而是更差的解?</p>
<p>退化问题的原因可以总结为:</p>
<ul>
<li>优化难度随着深度的增加非线性上升: 深层网络拥有更多参数, 解空间的维度和复杂性呈现指数级增长, 这使得优化算法需要在一个更大的高维空间内找到最优解,<br>
难度大大增加</li>
<li>误差积累: 深层网路中的每一层都会对后续层的输入产生影响, 如果某些层的输出有轻微偏差, 这些偏差可能随着层数的增加而累积放大,<br>
导致最终的误差增大</li>
</ul>
<p><strong>大白话解释</strong>：打个比方, 你有一张比别人更大的藏宝图(解空间), 理论上你能找到更多的保障, 但是如果你没有很好的工具(<br>
优化算法), 反而会因为地图太复杂, 找不到最好的路径, 结果挖了一些不值钱的东西(次优解), 甚至挖错地方.</p>
<h4 id="为什么训练集和测试集的精准度很高而验证集的精准度很低">为什么训练集和测试集的精准度很高,而验证集的精准度很低?</h4>
<p>可能的原因是在model_train.py文件中<br>
<code>train_transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor(), normalize])</code>未加上最后的那个归一化处理</p>
<h4 id="leakyrelu和relu函数的区别">LeakyReLU()和ReLU()函数的区别</h4>
<ul>
<li>ReLU():x&gt;0:输出x；x&lt;0，输出0</li>
<li>LeakyReLU():f(x):x&gt;0，输出x；x&lt;0，输出a*x（a表示一个很小的斜率值）</li>
</ul>
<h4 id="为什么要计算梯度">为什么要计算梯度？</h4>
<p>计算梯度的主要目的是通过优化算法调整模型的参数，以最小化损失函数。以下是具体原因：</p>
<h4 id="1优化模型参数">（1）<strong>优化模型参数</strong></h4>
<ul>
<li>
<p>梯度是优化算法（如梯度下降）的基础。通过计算损失函数对每个参数的梯度，可以知道如何调整参数以减少损失。</p>
</li>
<li>
<p>例如，在梯度下降中，参数更新公式为：</p>
<p><em>θ</em>new=<em>θ</em>old−<em>η</em>⋅∇<em>L</em>(<em>θ</em>)</p>
<p>其中，<em>η</em> 是学习率（learning rate），表示每次更新的步长。</p>
</li>
</ul>
<h4 id="2找到损失函数的最小值">（2）<strong>找到损失函数的最小值</strong></h4>
<ul>
<li>梯度指向损失函数增加最快的方向，而负梯度指向损失函数减少最快的方向。</li>
<li>通过沿着负梯度方向更新参数，可以逐步找到损失函数的最小值。</li>
</ul>
<h4 id="3自动微分">（3）<strong>自动微分</strong></h4>
<ul>
<li>神经网络的训练涉及复杂的函数组合，手动计算梯度非常困难且容易出错。</li>
<li>PyTorch 和 TensorFlow 等深度学习框架通过自动微分（Automatic Differentiation）技术，自动计算梯度，大大简化了训练过程。</li>
</ul>
<h4 id="_"></h4>
<h4 id="为什么有时需要权重初始化">为什么有时需要权重初始化？</h4>
<ul>
<li>避免梯度消失或爆炸。</li>
<li>加速模型收敛。</li>
<li>打破对称性，使每个神经元能够学习不同的特征。</li>
<li>提高模型的最终性能。</li>
</ul>
<blockquote>
<p>示例：<img src="https://lzz-1340752507.cos.ap-shanghai.myqcloud.com/lzz/image-20250310193316607.png" alt="image-20250310193316607" style="zoom: 50%"></p>
</blockquote>
<h4 id="常见的损失函数及其适用领域和任务的详细说明">常见的损失函数及其适用领域和任务的详细说明:</h4>
<blockquote>
<p><strong>1. 均方误差（MSE）</strong></p>
<ul>
<li>定义：
<ul>
<li>衡量预测值与真实值之间差的平方的平均值。</li>
</ul>
</li>
<li>适用领域和任务：
<ul>
<li>回归问题：适用于预测连续数值的任务，例如房价预测、股票价格预测等。</li>
<li>数据分布相对均匀，没有明显的异常值。</li>
<li>需要对较大的误差进行更严厉的惩罚。</li>
</ul>
</li>
<li>特点：
<ul>
<li>数学上易于处理，优化效果好。</li>
<li>对异常值非常敏感。</li>
</ul>
</li>
</ul>
<p><strong>2. 平均绝对误差（MAE）</strong></p>
<ul>
<li>定义：
<ul>
<li>衡量预测值与真实值之间差的绝对值的平均值。</li>
</ul>
</li>
<li>适用领域和任务：
<ul>
<li>回归问题：适用于预测连续数值的任务。</li>
<li>数据中存在异常值。</li>
<li>需要对所有误差进行相同程度的惩罚。</li>
</ul>
</li>
<li>特点：
<ul>
<li>对异常值不敏感。</li>
<li>数学上不如MSE易于处理。</li>
</ul>
</li>
</ul>
<p><strong>3. Huber 损失</strong></p>
<ul>
<li>定义：
<ul>
<li>结合了MSE和MAE的优点，在误差较小时类似于MSE，在误差较大时类似于MAE。</li>
</ul>
</li>
<li>适用领域和任务：
<ul>
<li>回归问题：适用于预测连续数值的任务。</li>
<li>数据中可能存在异常值。</li>
<li>需要在对小误差的敏感性和对大误差的鲁棒性之间取得平衡。</li>
</ul>
</li>
<li>特点：
<ul>
<li>对异常值具有一定的鲁棒性。</li>
<li>需要设置一个超参数。</li>
</ul>
</li>
</ul>
<p><strong>4. 交叉熵损失（Cross-Entropy Loss）</strong></p>
<ul>
<li>定义：
<ul>
<li>衡量两个概率分布之间的差异。</li>
</ul>
</li>
<li>适用领域和任务：
<ul>
<li>分类问题：适用于预测类别标签的任务，例如图像分类、文本分类等。</li>
<li>特别适用于多分类问题。</li>
</ul>
</li>
<li>特点：
<ul>
<li>在分类问题中表现良好。</li>
<li>对预测错误的惩罚力度较大。</li>
</ul>
</li>
</ul>
<p><strong>5. 二元交叉熵损失（Binary Cross-Entropy Loss）</strong></p>
<ul>
<li>定义：
<ul>
<li>交叉熵损失的特殊情况，适用于二分类问题。</li>
</ul>
</li>
<li>适用领域和任务：
<ul>
<li>二分类问题：适用于预测两个类别标签的任务，例如垃圾邮件检测、疾病诊断等。</li>
</ul>
</li>
<li>特点：
<ul>
<li>在二分类问题中表现良好。</li>
<li>对预测错误的惩罚力度较大。</li>
</ul>
</li>
</ul>
<p><strong>6. KL 散度（Kullback-Leibler Divergence）</strong></p>
<ul>
<li>定义：
<ul>
<li>衡量一个概率分布与另一个概率分布之间的差异。</li>
</ul>
</li>
<li>适用领域和任务：
<ul>
<li>生成模型：适用于训练生成模型，例如变分自编码器（VAE）、生成对抗网络（GAN）等。</li>
<li>概率分布估计。</li>
</ul>
</li>
<li>特点：
<ul>
<li>不对称性。</li>
<li>在生成模型中广泛应用</li>
</ul>
</li>
</ul>
<p>好的，以下是一些常见的损失函数及其适用领域和任务的详细说明：</p>
<p><strong>1. 均方误差（MSE）</strong></p>
<ul>
<li>定义：
<ul>
<li>衡量预测值与真实值之间差的平方的平均值。</li>
</ul>
</li>
<li>适用领域和任务：
<ul>
<li>回归问题：适用于预测连续数值的任务，例如房价预测、股票价格预测等。</li>
<li>数据分布相对均匀，没有明显的异常值。</li>
<li>需要对较大的误差进行更严厉的惩罚。</li>
</ul>
</li>
<li>特点：
<ul>
<li>数学上易于处理，优化效果好。</li>
<li>对异常值非常敏感。</li>
</ul>
</li>
</ul>
<p><strong>2. 平均绝对误差（MAE）</strong></p>
<ul>
<li>定义：
<ul>
<li>衡量预测值与真实值之间差的绝对值的平均值。</li>
</ul>
</li>
<li>适用领域和任务：
<ul>
<li>回归问题：适用于预测连续数值的任务。</li>
<li>数据中存在异常值。</li>
<li>需要对所有误差进行相同程度的惩罚。</li>
</ul>
</li>
<li>特点：
<ul>
<li>对异常值不敏感。</li>
<li>数学上不如MSE易于处理。</li>
</ul>
</li>
</ul>
<p><strong>3. Huber 损失</strong></p>
<ul>
<li>定义：
<ul>
<li>结合了MSE和MAE的优点，在误差较小时类似于MSE，在误差较大时类似于MAE。</li>
</ul>
</li>
<li>适用领域和任务：
<ul>
<li>回归问题：适用于预测连续数值的任务。</li>
<li>数据中可能存在异常值。</li>
<li>需要在对小误差的敏感性和对大误差的鲁棒性之间取得平衡。</li>
</ul>
</li>
<li>特点：
<ul>
<li>对异常值具有一定的鲁棒性。</li>
<li>需要设置一个超参数。</li>
</ul>
</li>
</ul>
<p><strong>4. 交叉熵损失（Cross-Entropy Loss）</strong></p>
<ul>
<li>定义：
<ul>
<li>衡量两个概率分布之间的差异。</li>
</ul>
</li>
<li>适用领域和任务：
<ul>
<li>分类问题：适用于预测类别标签的任务，例如图像分类、文本分类等。</li>
<li>特别适用于多分类问题。</li>
</ul>
</li>
<li>特点：
<ul>
<li>在分类问题中表现良好。</li>
<li>对预测错误的惩罚力度较大。</li>
</ul>
</li>
</ul>
<p><strong>5. 二元交叉熵损失（Binary Cross-Entropy Loss）</strong></p>
<ul>
<li>定义：
<ul>
<li>交叉熵损失的特殊情况，适用于二分类问题。</li>
</ul>
</li>
<li>适用领域和任务：
<ul>
<li>二分类问题：适用于预测两个类别标签的任务，例如垃圾邮件检测、疾病诊断等。</li>
</ul>
</li>
<li>特点：
<ul>
<li>在二分类问题中表现良好。</li>
<li>对预测错误的惩罚力度较大。</li>
</ul>
</li>
</ul>
<p><strong>6. KL 散度（Kullback-Leibler Divergence）</strong></p>
<ul>
<li>定义：
<ul>
<li>衡量一个概率分布与另一个概率分布之间的差异。</li>
</ul>
</li>
<li>适用领域和任务：
<ul>
<li>生成模型：适用于训练生成模型，例如变分自编码器（VAE）、生成对抗网络（GAN）等。</li>
<li>概率分布估计。</li>
</ul>
</li>
<li>特点：
<ul>
<li>不对称性。</li>
<li>在生成模型中广泛应用。</li>
</ul>
</li>
</ul>
<p><strong>选择损失函数的建议：</strong></p>
<ul>
<li>根据任务类型选择：回归问题选择MSE、MAE或Huber损失，分类问题选择交叉熵损失。</li>
<li>考虑数据特征：如果数据存在异常值，选择MAE或Huber损失。</li>
<li>尝试多种损失函数：比较不同损失函数的性能，选择最适合任务的损失函数。</li>
</ul>
<p>--from Gemini by lzz</p>
</blockquote>
<h4 id="独家技巧"><strong>独家技巧</strong></h4>
<ul>
<li>一般情况：在大多数CNN架构中，平均池化层后面通常会跟一个平展层，因为平展后的特征通常用于全连接层的输入，进行分类或回归任务。</li>
<li>注意:bn层里面放前一个通道数的大小</li>
<li>全连接层（Fully Connected Layer）和卷积层（Convolutional Layer）后一般会跟着一个激活函数（Activation Function）。</li>
<li>在 PyTorch 中，当你创建一个 nn.Module 的实例时，例如 self.c1 = Conv_Block(3, 64)，这会调用 Conv_Block 类的 <strong>init</strong><br>
方法，初始化模块的参数。而当你调用模块实例，例如 R1 = self.c1(x)，这会自动调用 Conv_Block 类的 forward 方法，执行前向传播逻辑。</li>
<li>必备公式：OH=(H+2*P-FH)/S+1；其中OH是计算后的高度，H是计算前的高度，FH是卷积和的高度。</li>
<li>在Gan网络中，判别器中的激活函数推荐使用LeakyReLU()，可以保留一定的梯度信息</li>
<li>在交叉熵损失中我们一般使用BECLoss()函数；交叉熵损失函数（Cross-Entropy Loss）常用于分类任务，尤其是二分类任务（Binary Cross-Entropy Loss，简称 BCELoss）和多分类任务（Categorical Cross-Entropy Loss）</li>
<li>len(dataloader)返回批次数，len(dataset)返回样本数</li>
<li>注意：nn.imshow() 默认期望的范围是 [0, 1] 或 [0, 255]</li>
<li>在数据加载的时候，训练阶段需要<code>shuffle=True</code>。而验证和测试阶段为<code>shuffle=False</code>，因为保持数据的原始顺序可以方便地与真实标签进行对比，从而更准确地评估模型的性能。</li>
</ul>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.023500738265046298" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-04-08 16:33">2025-04-08 16:20</span>&nbsp;
<a href="https://www.cnblogs.com/lizhongzheng">虾饺爱下棋</a>&nbsp;
阅读(<span id="post_view_count">63</span>)&nbsp;
评论(<span id="post_comment_count">2</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18814913" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18814913);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18814913', targetLink: 'https://www.cnblogs.com/lizhongzheng/p/18814913', title: '深度学习--个人总结' })">举报</a>
</div>
        