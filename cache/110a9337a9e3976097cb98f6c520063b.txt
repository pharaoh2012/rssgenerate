
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/wang_yb/p/18892531" title="发布于 2025-05-23 10:11">
    <span role="heading" aria-level="2">聚类是如何度量数据间的“远近”的？</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p>在聚类分析中，<strong>距离度量</strong>是核心概念之一，它决定了数据点之间的相似性或差异性，从而影响聚类结果的质量。</p>
<p>选择合适的距离度量方法，就像为数据选择合适的<strong>“观察视角”</strong>，能够帮助我们发现隐藏的模式结构。</p>
<p>本文将详细介绍几种常用的聚类距离度量方法，包括它们的原理、代码实现，以及这些方法满足的基本性质。</p>
<h1 id="1-常用距离度量">1. 常用距离度量</h1>
<h2 id="11-闵可夫斯基距离minkowski-distance">1.1. 闵可夫斯基距离（Minkowski Distance）</h2>
<p><strong>闵可夫斯基距离</strong>是一种通用的距离度量方法，它涵盖了多种常见的距离计算方式。</p>
<p>其公式为：<span class="math inline">\(D(x,y)=\left(\sum_{i=1}^{n}|x_i-y_i|^p\right)^{\frac{1}{p}}\)</span></p>
<ul>
<li>当$ p=1 $时，它是<strong>曼哈顿距离</strong>（<code>Manhattan Distance</code>），适用于网格状空间，例如城市街区。</li>
<li>当$ p=2 $时，它是<strong>欧几里得距离</strong>（<code>Euclidean Distance</code>），是最常用的距离度量方式，适用于连续变量。</li>
<li>当$ p\to\infty $时，它趋近于<strong>切比雪夫距离</strong>（<code>Chebyshev Distance</code>），即各维度差的最大值。</li>
</ul>
<p>基于<code>scikit-learn</code>库计算这些距离非常简单：</p>
<pre><code class="language-python">from sklearn.metrics.pairwise import pairwise_distances
import numpy as np

# 示例数据
x = np.array([[1, 2, 3]])
y = np.array([[4, 5, 6]])

# 计算不同 p 值的闵可夫斯基距离
manhattan_distance = pairwise_distances(x, y, metric='manhattan')[0][0]
euclidean_distance = pairwise_distances(x, y, metric='euclidean')[0][0]
chebyshev_distance = pairwise_distances(x, y, metric='chebyshev')[0][0]

print("曼哈顿距离:", manhattan_distance)
print("欧几里得距离:", euclidean_distance)
print("切比雪夫距离:", chebyshev_distance)

## 输出结果：
'''
曼哈顿距离: 9.0
欧几里得距离: 5.196152422706632
切比雪夫距离: 3.0
'''
</code></pre>
<h2 id="12-汉明距离hamming-distance">1.2. 汉明距离（Hamming Distance）</h2>
<p><strong>汉明距离</strong>用于衡量两个等长字符串之间的差异，即对应位置上不同字符的个数。</p>
<p>它常用于离散属性的比较。</p>
<p>代码示例如下：</p>
<pre><code class="language-python">from sklearn.metrics import hamming_loss

# 示例数据
x = np.array([0, 1, 1, 0])
y = np.array([1, 1, 0, 0])

# 计算汉明距离
hamming_distance = hamming_loss(x, y)
print("汉明距离:", hamming_distance)

## 输出结果：
'''
汉明距离: 0.5
'''
</code></pre>
<h2 id="13-杰卡德距离jaccard-distance">1.3. 杰卡德距离（Jaccard Distance）</h2>
<p><strong>杰卡德距离</strong>用于衡量两个集合之间的相似性，定义为两个集合交集的大小与并集大小的比值的补数。</p>
<p>它适用于稀疏数据。</p>
<p>代码示例如下：</p>
<pre><code class="language-python">from sklearn.metrics import jaccard_score

# 示例数据
x = np.array([0, 1, 1, 0])
y = np.array([1, 1, 0, 0])

# 计算杰卡德距离
jaccard_similarity = jaccard_score(x, y)
jaccard_distance = 1 - jaccard_similarity
print("杰卡德距离:", jaccard_distance)

## 输出结果：
'''
杰卡德距离: 0.6666666666666667
'''
</code></pre>
<h2 id="14-余弦距离">1.4. 余弦距离</h2>
<p><strong>余弦距离</strong>通过向量夹角衡量方向相似性，常用于文本分析。</p>
<p>它的公式是：$ cos(\theta)=\frac{X\cdot Y}{||X||\cdot ||Y||} $</p>
<p>实际使用常转换为余弦距离：<strong>距离 = 1 - 余弦相似度</strong>。</p>
<p>代码示例如下：</p>
<pre><code class="language-python">from sklearn.metrics.pairwise import cosine_distances
import numpy as np

# 示例数据
x = np.array([[1, 2, 3]])
y = np.array([[4, 5, 6]])

# 计算余弦距离
cosine_dist = cosine_distances(x, y)[0][0]

print("余弦距离:", cosine_dist)

## 输出结果：
'''
余弦距离: 0.025368153802923787
'''
</code></pre>
<h1 id="2-距离度量的基本性质">2. 距离度量的基本性质</h1>
<p><strong>距离度量方法</strong>通常需要满足以下基本性质，以确保其合理性和有效性：</p>
<ol>
<li><strong>非负性</strong>（<code>Non-negativity</code>）：距离必须是非负的，即$ D(x,y)\geq 0 $。</li>
</ol>
<p>这意味着任意两个点之间的距离不能为负值。</p>
<ol start="2">
<li><strong>同一性</strong>（<code>Identity</code>）：当且仅当两个点相同时，距离为零，即<span class="math inline">\(D(x,y)=0\)</span> 当且仅当<span class="math inline">\(x=y\)</span> 。</li>
</ol>
<p>这确保了距离能够区分不同的点。</p>
<ol start="3">
<li><strong>对称性</strong>（<code>Symmetry</code>）：距离是无方向的，即<span class="math inline">\(D(x,y)=D(y,x)\)</span> 。</li>
</ol>
<p>这意味着从点<span class="math inline">\(x\)</span> 到点<span class="math inline">\(y\)</span> 的距离与从点<span class="math inline">\(y\)</span> 到点<span class="math inline">\(x\)</span> 的距离相同。</p>
<ol start="4">
<li><strong>三角不等式</strong>（<code>Triangle Inequality</code>）：对于任意三个点<span class="math inline">\(x\)</span> 、<span class="math inline">\(y\)</span> 和<span class="math inline">\(z\)</span> ，满足<span class="math inline">\(D(x,z)\leq D(x,y)+D(y,z)\)</span> 。</li>
</ol>
<p>这确保了距离的合理性，即直接从<span class="math inline">\(x\)</span> 到<span class="math inline">\(z\)</span> 的距离不会超过经过<span class="math inline">\(y\)</span> 的距离。</p>
<p>这些性质的意义在于，它们为距离度量提供了数学上的合理性，使得距离能够正确地反映数据点之间的相似性或差异性。</p>
<h1 id="3-连续属性与离散属性的距离">3. 连续属性与离散属性的距离</h1>
<p>对于<strong>连续属性</strong>，常用的距离度量方法是<strong>欧几里得距离</strong>和<strong>曼哈顿距离</strong>。</p>
<p>这些方法基于数值的差值来计算距离，适用于数值型数据。</p>
<ul>
<li><strong>欧几里得距离</strong>：适用于多维空间中的连续数据，计算两点之间的直线距离。</li>
<li><strong>曼哈顿距离</strong>：适用于网格状空间，计算两点之间的“步数”距离。</li>
</ul>
<p>对于<strong>离散属性</strong>，常用的距离度量方法是<strong>汉明距离</strong>和<strong>杰卡德距离</strong>。</p>
<ul>
<li><strong>汉明距离</strong>：适用于二进制数据或分类数据，计算两个序列中不同位置的数量。</li>
<li><strong>杰卡德距离</strong>：适用于集合数据，计算两个集合之间的相似性。</li>
</ul>
<h1 id="4-总结">4. 总结</h1>
<p><strong>距离度量</strong>是聚类分析中的关键环节。通过选择合适的距离度量方法，可以更好地反映数据点之间的相似性或差异性。</p>
<p>本文介绍了几种常用的距离度量方法，包括闵可夫斯基距离、汉明距离和杰卡德距离等等，并通过代码示例展示了它们的使用方式。</p>
<p>同时，我们还探讨了距离度量的基本性质及其意义，以及如何针对连续属性和离散属性进行距离计算。</p>
<p>在实际应用中，选择哪种距离度量方法取决于数据的类型和聚类的目标。</p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.03812388596759259" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-05-23 10:14">2025-05-23 10:11</span>&nbsp;
<a href="https://www.cnblogs.com/wang_yb">wang_yb</a>&nbsp;
阅读(<span id="post_view_count">0</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18892531);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18892531', targetLink: 'https://www.cnblogs.com/wang_yb/p/18892531', title: '聚类是如何度量数据间的“远近”的？' })">举报</a>
</div>
        