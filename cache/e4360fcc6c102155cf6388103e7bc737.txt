
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/Johny-zhao/p/19047708" title="发布于 2025-08-20 00:25">
    <span role="heading" aria-level="2">基于 Docker 的 LLaMA-Factory 全流程部署指南</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body blogpost-body-html">
<p>LLaMA-Factory 是一个强大且高效的大语言模型（LLM）微调框架，支持多种微调方法（如 LoRA、QLoRA）、完整的训练流程（SFT、RM、PPO、DPO）以及丰富的模型和数据集支持，能帮助你在消费级显卡上高效微调大型语言模型。</p>
<p>这份指南将带你从零开始，完成基于 Docker 的环境搭建、数据准备、模型训练、推理测试到模型导出的全过程。</p>
<p><strong>🐳 基于 Docker 的 LLaMA-Factory 全流程部署指南</strong></p>
<p><strong>1. 环境准备与前置检查</strong></p>
<p>在开始部署之前，需要确保你的系统环境满足基本要求，并正确安装所需的软件依赖。</p>
<p><strong>1.1 硬件需求建议</strong></p>
<p>以下是对硬件配置的基本建议，实际需求会根据模型规模和数据集大小有所变化：</p>
<table border="0" cellspacing="0" cellpadding="0">
<thead>
<tr>
<td>
<p><strong>资源类型</strong></p>
</td>
<td>
<p><strong>最低配置要求</strong></p>
</td>
<td>
<p><strong>推荐配置</strong></p>
</td>
<td>
<p><strong>大型模型训练建议</strong></p>
</td>
</tr>
</thead>
<tbody>
<tr>
<td>
<p><strong>CPU</strong></p>
</td>
<td>
<p>4 核心</p>
</td>
<td>
<p>8 核心或以上</p>
</td>
<td>
<p>16 核心或以上</p>
</td>
</tr>
<tr>
<td>
<p><strong>内存</strong></p>
</td>
<td>
<p>16 GB</p>
</td>
<td>
<p>32 GB</p>
</td>
<td>
<p>64 GB 或以上</p>
</td>
</tr>
<tr>
<td>
<p><strong>GPU</strong></p>
</td>
<td>
<p>NVIDIA GPU (8GB VRAM)</p>
</td>
<td>
<p>NVIDIA RTX 3090/4090 (24GB VRAM)</p>
</td>
<td>
<p>NVIDIA A100 (80GB VRAM)</p>
</td>
</tr>
<tr>
<td>
<p><strong>存储空间</strong></p>
</td>
<td>
<p>50 GB (用于系统和依赖)</p>
</td>
<td>
<p>100 GB (含基础模型)</p>
</td>
<td>
<p>500 GB 或以上 (模型缓存)</p>
</td>
</tr>
</tbody>
</table>
<p><strong>1.2 软件依赖安装</strong></p>
<ol start="1">
<li><strong>安装 Docker</strong>：访问&nbsp;<a href="https://docs.docker.com/engine/install/" target="_blank" rel="noopener nofollow">Docker 官方网站</a>&nbsp;获取适合你操作系统（Windows/Linux/macOS）的安装指南。</li>
<ul>
<li><strong>Windows 用户注意</strong>：建议安装时<strong>更改默认安装路径</strong>到非系统盘（如 D 盘），避免后期占用过多 C 盘空间。可以通过命令行指定安装路径：</li>
</ul>
</ol>
<p>powershell</p>
<p>"Docker Desktop Installer.exe" install --installation-dir=D:\Docker</p>
<ol start="1">
<ul>
<li>安装完成后，启动 Docker 服务。</li>
</ul>
<li><strong>安装 NVIDIA Docker 支持</strong>（仅限 NVIDIA GPU 用户）：</li>
<ul>
<li>确保已安装最新的 NVIDIA 显卡驱动。</li>
<li>参照&nbsp;<a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html" target="_blank" rel="noopener nofollow">NVIDIA Container Toolkit 安装指南</a>&nbsp;安装和配置 NVIDIA Container Toolkit，以便 Docker 容器能够访问 GPU。</li>
<li>安装后，在终端执行&nbsp;<strong>docker run --rm --gpus all nvidia/cuda:11.6.2-base-ubuntu20.04 nvidia-smi</strong>&nbsp;测试 GPU 是否可在 Docker 中正常识别。如果成功，你将看到显卡信息输出。</li>
</ul>
<li><strong>（可选）安装 Docker Compose</strong>：新版本的 Docker Desktop 通常已包含 Compose。如果没有，请参照官方文档安装。</li>
</ol>
<p><strong>2. LLaMA-Factory 项目获取与 Docker 环境配置</strong></p>
<p><strong>2.1 获取 LLaMA-Factory 源代码</strong></p>
<p>使用&nbsp;<strong>git</strong>&nbsp;命令将 LLaMA-Factory 项目克隆到本地：</p>
<p>bash</p>
<p><em># 克隆项目代码（使用 --depth 1 只克隆最新提交，节省时间和空间）</em></p>
<p>git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git</p>
<p><em># 进入项目目录</em></p>
<p>cd LLaMA-Factory</p>
<p><strong>2.2 Docker 部署方式选择</strong></p>
<p>LLaMA-Factory 的 Docker 目录 (<strong>docker/</strong>) 下通常提供了针对不同硬件环境的配置1：</p>
<ul>
<li><strong>docker-cuda/</strong>:&nbsp;<strong>适用于 NVIDIA CUDA 用户</strong>（最常见）。</li>
<li><strong>docker-npu/</strong>: 适用于华为 Ascend NPU 用户。</li>
<li><strong>docker-rocm/</strong>: 适用于 AMD ROCm 用户。</li>
</ul>
<p>本指南以最常用的&nbsp;<strong>CUDA</strong>&nbsp;为例。</p>
<p><strong>2.2.1 使用 Docker Compose（推荐）</strong></p>
<p>Docker Compose 能简化容器的构建和运行过程。</p>
<ol start="1">
<li><strong>进入 CUDA 目录</strong>：</li>
</ol>
<p>bash</p>
<p>cd docker/docker-cuda/</p>
<ol start="2">
<li><strong>启动容器</strong>（后台运行）：</li>
</ol>
<p>bash</p>
<p>docker compose up -d</p>
<p>此命令会读取同目录下的&nbsp;<strong>docker-compose.yml</strong>&nbsp;文件，构建或拉取镜像，并在后台启动容器。</p>
<ol start="3">
<li><strong>进入容器内部</strong>：</li>
</ol>
<p>bash</p>
<p>docker compose exec llamafactory bash</p>
<p><em># 或者使用</em></p>
<p><em># docker exec -it llamafactory /bin/bash</em></p>
<p>执行后，你将进入一个名为&nbsp;<strong>llamafactory</strong>&nbsp;的容器内部，并可以开始在容器内操作。</p>
<p><strong>2.2.2 手动构建 Docker 镜像（可选）</strong></p>
<p>如果你需要更多自定义配置，可以手动构建镜像。</p>
<ol start="1">
<li><strong>编写 Dockerfile</strong>：你可以参考或修改&nbsp;<strong>docker/docker-cuda/Dockerfile</strong>。</li>
<li><strong>构建镜像</strong>：</li>
</ol>
<p>bash</p>
<p><em># 在 Dockerfile 所在目录执行</em></p>
<p>docker build -t llamafactory-cuda .</p>
<ol start="3">
<li><strong>运行容器并进入</strong>：</li>
</ol>
<p>bash</p>
<p>docker run -it --gpus all -v /path/to/your/data:/mnt/data llamafactory-cuda bash</p>
<ol start="3">
<ul>
<li><strong>--gpus all</strong>: 将主机所有 GPU 分配给容器。</li>
<li><strong>-v /path/to/your/data:/mnt/data</strong>: 将主机上的数据目录挂载到容器内的&nbsp;<strong>/mnt/data</strong>，方便容器内访问你的数据集和模型。</li>
</ul>
</ol>
<p><strong>2.3 解决常见 Docker 部署问题</strong></p>
<ul>
<li><strong>镜像拉取或构建错误</strong>：如果遇到&nbsp;<strong>ERROR load metadata for docker.io/hiyouga/pytorch:...</strong>&nbsp;类似的错误，可以尝试手动拉取基础镜像：</li>
</ul>
<p>bash</p>
<p>docker pull hiyouga/pytorch:th2.6.0-cu124-flashattn2.7.4-cxx11abi0-devel</p>
<p>然后再重新执行&nbsp;<strong>docker compose up -d</strong>。</p>
<ul>
<li><strong>内存分配错误 (cannot allocate memory in static TLS block)</strong>：如果在容器内运行某些命令时出现此错误，可以尝试在容器内的&nbsp;<strong>~/.bashrc</strong>&nbsp;文件末尾添加以下环境变量并&nbsp;<strong>source ~/.bashrc</strong>：</li>
</ul>
<p>bash</p>
<p>export LD_PRELOAD=/usr/local/python3.10.13/lib/python3.10/site-packages/sklearn/utils/../../scikit_learn.libs/libgomp-d22c30c5.so.1.0.0</p>
<ul>
<li><strong>共享内存不足</strong>：在运行训练或 WebUI 时，如果提示共享内存不足，可以在&nbsp;<strong>docker run</strong>&nbsp;命令中添加&nbsp;<strong>--shm-size</strong>&nbsp;参数，例如&nbsp;<strong>--shm-size 16g</strong>&nbsp;或&nbsp;<strong>--shm-size 32g</strong>。</li>
</ul>
<p><strong>3. 数据准备与格式规范</strong></p>
<p>模型微调的效果很大程度上依赖于数据质量。LLaMA-Factory 主要支持两种数据格式5。</p>
<p><strong>3.1 数据格式详解</strong></p>
<p><strong>3.1.1 Alpaca 格式（单轮对话）</strong></p>
<p>适用于指令微调（Instruction Tuning），结构简单，每条数据包含一个指令-输入-输出的三元组5。</p>
<ul>
<li><strong>示例 (JSON Line格式，.jsonl)</strong>：</li>
</ul>
<p>json</p>
<p>{"instruction": "写一个Python函数，实现斐波那契数列。", "input": "", "output": "def fibonacci(n):\n if n &lt;= 1:\n return n\n return fibonacci(n-1) + fibonacci(n-2)"}</p>
<p>{"instruction": "将以下英文翻译成中文", "input": "Hello, world!", "output": "你好，世界！"}</p>
<ul>
<li><strong>字段说明</strong>：</li>
<ul>
<li><strong>instruction</strong>: 希望模型执行的任务描述。</li>
<li><strong>input</strong>&nbsp;(可选): 任务所需的额外上下文或输入。</li>
<li><strong>output</strong>: 期望模型给出的回答。</li>
</ul>
</ul>
<p><strong>3.1.2 ShareGPT 格式（多轮对话）</strong></p>
<p>模拟真实对话场景，适合训练聊天助手，结构为一个包含多轮对话的数组。</p>
<ul>
<li><strong>示例 (JSON 格式，.json)</strong>：</li>
</ul>
<p>json</p>
<p>{</p>
<p>&nbsp; "conversations": [</p>
<p>&nbsp;&nbsp;&nbsp; {</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "role": "user",</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "content": "你好，可以给我写一段Python代码打印1到10吗？"</p>
<p>&nbsp;&nbsp;&nbsp; },</p>
<p>&nbsp;&nbsp;&nbsp; {</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "role": "assistant",</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "content": "当然可以：\n\n```python\nfor i in range(1, 11):\n print(i)\n```"</p>
<p>&nbsp;&nbsp;&nbsp; },</p>
<p>&nbsp;&nbsp;&nbsp; {</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "role": "user",</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "content": "那你能把它改成倒序输出吗？"</p>
<p>&nbsp;&nbsp;&nbsp; },</p>
<p>&nbsp;&nbsp;&nbsp; {</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "role": "assistant",</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "content": "当然，这是倒序输出的版本：\n\n```python\nfor i in range(10, 0, -1):\n print(i)\n```"</p>
<p>&nbsp;&nbsp;&nbsp; }</p>
<p>&nbsp; ]</p>
<p>}</p>
<ul>
<li><strong>字段说明</strong>：</li>
<ul>
<li><strong>conversations</strong>: 一个字典列表。</li>
<li><strong>role</strong>: 对话角色，通常是&nbsp;<strong>"user"</strong>&nbsp;或&nbsp;<strong>"assistant"</strong>。</li>
<li><strong>content</strong>: 该角色的对话内容。</li>
</ul>
</ul>
<p><strong>3.2 挂载数据并添加到项目</strong></p>
<ol start="1">
<li><strong>挂载数据卷</strong>：在启动 Docker 容器时，使用&nbsp;<strong>-v</strong>&nbsp;参数将存放数据集的本地目录挂载到容器内。</li>
</ol>
<p>bash</p>
<p><em># 在 docker run 命令中添加，例如</em></p>
<p>docker run -it --gpus all -v /path/on/host/data:/app/data llamafactory-cuda bash</p>
<p><em># 或者在 docker-compose.yml 文件中配置 volumes</em></p>
<p><em># volumes:</em></p>
<p><em>#&nbsp;&nbsp; - /path/on/host/data:/app/data</em></p>
<p><em>#&nbsp;&nbsp; - /path/on/host/output:/app/output</em></p>
<p><em>#&nbsp;&nbsp; - /path/on/host/hf_cache:/root/.cache/huggingface</em></p>
<ol start="2">
<li><strong>添加数据集信息</strong>：</li>
<ul>
<li>将你的数据文件（如&nbsp;<strong>my_data.json</strong>）放入已挂载的容器数据目录（如&nbsp;<strong>/app/data</strong>）。</li>
<li><strong>关键步骤</strong>：你还需要在容器内 LLaMA-Factory 项目的&nbsp;<strong>data</strong>&nbsp;目录下的&nbsp;<strong>dataset_info.json</strong>&nbsp;配置文件中，为你新添加的数据集添加一条记录，LLaMA-Factory 才能识别它。</li>
<li>例如，添加：</li>
</ul>
</ol>
<p>json</p>
<p>"my_custom_dataset": {</p>
<p>&nbsp; "file_name": "my_data.json",</p>
<p>&nbsp; "format": "sharegpt" <em>// </em><em>或者 "alpaca"</em></p>
<p>}</p>
<p><strong>4. 模型训练与微调</strong></p>
<p>进入 Docker 容器后，你就可以开始使用 LLaMA-Factory 进行模型微调了。</p>
<p><strong>4.1 训练方式</strong></p>
<p><strong>4.1.1 使用 Web UI（可视化界面）</strong></p>
<p>LLaMA-Factory 提供了友好的 Web 界面，方便进行参数配置和训练监控。</p>
<ol start="1">
<li><strong>启动 Web UI</strong>：</li>
</ol>
<p>bash</p>
<p><em># 在容器内执行</em></p>
<p>llamafactory-cli webui</p>
<p><em># 或者设置共享和CUDA设备</em></p>
<p><em># CUDA_VISIBLE_DEVICES=0 GRADIO_SHARE=1 llamafactory-cli webui</em></p>
<ol start="2">
<li>根据提示，在物理机的浏览器中打开&nbsp;<strong>http://localhost:7860</strong>（如果端口映射正确）。</li>
<li>在 Web 界面中：</li>
<ul>
<li>选择&nbsp;<strong>Train</strong>&nbsp;标签页。</li>
<li>在&nbsp;<strong>Model Name</strong>&nbsp;中填写或选择模型路径（如&nbsp;<strong>meta-llama/Meta-Llama-3-8B</strong>&nbsp;或&nbsp;<strong>/app/models/llama-3-8b</strong>）。</li>
<li>在&nbsp;<strong>Dataset</strong>&nbsp;中选择你在&nbsp;<strong>dataset_info.json</strong>&nbsp;中配置的数据集名称。</li>
<li>选择&nbsp;<strong>Fine-tuning Method</strong>&nbsp;(如 LoRA, QLoRA)。</li>
<li>调整其他超参数（学习率、批次大小、训练轮数等）。</li>
<li>点击&nbsp;<strong>Start Training</strong>&nbsp;开始训练。</li>
</ul>
</ol>
<p><strong>4.1.2 使用命令行（CLI）</strong></p>
<p>如果你更喜欢命令行操作，或者需要进行自动化脚本处理，CLI 方式更合适。</p>
<ol start="1">
<li><strong>准备配置文件</strong>：参考项目&nbsp;<strong>examples</strong>&nbsp;目录下的配置文件（如&nbsp;<strong>examples/train_lora/llama3_lora_sft.yaml</strong>）。</li>
<li><strong>修改配置文件</strong>：指定模型路径、数据集名称、训练参数等。</li>
<li><strong>启动训练</strong>：</li>
</ol>
<p>bash</p>
<p><em># 在容器内执行</em></p>
<p>CUDA_VISIBLE_DEVICES=0 llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml</p>
<ol start="3">
<ul>
<li><strong>CUDA_VISIBLE_DEVICES=0</strong>&nbsp;指定使用第一块 GPU。</li>
</ul>
</ol>
<p><strong>4.2 训练注意事项</strong></p>
<ul>
<li><strong>显存管理</strong>：根据你的 GPU 显存调整&nbsp;<strong>per_device_train_batch_size</strong>&nbsp;和&nbsp;<strong>gradient_accumulation_steps</strong>。如果遇到 OOM（内存溢出）错误，请减小批次大小或使用 QLoRA 等更节省显存的方法。</li>
<li><strong>性能优化</strong>：可以尝试启用&nbsp;<strong>DeepSpeed</strong>（配置&nbsp;<strong>deepspeed</strong>&nbsp;参数）或&nbsp;<strong>Unsloth</strong>（添加&nbsp;<strong>--use_unsloth True</strong>&nbsp;参数）来加速训练并减少显存消耗。注意，Unsloth 可能需要额外的安装步骤。</li>
<li><strong>模型缓存</strong>：建议将 Hugging Face 模型缓存目录 (<strong>~/.cache/huggingface/</strong>) 通过&nbsp;<strong>-v</strong>&nbsp;参数挂载到主机，避免每次重启容器后重复下载模型。</li>
</ul>
<p><strong>5. 模型推理、测试与导出</strong></p>
<p>训练完成后，你可以对微调后的模型进行测试和导出。</p>
<p><strong>5.1 使用 Web UI 进行聊天测试</strong></p>
<ol start="1">
<li>启动 Web UI（同上）：<strong>llamafactory-cli webui</strong>。</li>
<li>在&nbsp;<strong>Chat</strong>&nbsp;标签页中：</li>
<ul>
<li>选择或输入模型路径。</li>
<li>如果使用 LoRA 等适配器微调，需要在&nbsp;<strong>Adapter</strong>&nbsp;选项中选择对应的适配器检查点路径（通常在&nbsp;<strong>output</strong>&nbsp;目录下）。</li>
<li>在下方输入框与模型进行对话，测试微调效果。</li>
</ul>
</ol>
<p><strong>5.2 使用命令行进行推理</strong></p>
<p>bash</p>
<p><em># 在容器内执行</em></p>
<p>CUDA_VISIBLE_DEVICES=0 llamafactory-cli chat examples/inference/llama3_lora_sft.yaml</p>
<p>你需要准备并修改对应的推理配置文件，指定模型和适配器路径。</p>
<p><strong>5.3 模型导出与部署</strong></p>
<p><strong>5.3.1 合并 LoRA 适配器（如果使用了 LoRA）</strong></p>
<p>如果你希望将微调得到的 LoRA 权重合并到基础模型中，得到一个完整的、可独立部署的模型，可以执行：</p>
<p>bash</p>
<p><em># 在容器内执行</em></p>
<p>CUDA_VISIBLE_DEVICES=0 llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml</p>
<p>合并后的模型将保存在配置文件中指定的&nbsp;<strong>output_dir</strong>&nbsp;目录。</p>
<p><strong>5.3.2 转换为 GGUF 格式（用于 Ollama 等框架）</strong></p>
<p>合并后的模型可以转换为 GGUF 格式，以便在 Ollama 或其他支持该格式的推理引擎中使用7。</p>
<ol start="1">
<li><strong>获取转换工具</strong>：</li>
</ol>
<p>bash</p>
<p>git clone https://github.com/ggerganov/llama.cpp.git</p>
<p>pip install -r llama.cpp/requirements.txt</p>
<ol start="2">
<li><strong>执行转换</strong>：</li>
</ol>
<p>bash</p>
<p>python llama.cpp/convert-hf-to-gguf.py /path/to/your/merged_model --outfile my-model.fp16.gguf --outtype f16</p>
<ol start="3">
<li><strong>在 Ollama 中使用</strong>：</li>
<ul>
<li>创建一个 Modelfile：</li>
</ul>
</ol>
<p>text</p>
<p>FROM ./my-model.fp16.gguf</p>
<p># 添加适当的模板，例如对于 Llama 3：</p>
<p>TEMPLATE "{{ if .System }}&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\n\n{{ .System }}&lt;|eot_id|&gt;{{ end }}{{ if .Prompt }}&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n\n{{ .Prompt }}&lt;|eot_id|&gt;{{ end }}&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n\n{{ .Response }}&lt;|eot_id|&gt;"</p>
<ol start="3">
<ul>
<li>创建并运行模型：</li>
</ul>
</ol>
<p>bash</p>
<p>ollama create my-model -f Modelfile</p>
<p>ollama run my-model</p>
<p><strong>6. 进阶配置与优化</strong></p>
<p><strong>6.1 多卡分布式训练</strong></p>
<p>对于更大的模型或更快的训练速度，可以使用多 GPU 进行分布式训练。</p>
<ul>
<li><strong>使用&nbsp;accelerate&nbsp;配置</strong>：LLaMA-Factory 集成了 Hugging Face Accelerate 库。你可能需要配置&nbsp;<strong>/root/.cache/huggingface/accelerate/default_config.yaml</strong>&nbsp;文件来设置分布式训练参数（如&nbsp;<strong>deepspeed_config</strong>）。</li>
<li><strong>使用 Docker Compose 扩展</strong>：确保 Docker Compose 配置能够访问所有 GPU (<strong>docker compose.yml</strong>&nbsp;中配置&nbsp;<strong>deploy</strong>&nbsp;资源或使用&nbsp;<strong>--gpus all</strong>)。</li>
</ul>
<p><strong>6.2 自定义 Docker 镜像</strong></p>
<p>如果你有特殊需求（如特定版本的库、额外工具），可以基于提供的 Dockerfile 进行自定义构建。</p>
<p>Dockerfile</p>
<p># 示例：基于官方CUDA镜像构建，安装中文依赖和常用工具</p>
<p>FROM nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04</p>
<p>&nbsp;</p>
<p>ENV DEBIAN_FRONTEND=noninteractive</p>
<p>RUN apt-get update &amp;&amp; apt-get install -y wget git vim unzip fonts-wqy-microhei</p>
<p># ... 其余步骤参考 LLaMA-Factory 的 Dockerfile 或上文手动构建部分</p>
<p>构建自定义镜像：<strong>docker build -t my-llamafactory:latest .</strong></p>
<p><strong>7. 故障排除与常见问题</strong></p>
<table border="0" cellspacing="0" cellpadding="0">
<thead>
<tr>
<td>
<p><strong>问题现象</strong></p>
</td>
<td>
<p><strong>可能原因及解决方案</strong></p>
</td>
</tr>
</thead>
<tbody>
<tr>
<td>
<p><strong>Docker 构建时无法下载依赖或镜像</strong></p>
</td>
<td>
<p>更换国内镜像源（如清华大学源）。在 Dockerfile 中使用&nbsp;<strong>pip install -i https://pypi.tuna.tsinghua.edu.cn/simple some-package</strong>。检查网络连接。</p>
</td>
</tr>
<tr>
<td>
<p><strong>训练时 GPU 显存不足 (OOM)</strong></p>
</td>
<td>
<p>减小&nbsp;<strong>per_device_train_batch_size</strong>。增加&nbsp;<strong>gradient_accumulation_steps</strong>&nbsp;补偿。尝试使用&nbsp;<strong>--fp16</strong>&nbsp;或&nbsp;<strong>--bf16</strong>。使用 QLoRA 而非全量微调。使用梯度检查点 (gradient checkpointing)。</p>
</td>
</tr>
<tr>
<td>
<p><strong>训练速度很慢</strong></p>
</td>
<td>
<p>启用 DeepSpeed Stage 2 或 33。尝试使用 Unsloth (如果支持)6。检查 CPU 内存是否不足导致频繁交换。检查 Docker 的共享内存 (<strong>--shm-size</strong>) 是否设置过小。</p>
</td>
</tr>
<tr>
<td>
<p><strong>Web UI 无法访问或报错</strong></p>
</td>
<td>
<p>检查 Docker 容器的端口映射是否正确（主机端口:容器端口，容器内默认是7860）。检查防火墙设置。</p>
</td>
</tr>
<tr>
<td>
<p><strong>无法识别自定义数据集</strong></p>
</td>
<td>
<p>检查数据格式是否正确（JSON/JSONL）。<strong>确认是否在&nbsp;data/dataset_info.json&nbsp;中添加了数据集信息</strong>5。检查文件路径和挂载点是否正确。</p>
</td>
</tr>
<tr>
<td>
<p><strong>cannot allocate memory in static TLS block</strong></p>
</td>
<td>
<p>在容器内的&nbsp;<strong>~/.bashrc</strong>&nbsp;中添加&nbsp;<strong>export LD_PRELOAD=...</strong>&nbsp;并&nbsp;<strong>source ~/.bashrc</strong>4。</p>
</td>
</tr>
</tbody>
</table>
<p>希望这份详细的指南能帮助你顺利完成 LLaMA-Factory 的 Docker 化部署和模型微调工作。如果在实际操作中遇到更多问题，查阅项目的 GitHub Issues 和官方文档通常能找到解决方案。</p>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0" data-date-updated="2025-08-20 00:25">2025-08-20 00:25</span>&nbsp;
<a href="https://www.cnblogs.com/Johny-zhao">Johny_Zhao</a>&nbsp;
阅读(<span id="post_view_count">23</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(19047708);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '19047708', targetLink: 'https://www.cnblogs.com/Johny-zhao/p/19047708', title: '基于 Docker 的 LLaMA-Factory 全流程部署指南' })">举报</a>
</div>
        