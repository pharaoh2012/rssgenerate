
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/HaiJun-Aion/p/19037450" title="发布于 2025-08-14 11:39">
    <span role="heading" aria-level="2">MCP神器！MCP-USE 一键部署连接任何MCP服务器</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                    <div id="cnblogs_post_description" style="display: none">
        <img src="https://img2024.cnblogs.com/blog/1654515/202508/1654515-20250814113912776-1275464309.png" alt="MCP神器！MCP-USE 一键部署连接任何MCP服务器" class="desc_img">
        `mcp-use` 是连接任何 LLM 到任何 MCP 服务器并构建自定义 MCP 智能体最简单的开源方式，无需依赖闭源或特定应用客户端。 它解决了开发者在构建 AI 智能体时面临的工具集成复杂性问题，让开发者能够轻松地将 LLM 连接到各种工具，如网页浏览、文件操作等。
    </div>
<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p>Hello， 大家好，我是程序员海军, <code>全栈开发</code> |<code>AI爱好者</code> ｜ <code>独立开发</code>。<br>
最近一直在研究MCP方面的事情，使用的技术栈是Python + FastAPi + FastMCP，开发了多个MCP-Server，本地化访问没啥问题，准备部署试着玩一下，调研发现这样的一个 MCP 神器，可一键部署MCP 服务器托管，并且它简化了很多操作，简直太方便了。<br>
<img alt="0a6ef4f320048cb65b892cfff1a4ac78" loading="lazy" data-src="https://img2024.cnblogs.com/blog/1654515/202508/1654515-20250814111955920-1316796452.png" class="lazyload"></p>
<p><code>mcp-use</code> 是连接任何 LLM 到任何 MCP 服务器并构建自定义 MCP 智能体最简单的开源方式，无需依赖闭源或特定应用客户端。 它解决了开发者在构建 AI 智能体时面临的工具集成复杂性问题，让开发者能够轻松地将 LLM 连接到各种工具，如网页浏览、文件操作等。</p>
<h2 id="什么是-mcp-use">什么是 <code>mcp-use</code></h2>
<p><img alt="65b58369f32b63e42f5c0769728099b3" loading="lazy" data-src="https://img2024.cnblogs.com/blog/1654515/202508/1654515-20250814112005102-1652038543.png" class="lazyload"></p>
<p><code>mcp-use</code> 是一个开源 <code>Python</code> 库，专门用于连接<code>LLM</code> 和 <code>MCP服务器</code>。它充当了 <code>LLM</code> 和各种工具服务之间的桥梁，让开发者能够创建具有工具访问能力的自定义智能体。</p>
<p>核心价值</p>
<ul>
<li>开放性：完全开源，不依赖任何闭源或特定应用的客户端</li>
<li>通用性：支持任何 LangChain 兼容的 LLM 提供商（OpenAI、Anthropic、Groq 等）</li>
<li>灵活性：通过简单的 JSON 配置即可连接各种 MCP 服务器</li>
<li>易用性：提供简洁的 Python API，几行代码即可创建功能强大的智能体</li>
</ul>
<h2 id="mcp-use-功能">mcp-use 功能</h2>
<h3 id="llm-灵活性">LLM 灵活性</h3>
<ul>
<li>支持各大模型系列的模型</li>
</ul>
<h3 id="多种连接方式">多种连接方式</h3>
<ul>
<li>Stdio 连接：标准输入输出连接方式</li>
<li>HTTP 连接：支持连接到特定端口的 MCP 服务器</li>
<li>SSE 连接：支持服务端事件流连接</li>
<li>沙盒执行：通过 E2B 云基础设施运行 MCP 服务器</li>
</ul>
<h3 id="高级功能">高级功能</h3>
<p><img alt="image" loading="lazy" data-src="https://img2024.cnblogs.com/blog/1654515/202508/1654515-20250814112011469-1025398400.png" class="lazyload"></p>
<ul>
<li>多服务器支持：同时连接多个 MCP 服务器</li>
<li>动态服务器选择：智能选择最合适的服务器执行任务</li>
<li>工具访问控制：限制智能体可使用的工具范围</li>
<li>流式输出：支持实时输出智能体的执行过程</li>
<li>调试模式：提供详细的调试信息帮助开发</li>
</ul>
<h3 id="24-配置管理">2.4 配置管理</h3>
<p><img alt="image" loading="lazy" data-src="https://img2024.cnblogs.com/blog/1654515/202508/1654515-20250814112016974-1907367480.png" class="lazyload"></p>
<ul>
<li>支持 JSON 配置文件</li>
<li>支持字典配置</li>
<li>环境变量管理</li>
<li>灵活的服务器配置选项</li>
</ul>
<h2 id="mcp-use-如何使用">mcp-use 如何使用</h2>
<h2 id="安装">安装</h2>
<pre><code class="language-python"># 基础安装
pip install mcp-use

# 安装 LLM 提供商依赖
pip install langchain-openai  # OpenAI
pip install langchain-anthropic  # Anthropic

# 安装沙盒支持（可选）
pip install "mcp-use[e2b]"
</code></pre>
<h3 id="基本使用">基本使用</h3>
<pre><code class="language-python">import asyncio
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from mcp_use import MCPAgent, MCPClient

async def main():
    load_dotenv()
    
    # 配置 MCP 服务器
    config = {
        "mcpServers": {
            "playwright": {
                "command": "npx",
                "args": ["@playwright/mcp@latest"],
                "env": {"DISPLAY": ":1"}
            }
        }
    }
    
    # 创建客户端和智能体
    client = MCPClient.from_dict(config)
    llm = ChatOpenAI(model="gpt-4o")
    agent = MCPAgent(llm=llm, client=client, max_steps=30)
    
    # 执行任务
    result = await agent.run(
        "上海有哪些美食"
    )
    print(f"结果: {result}")

if __name__ == "__main__":
    asyncio.run(main())
</code></pre>
<h3 id="使用配置文件">使用配置文件</h3>
<p>创建 mcp-config.json 文件：</p>
<pre><code class="language-python">{
  "mcpServers": {
    "playwright": {
      "command": "npx",
      "args": ["@playwright/mcp@latest"],
      "env": {
        "DISPLAY": ":1"
      }
    },
    "airbnb": {
      "command": "npx",
      "args": ["-y", "@openbnb/mcp-server-airbnb"]
    }
  }
}
</code></pre>
<p>在代码中使用mcpserver</p>
<pre><code class="language-python">client = MCPClient.from_config_file("mcp-config.json")
</code></pre>
<h3 id="流式输出">流式输出</h3>
<pre><code class="language-python">async for chunk in agent.astream("山西哪里好玩？"):
    print(chunk["messages"], end="", flush=True)
</code></pre>
<h3 id="工具访问控制">工具访问控制</h3>
<pre><code class="language-python"># 限制智能体可使用的工具
agent = MCPAgent(
    llm=llm,
    client=client,
    disallowed_tools=["get_Personal",]  # 禁用一些工具调用
)
</code></pre>
<h3 id="调试模式">调试模式</h3>
<pre><code class="language-PYTHON">
#在代码中设置
import mcp_use
mcp_use.set_debug(2)  # 启用详细调试信息
</code></pre>
<h2 id="总结">总结</h2>
<p><code>mcp-use</code> 为开发者提供了一个强大而灵活的解决方案，解决了 <code>LLM</code> 与外部工具集成的复杂性问题。</p>
<p>我们可以通过几行代码快速构建AI Agent，并且还可以轻松的集成MCP 服务器和工具了。</p>
<p>随着 MCP 生态系统的不断发展，我觉得不管是大模型的开发还是Agent 开发等等，门槛都会被降低下来了，现在已经是这个趋势了。AI 的飞速发展，以往的很多知识点可能被推翻，化繁为简，变的更简单。</p>
<p>mcp-use:  <code>https://mcp-use.com/</code></p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0" data-date-updated="2025-08-14 11:39">2025-08-14 11:39</span>&nbsp;
<a href="https://www.cnblogs.com/HaiJun-Aion">程序员海军</a>&nbsp;
阅读(<span id="post_view_count">228</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(19037450);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '19037450', targetLink: 'https://www.cnblogs.com/HaiJun-Aion/p/19037450', title: 'MCP神器！MCP-USE 一键部署连接任何MCP服务器' })">举报</a>
</div>
        