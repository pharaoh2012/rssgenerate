
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/carpell/p/18890267" title="å‘å¸ƒäº 2025-05-22 10:24">
    <span role="heading" aria-level="2">ã€è¯­ä¹‰åˆ†å‰²ä¸“æ ã€‘ï¼šFCNå®æˆ˜ç¯‡(é™„ä¸Šå®Œæ•´å¯è¿è¡Œçš„ä»£ç pytorch)</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                    <div id="cnblogs_post_description" style="display: none">
        
        æœ¬æ–‡è¯¦ç»†ä»‹ç»äº†å¦‚ä½•æ‰‹åŠ¨å¤ç°FCNï¼ˆå…¨å·ç§¯ç½‘ç»œï¼‰è¯­ä¹‰åˆ†å‰²æ¨¡å‹ï¼Œå¹¶æ·±å…¥è®²è§£äº†ä»£ç å®ç°ã€‚æ–‡ç« é¦–å…ˆå›é¡¾äº†FCNçš„åŸç†ï¼Œå¼ºè°ƒäº†ç†è§£ä»£ç çš„é‡è¦æ€§ï¼ŒéšåæŒ‰ç…§æ¨¡å‹æ„å»ºã€æ•°æ®é›†å¤„ç†ã€è¯„ä»·æŒ‡æ ‡è®¾å®šã€è®­ç»ƒæµç¨‹å’Œæµ‹è¯•äº”ä¸ªéƒ¨åˆ†è¿›è¡Œè®²è§£ã€‚
    </div>
<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p></p><div class="toc"><div class="toc-container-header">ç›®å½•</div><ul><li><a href="#å‰è¨€" rel="noopener nofollow">å‰è¨€</a></li><li><a href="#fcnå…¨æµç¨‹ä»£ç " rel="noopener nofollow">FCNå…¨æµç¨‹ä»£ç </a><ul><li><a href="#æ¨¡å‹æ­å»ºmodel" rel="noopener nofollow">æ¨¡å‹æ­å»º(model)</a></li><li><a href="#æ•°æ®å¤„ç†dataloader" rel="noopener nofollow">æ•°æ®å¤„ç†(dataloader)</a></li><li><a href="#è¯„ä»·æŒ‡æ ‡metric" rel="noopener nofollow">è¯„ä»·æŒ‡æ ‡(metric)</a></li><li><a href="#è®­ç»ƒæµç¨‹train" rel="noopener nofollow">è®­ç»ƒæµç¨‹(train)</a></li><li><a href="#æ¨¡å‹æµ‹è¯•test" rel="noopener nofollow">æ¨¡å‹æµ‹è¯•(test)</a></li></ul></li><li><a href="#æ•ˆæœå›¾" rel="noopener nofollow">æ•ˆæœå›¾</a></li><li><a href="#ç»“è¯­" rel="noopener nofollow">ç»“è¯­</a></li></ul></div><p></p>
<h1 id="å‰è¨€">å‰è¨€</h1>
<blockquote>
<p>FCNåŸç†ç¯‡è®²è§£ï¼š<a href="https://www.cnblogs.com/carpell/p/18883197" target="_blank">ã€è¯­ä¹‰åˆ†å‰²ä¸“æ ã€‘ï¼šFCNåŸç†ç¯‡ - carpell - åšå®¢å›­</a></p>
<p>ä»£ç åœ°å€ï¼Œä¸‹è½½å¯å¤ç°ï¼š<a href="https://github.com/fouen6/FCN_semantic-segmentation" target="_blank" rel="noopener nofollow">fouen6/FCN_semantic-segmentation</a></p>
</blockquote>
<p>æœ¬ç¯‡æ–‡ç« æ”¶å½•äºè¯­ä¹‰åˆ†å‰²ä¸“æ ï¼Œå¦‚æœå¯¹è¯­ä¹‰åˆ†å‰²é¢†åŸŸæ„Ÿå…´è¶£çš„ï¼Œå¯ä»¥å»çœ‹çœ‹ä¸“æ ï¼Œä¼šå¯¹ç»å…¸çš„æ¨¡å‹ä»¥åŠä»£ç è¿›è¡Œè¯¦ç»†çš„è®²è§£å“¦ï¼å…¶ä¸­ä¼šåŒ…å«å¯å¤ç°çš„ä»£ç ï¼</p>
<p>ä¸Šç¯‡æ–‡ç« å·²ç»å¸¦å¤§å®¶å­¦ä¹ è¿‡äº†FCNçš„åŸç†ï¼Œç›¸ä¿¡å¤§å®¶å¯¹äºåŸç†åº”è¯¥æœ‰äº†æ¯”è¾ƒæ·±çš„äº†è§£ã€‚æœ¬æ–‡å°†ä¼šå¸¦å¤§å®¶å»æ‰‹åŠ¨å¤ç°å±äºè‡ªå·±çš„ä¸€ä¸ªè¯­ä¹‰åˆ†å‰²æ¨¡å‹ã€‚å°†ä¼šæ·±å…¥ä»£ç è¿›è¡Œè®²è§£ï¼Œå¦‚æœæœ‰è®²é”™çš„åœ°æ–¹æ¬¢è¿å¤§å®¶æ‰¹è¯„æŒ‡æ­£ï¼</p>
<p>å…¶å®æ‰€æœ‰çš„æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ­å»ºæˆ‘è®¤ä¸ºå¯ä»¥æ€»ç»“æˆäº”éƒ¨åˆ†ï¼šæ¨¡å‹çš„æ„å»ºï¼Œæ•°æ®é›†çš„å¤„ç†ï¼Œè¯„ä»·æŒ‡æ ‡çš„è®¾å®šï¼Œè®­ç»ƒæµç¨‹ï¼Œæµ‹è¯•ã€‚å…¶å®æ„Ÿè§‰æœ‰ç‚¹æ·±åº¦å­¦ä¹ ä»£ç å…«è‚¡æ–‡çš„é‚£ç§æ„æ€ã€‚æœ¬ç¯‡åŒæ ·çš„ä¹Ÿä¼šæŒ‰ç…§è¿™æ ·çš„æ–¹å¼è¿›è¡Œè®²è§£ï¼Œå¸Œæœ›å¤§å®¶èƒ½å¤Ÿæ·±å…¥ä»£ç å»è¿›è¡Œäº†è§£å­¦ä¹ ã€‚</p>
<p>è¯·è®°ä½ï¼š<strong>åªæ‡‚åŸç†ä¸æ‡‚ä»£ç ï¼Œä½ å°±ç®—æœ‰äº†å¾ˆå¥½çš„æƒ³æ³•åˆ›æ–°ç‚¹ï¼Œä½ ä¹Ÿéš¾ä»¥å»å®ç°ï¼Œæ‰€ä»¥å¸Œæœ›å¤§å®¶èƒ½å¤Ÿæ·±å…¥å»äº†è§£</strong>ï¼Œæœ€å¥½èƒ½å¤Ÿå‚è€ƒç€æœ¬æ–‡è‡ªå·±å¤ç°ä¸€ä¸‹ã€‚</p>
<p><img src="https://gitee.com/fouen/image/raw/master/image/20250502185328903.png" alt="image-20250428005507704" loading="lazy"></p>
<h1 id="fcnå…¨æµç¨‹ä»£ç ">FCNå…¨æµç¨‹ä»£ç </h1>
<h2 id="æ¨¡å‹æ­å»ºmodel">æ¨¡å‹æ­å»º(model)</h2>
<p>æˆ‘ä»¬è¿™é‡Œæ ¹æ®åŸè®ºæ–‡ä¸€æ ·é‡‡ç”¨VGGä½œä¸ºæˆ‘ä»¬çš„ç‰¹å¾æå–ç½‘ç»œï¼Œå¦‚æœä½ å¯¹VGGç½‘ç»œè¿˜ä¸å¤ªäº†è§£çš„è¯ï¼Œå¯ä»¥å…ˆå»çœ‹çœ‹æˆ‘å¯¹VGGç½‘ç»œçš„è®²è§£ã€‚</p>
<p>æˆ‘ä»¬éƒ½çŸ¥é“VGGé‡‡ç”¨äº†ä¸€äº›é‡å¤çš„ç»“æ„ï¼Œæ‰€ä»¥æˆ‘ä»¬æ ¹æ®maxpoolå‡ºç°çš„ä½ç½®å°†å…¶åˆ’åˆ†ä¸º5ä¸ªstageã€‚è¿™æ ·æˆ‘ä»¬å¯ä»¥åŒæ—¶ç”¨ä¸åŒæ·±åº¦çš„VGGçš„ç½‘ç»œï¼ŒVGG11åˆ°VGG19éƒ½å¯ä»¥ä½¿ç”¨ï¼Œå› ä¸ºå…¶ç»“æ„æ˜¯ä¸€æ ·çš„ã€‚</p>
<pre><code class="language-python">		backbone = get_backbone(backbone=backbone, pretrained=True)
        features = list(backbone.features.children())

        pool_indices = [i + 1 for i, layer in enumerate(features) if isinstance(layer, nn.MaxPool2d)]
        pool_indices = [0] + pool_indices + [len(features)]

        # åˆ’åˆ†é˜¶æ®µ
        self.stage1 = nn.Sequential(*features[pool_indices[0]:pool_indices[1]])
        self.stage2 = nn.Sequential(*features[pool_indices[1]:pool_indices[2]])
        self.stage3 = nn.Sequential(*features[pool_indices[2]:pool_indices[3]])
        self.stage4 = nn.Sequential(*features[pool_indices[3]:pool_indices[4]])
        self.stage5 = nn.Sequential(*features[pool_indices[4]:pool_indices[5]])
</code></pre>
<p>ç„¶åä¸€ä¸ªéå¸¸é‡è¦çš„ï¼Œæˆ‘ä»¬é‡‡ç”¨æˆ‘ä»¬çš„åŒçº¿æ€§æ’å€¼æ¥åˆå§‹åŒ–æˆ‘ä»¬çš„åå·ç§¯ï¼Œä½¿ç”¨åŒçº¿æ€§æ’å€¼æ¥åˆå§‹åŒ–ï¼Œå¯ä»¥åœ¨è®­ç»ƒåˆæœŸä¿è¯æ¨¡å‹æœ‰ä¸€ä¸ªæ¯”è¾ƒå¥½çš„è¾“å‡ºç„¶ååœ¨é€šè¿‡è®­ç»ƒè°ƒæ•´ã€‚</p>
<pre><code class="language-python">def _make_bilinear_weights(size,num_channels):
    factor = (size+1)//2

    if size % 2 == 1:
        center = factor - 1
    else:
        center = factor - 0.5
    og = torch.FloatTensor(size, size)
    for i in range(size):
        for j in range(size):
            og[i, j] = (1-abs((i-center)/factor)) * (1-abs((j-center)/factor))
    filter = torch.zeros(num_channels,num_channels,size,size)
    for i in range(num_channels):
        filter[i,i] = og
    return filter

</code></pre>
<p>æœ€åæˆ‘ä»¬å…ˆæ­å»ºæˆ‘ä»¬çš„FCN32sæ¨¡å‹ï¼Œé¦–å…ˆæˆ‘ä»¬åŠ è½½é¢„è®­ç»ƒçš„VGG16ï¼Œå½“ç„¶åˆ«çš„VGGä¹Ÿè¡Œï¼Œä½ è‡ªå·±é€‰æ‹©å°±è¡Œäº†ã€‚æˆ‘ä»¬åªéœ€è¦å…¶å…¨è¿æ¥å±‚å‰çš„æ‰€æœ‰å±‚ï¼Œåˆ’åˆ†ä¸åŒçš„stageï¼Œç„¶åæ„å»ºfcnæ£€æµ‹å¤´ï¼Œå…ˆ7x7çš„å·ç§¯ï¼Œç­‰æ•ˆäºå¯¹7Ã—7æ„Ÿå—é‡åšå…¨è¿æ¥ï¼Œè¾“å‡º4096é€šé“ï¼Œç„¶åå†1x1çš„å·ç§¯ï¼Œæå–è¯­ä¹‰ç‰¹å¾ï¼Œæœ€ååœ¨1x1å·ç§¯è¾“å‡ºæ¯ä¸ªç©ºé—´ä½ç½®ä¸Šçš„ç±»åˆ«åˆ†å¸ƒï¼ˆå¦‚21ç±»ï¼‰ã€‚FCN32sæ˜¯ç›´æ¥ä»æœ€åçš„å±‚è¿›è¡Œ32å€ä¸Šé‡‡æ ·ï¼Œå½“ç„¶äº†è¿™æ ·çš„ç»“æœå°±æ¯”è¾ƒç²—ç³™äº†ã€‚æ‰€ä»¥æ•ˆæœä¸ä¼šå¤ªå¥½ã€‚</p>
<p>è¿™é‡Œæœ‰ä¸ªç»†èŠ‚å“ˆï¼Œx = x[:, :, :input_size[0], :input_size[1]]ï¼Œæˆ‘ä»¬è£å‰ªäº†ä¿è¯åˆå§‹å¤§å°ï¼Œå› ä¸ºä¸Šé‡‡æ ·è¿‡ç¨‹ä¸­å¯èƒ½ä¼šé€ æˆå›¾åƒçš„å°ºåº¦è¶…å‡ºä¸€ç‚¹ç‚¹çš„ï¼Œæ¯”å¦‚ä¸Šé‡‡æ ·ååº”è¯¥æ˜¯224ï¼Œç„¶åæœ€åæ˜¯225ï¼Œæ‰€ä»¥è£å‰ªä¿è¯ä¸åˆå§‹å¤§å°ç›¸åŒã€‚</p>
<pre><code class="language-python">class FCN32s(nn.Module):
    def __init__(self,num_classes = 21,backbone='vgg16'):
        super(FCN32s, self).__init__()
        self.num_classes = num_classes
        backbone = get_backbone(backbone=backbone, pretrained=True)
        features = list(backbone.features.children())

        pool_indices = [i + 1 for i, layer in enumerate(features) if isinstance(layer, nn.MaxPool2d)]
        pool_indices = [0] + pool_indices + [len(features)]

        # åˆ’åˆ†é˜¶æ®µ
        self.stage1 = nn.Sequential(*features[pool_indices[0]:pool_indices[1]])
        self.stage2 = nn.Sequential(*features[pool_indices[1]:pool_indices[2]])
        self.stage3 = nn.Sequential(*features[pool_indices[2]:pool_indices[3]])
        self.stage4 = nn.Sequential(*features[pool_indices[3]:pool_indices[4]])
        self.stage5 = nn.Sequential(*features[pool_indices[4]:pool_indices[5]])

        self.fcn_head = nn.Sequential(
            nn.Conv2d(512, 4096, kernel_size=7,padding=3),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Conv2d(4096,4096,kernel_size=1),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Conv2d(4096, self.num_classes, kernel_size=1),
        )

        self.upsample32 = nn.ConvTranspose2d(self.num_classes,self.num_classes,kernel_size = 64,stride = 32,padding = 16,bias = False)

        for m in self.modules():
            if isinstance(m, nn.ConvTranspose2d):
                m.weight.data.zero_()
                m.weight.data = _make_bilinear_weights(m.kernel_size[0], m.out_channels)

    def forward(self, x):
        input_size = x.size()[2:]
        x = self.stage1(x)
        x = self.stage2(x)
        x = self.stage3(x)
        x = self.stage4(x)
        x = self.stage5(x)

        x = self.fcn_head(x)
        x = self.upsample32(x)
        x = x[:, :, :input_size[0], :input_size[1]]
        return x
</code></pre>
<p>ç„¶åå°±æ˜¯FCN16säº†ï¼Œæˆ‘ä»¬é€šè¿‡å°†stage4çš„è¾“å‡ºä½œä¸ºæˆ‘ä»¬çš„pool4ï¼ŒåŒæ—¶æˆ‘ä»¬å°†pool4ç»è¿‡å·ç§¯è¾“å‡ºé€šé“åˆ°å˜ä¸ºç±»åˆ«åˆ†æ•°ï¼Œåˆ°æ—¶å€™æ–¹ä¾¿è·Ÿæœ€ç»ˆçš„è¾“å‡ºåšè·³è·ƒè¿æ¥ã€‚ç»è¿‡fcn_headè¾“å‡ºçš„xå…ˆä¸Šé‡‡æ ·ä¸¤å€åˆ°è·Ÿpool4ç›¸åŒçš„shapeï¼Œç„¶åä¸¤è€…åšè·³è·ƒè¿æ¥ç›¸åŠ åå†ä¸Šé‡‡æ ·16å€åˆ°è¾“å…¥å›¾åƒçš„shapeå¤§å°ã€‚</p>
<pre><code class="language-python">class FCN16s(nn.Module):
    def __init__(self,num_classes = 21,backbone='vgg16'):
        super(FCN16s, self).__init__()
        self.num_classes = num_classes
        backbone = get_backbone(backbone=backbone, pretrained=True)
        features = list(backbone.features.children())

        pool_indices = [i + 1 for i, layer in enumerate(features) if isinstance(layer, nn.MaxPool2d)]
        pool_indices = [0] + pool_indices + [len(features)]

        # åˆ’åˆ†é˜¶æ®µ
        self.stage1 = nn.Sequential(*features[pool_indices[0]:pool_indices[1]])
        self.stage2 = nn.Sequential(*features[pool_indices[1]:pool_indices[2]])
        self.stage3 = nn.Sequential(*features[pool_indices[2]:pool_indices[3]])
        self.stage4 = nn.Sequential(*features[pool_indices[3]:pool_indices[4]])
        self.stage5 = nn.Sequential(*features[pool_indices[4]:pool_indices[5]])

        self.fcn_head = nn.Sequential(
            nn.Conv2d(512, 4096, kernel_size=7,padding=3),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Conv2d(4096,4096,kernel_size=1),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Conv2d(4096, self.num_classes, kernel_size=1),
        )

        self.pool4_score = nn.Conv2d(512,self.num_classes, kernel_size=1)

        self.upsample2 = nn.ConvTranspose2d(self.num_classes,self.num_classes,kernel_size = 4,stride = 2,padding = 1,
                                            bias = False)
        self.upsample16 = nn.ConvTranspose2d(self.num_classes, self.num_classes, kernel_size=32, stride=16, padding=8,
                                            bias=False)

        for m in self.modules():
            if isinstance(m, nn.ConvTranspose2d):
                m.weight.data.zero_()
                m.weight.data = _make_bilinear_weights(m.kernel_size[0], m.out_channels)

    def forward(self, x):
        input_size = x.size()[2:]
        x = self.stage1(x)
        x = self.stage2(x)
        x = self.stage3(x)
        x = self.stage4(x)
        pool4 = x
        x = self.stage5(pool4)

        x = self.fcn_head(x)
        x = self.upsample2(x)

        pool4_score = self.pool4_score(pool4)
        pool4_score = pool4_score[:, :, :x.size()[2], :x.size()[3]]
        x = x + pool4_score

        x = self.upsample16(x)
        x = x[:, :, :input_size[0], :input_size[1]]
        return x
</code></pre>
<p>ç„¶åå°±æ˜¯FCN8säº†ï¼Œæˆ‘ä»¬é€šè¿‡å°†stage3å’Œstage4çš„è¾“å‡ºä½œä¸ºæˆ‘ä»¬çš„pool3å’Œpool4ï¼ŒåŒæ—¶æˆ‘ä»¬å°†åˆ†åˆ«å°†pool3å’Œpool4ç»è¿‡å·ç§¯è¾“å‡ºå°†é€šé“åˆ°å˜ä¸ºç±»åˆ«åˆ†æ•°ï¼Œåˆ°æ—¶å€™æ–¹ä¾¿è·Ÿæœ€ç»ˆçš„è¾“å‡ºåšè·³è·ƒè¿æ¥ã€‚ç»è¿‡fcn_headè¾“å‡ºçš„xå…ˆä¸Šé‡‡æ ·ä¸¤å€åˆ°è·Ÿpool4ç›¸åŒçš„shapeï¼Œç„¶åä¸¤è€…åšè·³è·ƒè¿æ¥ç›¸åŠ åå†ä¸Šé‡‡æ ·2å€åˆ°pool3çš„shapeå¤§å°ã€‚å†ä¸pool3åšè·³è·ƒè¿æ¥ç›¸åŠ ï¼Œä¸Šé‡‡æ ·8å€æ•°åˆ°è¾“å‡ºå›¾åƒçš„shapeå¤§å°ã€‚</p>
<pre><code class="language-python">class FCN8s(nn.Module):
    def __init__(self,num_classes = 21,backbone='vgg16'):
        super(FCN8s, self).__init__()
        self.num_classes = num_classes
        backbone = get_backbone(backbone=backbone,pretrained=True)
        features = list(backbone.features.children())

        pool_indices = [i +1 for i, layer in enumerate(features) if isinstance(layer, nn.MaxPool2d)]
        pool_indices = [0] + pool_indices + [len(features)]

        # åˆ’åˆ†é˜¶æ®µ
        self.stage1 = nn.Sequential(*features[pool_indices[0]:pool_indices[1]])
        self.stage2 = nn.Sequential(*features[pool_indices[1]:pool_indices[2]])
        self.stage3 = nn.Sequential(*features[pool_indices[2]:pool_indices[3]])
        self.stage4 = nn.Sequential(*features[pool_indices[3]:pool_indices[4]])
        self.stage5 = nn.Sequential(*features[pool_indices[4]:pool_indices[5]])

        self.fcn_head = nn.Sequential(
            nn.Conv2d(512, 4096, kernel_size=7,padding=3),
            nn.ReLU(inplace=True),
            nn.Dropout2d(),
            nn.Conv2d(4096,4096,kernel_size=1),
            nn.ReLU(inplace=True),
            nn.Dropout2d(),
            nn.Conv2d(4096, self.num_classes, kernel_size=1),
        )


        self.pool3_score = nn.Conv2d(256,self.num_classes, kernel_size=1)
        self.pool4_score = nn.Conv2d(512, self.num_classes, kernel_size=1)

        self.upsample2_1 = nn.ConvTranspose2d(self.num_classes,self.num_classes,kernel_size = 4,stride = 2,padding = 1,
                                            bias = False)
        self.upsample2_2 = nn.ConvTranspose2d(self.num_classes, self.num_classes, kernel_size=4, stride=2, padding=1,
                                              bias=False)
        self.upsample8 = nn.ConvTranspose2d(self.num_classes, self.num_classes, kernel_size=16, stride=8, padding=4,
                                            bias=False)
        for m in self.modules():
            if isinstance(m, nn.ConvTranspose2d):
                m.weight.data.zero_()
                m.weight.data=_make_bilinear_weights(m.kernel_size[0],m.out_channels)


    def forward(self, x):
        input_size = x.size()[2:]
        x = self.stage1(x)
        x = self.stage2(x)
        pool3 = self.stage3(x)
        pool4 = self.stage4(pool3)
        x = self.stage5(pool4)

        x = self.fcn_head(x)
        x = self.upsample2_1(x)
        pool4_score = self.pool4_score(pool4)
        pool4_score = pool4_score[:, :, :x.size()[2], :x.size()[3]]
        x = x + pool4_score
        x = self.upsample2_2(x)
        pool3_score = self.pool3_score(pool3)
        pool3_score = pool3_score[:, :, :x.size()[2], :x.size()[3]]
        x = x + pool3_score

        x = self.upsample8(x)
        x = x[:, :, :input_size[0], :input_size[1]]
        return x
</code></pre>
<h2 id="æ•°æ®å¤„ç†dataloader">æ•°æ®å¤„ç†(dataloader)</h2>
<blockquote>
<p>æ•°æ®é›†åç§°ï¼šVOC2012</p>
<p>æ•°æ®é›†ä¸‹è½½åœ°å€ï¼š<a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html#data" target="_blank" rel="noopener nofollow">The PASCAL Visual Object Classes Challenge 2012 (VOC2012)</a></p>
</blockquote>
<p><img src="https://gitee.com/fouen/image/raw/master/image/20250502190727665.png" alt="image-20250502190727617" loading="lazy"></p>
<p>åœ¨è¿™é‡Œä¸‹è½½å“ˆï¼Œ2GBçš„é‚£ä¸ªã€‚</p>
<p>è¿™é‡Œæˆ‘å·²ç»ä¸“é—¨å‘äº†ä¸€ç¯‡åšå®¢å¯¹è¯­ä¹‰åˆ†å‰²ä»»åŠ¡å¸¸ç”¨çš„æ•°æ®é›†åšäº†æ·±å…¥çš„ä»‹ç»ï¼Œå·²ç»å…·ä½“è®²è§£äº†å…¶å®ç°çš„å¤„ç†ä»£ç ã€‚å¦‚æœä½ å¯¹è¯­ä¹‰åˆ†å‰²å¸¸ç”¨æ•°æ®é›†æœ‰ä¸äº†è§£çš„è¯ï¼Œ<strong>å¯ä»¥å…ˆå»æˆ‘çš„è¯­ä¹‰åˆ†å‰²ä¸“æ ä¸­è¿›è¡Œäº†è§£å“¦</strong>ï¼ï¼  æˆ‘è¿™é‡Œå°±ç›´æ¥é™„ä¸Šä»£ç äº†ã€‚</p>
<pre><code class="language-python">import torch
import numpy as np
from PIL import Image
from torch.utils.data import Dataset, DataLoader
import os
import random
import torchvision.transforms as T

VOC_CLASSES = [
    'background','aeroplane','bicycle','bird','boat','bottle','bus',
    'car','cat','chair','cow','diningtable','dog','horse',
    'motorbike','person','potted plant','sheep','sofa','train','tv/monitor'
]



class VOCSegmentation(Dataset):
    def __init__(self, root, split='train', img_size=320, augment=True):
        super(VOCSegmentation, self).__init__()
        self.root = root
        self.split = split
        self.img_size = img_size
        self.augment = augment

        img_dir = os.path.join(root, 'JPEGImages')
        mask_dir = os.path.join(root, 'SegmentationClass')
        split_file = os.path.join(root, 'ImageSets', 'Segmentation', f'{split}.txt')
        if not os.path.exists(split_file):
            raise FileNotFoundError(split_file)

        with open(split_file, 'r') as f:
            file_names = [x.strip() for x in f.readlines()]

        self.images = [os.path.join(img_dir, x + '.jpg') for x in file_names]
        self.masks = [os.path.join(mask_dir, x + '.png') for x in file_names]
        assert len(self.images) == len(self.masks)
        print(f"âœ… {split} set loaded: {len(self.images)} samples")

        self.normalize = T.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225])

    def __getitem__(self, index):
        img = Image.open(self.images[index]).convert('RGB')
        mask = Image.open(self.masks[index])  # maskä¸ºPæ¨¡å¼ï¼ˆ0~20çš„ç±»åˆ«ï¼‰

        # Resize
        img = img.resize((self.img_size, self.img_size), Image.BILINEAR)
        mask = mask.resize((self.img_size, self.img_size), Image.NEAREST)

        # è½¬Tensor
        img = T.functional.to_tensor(img)
        mask = torch.from_numpy(np.array(mask)).long()  # 0~20

        # æ•°æ®å¢å¼º
        if self.augment:
            if random.random() &gt; 0.5:
                img = T.functional.hflip(img)
                mask = T.functional.hflip(mask)
            if random.random() &gt; 0.5:
                img = T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2)(img)

        img = self.normalize(img)
        return img, mask

    def __len__(self):
        return len(self.images)


def get_dataloader(data_path, batch_size=4, img_size=320, num_workers=4):
    train_dataset = VOCSegmentation(root=data_path, split='train', img_size=img_size, augment=True)
    val_dataset = VOCSegmentation(root=data_path, split='val', img_size=img_size, augment=False)

    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, pin_memory=True, num_workers=num_workers)
    val_loader = DataLoader(val_dataset, shuffle=False, batch_size=batch_size, pin_memory=True, num_workers=num_workers)
    return train_loader, val_loader
</code></pre>
<h2 id="è¯„ä»·æŒ‡æ ‡metric">è¯„ä»·æŒ‡æ ‡(metric)</h2>
<p>æˆ‘ä»¬è¿™é‡Œè¯­ä¹‰åˆ†å‰²é‡‡ç”¨çš„è¯„ä»·æŒ‡æ ‡ä¸ºï¼šPA(åƒç´ å‡†ç¡®ç‡)ï¼ŒCPA(ç±»åˆ«åƒç´ å‡†ç¡®ç‡)ï¼ŒMPA(ç±»åˆ«å¹³å‡åƒç´ å‡†ç¡®ç‡)ï¼ŒIoU(äº¤å¹¶æ¯”)ï¼ŒmIoU(å¹³å‡äº¤å¹¶æ¯”)ï¼ŒFWIoU(é¢‘ç‡åŠ æƒäº¤å¹¶æ¯”)ï¼ŒmF1(å¹³å‡F1åˆ†æ•°)ã€‚</p>
<p>è¿™é‡Œæˆ‘å·²ç»ä¸“é—¨å‘äº†ä¸€ç¯‡åšå®¢å¯¹è¿™äº›å¹³å‡æŒ‡æ ‡åšäº†æ·±å…¥çš„ä»‹ç»ï¼Œå·²ç»å…·ä½“è®²è§£äº†å…¶å®ç°çš„ä»£ç ã€‚å¦‚æœä½ å¯¹è¿™äº›è¯„ä»·æŒ‡æ ‡æœ‰ä¸äº†è§£çš„è¯ï¼Œ<strong>å¯ä»¥å…ˆå»æˆ‘çš„è¯­ä¹‰åˆ†å‰²ä¸“æ ä¸­è¿›è¡Œäº†è§£å“¦</strong>ï¼ï¼  æˆ‘è¿™é‡Œå°±ç›´æ¥é™„ä¸Šä»£ç äº†ã€‚</p>
<pre><code class="language-python">import numpy as np

__all__ = ['SegmentationMetric']


class SegmentationMetric(object):
    def __init__(self, numClass):
        self.numClass = numClass
        self.confusionMatrix = np.zeros((self.numClass,) * 2)

    def genConfusionMatrix(self, imgPredict, imgLabel):
        mask = (imgLabel &gt;= 0) &amp; (imgLabel &lt; self.numClass)
        label = self.numClass * imgLabel[mask] + imgPredict[mask]
        count = np.bincount(label, minlength=self.numClass ** 2)
        confusionMatrix = count.reshape(self.numClass, self.numClass)
        return confusionMatrix

    def addBatch(self, imgPredict, imgLabel):
        assert imgPredict.shape == imgLabel.shape
        self.confusionMatrix += self.genConfusionMatrix(imgPredict, imgLabel)
        return self.confusionMatrix

    def pixelAccuracy(self):
        acc = np.diag(self.confusionMatrix).sum() / self.confusionMatrix.sum()
        return acc

    def classPixelAccuracy(self):
        denominator = self.confusionMatrix.sum(axis=1)
        denominator = np.where(denominator == 0, 1e-12, denominator)
        classAcc = np.diag(self.confusionMatrix) / denominator
        return classAcc

    def meanPixelAccuracy(self):
        classAcc = self.classPixelAccuracy()
        meanAcc = np.nanmean(classAcc)
        return meanAcc

    def IntersectionOverUnion(self):
        intersection = np.diag(self.confusionMatrix)
        union = np.sum(self.confusionMatrix, axis=1) + np.sum(self.confusionMatrix, axis=0) - np.diag(
            self.confusionMatrix)
        union = np.where(union == 0, 1e-12, union)
        IoU = intersection / union
        return IoU

    def meanIntersectionOverUnion(self):
        mIoU = np.nanmean(self.IntersectionOverUnion())
        return mIoU

    def Frequency_Weighted_Intersection_over_Union(self):
        denominator1 = np.sum(self.confusionMatrix)
        denominator1 = np.where(denominator1 == 0, 1e-12, denominator1)
        freq = np.sum(self.confusionMatrix, axis=1) / denominator1
        denominator2 = np.sum(self.confusionMatrix, axis=1) + np.sum(self.confusionMatrix, axis=0) - np.diag(
            self.confusionMatrix)
        denominator2 = np.where(denominator2 == 0, 1e-12, denominator2)
        iu = np.diag(self.confusionMatrix) / denominator2
        FWIoU = (freq[freq &gt; 0] * iu[freq &gt; 0]).sum()
        return FWIoU

    def classF1Score(self):
        tp = np.diag(self.confusionMatrix)
        fp = self.confusionMatrix.sum(axis=0) - tp
        fn = self.confusionMatrix.sum(axis=1) - tp

        precision = tp / (tp + fp + 1e-12)
        recall = tp / (tp + fn + 1e-12)

        f1 = 2 * precision * recall / (precision + recall + 1e-12)
        return f1

    def meanF1Score(self):
        f1 = self.classF1Score()
        mean_f1 = np.nanmean(f1)
        return mean_f1

    def reset(self):
        self.confusionMatrix = np.zeros((self.numClass, self.numClass))

    def get_scores(self):
        scores = {
            'Pixel Accuracy': self.pixelAccuracy(),
            'Class Pixel Accuracy': self.classPixelAccuracy(),
            'Intersection over Union': self.IntersectionOverUnion(),
            'Class F1 Score': self.classF1Score(),
            'Frequency Weighted Intersection over Union': self.Frequency_Weighted_Intersection_over_Union(),
            'Mean Pixel Accuracy': self.meanPixelAccuracy(),
            'Mean Intersection over Union(mIoU)': self.meanIntersectionOverUnion(),
            'Mean F1 Score': self.meanF1Score()
        }
        return scores

</code></pre>
<h2 id="è®­ç»ƒæµç¨‹train">è®­ç»ƒæµç¨‹(train)</h2>
<p>åˆ°è¿™é‡Œï¼Œæ‰€æœ‰çš„å‰æœŸå‡†å¤‡éƒ½å·²ç»å°±ç»ªï¼Œæˆ‘ä»¬å°±è¦å¼€å§‹è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹äº†ã€‚</p>
<pre><code class="language-python">def parse_arguments():
    parser = argparse.ArgumentParser()
    parser.add_argument('--data_root', type=str, default='./datasets/VOC2012', help='Dataset root path')
    parser.add_argument('--classes_name', type=str, default='VOC', help='Dataset class names')
    parser.add_argument('--backbone', type=str, default='vgg16', help='Backbone model')
    parser.add_argument('--head', type=str, default='fcn8s', help='Segmentation head')
    parser.add_argument('--num_classes', type=int, default=21, help='Number of classes')
    parser.add_argument('--epochs', type=int, default=50, help='Epochs')
    parser.add_argument('--lr', type=float, default=0.005, help='Learning rate')
    parser.add_argument('--momentum', type=float, default=0.9, help='Momentum')
    parser.add_argument('--weight-decay', type=float, default=1e-4, help='Weight decay')
    parser.add_argument('--batch_size', type=int, default=8, help='Batch size')
    parser.add_argument('--checkpoint', type=str, default='./checkpoint', help='Checkpoint directory')
    parser.add_argument('--resume', type=str, default=None, help='Resume checkpoint path')
    return parser.parse_args()
</code></pre>
<p>é¦–å…ˆæ¥çœ‹çœ‹æˆ‘ä»¬çš„ä¸€äº›å‚æ•°çš„è®¾å®šï¼Œä¸€èˆ¬æˆ‘ä»¬éƒ½æ˜¯è¿™æ ·æ”¾åœ¨æœ€å‰é¢ï¼Œèƒ½å¤Ÿè®©äººæ›´åŠ å¿«é€Ÿçš„äº†è§£å…¶ä»£ç çš„ä¸€äº›æ ¸å¿ƒå‚æ•°è®¾ç½®ã€‚é¦–å…ˆå°±æ˜¯æˆ‘ä»¬çš„æ•°æ®é›†ä½ç½®()ï¼Œç„¶åå°±æ˜¯æˆ‘ä»¬çš„æ•°æ®é›†åç§°(classes_name)ï¼Œè¿™ä¸ªæš‚æ—¶æ²¡ä»€ä¹ˆç”¨ï¼Œå› ä¸ºæˆ‘ä»¬ç›®å‰åªç”¨äº†VOCæ•°æ®é›†ï¼Œç„¶åå°±æ˜¯ç‰¹å¾æå–ç½‘ç»œçš„é€‰æ‹©(backbone)ï¼Œè¿™é‡Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©ä¸åŒæ·±åº¦çš„VGGç½‘ç»œï¼Œæ£€æµ‹æ¨¡å‹çš„é€‰æ‹©(head)ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©ä¸åŒçš„fcnæ¨¡å‹ï¼Œæ•°æ®é›†çš„ç±»åˆ«æ•°(num_classes)ï¼Œè®­ç»ƒepochæ•°ï¼Œè¿™ä¸ªä½ è®¾ç½®å¤§ä¸€ç‚¹ä¹Ÿè¡Œï¼Œå› ä¸ºæˆ‘ä»¬ä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿å­˜æœ€å¥½ç»“æœçš„æ¨¡å‹çš„ã€‚å­¦ä¹ ç‡(lr)ï¼ŒåŠ¨é‡(momentum)ï¼Œæƒé‡è¡°å‡(weight-decay)ï¼Œè¿™äº›éƒ½å±äºæ¨¡å‹è¶…å‚æ•°ï¼Œå¤§å®¶å¯ä»¥å°è¯•ä¸åŒçš„æ•°å€¼ï¼Œå¤šè¯•è¯•ï¼Œå°±ä¼šæœ‰ä¸ªå¤§è‡´çš„äº†è§£çš„ï¼Œæ‰¹é‡å¤§å°(batch_size)æ ¹æ®è‡ªå·±ç”µè„‘æ€§èƒ½æ¥è®¾ç½®ï¼Œä¸€èˆ¬éƒ½æ˜¯ä¸º2çš„å€æ•°ï¼Œä¿å­˜æƒé‡çš„æ–‡ä»¶å¤¹(checkpoint)ï¼Œæ˜¯å¦ç»§ç»­è®­ç»ƒ(resume)ã€‚</p>
<pre><code class="language-python">def train(args):
    if not os.path.exists(args.checkpoint):
        os.makedirs(args.checkpoint)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    n_gpu = torch.cuda.device_count()
    print(f"Device: {device}, GPUs available: {n_gpu}")

    # Dataloader
    train_loader, val_loader = get_dataloader(args.data_root, batch_size=args.batch_size)
    train_dataset_size = len(train_loader.dataset)
    val_dataset_size = len(val_loader.dataset)
    print(f"Train samples: {train_dataset_size}, Val samples: {val_dataset_size}")

    # Model
    model = get_model(args.head, backbone=args.backbone, num_classes=args.num_classes)
    model.to(device)

    # Loss + Optimizer + Scheduler
    criterion = nn.CrossEntropyLoss(ignore_index=255)
    #optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)
    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)
    scaler = torch.cuda.amp.GradScaler()

    # Resume
    start_epoch = 0
    best_miou = 0.0
    if args.resume and os.path.isfile(args.resume):
        print(f"Loading checkpoint '{args.resume}'")
        checkpoint = torch.load(args.resume)
        start_epoch = checkpoint['epoch']
        best_miou = checkpoint['best_miou']
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        print(f"Loaded checkpoint (epoch {start_epoch})")

    # Training history
    history = {
        'train_loss': [],
        'val_loss': [],
        'pixel_accuracy': [],
        'miou': []
    }

    print(f"ğŸš€ Start training ({args.head})")
    for epoch in range(start_epoch, args.epochs):
        model.train()
        train_loss = 0.0
        t0 = time.time()
        for images, masks in tqdm(train_loader, desc=f'Epoch {epoch+1}/{args.epochs} [Train]'):
            images = images.to(device)
            masks = masks.to(device)


            optimizer.zero_grad()
            with torch.cuda.amp.autocast():
                outputs = model(images)
                loss = criterion(outputs, masks)
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            train_loss += loss.item() * images.size(0)

        train_loss /= train_dataset_size
        history['train_loss'].append(train_loss)

        # Validation
        model.eval()
        val_loss = 0.0
        evaluator = SegmentationMetric(args.num_classes)
        with torch.no_grad():
            for images, masks in tqdm(val_loader, desc=f'Epoch {epoch+1}/{args.epochs} [Val]'):
                images = images.to(device)
                masks = masks.to(device)


                outputs = model(images)
                loss = criterion(outputs, masks)
                val_loss += loss.item() * images.size(0)

                predictions = torch.argmax(outputs, dim=1)
                if isinstance(predictions, torch.Tensor):
                    predictions = predictions.cpu().numpy()
                if isinstance(masks, torch.Tensor):
                    masks = masks.cpu().numpy()

                evaluator.addBatch(predictions, masks)

        val_loss /= val_dataset_size
        history['val_loss'].append(val_loss)

        scores = evaluator.get_scores()
        print(f"\nğŸ“ˆ Validation Epoch {epoch+1}:")
        for k, v in scores.items():
            if isinstance(v, np.ndarray):
                print(f"{k}: {np.round(v, 3)}")
            else:
                print(f"{k}: {v:.4f}")

        history['pixel_accuracy'].append(scores['Pixel Accuracy'])
        history['miou'].append(scores['Mean Intersection over Union(mIoU)'])

        # Save best
        if scores['Mean Intersection over Union(mIoU)'] &gt; best_miou:
            best_miou = scores['Mean Intersection over Union(mIoU)']
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'best_miou': best_miou,
            }, os.path.join(args.checkpoint, f'{args.head}_best.pth'))
            print(f"Saved best model! mIoU: {best_miou:.4f}")

        scheduler.step()

        print(f"ğŸ•’ Epoch time: {time.time() - t0:.2f}s\n")

    print("ğŸ‰ Training complete!")
</code></pre>
<p>ç„¶åå°±æ˜¯æˆ‘ä»¬çš„è®­ç»ƒæµç¨‹äº†ã€‚è®­ç»ƒæµç¨‹ä¹Ÿæ˜¯æœ‰å¥—è·¯çš„å“¦ï¼Œæˆ‘ä»¬è¯¥æ€ä¹ˆå»æ­å»ºä¸€ä¸ªæ›´å¥½çš„è®­ç»ƒæµç¨‹ï¼Œå¯ä»¥ä»å¤šæ–¹é¢å…¥æ‰‹çš„ã€‚</p>
<p>é¦–å…ˆæˆ‘ä»¬ç¡®å®šæˆ‘ä»¬çš„ä»£ç è¿è¡Œè®¾å¤‡ï¼ŒåŸºæœ¬ä¸Šéƒ½æ˜¯è¦GPUçš„ã€‚ç„¶åå°±æ˜¯åŠ è½½æˆ‘ä»¬å¤„ç†å¥½çš„æ•°æ®ï¼Œè¿™é‡Œå°±æ˜¯dataloaderçš„é‚£éƒ¨åˆ†äº†ï¼ŒåŠ è½½å¥½æ•°æ®ä¹‹åï¼Œæˆ‘ä»¬åŠ è½½æˆ‘ä»¬æ„å»ºå¥½çš„æ¨¡å‹ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬åœ¨modelé‚£éƒ¨åˆ†åšçš„å·¥ä½œã€‚ç„¶åå°±æ˜¯losså‡½æ•°ï¼ŒOptimizer å’Œ Schedulerï¼Œè¿™æ˜¯æˆ‘ä»¬æ¯”è¾ƒé‡è¦çš„å‡ ä¸ªéƒ¨åˆ†ã€‚losså‡½æ•°çš„é€‰æ‹©æœ‰å¾ˆå¤šï¼Œä¸åŒçš„losså‡½æ•°åœ¨ä¸€å®šç¨‹åº¦ä¸Šä¼šå†³å®šæˆ‘ä»¬çš„æ¨¡å‹æ”¶æ•›å¥½åï¼Œåƒè¯­ä¹‰åˆ†å‰²ä»»åŠ¡å°±åŸºæœ¬ä¸Šéƒ½æ˜¯ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°äº†ã€‚Optimizer ä¹Ÿæœ‰å¾ˆå¤šï¼ŒSGDï¼ŒAdamä¹‹ç±»çš„ï¼Œéƒ½å¯ä»¥å»å°è¯•ä¸‹ã€‚Schedulerå°±æ˜¯æˆ‘ä»¬çš„å­¦ä¹ ç­–ç•¥ï¼Œå­¦ä¹ ç‡çš„æ›´æ–°ï¼Œå¸Œæœ›ä¸€å¼€å§‹å­¦ä¹ ç‡å¤§ï¼Œè®­ç»ƒåˆ°åæœŸå­¦ä¹ ç‡å°ï¼Œè¿™æ ·åŠ é€Ÿæ”¶æ•›ï¼Œé¿å…éœ‡è¡ã€‚ç„¶åè¿˜æœ‰ä¸ªscalerï¼Œè¿™æ˜¯AMP(è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒ)ï¼Œèƒ½å¤ŸèŠ‚çœæˆ‘ä»¬çš„å†…å­˜ï¼Œè®©æˆ‘ä»¬çš„å°ç”µè„‘ä¹Ÿèƒ½è·‘èµ·æ¥æ¨¡å‹ã€‚</p>
<p>è¿˜æœ‰ä¸ªæ–­ç‚¹é‡è®­åŠŸèƒ½ï¼Œä¸ºäº†é¿å…å› ä¸ºä¸€äº›æ„å¤–çš„æƒ…å†µå¯¼è‡´è®­ç»ƒä¸­æ–­ï¼Œå¯èƒ½è¿™æ˜¯æˆ‘ä»¬è®­ç»ƒå¥½ä¹…çš„ç»“æœï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥é€šè¿‡è¿™ä¸ªåŠŸèƒ½ç»§ç»­ä»æ–­ç‚¹è¿›è¡Œè®­ç»ƒã€‚ç„¶åå°±æ˜¯è®­ç»ƒäº†ï¼Œæˆ‘ä»¬åŠ è½½æ•°æ®ï¼Œé€šè¿‡æ¨¡å‹çš„é¢„æµ‹ä¸maskå¾—åˆ°æŸå¤±ï¼Œç„¶åæ¢¯åº¦è¯¯å·®åä¼ ï¼Œæ›´æ–°æ¨¡å‹å‚æ•°ã€‚å½“ä¸€ä¸ªepochä¸­çš„æ•°æ®éƒ½è®­ç»ƒç»“æŸä¹‹åï¼Œæˆ‘ä»¬å°±éœ€è¦è¯„ä¼°ä¸‹æˆ‘ä»¬çš„æ¨¡å‹æ€ä¹ˆæ ·äº†ï¼Œè¿™é‡Œå°±æ˜¯æ ¹æ®æˆ‘ä»¬çš„è¯„ä»·æŒ‡æ ‡è¿›è¡Œè¯„ä»·ï¼Œå…¶ä¸­æˆ‘ä»¬æ ‡è®°best_mIoUï¼Œå½“æ›´å¥½çš„æ—¶å€™å°±é‡æ–°ä¿å­˜æ¨¡å‹æ–‡ä»¶ã€‚</p>
<p>æœ€åå½“è®­ç»ƒç»“æŸåæˆ‘ä»¬å°±ä¼šè·å¾—æœ€å¥½çš„æ¨¡å‹å‚æ•°çš„æ–‡ä»¶äº†ã€‚</p>
<p>å®Œæ•´ä»£ç ï¼š</p>
<pre><code class="language-python">import argparse
import os
import time
from tqdm import tqdm
import numpy as np
import torch
import torch.nn as nn
from datasets.VOC_dataloader import get_dataloader
from model import get_model
from metric import SegmentationMetric

def parse_arguments():
    parser = argparse.ArgumentParser()
    parser.add_argument('--data_root', type=str, default='./datasets/VOC2012', help='Dataset root path')
    parser.add_argument('--classes_name', type=str, default='VOC', help='Dataset class names')
    parser.add_argument('--backbone', type=str, default='vgg16', help='Backbone model')
    parser.add_argument('--head', type=str, default='fcn8s', help='Segmentation head')
    parser.add_argument('--num_classes', type=int, default=21, help='Number of classes')
    parser.add_argument('--epochs', type=int, default=50, help='Epochs')
    parser.add_argument('--lr', type=float, default=0.005, help='Learning rate')
    parser.add_argument('--momentum', type=float, default=0.9, help='Momentum')
    parser.add_argument('--weight-decay', type=float, default=1e-4, help='Weight decay')
    parser.add_argument('--batch_size', type=int, default=8, help='Batch size')
    parser.add_argument('--checkpoint', type=str, default='./checkpoint', help='Checkpoint directory')
    parser.add_argument('--resume', type=str, default=None, help='Resume checkpoint path')
    return parser.parse_args()

def train(args):
    if not os.path.exists(args.checkpoint):
        os.makedirs(args.checkpoint)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    n_gpu = torch.cuda.device_count()
    print(f"Device: {device}, GPUs available: {n_gpu}")

    # Dataloader
    train_loader, val_loader = get_dataloader(args.data_root, batch_size=args.batch_size)
    train_dataset_size = len(train_loader.dataset)
    val_dataset_size = len(val_loader.dataset)
    print(f"Train samples: {train_dataset_size}, Val samples: {val_dataset_size}")

    # Model
    model = get_model(args.head, backbone=args.backbone, num_classes=args.num_classes)
    model.to(device)

    # Loss + Optimizer + Scheduler
    criterion = nn.CrossEntropyLoss(ignore_index=255)
    #optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)
    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)
    scaler = torch.cuda.amp.GradScaler()

    # Resume
    start_epoch = 0
    best_miou = 0.0
    if args.resume and os.path.isfile(args.resume):
        print(f"Loading checkpoint '{args.resume}'")
        checkpoint = torch.load(args.resume)
        start_epoch = checkpoint['epoch']
        best_miou = checkpoint['best_miou']
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        print(f"Loaded checkpoint (epoch {start_epoch})")

    # Training history
    history = {
        'train_loss': [],
        'val_loss': [],
        'pixel_accuracy': [],
        'miou': []
    }

    print(f"ğŸš€ Start training ({args.head})")
    for epoch in range(start_epoch, args.epochs):
        model.train()
        train_loss = 0.0
        t0 = time.time()
        for images, masks in tqdm(train_loader, desc=f'Epoch {epoch+1}/{args.epochs} [Train]'):
            images = images.to(device)
            masks = masks.to(device)


            optimizer.zero_grad()
            with torch.cuda.amp.autocast():
                outputs = model(images)
                loss = criterion(outputs, masks)
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            train_loss += loss.item() * images.size(0)

        train_loss /= train_dataset_size
        history['train_loss'].append(train_loss)

        # Validation
        model.eval()
        val_loss = 0.0
        evaluator = SegmentationMetric(args.num_classes)
        with torch.no_grad():
            for images, masks in tqdm(val_loader, desc=f'Epoch {epoch+1}/{args.epochs} [Val]'):
                images = images.to(device)
                masks = masks.to(device)


                outputs = model(images)
                loss = criterion(outputs, masks)
                val_loss += loss.item() * images.size(0)

                predictions = torch.argmax(outputs, dim=1)
                if isinstance(predictions, torch.Tensor):
                    predictions = predictions.cpu().numpy()
                if isinstance(masks, torch.Tensor):
                    masks = masks.cpu().numpy()

                evaluator.addBatch(predictions, masks)

        val_loss /= val_dataset_size
        history['val_loss'].append(val_loss)

        scores = evaluator.get_scores()
        print(f"\nğŸ“ˆ Validation Epoch {epoch+1}:")
        for k, v in scores.items():
            if isinstance(v, np.ndarray):
                print(f"{k}: {np.round(v, 3)}")
            else:
                print(f"{k}: {v:.4f}")

        history['pixel_accuracy'].append(scores['Pixel Accuracy'])
        history['miou'].append(scores['Mean Intersection over Union(mIoU)'])

        # Save best
        if scores['Mean Intersection over Union(mIoU)'] &gt; best_miou:
            best_miou = scores['Mean Intersection over Union(mIoU)']
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'best_miou': best_miou,
            }, os.path.join(args.checkpoint, f'{args.head}_best.pth'))
            print(f"Saved best model! mIoU: {best_miou:.4f}")

        scheduler.step()

        print(f"ğŸ•’ Epoch time: {time.time() - t0:.2f}s\n")

    print("ğŸ‰ Training complete!")

if __name__ == '__main__':
    args = parse_arguments()
    train(args)

</code></pre>
<h2 id="æ¨¡å‹æµ‹è¯•test">æ¨¡å‹æµ‹è¯•(test)</h2>
<p>è¿™é‡Œå°±åˆ°äº†æˆ‘ä»¬çš„æœ€åä¸€æ­¥äº†ï¼Œæµ‹è¯•æˆ‘ä»¬çš„æ¨¡å‹ã€‚</p>
<pre><code class="language-python">def parse_arguments():
    parser = argparse.ArgumentParser()
    parser.add_argument('--image_dir', type=str, default='./datasets/test', help='Input image or folder')
    parser.add_argument('--checkpoint', type=str, default='./checkpoint/fcn8s_best.pth', help='Checkpoint path')
    parser.add_argument('--backbone', type=str, default='vgg16', help='Backbone model')
    parser.add_argument('--head', type=str, default='fcn8s', help='Segmentation head')
    parser.add_argument('--num_classes', type=int, default=21, help='Number of classes')
    parser.add_argument('--save_dir', type=str, default='./predictions', help='Directory to save results')
    parser.add_argument('--overlay', type=bool, default=True, help='Save overlay image')
    return parser.parse_args()
</code></pre>
<p>åŒæ ·çš„æ¥çœ‹ï¼Œæˆ‘ä»¬æ‰€éœ€è¦çš„ä¸€äº›å‚æ•°è®¾å®šå“ˆï¼æˆ‘ä»¬æ‰€éœ€è¦è¿›è¡Œæµ‹è¯•çš„å›¾ç‰‡æ–‡ä»¶å¤¹(image_dir)ï¼Œæˆ‘ä»¬è®­ç»ƒæ—¶å€™æ‰€ä¿å­˜çš„æƒé‡æ–‡ä»¶å¤¹(checkpoint)ï¼Œæˆ‘ä»¬é€‰æ‹©çš„ç‰¹å¾æå–ç½‘ç»œ(backbone)ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ£€æµ‹æ¨¡å‹(head)ï¼Œè¿˜æœ‰æ•°æ®é›†çš„ç±»åˆ«æ•°(num_classes)ï¼Œä¿æŒçš„ç»“æœçš„æ–‡ä»¶å¤¹(save_dir)ï¼Œè¿˜è¦ä¸ªéå¸¸é‡è¦çš„å‚æ•°ï¼Œæ˜¯å¦å°†é¢„æµ‹å›¾è¦†ç›–åœ¨åŸå›¾ä¸Š(overlay)ï¼Œé€šè¿‡è¿™ä¸ªæˆ‘ä»¬å¯ä»¥æ›´å¥½çš„çœ‹è¯­ä¹‰åˆ†å‰²çš„æ•ˆæœæ€ä¹ˆæ ·ã€‚</p>
<pre><code class="language-python">def load_image(image_path):
    image = Image.open(image_path).convert('RGB')
    transform = T.Compose([
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406],
                    std=[0.229, 0.224, 0.225])
    ])
    return transform(image).unsqueeze(0), image  # tensor, PIL image

#æŠŠç±»åˆ«mask â” å½©è‰²å›¾ (ç”¨VOC_COLORMAP)
def mask_to_color(mask):
    color_mask = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)
    for label in range(len(VOC_COLORMAP)):
        color_mask[mask == label] = VOC_COLORMAP[label]
    return color_mask

def save_mask(mask, save_path):
    color_mask = mask_to_color(mask)
    Image.fromarray(color_mask).save(save_path)

def overlay_mask_on_image(raw_image, mask, alpha=0.6):
    mask_color = mask_to_color(mask)
    mask_pil = Image.fromarray(mask_color)
    mask_pil = mask_pil.resize(raw_image.size, resample=Image.NEAREST)
    blended = Image.blend(raw_image, mask_pil, alpha=alpha)
    return blended
</code></pre>
<p>ç„¶åæ¥çœ‹æµ‹è¯•è¿‡ç¨‹ä¸­ä¼šç”¨åˆ°çš„ä¸€äº›å‡½æ•°ï¼Œå½“ç„¶æµ‹è¯•é¦–å…ˆè‚¯å®šè¦åŠ è½½æˆ‘ä»¬çš„å›¾ç‰‡å‘ã€‚æ³¨æ„çœ‹è¿™é‡Œæœ‰ä¸ªç»†èŠ‚ï¼ŒåŠ è½½å›¾ç‰‡çš„æ—¶å€™æˆ‘ä»¬è¿›è¡Œäº†æ ‡å‡†åŒ–çš„ï¼Œä¸ºä»€ä¹ˆè¿™ä¹ˆåšï¼Ÿå› ä¸ºæˆ‘ä»¬åœ¨è®­ç»ƒæ¨¡å‹çš„æ—¶å€™ï¼Œå›¾ç‰‡å°±è¿›è¡Œäº†æ ‡å‡†åŒ–çš„æ“ä½œï¼Œæ‰€æœ‰æµ‹è¯•å›¾ç‰‡ï¼Œæˆ‘ä»¬è‚¯å®šè¦ä¿æŒå›¾ç‰‡å’Œè®­ç»ƒæ—¶å€™çš„æ¡ä»¶ä¸€æ ·ã€‚ç„¶åä¸ºäº†æ›´å¥½çš„å¯è§†åŒ–ï¼Œæˆ‘ä»¬éœ€è¦å°†é¢„æµ‹çš„maskå›¾è½¬æ¢ä¸ºå½©è‰²å›¾ã€‚æ ¹æ®VOC_COLORMAPçš„é¢œè‰²è¿›è¡Œè½¬æ¢å³å¯ã€‚è¿˜æœ‰ä¸ªoverlay_mask_on_imageå‡½æ•°ï¼Œé€šè¿‡å°†é¢„æµ‹çš„å¯è§†åŒ–å›¾ä¸åŸå›¾è¿›è¡Œå åŠ æ··åˆèƒ½å¤Ÿè®©æˆ‘ä»¬æ›´åŠ ç›´è§‚ã€‚</p>
<pre><code class="language-python">def predict(args):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Device: {device}")

    # æ¨¡å‹
    model = get_model(args.head, backbone=args.backbone, num_classes=args.num_classes)
    checkpoint = torch.load(args.checkpoint, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()

    os.makedirs(args.save_dir, exist_ok=True)

    # é¢„æµ‹å•å¼  or æ‰¹é‡
    if os.path.isdir(args.image):
        image_list = [os.path.join(args.image, f) for f in os.listdir(args.image) if f.lower().endswith(('jpg', 'png', 'jpeg'))]
    else:
        image_list = [args.image]

    print(f"ğŸ” Found {len(image_list)} images to predict.")

    for img_path in tqdm(image_list):
        img_tensor, raw_img = load_image(img_path)
        img_tensor = img_tensor.to(device)

        with torch.no_grad():
            output = model(img_tensor)
            pred = torch.argmax(output.squeeze(), dim=0).cpu().numpy()

        # ä¿å­˜ mask
        base_name = os.path.basename(img_path).split('.')[0]
        mask_save_path = os.path.join(args.save_dir, f"{base_name}_mask.png")
        save_mask(pred, mask_save_path)

        # ä¿å­˜ overlay
        if args.overlay:
            overlay_img = overlay_mask_on_image(raw_img, pred)
            overlay_save_path = os.path.join(args.save_dir, f"{base_name}_overlay.png")
            overlay_img.save(overlay_save_path)

        print(f"Saved: {mask_save_path}")
        if args.overlay:
            print(f"Saved overlay: {overlay_save_path}")

    print("ğŸ‰ Prediction done!")
</code></pre>
<p>ç„¶åå°±åˆ°äº†é¢„æµ‹ç¯èŠ‚ï¼Œå…¶å®æµç¨‹è·Ÿtrainçš„æµç¨‹å·®ä¸å¤šï¼Œä½†æ˜¯ä¸åœ¨éœ€è¦åƒtrainçš„æ—¶å€™ä»€ä¹ˆæ¢¯åº¦åä¼ æ›´æ–°å‚æ•°äº†ï¼Œç›´æ¥é¢„æµ‹å¾—å‡ºç»“æœç„¶åä¿å­˜å³å¯ã€‚</p>
<p>é¦–å…ˆç¡®å®šè®¾å¤‡å“ˆï¼Œä¸€èˆ¬éƒ½æ˜¯GPUçš„ï¼Œç„¶åå°±æ˜¯å°±æ˜¯åŠ è½½æ•°æ®å’Œæ¨¡å‹äº†ï¼Œæœ€åé¢„æµ‹ä¿å­˜ç»“æœå³å¯ï¼Œè¿™äº›ä»£ç åº”è¯¥è¿˜æ˜¯æ¯”è¾ƒå®¹æ˜“ç†è§£çš„ï¼Œç›´æ¥çœ‹ä»£ç å³å¯ã€‚</p>
<p>å®Œæ•´ä»£ç ï¼š</p>
<pre><code class="language-python">import argparse
import os
import torch
import numpy as np
from PIL import Image
from tqdm import tqdm
from model import get_model
import torchvision.transforms as T
from datasets import *



def parse_arguments():
    parser = argparse.ArgumentParser()
    parser.add_argument('--image_dir', type=str, default='./datasets/test', help='Input image or folder')
    parser.add_argument('--checkpoint', type=str, default='./checkpoint/fcn8s_best.pth', help='Checkpoint path')
    parser.add_argument('--backbone', type=str, default='vgg16', help='Backbone model')
    parser.add_argument('--head', type=str, default='fcn8s', help='Segmentation head')
    parser.add_argument('--num_classes', type=int, default=21, help='Number of classes')
    parser.add_argument('--save_dir', type=str, default='./predictions', help='Directory to save results')
    parser.add_argument('--overlay', type=bool, default=True, help='Save overlay image')
    return parser.parse_args()

def load_image(image_path):
    image = Image.open(image_path).convert('RGB')
    transform = T.Compose([
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406],
                    std=[0.229, 0.224, 0.225])
    ])
    return transform(image).unsqueeze(0), image  # tensor, PIL image

#æŠŠç±»åˆ«mask â” å½©è‰²å›¾ (ç”¨VOC_COLORMAP)
def mask_to_color(mask):
    color_mask = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)
    for label in range(len(VOC_COLORMAP)):
        color_mask[mask == label] = VOC_COLORMAP[label]
    return color_mask

def save_mask(mask, save_path):
    color_mask = mask_to_color(mask)
    Image.fromarray(color_mask).save(save_path)

def overlay_mask_on_image(raw_image, mask, alpha=0.6):
    mask_color = mask_to_color(mask)
    mask_pil = Image.fromarray(mask_color)
    mask_pil = mask_pil.resize(raw_image.size, resample=Image.NEAREST)
    blended = Image.blend(raw_image, mask_pil, alpha=alpha)
    return blended

def predict(args):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Device: {device}")

    # æ¨¡å‹
    model = get_model(args.head, backbone=args.backbone, num_classes=args.num_classes)
    checkpoint = torch.load(args.checkpoint, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()

    os.makedirs(args.save_dir, exist_ok=True)

    # é¢„æµ‹å•å¼  or æ‰¹é‡
    if os.path.isdir(args.image):
        image_list = [os.path.join(args.image, f) for f in os.listdir(args.image) if f.lower().endswith(('jpg', 'png', 'jpeg'))]
    else:
        image_list = [args.image]

    print(f"ğŸ” Found {len(image_list)} images to predict.")

    for img_path in tqdm(image_list):
        img_tensor, raw_img = load_image(img_path)
        img_tensor = img_tensor.to(device)

        with torch.no_grad():
            output = model(img_tensor)
            pred = torch.argmax(output.squeeze(), dim=0).cpu().numpy()

        # ä¿å­˜ mask
        base_name = os.path.basename(img_path).split('.')[0]
        mask_save_path = os.path.join(args.save_dir, f"{base_name}_mask.png")
        save_mask(pred, mask_save_path)

        # ä¿å­˜ overlay
        if args.overlay:
            overlay_img = overlay_mask_on_image(raw_img, pred)
            overlay_save_path = os.path.join(args.save_dir, f"{base_name}_overlay.png")
            overlay_img.save(overlay_save_path)

        print(f"Saved: {mask_save_path}")
        if args.overlay:
            print(f"Saved overlay: {overlay_save_path}")

    print("ğŸ‰ Prediction done!")

if __name__ == '__main__':
    args = parse_arguments()
    predict(args)

</code></pre>
<h1 id="æ•ˆæœå›¾">æ•ˆæœå›¾</h1>
<p>æˆ‘å°±è®­ç»ƒäº†50ä¸ªepochï¼Œæ•ˆæœè¿˜è¡Œï¼Œæ•ˆæœå›¾å¦‚ä¸‹æ‰€ç¤º</p>
<p><img src="https://gitee.com/fouen/image/raw/master/image/20250502184235110.png" alt="image-20250502184234645" loading="lazy"></p>
<p><img src="https://gitee.com/fouen/image/raw/master/image/20250502184254074.png" alt="image-20250502184254016" loading="lazy"></p>
<h1 id="ç»“è¯­">ç»“è¯­</h1>
<p>å¸Œæœ›ä¸Šåˆ—æ‰€è¿°å†…å®¹å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œå¦‚æœæœ‰é”™è¯¯çš„åœ°æ–¹æ¬¢è¿å¤§å®¶æ‰¹è¯„æŒ‡æ­£ï¼</p>
<p>å¹¶ä¸”å¦‚æœå¯ä»¥çš„è¯å¸Œæœ›å¤§å®¶èƒ½å¤Ÿä¸‰è¿é¼“åŠ±ä¸€ä¸‹ï¼Œè°¢è°¢å¤§å®¶ï¼</p>
<p>å¦‚æœä½ è§‰å¾—è®²çš„è¿˜ä¸é”™æƒ³è½¬è½½ï¼Œå¯ä»¥ç›´æ¥è½¬è½½ï¼Œä¸è¿‡éº»çƒ¦æŒ‡å‡ºæœ¬æ–‡æ¥æºå‡ºå¤„å³å¯ï¼Œè°¢è°¢ï¼</p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.3288012015787037" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-05-22 10:24">2025-05-22 10:24</span>&nbsp;
<a href="https://www.cnblogs.com/carpell">carpell</a>&nbsp;
é˜…è¯»(<span id="post_view_count">11</span>)&nbsp;
è¯„è®º(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18890267);return false;">æ”¶è—</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18890267', targetLink: 'https://www.cnblogs.com/carpell/p/18890267', title: 'ã€è¯­ä¹‰åˆ†å‰²ä¸“æ ã€‘ï¼šFCNå®æˆ˜ç¯‡(é™„ä¸Šå®Œæ•´å¯è¿è¡Œçš„ä»£ç pytorch)' })">ä¸¾æŠ¥</a>
</div>
        