
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/wang_yb/p/18803693" title="发布于 2025-04-01 10:47">
    <span role="heading" aria-level="2">直线思维的进化：线性到广义线性</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p>在数据科学领域，<strong>线性模型</strong>和<strong>广义线性模型</strong>是两种基础且重要的统计工具，</p>
<p>它们被广泛应用于各种预测和分析任务中，从简单的回归问题到复杂的分类场景。</p>
<p>今天，让我们深入探讨这两种模型，了解它们的原理、区别以及实际应用。</p>
<h1 id="1-线性模型统计分析的基石">1. 线性模型：统计分析的基石</h1>
<p><strong>线性模型</strong>是统计学中最早被提出和广泛应用的一类模型。</p>
<p>其基本思想是假设<strong>因变量</strong>（响应变量）与<strong>自变量</strong>（解释变量）之间存在线性关系。</p>
<p>数学上，<strong>线性回归模型</strong>可以表示为：$ y = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p + \epsilon $</p>
<p>其中：</p>
<p><font style="color: rgba(6, 6, 7, 1)">其中，</font>$ y <span class="math inline">\(&lt;font style="color:rgb(6, 6, 7);"&gt;是因变量，&lt;/font&gt;\)</span> x_1,x_2,...,x_p <span class="math inline">\(&lt;font style="color:rgb(6, 6, 7);"&gt;是自变量，&lt;/font&gt;\)</span> \beta_1,\beta_2,...,\beta_p <span class="math inline">\(&lt;font style="color:rgb(6, 6, 7);"&gt;是模型参数，&lt;/font&gt;\)</span> \epsilon $<font style="color: rgba(6, 6, 7, 1)">是误差项，通常假设误差项服从均值为0的</font><strong><font style="color: rgba(6, 6, 7, 1)">正态分布</font></strong><font style="color: rgba(6, 6, 7, 1)">。</font></p>
<p><font style="color: rgba(6, 6, 7, 1)"></font></p>
<p>这种模型通过<strong>最小二乘法</strong>估计参数，广泛应用于房价预测、销量分析等连续值预测场景。其优势在于：</p>
<ol>
<li><strong>可解释性强</strong>：参数直接反映变量影响程度</li>
<li><strong>计算高效</strong>：存在解析解（当矩阵可逆时）</li>
<li><strong>易于实现</strong>：几乎所有统计软件都支持</li>
</ol>
<h1 id="2-线性模型的软肋">2. 线性模型的"软肋"</h1>
<p>然而，现实世界的数据往往比直线复杂得多。<strong>线性模型</strong>的局限性开始显现：</p>
<ol>
<li><strong>关系局限性</strong>：只能捕捉线性关系，对非线性模式（如指数增长、周期性波动）无能为力</li>
<li><strong>分布局限性</strong>：要求误差项服从正态分布，当数据存在异方差或重尾分布时效果骤降</li>
<li><strong>因变量局限性</strong>：只能处理连续型因变量，无法直接处理分类变量或计数数据</li>
<li><strong>边界局限性</strong>：预测值可能超出合理范围（如概率预测时出现&gt;1或&lt;0的值）</li>
</ol>
<p>这些局限促使统计学家们思考：能否扩展线性模型的核心思想，同时突破这些限制？</p>
<p>答案正是<strong>广义线性模型</strong>（<code>GLM</code>）。</p>
<h1 id="3-线性模型的进化">3. 线性模型的"进化"</h1>
<p><strong>广义线性模型</strong>在传统线性模型的基础上进行了扩展，放宽了对响应变量分布和线性关系的限制，使其能够适应更广泛的数据类型和复杂关系。</p>
<p>与<strong>线性模型</strong>相比，<strong>广义线性模型</strong>主要改进的地方有3个：</p>
<h2 id="31-因变量分布">3.1. 因变量分布</h2>
<p><strong>线性模型</strong>假设<strong>因变量</strong>($ y $)是连续型变量，且服从<strong>正态分布</strong>（误差项服从独立同分布的正态分布）。</p>
<p>例如：简单线性回归、多元线性回归。</p>
<p>而广义线性模型允许响应变量来自指数族分布，包括正态分布、二项分布、泊松分布、伽玛分布等。</p>
<p>这意味着广义线性模型可以适用于更多类型的数据，如二分类数据（使用二项分布）、计数数据（使用泊松分布）等。</p>
<p>例如：逻辑回归（二分类问题，二项分布）、泊松回归（计数数据，泊松分布）。</p>
<h2 id="32-模型结构">3.2. 模型结构</h2>
<p><strong>线性模型</strong>直接假设<strong>因变量</strong>的均值与线性预测器（$ \eta = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p $）相等：</p>
<p>$ y = \eta + \epsilon $误差项独立同分布于正态分布</p>
<p>而<strong>广义线性模型</strong>（<code>GLM</code>）通过<strong>连接函数</strong>$ g(\mu) $将均值 $ \mu $ 与线性预测器 ($ \eta = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p $) 关联：</p>
<p>$ g(\mu) = \eta = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p $</p>
<p>具体来说，比如<strong>逻辑回归</strong>模型，使用<strong>对数几率函数</strong>作为<strong>连接函数</strong>：</p>
<p>$ \mu = p <span class="math inline">\(，\)</span> g(\mu) = \text{logit}(p) = \ln\left(\frac{p}{1-p}\right) = \eta $</p>
<p>其中，<strong>连接函数</strong>必须是单调可微的，例如：</p>
<ul>
<li>逻辑函数（逻辑回归，连接二项分布）</li>
<li>对数函数（泊松回归，连接泊松分布）。</li>
</ul>
<h2 id="33-参数估计方法">3.3. 参数估计方法</h2>
<p>最后，在参数估计方面，<strong>线性模型</strong>使用<strong>最小二乘法</strong>（<code>OLS</code>），通过最小化残差平方和求解参数。</p>
<p>而<strong>广义线性模型</strong>（<code>GLM</code>）使用<strong>极大似然估计</strong>（<code>MLE</code>），通过最大化似然函数求解参数，通常需迭代优化（如牛顿-拉夫森算法）。</p>
<h2 id="34-两者区别">3.4. 两者区别</h2>
<p>总得来说，两者的主要区别如下表：</p>
<table>
<thead>
<tr>
<th><strong>特性</strong></th>
<th><strong>线性模型</strong></th>
<th><strong>广义线性模型</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><font style="color: rgba(64, 64, 64, 1)">目标变量分布</font></td>
<td><font style="color: rgba(64, 64, 64, 1)">正态分布</font></td>
<td><font style="color: rgba(64, 64, 64, 1)">指数分布族（泊松、二项、伽马等）</font></td>
</tr>
<tr>
<td><font style="color: rgba(64, 64, 64, 1)">连接函数</font></td>
<td><font style="color: rgba(64, 64, 64, 1)">恒等函数</font></td>
<td><font style="color: rgba(64, 64, 64, 1)">Logit、Log、逆函数等</font></td>
</tr>
<tr>
<td><font style="color: rgba(64, 64, 64, 1)">适用场景</font></td>
<td><font style="color: rgba(64, 64, 64, 1)">连续值预测</font></td>
<td><font style="color: rgba(64, 64, 64, 1)">分类、计数、偏态数据等</font></td>
</tr>
<tr>
<td><font style="color: rgba(64, 64, 64, 1)">参数估计方法</font></td>
<td><font style="color: rgba(64, 64, 64, 1)">最小二乘法</font></td>
<td><font style="color: rgba(64, 64, 64, 1)">最大似然估计</font></td>
</tr>
</tbody>
</table>
<h1 id="4-示例比较">4. 示例比较</h1>
<p>理论再多，不如一个示例来的直接，下面我们先通过<code>scikit-learn</code>中的<code>make_moons</code>函数生成一个包含 <code>1000</code> 个样本的月牙形数据集。</p>
<pre><code class="language-python">import matplotlib.pyplot as plt
from sklearn.datasets import make_moons

# 生成月牙形数据集
X, y = make_moons(n_samples=1000, noise=0.1, random_state=42)

plt.scatter(X[:, 0], X[:, 1], marker="o", c=y, s=25)
plt.show()
</code></pre>
<p><img src="https://img2024.cnblogs.com/blog/83005/202504/83005-20250401104643016-238415106.png" alt="" loading="lazy"></p>
<p>从图中可以明显看出，这个数据集呈现出<strong>非线性</strong>的分布。</p>
<p>然后比较使用<strong>线性模型</strong>和<strong>广义线性模型</strong>训练之后的准确率。</p>
<pre><code class="language-python">from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import PolynomialFeatures


# 生成月牙形数据集
X, y = make_moons(n_samples=1000, noise=0.1, random_state=42)

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 线性模型（线性回归）
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)
# 线性回归预测的是连续值，我们将其转换为分类结果
linear_pred = (linear_model.predict(X_test) &gt; 0.5).astype(int)
linear_accuracy = accuracy_score(y_test, linear_pred)

# 广义线性模型（逻辑回归）
# 先进行多项式特征转换
poly = PolynomialFeatures(degree=3)
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

logistic_model = LogisticRegression()
logistic_model.fit(X_train_poly, y_train)
logistic_pred = logistic_model.predict(X_test_poly)
logistic_accuracy = accuracy_score(y_test, logistic_pred)

print(f"线性回归的准确率: {linear_accuracy:.2f}")
print(f"逻辑回归的准确率: {logistic_accuracy:.2f}")

</code></pre>
<p>训练结果：</p>
<pre><code class="language-plain">线性回归的准确率: 0.87
逻辑回归的准确率: 0.99
</code></pre>
<p>在非线性数据集上，明显看出<strong>广义线性模型</strong>（<strong>逻辑回归</strong>）的准确率要高出一截。</p>
<h1 id="5-总结">5. 总结</h1>
<p>总之，<strong>线性模型</strong>和<strong>广义线性模型</strong>都是数据科学中重要的建模工具。</p>
<p><strong>线性模型</strong>以其简单性和可解释性在连续型数据的回归分析中表现出色，但在面对非正态分布的响应变量和非线性关系时存在局限。</p>
<p><strong>广义线性模型</strong>通过放宽对响应变量分布的假设并引入链接函数，能够适应更广泛的数据类型和复杂关系，在分类、计数等场景中具有明显优势。</p>
<p>在实际应用中，我们需要根据数据的特点和分析目标选择合适的模型，并结合具体的算法和工具进行实现和优化。</p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.016472901649305555" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-04-01 10:48">2025-04-01 10:47</span>&nbsp;
<a href="https://www.cnblogs.com/wang_yb">wang_yb</a>&nbsp;
阅读(<span id="post_view_count">0</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18803693" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18803693);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18803693', targetLink: 'https://www.cnblogs.com/wang_yb/p/18803693', title: '直线思维的进化：线性到广义线性' })">举报</a>
</div>
        