
        <h2>
            <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/leslies2/p/18808145" title="发布于 2025-04-10 11:22">
    <span role="heading" aria-level="2">基于大模型的 RAG 核心开发——详细介绍 DeepSeek R1 本地化部署流程</span>
    

</a>

        </h2>
            <div id="cnblogs_post_description" style="display: none">
        <img src="https://img2024.cnblogs.com/blog/64989/202504/64989-20250410122616520-1907341897.png" alt="基于大模型的 RAG 核心开发——详细介绍 DeepSeek R1 本地化部署流程" class="desc_img">
        自从 DeepSeek 发布后，对 AI 行业产生了巨大的影响，以 OpenAI、Google 为首的国际科技集团为之震惊，它的出现标志着全球AI竞争进入新阶段。DeepSeek 是一个开源的产品，任何人都可通过 GitHub 等途径下载它的核心源代码，它的开源性意味着任何人都可以为 DeepSeek 开发某项额外的功能，为DeepSeek 的茁壮成长贡献自己的一份力量。通过本地化部署不仅能保障数据安全，更能通过灵活定制实现业务场景的高效适配，为企业智能化转型提供可靠的技术底座。
    </div>
<div id="cnblogs_post_body" class="blogpost-body blogpost-body-html">
<p>前言</p>
<p>自从 DeepSeek 发布后，对 AI 行业产生了巨大的影响，以 OpenAI、Google 为首的国际科技集团为之震惊，它的出现标志着全球AI竞争进入新阶段。从以往单纯的技术比拼转向效率、生态与战略的综合较量。其影响已超越企业层面，涉及地缘政治、产业政策与全球技术治理，它彻底改变“美国主导创新、中国跟随应用”的传统格局，形成多极化的技术权力分布。<br>DeepSeek 的开源性彻底打破了 OpenAI 等公司通过 API 接口调用，依赖 token 计费的单一规则。因为 DeepSeek 是一个开源的产品，任何人都可通过 GitHub 等途径下载它的核心源代码，这种开源方案有点类似当年的 Android / 鸿蒙发展策略。任何人都可以为 DeepSeek 开发某项额外的功能，为DeepSeek 的茁壮成长贡献自己的一份力量。<br>它包括了 DeepSeek R1 / DeepSeek V3 / DeepSeek Coder V2 / DeepSeek VL / DeepSeek V2 / DeepSeek Coder / DeepSeek Math / DeepSeek LLM&nbsp; 等多个不同的模型，以适应不同领域的应用。私人开发者可以下载 DeepSeek R1 检心框架进行调试，如果企业调用 DeepSeek 的 API 接口，也需要按 token 收费，然而费用不到 ChatGDP 的十分之一，对企业来说是相当有良心。DeepSeek 的 R1 模型支持本地化部署，用户可以在企业服务器内单独部署自己的 DeepSeek 模型，以适应各自的领域需求。<br>废话不多说，下面为大家介绍 DeepSeek R1 的本地化部署流程。</p>
<p>&nbsp;</p>
<p><span style="font-size: 16px"><strong>一、运行环境要求</strong></span></p>
<p>‌<span style="font-size: 14px">1.&nbsp;<strong>硬件配置‌</strong></span></p>
<ul>
<li>独立显卡（推荐 NVIDIA 1060 以上 GPU显存 ≥ 6GB）‌<span><span><span class="cos-tooltip cosd-citation"><span class="cosd-citation-citationId "><span><span class="cos-tooltip cosd-citation"><span class="cosd-citation-citationId "><br></span></span></span></span></span></span></span></li>
<li>CPU、内存及存储需满足模型参数规模（如1.5B/7B/14B模型对应不同配置）‌</li>

















</ul>
<p>进入 DeepSeek 的官网 <span style="color: rgba(0, 0, 255, 1)"><a href="https://www.deepseek.com/" target="_blank" rel="noopener nofollow"><span style="color: rgba(0, 0, 255, 1)">https://www.deepseek.com/</span></a></span>，点激 DeepSeek R1 的模型连接，可以进入 GitHub 的源代码页面。里面可看到 DeepSeek R1 包含了多个不同大小的模型，每个模型需要使用的资源不一样。一般情况下建议使用 1.5B 的轻量级模型，GPU 在 6G~8G 可以尝试使用 7B 的平衡型模型。</p>
<p><img src="https://img2024.cnblogs.com/blog/64989/202504/64989-20250403175228652-718791279.png" width="534" height="466" loading="lazy" style="border: 1px solid rgba(0, 0, 0, 1)"></p>
<p>显卡要求可参考下表</p>
<p><img src="https://img2024.cnblogs.com/blog/64989/202504/64989-20250403183759745-1791489902.png" width="537" height="448" loading="lazy" style="border: 1px solid rgba(0, 0, 0, 1)"></p>
<p>&nbsp;</p>
<p><strong><span style="font-size: 14px">‌2. 依赖工具‌</span></strong></p>
<ul>
<li>Ollama 或 HFD 部署工具及模型库<span><span><span class="cos-tooltip cosd-citation"><span class="cosd-citation-citationId "><span><span class="cos-tooltip cosd-citation"><span class="cosd-citation-citationId "><br></span></span></span></span></span></span></span></li>
<li>Docker、Python等基础环境‌<span><span><span class="cos-tooltip cosd-citation"><span class="cosd-citation-citationId "><span><span class="cos-tooltip cosd-citation"><span class="cosd-citation-citationId "><br></span></span></span></span></span></span></span></li>

















</ul>
<p>常用下载模型的方法主要有两种，一是通过 Ollama，二是通过 HuggingFace。虽然 HuggingFace 的镜像比较丰富全面，但由于在2023年底，HuggingFace 的官网已经彻底被封，想要下载镜像需要使用 <span style="color: rgba(0, 0, 255, 1)"><a href="https://hf-mirror.com/" rel="noopener nofollow"><span style="color: rgba(0, 0, 255, 1)">https://hf-mirror.com</span></a></span>&nbsp;里面的 HFD 工具通过命令执行，对新手来说相对不太友好，所以本文就选择相对轻量级的 Ollama 工具进行安装。</p>
<p>&nbsp;</p>
<p><strong><span style="font-size: 16px">二、安装步骤</span></strong></p>
<p><span style="font-size: 14px"><strong>1. 安装 Ollama</strong></span>&nbsp;</p>
<p>首先到 Ollama 官网 <span style="color: rgba(0, 0, 255, 1)"><a href="https://www.ollama.com%20" target="_blank"><span style="color: rgba(0, 0, 255, 1)">https://www.ollama.com</span></a></span> 下载 ollama，可以选择 Windows、Linux、masOS 三个不同的版本<a href="https://ollama.com/" rel="noopener nofollow"><br></a></p>
<p><img src="https://img2024.cnblogs.com/blog/64989/202504/64989-20250403182232182-1091546442.png" width="548" height="396" loading="lazy" style="border: 1px solid rgba(0, 0, 0, 1)"></p>
<p>&nbsp;下载后点激安装，默认安装路径在&nbsp;C:\Users\username\AppData\Local\Programs\Ollama 下</p>
<p><img src="https://img2024.cnblogs.com/blog/64989/202504/64989-20250403182607744-989212057.png" width="550" height="406" style="border: 1px solid rgba(0, 0, 0, 1)"></p>
<p>安装完成后，打开 Windows 的环境变量，修改用户变量中的 Path 值，加入 Ollama 的路径&nbsp;&nbsp;C:\Users\username\AppData\Local\Programs\Ollama</p>
<p><img src="https://img2024.cnblogs.com/blog/64989/202504/64989-20250403183047376-318181332.png" width="548" height="331" style="border: 1px solid rgba(0, 0, 0, 1)"></p>
<p>完成设置后，点激 Ollama.exe 按钮，然后在命令提示符中输入 ollama -v，见到 ollama 版本号代表安装成功。</p>
<p><img src="https://img2024.cnblogs.com/blog/64989/202504/64989-20250403183541200-375160975.png" alt="" width="553" height="310" loading="lazy"></p>
<p>也打开浏览器，输入Ollama 运行地址 “http://127.0.0.1:11434”<br>看到 “Ollama is running”&nbsp; 字样证明 Ollama 已经正常运行。</p>
<p><img src="https://img2024.cnblogs.com/blog/64989/202504/64989-20250405111915887-1154233999.png" width="550" height="394" loading="lazy" style="border: 1px solid rgba(0, 0, 0, 1)"></p>
<p>&nbsp;</p>
<p><span style="font-size: 14px"><strong>2. 下载 deepseek v1 模型</strong></span></p>
<p>ollama 的命令与 docker 有点类似，输入命令 ollama pull deepseek-r1:7b 系统开始下载模型 deepseek v1:7b<br>最后看到 success 代表下载成功</p>
<p><img src="https://img2024.cnblogs.com/blog/64989/202504/64989-20250403184438019-303161315.png" alt="" width="552" height="298" loading="lazy"></p>
<p>&nbsp;此时输入命令 ollama ls 可以查看已下载的模型</p>
<p><img src="https://img2024.cnblogs.com/blog/64989/202504/64989-20250403184709239-2005958239.png" width="551" height="297" loading="lazy" style="border: 1px solid rgba(0, 0, 0, 1)"></p>
<p>&nbsp;</p>
<p><span style="font-size: 14px"><strong>3. 运行模型</strong></span></p>
<p>输入命令 “ollama run deepseek-r1:7b” 启动模型<br>成功启动后就可以尝试输入问题让 deepseek 回答。</p>
<p><img src="https://img2024.cnblogs.com/blog/64989/202504/64989-20250403185517825-1276174903.png" alt="" width="555" height="318" loading="lazy"></p>
<p>&nbsp;按下 CTRL+D&nbsp; 可以退出当前对话<br>若要查看当前运行的模型，可以输入 ollama ps</p>
<p><img src="https://img2024.cnblogs.com/blog/64989/202504/64989-20250403185947314-1599857437.png" width="553" height="352" loading="lazy" style="border: 1px solid rgba(0, 0, 0, 1)"></p>
<p>若要停止模式运行，可输入 ollama stop deepseek-r1:7b。<br>停止后再输入 ollama ps，可以知道停止命令是否成功</p>
<p><img src="https://img2024.cnblogs.com/blog/64989/202504/64989-20250403190250818-1100663577.png" width="557" height="354" loading="lazy" style="border: 1px solid rgba(0, 0, 0, 1)"></p>
<p>&nbsp;</p>
<p><strong><span style="font-size: 16px">三、可视化部署</span></strong></p>
<p>DeepSeek R1 不仅可以通过命令执行，还可通过插件进行可视化部署，布置出与官网应用类似的应用场景。<br>首先选择浏览器的扩展按键，填入 Page Assist 进行搜索，安装插件。</p>
<p><img src="https://img2024.cnblogs.com/blog/64989/202504/64989-20250403190808865-1176847149.png" width="540" height="293" loading="lazy" style="border: 1px solid rgba(0, 0, 0, 1)"></p>
<p>&nbsp;完成安装后，若要选择中文版可点激右上角设置按钮，在language中选择 “简体中文”</p>
<p><img src="https://img2024.cnblogs.com/blog/64989/202504/64989-20250403191702822-1930056834.png" width="538" height="293" loading="lazy" style="border: 1px solid rgba(0, 0, 0, 1)"></p>
<p>在命令提示符输入 ollama run deepseek-r1:7b ，确定 deepseek 模型已经正常运行后， 在 Ollama URL 处填入默认的运行地址&nbsp; http://127.0.0.1:11434</p>
<p><img src="https://img2024.cnblogs.com/blog/64989/202504/64989-20250403191513775-69810052.png" width="540" height="292" loading="lazy" style="border: 1px solid rgba(0, 0, 0, 1)"></p>
<p>回到首页，在选项中可以查到系统中正在运行的模型，选择你要有的模型类别</p>
<p><img src="https://img2024.cnblogs.com/blog/64989/202504/64989-20250403191347560-1297894464.png" width="538" height="293" loading="lazy" style="border: 1px solid rgba(0, 0, 0, 1)"></p>
<p>&nbsp;此时，你已经可以在本机尽情享受 DeepSeek 给你带来的乐趣。</p>
<p><img src="https://img2024.cnblogs.com/blog/64989/202504/64989-20250403192631707-528347824.png" width="543" height="320" loading="lazy" style="border: 1px solid rgba(0, 0, 0, 1)"></p>
<p>&nbsp;</p>
<p><span style="font-size: 16px"><strong>本章小结</strong></span></p>
<p>前面已经介绍了 DeepSeek R1 本地化部署流程，本地化部署不仅能保障数据安全，更能通过灵活定制实现业务场景的高效适配，为企业智能化转型提供可靠的技术底座。DeepSeek 模型从环境准备、模型加载到 RAG 功能集成，每一个环节都体现了大模型与企业私有化场景深度融合的技术潜力。接下来一连几章将会为大家介绍基于大模型 RAG 的核心开发，敬请留意。</p>
<p>&nbsp;</p>
<div style="text-align: right">作者：风尘浪子</div>
<div style="text-align: right">&nbsp;<span style="color: rgba(0, 0, 255, 1)"><a class="ng-star-inserted" href="https://www.cnblogs.com/leslies2/p/18808145" target="_blank"><span style="color: rgba(0, 0, 255, 1)">https://www.cnblogs.com/leslies2/p/18808145</span></a></span></div>
<div style="text-align: right"><span style="color: rgba(255, 0, 0, 1)"><strong>原创作品，转载时请注明作者及出处</strong></span></div>
</div>
<div class="clear"></div>

        <p class="postfoot">
            posted on 
<span id="post-date" data-last-update-days="0.43734985875694443" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-04-10 13:51">2025-04-10 11:22</span>&nbsp;
<a href="https://www.cnblogs.com/leslies2">风尘浪子</a>&nbsp;
阅读(<span id="post_view_count">86</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18808145" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18808145);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18808145', targetLink: 'https://www.cnblogs.com/leslies2/p/18808145', title: '基于大模型的 RAG 核心开发——详细介绍 DeepSeek R1 本地化部署流程' })">举报</a>

        </p>
    