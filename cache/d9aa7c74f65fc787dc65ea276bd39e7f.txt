
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/Sun-Wind/p/18894486" title="发布于 2025-05-24 16:45">
    <span role="heading" aria-level="2">stable diffusion论文解读</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                    <div id="cnblogs_post_description" style="display: none">
        <img src="https://img2024.cnblogs.com/blog/2489686/202505/2489686-20250524164642236-373081331.png" alt="stable diffusion论文解读" class="desc_img">
        High-Resolution Image Synthesis with Latent Diffusion Models论文解读
    </div>
<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h1 id="high-resolution-image-synthesis-with-latent-diffusion-models">High-Resolution Image Synthesis with Latent Diffusion Models</h1>
<h2 id="论文背景">论文背景</h2>
<p><strong>LDM是Stable Diffusion模型的奠基性论文</strong></p>
<p>于2022年6月在CVPR上发表<br>
<img src="https://img2023.cnblogs.com/blog/2489686/202505/2489686-20250524130805691-684308184.png" alt="img" loading="lazy"><br>
传统生成模型具有局限性：</p>
<ul>
<li>扩散模型（DM）通过逐步去噪生成图像，质量优于GAN，但直接在像素空间操作导致高计算开销。</li>
<li>随着分辨率提升，扩散模型的优化和推理成本呈指数级增长，限制了实际应用</li>
</ul>
<blockquote>
<p>如DDPM生成的图像分辨率普遍不超过256×256，而LDM生成的图像分辨率可以超过1024×1024.<br>
而LDM通过将扩散过程迁移至<strong>潜在空间</strong>，解决了传统模型的计算瓶颈，同时保持<strong>生成质量与灵活性</strong>。</p>
</blockquote>
<h2 id="论文框架方法">论文框架方法</h2>
<p>论文中框架示意图如图所示：<br>
<img src="https://img2023.cnblogs.com/blog/2489686/202505/2489686-20250524130200561-1587125299.png" alt="img" loading="lazy"><br>
在训练阶段：</p>
<ul>
<li>预训练自动编码器（AE）和条件生成编码器（如clip）</li>
<li>输入图片x，经过自动编码器压缩到隐空间ε(x)=z</li>
<li>随机采样时间步T，对Z进行加噪到<span class="math inline">\(Z_{T}\)</span></li>
<li>对右边框里的条件进行条件编码<span class="math inline">\(\tau_\theta(y)\)</span>和<span class="math inline">\(Z_{T}\)</span>一起输入UNet网络中</li>
<li>进行交叉注意力计算，其中<span class="math inline">\(Z_{T}\)</span>作为Q向量，<span class="math inline">\(\tau_\theta(y)\)</span>作为K，V向量计算注意力，这样做是让图像的每个位置根据文本的语义来决定关注哪些部分</li>
<li>最后Unet输出两个向量，一个是无条件预测噪声，一个是文本预测噪声。</li>
</ul>
<blockquote>
<p>无条件预测噪声输入是空字符串</p>
</blockquote>
<ul>
<li>使用CFG计算最终预测噪声<span class="math inline">\(\epsilon_{\text{guided}}(z_t, t, \tau_\theta(y)) = \epsilon_\theta(z_t, t,\tau_\theta(y)) + s \cdot (\epsilon_\theta(z_t, t, \tau_\theta(y)) - \epsilon_\theta(z_t, t, \varnothing))\)</span></li>
<li>使用损失函数进行反向传播计算<img src="https://img2023.cnblogs.com/blog/2489686/202505/2489686-20250524142351764-1390319244.png" alt="img" loading="lazy"></li>
</ul>
<p>在生成阶段：</p>
<ul>
<li>以随机噪声<span class="math inline">\(Z_{T}\)</span>作为起点</li>
<li>输入文本作为条件，编码后一起进入Unet进行交叉注意力计算</li>
<li>输出预测噪声<span class="math inline">\(\epsilon_{\text{guided}}(z_t, t, \tau_\theta(y))\)</span></li>
<li>使用调度器进行逐步去噪计算（如DDPM，DDIM)成为<span class="math inline">\(Z_{T-1}\)</span></li>
<li>重复以上过程，直到Z</li>
<li>通过自动编码器的解码器部分把Z迁移到像素空间，D(z),即生成图像</li>
</ul>
<blockquote>
<p>交叉注意力机制中的维度变换</p>
</blockquote>
<blockquote>
<p>图像编码后变成 C=4, H'=64, W'=64，展平后作为Q(<span class="math inline">\(z_t \Rightarrow Q \in \mathbb{R}^{(H'W') \times d}\)</span>),文本通过编码器的编码表示为<span class="math inline">\(c = [t_1, t_2, ..., t_L] \Rightarrow \text{Embedding} \in \mathbb{R}^{L \times d}\)</span>,K和V表示为<span class="math inline">\(K, V \in \mathbb{R}^{L \times d}\)</span>,计算注意力权重<span class="math inline">\(A = \text{softmax}\left( \frac{Q K^\top}{\sqrt{d}} \right) \in \mathbb{R}^{(H'W') \times L}\)</span>,输出为<span class="math inline">\(\text{Attention}(Q, K, V) = A \cdot V \in \mathbb{R}^{(H'W') \times d}\)</span></p>
</blockquote>
<pre><code class="language-python">        # 潜在空间输入（prepare_latents生成）
latents.shape = (batch_size * num_images_per_prompt, 4, H//8, W//8)

# 文本嵌入处理（encode_prompt输出）
prompt_embeds.shape = (batch_size, max_sequence_length, embedding_dim)

# IP适配器图像嵌入处理
image_embeds[0].shape = (batch_size * num_images_per_prompt, num_images, emb_dim)

# UNet输入/输出维度
latent_model_input.shape = [batch*2, 4, H//8, W//8]  # 当启用CFG时
noise_pred.shape = [batch*2, 4, H//8, W//8]          # UNet输出噪声预测
假设参数设置
prompt = "一只坐在月球上的猫"
height = 512
width = 512
num_images_per_prompt = 1
guidance_scale = 7.5
batch_size = 1  # 根据prompt长度自动确定

# 关键计算步骤演示
# ---------------------------
# 步骤1：潜在空间(latents)维度计算
latents_shape = (
    batch_size * num_images_per_prompt,  # 1*1=1
    4,  # UNet输入通道数
    height // 8,  # 512/8=64
    width // 8    # 512/8=64
)
print(f"潜在空间维度: {latents_shape}")  # -&gt; (1, 4, 64, 64)

# 步骤2：文本编码维度（假设使用CLIP模型）
prompt_embeds_shape = (
    batch_size, 
    77,  # CLIP最大序列长度
    768  # CLIP文本编码维度
) 
print(f"文本嵌入维度: {prompt_embeds_shape}")  # -&gt; (1, 77, 768)

# 步骤3：CFG处理后的嵌入
if guidance_scale &gt; 1:
    prompt_embeds = torch.cat([negative_embeds, positive_embeds])
    print(f"CFG嵌入维度: {prompt_embeds.shape}")  # -&gt; (2, 77, 768)

# 步骤4：UNet输入维度（假设启用CFG）
latent_model_input = torch.cat([latents] * 2)
print(f"UNet输入维度: {latent_model_input.shape}")  # -&gt; (2, 4, 64, 64)

# 步骤5：噪声预测输出
noise_pred = unet(latent_model_input, ...)[0]
print(f"噪声预测维度: {noise_pred.shape}")  # -&gt; (2, 4, 64, 64)

# 步骤6：CFG调整后的噪声
noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)
print(f"调整后噪声维度: {noise_pred.shape}")  # -&gt; (1, 4, 64, 64)

# 最终输出图像
image = vae.decode(latents / vae.config.scaling_factor)[0]
print(f"输出图像维度: {image.shape}")  # -&gt; (1, 3, 512, 512)
    def cross_attention(query, key, value):
    # 输入维度说明
    # query: 来自潜在噪声 [batch=2, 4*64*64=16384] → 投影为 [2, 16384, 768]
    # key/value: 来自文本嵌入 [2, 77, 768]
    
    # 步骤1：计算注意力分数
    attention_scores = torch.matmul(
        query,  # [2, 16384, 768] 
        key.transpose(-1, -2)  # [2, 768, 77] → 转置后维度
    )  # 矩阵乘法结果 → [2, 16384, 77]
    
    # 步骤2：计算注意力权重
    attention_probs = torch.softmax(
        attention_scores,  # [2, 16384, 77]
        dim=-1  # 对最后一个维度（文本标记维度）做归一化
    )  # 保持维度 [2, 16384, 77]
    
    # 步骤3：应用注意力到value
    output = torch.matmul(
        attention_probs,  # [2, 16384, 77]
        value  # [2, 77, 768]
    )  # 结果维度 → [2, 16384, 768]
    
    # 步骤4：重塑为潜在空间维度
    output = output.view(2, 4, 64, 64, 768)  # 恢复空间结构
    output = output.permute(0, 4, 1, 2, 3)  # [2, 768, 4, 64, 64]
    output = self.to_out(output)  # 通过最后的线性层投影回4通道
    return output  # [2, 4, 64, 64]
</code></pre>
<h2 id="数据集以及指标介绍">数据集以及指标介绍</h2>
<h3 id="数据集介绍">数据集介绍</h3>
<p><img src="https://img2023.cnblogs.com/blog/2489686/202505/2489686-20250524151233145-1428716968.png" alt="img" loading="lazy"></p>
<p>CelebA-HQ 256 × 256数据集，是一个大规模的人脸属性数据集，拥有超过200K张名人图片，每张图片都有40个属性注释（如身份，年龄、表情、发型等）。</p>
<p><img src="https://img2023.cnblogs.com/blog/2489686/202505/2489686-20250524151954069-798614626.png" alt="img" loading="lazy"></p>
<p>从Flickr网站爬取的人脸数据集集合，涵盖多样化的年龄、种族、表情、配饰（如眼镜、帽子）等属性</p>
<p><img src="https://img2023.cnblogs.com/blog/2489686/202505/2489686-20250524152215385-1752966306.png" alt="img" loading="lazy"></p>
<p>这两个数据集都是LSUN（大规模场景理解）数据集的子集，两个数据集分别表示教堂和卧室场景的数据，包含教堂建筑的不同视角、结构和环境条件，覆盖多样化的卧室场景，包括不同装修风格、家具布局和光照条件</p>
<h3 id="指标介绍">指标介绍</h3>
<h4 id="is分数介绍">IS分数介绍</h4>
<p>Inception Score 的定义为：<br>
<span class="math inline">\(IS(G) = \exp \left( \mathbb{E}_{x \sim p_g} \left[ D_{KL} ( p(y|x) \| p(y) ) \right] \right)\)</span></p>
<p>x~pg：生成图像样本来自生成模型的分布 。</p>
<p>p(y|x)：通过预训练分类器（如Inception v3）对生成图像的类别预测概率分布。</p>
<p>p(y)：预测类别的边缘分布。类别可以是猫，狗，猪等诸如此类的动物。<br>
其中如果生成图像明确、质量高，则p(y|x)的熵就会比较低，如果生成图像比较多样，则p(y)的熵就会较高，体现在公式中则IS分数会较高。</p>
<h4 id="fid分数介绍">FID分数介绍</h4>
<p>主要是计算生成图像分布和真实图像分布在特征空间中的距离<br>
公式<span class="math inline">\(\text{FID} = \| \mu_r - \mu_g \|_2^2 + \text{Tr}(\Sigma_r + \Sigma_g - 2 (\Sigma_r \Sigma_g)^{\frac{1}{2}})\)</span></p>
<p><span class="math inline">\(\mu_r,\Sigma_r\)</span>：真实图像分布的均值和协方差矩阵。</p>
<p><span class="math inline">\(\mu_g,\Sigma_g\)</span>：生成图像分布的均值和协方差矩阵。</p>
<p><span class="math inline">\(\| \mu_r - \mu_g \|_2^2\)</span>：欧几里得距离的平方。</p>
<p><span class="math inline">\(\text{Tr}\)</span>：矩阵的迹。</p>
<p><span class="math inline">\((\Sigma_r \Sigma_g)^{\frac{1}{2}}\)</span>：协方差矩阵的乘积的平方根。</p>
<p>两个分布的均值和协方差越低，FID越低，生成图像质量越接近生成的图像</p>
<h4 id="prec和recall">prec和recall</h4>
<p>这里的指标和一般理解的不一样。</p>
<p>会先用Inception网络分别提取真实图像和生成图像的特征点</p>
<p>用集合的角度解释：</p>
<p>Precision ≈ 生成图像中，有多少落在真实图像分布的“支持区域”里（真实性）</p>
<p>Recall    ≈ 真实图像中，有多少被生成图像的“支持区域”覆盖（多样性）</p>
<p>注：支持区域的计算<br>
基于 kNN — 提取 Inception 特征后，设置一个半径 ε<br>
看有多少点落入对方的球内；<br>
也有基于球体体积估计的方法。</p>
<h2 id="实验分析">实验分析</h2>
<p><img src="https://img2023.cnblogs.com/blog/2489686/202505/2489686-20250524160708837-247624707.png" alt="img" loading="lazy"></p>
<p>研究不同下采样因子f对生成图像质量和训练效率的影响</p>
<blockquote>
<p>下采样因子：指的是自动编码器中的参数。</p>
</blockquote>
<p>可以看到下采样因子为4或8时表现最好。因为如果因子过小，会导致维度高，计算缓慢，因子过大，会损失很多信息，导致最后生成图像生成质量较差</p>
<p>后续的实验将基于此展开</p>
<p><img src="https://img2023.cnblogs.com/blog/2489686/202505/2489686-20250524161814398-1305823952.png" alt="img" loading="lazy"></p>
<p>在这个实验里可以看到，LDM在CelebA-HQ中取得了最优的FID分数，在其他数据集上的表现也是中规中矩。</p>
<p><img src="https://img2023.cnblogs.com/blog/2489686/202505/2489686-20250524163947744-643239303.png" alt="img" loading="lazy"></p>
<p>这个实验里展示了LDM在类别生成任务中的表现，可以看到使用cfg引导的LDM展现出了非常优秀的性能，在FID和IS分数上表现优异，虽然recall略低，但是使用的参数量也大幅减少了。</p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.0450597527650463" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-05-25 11:18">2025-05-24 16:45</span>&nbsp;
<a href="https://www.cnblogs.com/Sun-Wind">Sun-Wind</a>&nbsp;
阅读(<span id="post_view_count">50</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18894486);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18894486', targetLink: 'https://www.cnblogs.com/Sun-Wind/p/18894486', title: 'stable diffusion论文解读' })">举报</a>
</div>
        