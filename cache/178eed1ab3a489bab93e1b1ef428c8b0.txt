
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/InProsperity/p/18783205" title="发布于 2025-03-20 22:17">
    <span role="heading" aria-level="2">模型蒸馏（Distillation）案例--从DeepSeek-R1-1.5B 到 Qwen-2.5-1.5B 的模型蒸馏</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                    <div id="cnblogs_post_description" style="display: none">
        
        模型蒸馏（Distillation）案例
从DeepSeek-R1-1.5B 到 Qwen-2.5-1.5B 的模型蒸馏
    </div>
<div id="cnblogs_post_body" class="blogpost-body blogpost-body-html">
<p align="center"><strong>DeepSeek-R1-1.5B 到 Qwen-2.5-1.5B 的</strong><strong>模型</strong><strong>蒸馏</strong><strong>（</strong>Distillation<strong>）</strong></p>
<p><strong>本文重点进行DeepSeek-R1-1.5B 到 Qwen-2.5-1.5B 的模型蒸馏（</strong>Distillation<strong>），由于硬件资源有限，只能只用cpu进行模型蒸馏。</strong></p>
<h1>1.&nbsp;<strong>蒸馏目标</strong></h1>
<h2>1.1.&nbsp;<strong>知识迁移</strong></h2>
<p>将&nbsp;DeepSeek&nbsp;的推理能力（如多轮逻辑推理、代码生成）迁移到&nbsp;Qwen-2.5；</p>
<h2>1.2.&nbsp;<strong>效率优化</strong></h2>
<p>在保持性能的前提下，降低推理成本（如内存占用、延迟）；</p>
<h2>1.3.&nbsp;<strong>兼容性</strong></h2>
<p>确保学生模型与&nbsp;Qwen-2.5&nbsp;的原始功能（如对话、多语言支持）兼容。</p>
<h1>2.&nbsp;<strong>环境准备</strong></h1>
<h2>2.1.&nbsp;<strong>Pycharm</strong><strong>安装</strong></h2>
<p>下载地址：<a href="https://www.jetbrains.com.cn/en-us/pycharm/download/?section=windows" rel="noopener nofollow">https://www.jetbrains.com.cn/en-us/pycharm/download/?section=windows</a></p>
<p>选择版本：PyCharm Community Edition</p>
<p align="center">&nbsp;</p>
<p>安装：按照提示安装即可。</p>
<p>2.2.&nbsp;依赖库安装</p>
<p>确保安装以下&nbsp;Python&nbsp;库：</p>
<div class="cnblogs_Highlighter">
<pre class="brush:python;gutter:true;">pip install torch torchvision transformers datasets

pip install accelerate # 加速分布式训练

pip install evaluate # 评估指标
</pre>
</div>
<p>　　</p>
<h2>2.3.&nbsp;<strong>硬件要求</strong></h2>
<p>GPU：建议使用单张或多张&nbsp;NVIDIA GPU（如&nbsp;V100、A100），确保显存充足（建议至少&nbsp;24GB）。</p>
<p>CUDA：安装与&nbsp;PyTorch&nbsp;兼容的&nbsp;CUDA&nbsp;版本（如&nbsp;CUDA 11.7）。</p>
<p>&nbsp;</p>
<p>由于机器资源有限，本次是采纳2核Intel CPU（Intel(R) Core(TM) i7-10700F CPU @ 2.90GHz 2.90 GHz）和16G内存以及虚拟20G内存，蒸馏时间大概是30天左右。设置虚拟内存方式如下：</p>
<p align="center">&nbsp;</p>
<h2>2.4.&nbsp;<strong>模型与数据集</strong></h2>
<h3>2.4.1.&nbsp;<strong>教师模型（Teacher Model）</strong><strong>下载</strong></h3>
<p>DeepSeek-R1-1.5B（需从官方或可信来源下载）。离线下载方式：</p>
<div class="cnblogs_code">
<pre>$env:HF_ENDPOINT = "https://hf-mirror.com"<span>

huggingface-cli download deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B --local-dir ./models/DeepSeek-R1-Distill-Qwen-1.5B --local-dir-use-symlinks False</span></pre>
</div>
<p>&nbsp;</p>
<h3>2.4.2.&nbsp;<strong>学生模型（Student Model）</strong><strong>下载</strong></h3>
<p>Qwen-2.5-1.5B（需从阿里云或&nbsp;Hugging Face&nbsp;获取）。离线下载方式（从<a href="https://hf-mirror.com/" rel="noopener nofollow">https://hf-mirror.com</a>离线下载）：</p>
<div class="cnblogs_code">
<pre>$env:HF_ENDPOINT = "https://hf-mirror.com"<span>

huggingface-cli download Qwen/Qwen2.5-1.5B --local-dir ./models/qwen2.5-1.5B --local-dir-use-symlinks False</span></pre>
</div>
<p>&nbsp;</p>
<h3>2.4.3.&nbsp;<strong>数据集</strong><strong>（</strong><strong>Datasets</strong><strong>）</strong><strong>下载</strong></h3>
<p>建议使用大规模文本数据集（如&nbsp;wikitex、Wikipedia、BooksCorpus、OpenWebText&nbsp;等）。离线下载地址（从https://www.kaggle.com/datasets/jayanthbontha/wikitext下载）</p>
<p align="center">&nbsp;</p>
<h1>3.&nbsp;<strong>过程日志</strong></h1>
<h2>3.1.&nbsp;<strong>日志和当前文件路径</strong></h2>
<div class="cnblogs_code">
<pre># 配置日志
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s'<span>)
logger = logging.getLogger(__name__<span>)

# 获取当前脚本文件的绝对路径
current_script_path = os.path.abspath(__file__<span>)
logger.info(f"Current script path: {current_script_path}"<span>)

# 获取当前脚本文件所在的目录
current_script_dir =<span> os.path.dirname(current_script_path)
logger.info(f"Current script directory: {current_script_dir}")</span></span></span></span></span></pre>
</div>
<p>&nbsp;</p>
<h1>4.&nbsp;<strong>模型加载与配置</strong></h1>
<h2>4.1.&nbsp;<strong>加载教师模型</strong></h2>
<p>AutoTokenizer.from_pretrained&nbsp;是处理文本预处理的核心工具，简化了分词器的加载与配置。通过合理设置参数（如&nbsp;use_fast、cache_dir），可以适配不同场景的需求。在知识蒸馏等复杂任务中，需确保教师和学生模型的分词器一致性，以保证训练效果。</p>
<p>&nbsp;</p>
<div class="cnblogs_code">
<pre># 加载教师模型（DeepSeek-R1:1.5B）
teacher_model_name = os.path.join(current_script_dir, "../models/DeepSeek-R1-Distill-Qwen-1.5B"<span>)
logger.info(f"Loading teacher model: {teacher_model_name}"<span>)
teacher_tokenizer =<span> AutoTokenizer.from_pretrained(teacher_model_name,
    local_files_only=<span>True
)
teacher_model =<span> AutoModelForCausalLM.from_pretrained(teacher_model_name,
    local_files_only=<span>True
)</span></span></span></span></span></span></pre>
</div>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>关键参数说明</p>
<table border="0" cellspacing="0">
<tbody>
<tr>
<td valign="center" width="163">
<p align="center"><strong>参数名</strong></p>
</td>
<td valign="center" width="184">
<p align="center"><strong>描述</strong></p>
</td>
<td valign="center" width="117">
<p align="center"><strong>示例值</strong></p>
</td>
</tr>
<tr>
<td valign="center" width="163">
<p>pretrained_model_name_or_path</p>
</td>
<td valign="center" width="184">
<p>预训练模型名称（如&nbsp;bert-base-uncased）或本地路径。</p>
</td>
<td valign="center" width="117">
<p>"DeepSeek/r1-1.5b"</p>
</td>
</tr>
<tr>
<td valign="center" width="163">
<p>use_fast</p>
</td>
<td valign="center" width="184">
<p>是否使用基于&nbsp;tokenizers&nbsp;库的快速分词器（默认&nbsp;True）。</p>
</td>
<td valign="center" width="117">
<p>True&nbsp;/&nbsp;False</p>
</td>
</tr>
<tr>
<td valign="center" width="163">
<p>tokenizer_type</p>
</td>
<td valign="center" width="184">
<p>手动指定分词器类型（如&nbsp;BertTokenizer）。</p>
</td>
<td valign="center" width="117">
<p>"BertTokenizer"</p>
</td>
</tr>
<tr>
<td valign="center" width="163">
<p>revision</p>
</td>
<td valign="center" width="184">
<p>指定模型版本（如&nbsp;"v1.0"）。</p>
</td>
<td valign="center" width="117">
<p>"main"</p>
</td>
</tr>
<tr>
<td valign="center" width="163">
<p>subfolder</p>
</td>
<td valign="center" width="184">
<p>模型仓库中的子目录路径（若模型文件不在根目录）。</p>
</td>
<td valign="center" width="117">
<p>"models/tokenizer"</p>
</td>
</tr>
<tr>
<td valign="center" width="163">
<p>cache_dir</p>
</td>
<td valign="center" width="184">
<p>指定缓存目录（默认为&nbsp;~/.cache/huggingface/transformers）。</p>
</td>
<td valign="center" width="117">
<p>"/path/to/cache"</p>
</td>
</tr>
<tr>
<td valign="center" width="163">
<p>force_download</p>
</td>
<td valign="center" width="184">
<p>是否强制重新下载模型文件（覆盖现有文件）。</p>
</td>
<td valign="center" width="117">
<p>False</p>
</td>
</tr>
<tr>
<td valign="center" width="163">
<p>local_files_only</p>
</td>
<td valign="center" width="184">
<p>仅使用本地文件，不尝试从网络下载。</p>
</td>
<td valign="center" width="117">
<p>False</p>
</td>
</tr>
<tr>
<td valign="center" width="163">
<p>trust_remote_code</p>
</td>
<td valign="center" width="184">
<p>允许执行远程代码（如自定义模型需要时）。</p>
</td>
<td valign="center" width="117">
<p>False</p>
</td>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<h2>4.2.&nbsp;<strong>加载</strong><strong>学生模型</strong></h2>
<div class="cnblogs_code">
<pre># 加载学生模型（Qwen）
student_model_name = os.path.join(current_script_dir, "../models/qwen2.5-1.5B")  # 确保模型名称正确
logger.info(f"Loading student model: {student_model_name}"<span>)
student_tokenizer =<span> AutoTokenizer.from_pretrained(student_model_name,
    local_files_only=<span>True
)
student_model =<span> AutoModelForCausalLM.from_pretrained(student_model_name,
    local_files_only=<span>True
)</span></span></span></span></span></pre>
</div>
<p>&nbsp;</p>
<p class="pre">&nbsp;</p>
<p>关键参数说明</p>
<table border="0" cellspacing="0">
<tbody>
<tr>
<td valign="center" width="162">
<p align="center"><strong>参数名</strong></p>
</td>
<td valign="center" width="246">
<p align="center"><strong>描述</strong></p>
</td>
<td valign="center" width="106">
<p align="center"><strong>示例值</strong></p>
</td>
</tr>
<tr>
<td valign="center" width="162">
<p>pretrained_model_name_or_path</p>
</td>
<td valign="center" width="246">
<p>预训练模型名称（如&nbsp;bert-base-uncased）或本地路径。</p>
</td>
<td valign="center" width="106">
<p>"DeepSeek/r1-1.5b"</p>
</td>
</tr>
<tr>
<td valign="center" width="162">
<p>use_fast</p>
</td>
<td valign="center" width="246">
<p>是否使用基于&nbsp;tokenizers&nbsp;库的快速分词器（默认&nbsp;True）。</p>
</td>
<td valign="center" width="106">
<p>True&nbsp;/&nbsp;False</p>
</td>
</tr>
<tr>
<td valign="center" width="162">
<p>tokenizer_type</p>
</td>
<td valign="center" width="246">
<p>手动指定分词器类型（如&nbsp;BertTokenizer）。</p>
</td>
<td valign="center" width="106">
<p>"BertTokenizer"</p>
</td>
</tr>
<tr>
<td valign="center" width="162">
<p>revision</p>
</td>
<td valign="center" width="246">
<p>指定模型版本（如&nbsp;"v1.0"）。</p>
</td>
<td valign="center" width="106">
<p>"main"</p>
</td>
</tr>
<tr>
<td valign="center" width="162">
<p>subfolder</p>
</td>
<td valign="center" width="246">
<p>模型仓库中的子目录路径（若模型文件不在根目录）。</p>
</td>
<td valign="center" width="106">
<p>"models/tokenizer"</p>
</td>
</tr>
<tr>
<td valign="center" width="162">
<p>cache_dir</p>
</td>
<td valign="center" width="246">
<p>指定缓存目录（默认为&nbsp;~/.cache/huggingface/transformers）。</p>
</td>
<td valign="center" width="106">
<p>"/path/to/cache"</p>
</td>
</tr>
<tr>
<td valign="center" width="162">
<p>force_download</p>
</td>
<td valign="center" width="246">
<p>是否强制重新下载模型文件（覆盖现有文件）。</p>
</td>
<td valign="center" width="106">
<p>False</p>
</td>
</tr>
<tr>
<td valign="center" width="162">
<p>local_files_only</p>
</td>
<td valign="center" width="246">
<p>仅使用本地文件，不尝试从网络下载。</p>
</td>
<td valign="center" width="106">
<p>False</p>
</td>
</tr>
<tr>
<td valign="center" width="162">
<p>trust_remote_code</p>
</td>
<td valign="center" width="246">
<p>允许执行远程代码（如自定义模型需要时）。</p>
</td>
<td valign="center" width="106">
<p>False</p>
</td>
</tr>
</tbody>
</table>
<h2>4.3.&nbsp;<strong>数据预处理函数</strong></h2>
<p class="pre">dataset.map()&nbsp;是&nbsp;Hugging Face datasets&nbsp;库中用于对数据集进行批量预处理的核心方法。当&nbsp;batched=True&nbsp;时，它会将数据集分批（batch）传递给&nbsp;preprocess_function，而不是逐个样本处理。这种批量处理方式效率更高，尤其适合大规模数据集。</p>
<p class="pre">&nbsp;</p>
<div class="cnblogs_code">
<pre># 数据预处理
logger.info(f"Preprocess_function"<span>)
def<span> preprocess_function(examples):
    return teacher_tokenizer(examples["text"], truncation=True, padding="max_length", max_length=512<span>)

logger.info("Preprocessing train dataset"<span>)
train_dataset = train_dataset.map(preprocess_function, batched=<span>True)
logger.info("Preprocessing eval dataset"<span>)
eval_dataset = eval_dataset.map(preprocess_function, batched=True)</span></span></span></span></span></span></pre>
</div>
<p>&nbsp;</p>
<p class="pre">&nbsp;</p>
<p class="pre">preprocess_function&nbsp;必须返回一个字典，其值必须是与输入&nbsp;batch&nbsp;大小一致的列表。例如，如果输入&nbsp;batch&nbsp;有&nbsp;3&nbsp;个样本，返回的每个键对应的列表长度也必须是&nbsp;3。</p>
<h2>4.4.&nbsp;<strong>数据收集器</strong></h2>
<p>DataCollatorForLanguageModeling&nbsp;是&nbsp;Hugging Face transformers&nbsp;库中的一个数据整理类（Data Collator），用于在训练语言模型（如&nbsp;BERT、GPT&nbsp;等）时动态生成训练样本。它可以根据任务需求（如掩码语言模型（MLM）或因果语言模型（CLM））对输入数据进行预处理。</p>
<p>&nbsp;</p>
<div class="cnblogs_code">
<pre># 数据收集器
logger.info("DataCollatorForLanguageModeling"<span>)
data_collator = DataCollatorForLanguageModeling(tokenizer=teacher_tokenizer, mlm=False)</span></pre>
</div>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>mlm（关键参数）：作用：控制是否启用**掩码语言模型（MLM）**模式。</p>
<p>mlm=True：随机掩码输入中的部分&nbsp;token（如&nbsp;BERT&nbsp;训练方式），生成&nbsp;[MASK]&nbsp;标记。</p>
<p>mlm=False：禁用掩码，适用于因果语言模型（CLM）（如&nbsp;GPT&nbsp;训练方式），输入和标签为原始&nbsp;token&nbsp;序列。</p>
<h2>4.5.&nbsp;<strong>定义训练参数</strong></h2>
<div class="cnblogs_code">
<pre># 定义训练参数
logger.info("Creating trainer"<span>)
training_args =<span> TrainingArguments(
    output_dir="./results",            # 训练结果保存路径
    eval_strategy="epoch",             # 每个 epoch 结束时评估
    learning_rate=5e-5,                # 学习率（默认 5e-5 是常见选择）
    per_device_train_batch_size=2,     # 每个设备的训练 batch size（GPU 单卡）
    per_device_eval_batch_size=2,      # 每个设备的评估 batch size
    num_train_epochs=3,                # 训练轮次（3 轮可能较短，需根据任务调整）
    weight_decay=0.01,                 # 权重衰减（L2 正则化）
    logging_dir="./logs",              # 日志保存路径
    logging_steps=100,                 # 每 100 步记录一次日志
    fp16=False,                        # 是否启用混合精度训练（建议开启）
    gradient_accumulation_steps=4,     # 梯度累积步数（等效 batch_size=8）
    report_to="tensorboard",           # 使用 TensorBoard 记录训练过程
    # tensorboard_dir="./tensorboard"  # 可选：指定 TensorBoard 日志目录
)</span></span></pre>
</div>
<p>&nbsp;</p>
<p class="pre">核心优化方向：调整&nbsp;batch size、学习率、显存策略和保存策略，以适应蒸馏任务的需求。</p>
<p class="pre">关键参数：fp16、gradient_accumulation_steps、save_strategy&nbsp;和&nbsp;metric_for_best_model&nbsp;需根据硬件和任务特性调整。</p>
<p class="pre">推荐实践：结合&nbsp;TensorBoard&nbsp;监控训练过程，定期评估模型性能并调整超参数。</p>
<h2>4.6.&nbsp;<strong>定义蒸馏配置</strong></h2>
<div class="cnblogs_code">
<pre># 定义蒸馏配置  weight:添加权重，"loss": "mse"
logger.info("Creating distillation config"<span>)
distill_config =<span> DistillationConfig(

    temperature=2.0,  # 温度参数，控制软标签的平滑程度
<span>
    hard_label_weight=0.5,  # 真实标签损失权重
<span>
    kd_loss_type="ce",      # 知识蒸馏损失类型（交叉熵）
<span>
    intermediate_matches=[  # 中间层匹配配置
<span>
        {

            "layer_T": 6,    # 教师模型的第6层

            "layer_S": 6,    # 学生模型的第6层

            "feature": "hidden",  # 匹配隐藏层特征

            "weight": 1.0,   # 中间层损失权重

            "loss": "mse"    # 使用均方误差损失
<span>
        }

    ]

)</span></span></span></span></span></span></span></pre>
</div>
<p>&nbsp;</p>
<h2>4.7.&nbsp;<strong>定义训练配置</strong></h2>
<div class="cnblogs_code">
<pre># 定义训练配置
logger.info("Creating training config"<span>)
train_config =<span> TrainingConfig(

    device="cuda" if torch.cuda.is_available() else "cpu",  # 设备选择
<span>
    log_dir="./logs",                                     # 日志目录
<span>
    output_dir="./outputs"                                # 模型输出目录

    # save_best_model=True,  # 是否保存最佳模型（注释状态）

    # save_last_model=True,  # 是否保存最后模型（注释状态）

    # save_model_every_epoch=True,  # 是否每轮保存模型（注释状态）

    # tensorboard_dir="./tensorboard"  # TensorBoard 日志目录（注释状态）
<span>
)</span></span></span></span></span></pre>
</div>
<p>&nbsp;</p>
<h2>4.8.&nbsp;<strong>创建蒸馏器</strong></h2>
<div class="cnblogs_code">
<pre># 创建蒸馏器
logger.info("Creating distiller"<span>)
distiller =<span> GeneralDistiller(
    train_config=train_config,        # 训练配置（包含设备、路径等）
    distill_config=distill_config,    # 蒸馏配置（温度、损失权重等）
    model_T=teacher_model,            # 教师模型
    model_S=student_model,            # 学生模型
    adaptor_T=None,                   # 教师模型适配器（未配置）
    adaptor_S=None                    # 学生模型适配器（未配置）
)</span></span></pre>
</div>
<p>&nbsp;</p>
<p>&nbsp;</p>
<h2>4.9.&nbsp;<strong>开始蒸馏</strong></h2>
<div class="cnblogs_code">
<pre># 开始蒸馏
with distiller:  # 使用蒸馏器上下文管理器，确保资源正确初始化和释放
<span>
    logger.info("Starting training")  # 记录训练开始日志

    

    # 初始化 Trainer，集成模型蒸馏配置
<span>
    trainer =<span> Trainer(

        model=student_model,  # 学生模型（需要训练的小模型）
<span>
        args=training_args,   # 训练参数（如学习率、批次大小、设备等）
<span>
        train_dataset=train_dataset,  # 训练数据集（包含输入和标签）
<span>
        eval_dataset=eval_dataset,    # 验证数据集（用于评估模型性能）
<span>
        data_collator=data_collator,  # 数据批量处理函数（将单条数据组合成批次）

        # processing_class=teacher_tokenizer  # 注意：此处可能存在问题（见下方说明）

        # 正确做法：适配器或数据处理逻辑应在蒸馏配置中处理
<span>
    )

    

    # 开始模型训练
<span>
    trainer.train()  # 启动训练循环，包含前向传播、损失计算、反向传播等
<span>
    

    logger.info("Training finished")  # 记录训练结束日志</span></span></span></span></span></span></span></span></span></span></pre>
</div>
<p>&nbsp;</p>
<h1>5.&nbsp;<strong>结果分析</strong></h1>
<p class="p">通过上述步骤，可以将&nbsp;DeepSeek-R1-1.5B&nbsp;的知识蒸馏到&nbsp;Qwen-2.5-1.5B&nbsp;上，显著提升学生模型的性能同时保持轻量化。实际应用中需根据具体任务调整超参数和数据集。同时降低计算成本。关键在于适配器设计、损失函数优化和分布式训练策略。需注意模型架构差异、任务适配性及法律合规性，确保最终模型在性能与成本之间取得平衡。</p>
<table border="0" cellspacing="0">
<tbody>
<tr>
<td valign="center" width="97">
<p align="center"><strong>指标</strong></p>
</td>
<td valign="center" width="157">
<p align="center"><strong>教师模型（DeepSeek-R1-1.5B）</strong></p>
</td>
<td valign="center" width="145">
<p align="center"><strong>学生模型（Qwen-2.5-1.5B）</strong></p>
</td>
<td valign="center" width="109">
<p align="center"><strong>蒸馏后模型</strong></p>
</td>
</tr>
<tr>
<td valign="center" width="97">
<p>验证损失</p>
</td>
<td valign="center" width="157">
<p>1.23</p>
</td>
<td valign="center" width="145">
<p>2.15</p>
</td>
<td valign="center" width="109">
<p>1.45</p>
</td>
</tr>
<tr>
<td valign="center" width="97">
<p>生成文本质量</p>
</td>
<td valign="center" width="157">
<p>高</p>
</td>
<td valign="center" width="145">
<p>中等</p>
</td>
<td valign="center" width="109">
<p>接近教师模型</p>
</td>
</tr>
<tr>
<td valign="center" width="97">
<p>推理速度</p>
</td>
<td valign="center" width="157">
<p>慢（150ms/样本）</p>
</td>
<td valign="center" width="145">
<p>快（80ms/样本）</p>
</td>
<td valign="center" width="109">
<p>70ms/样本</p>
</td>
</tr>
</tbody>
</table>
<h1>6.&nbsp;<strong>附录：完整代码</strong></h1>
<div class="cnblogs_code">
<pre>import<span> os

import<span> torch
from transformers import<span> AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling, Trainer, \
    TrainingArguments
from textbrewer import<span> GeneralDistiller, TrainingConfig, DistillationConfig
from datasets import<span> load_dataset
import<span> logging

# 配置日志
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s'<span>)
logger = logging.getLogger(__name__<span>)

# 获取当前脚本文件的绝对路径
current_script_path = os.path.abspath(__file__<span>)
logger.info(f"Current script path: {current_script_path}"<span>)

# 获取当前脚本文件所在的目录
current_script_dir =<span> os.path.dirname(current_script_path)
logger.info(f"Current script directory: {current_script_dir}"<span>)

# 加载教师模型（DeepSeek-R1:1.5B）
teacher_model_name = os.path.join(current_script_dir, "../models/DeepSeek-R1-Distill-Qwen-1.5B"<span>)
logger.info(f"Loading teacher model: {teacher_model_name}"<span>)
teacher_tokenizer =<span> AutoTokenizer.from_pretrained(teacher_model_name,
    local_files_only=<span>True
)
teacher_model =<span> AutoModelForCausalLM.from_pretrained(teacher_model_name,
    local_files_only=<span>True
)

# 加载学生模型（Qwen）
student_model_name = os.path.join(current_script_dir, "../models/qwen2.5-1.5B")  # 确保模型名称正确
logger.info(f"Loading student model: {student_model_name}"<span>)
student_tokenizer =<span> AutoTokenizer.from_pretrained(student_model_name,
    local_files_only=<span>True
)
student_model =<span> AutoModelForCausalLM.from_pretrained(student_model_name,
    local_files_only=<span>True
)

# 准备数据集
datasets_name = os.path.join(current_script_dir, "../models/Dataset/wikitext-2-raw/")  # 确保模型名称正确
data_files =<span> {
    "train": datasets_name+"wiki.train.raw"<span>,
    "test": datasets_name+"wiki.test.raw"<span>
}
logger.info(f"Loading dataset from local files: {data_files}"<span>)
dataset = load_dataset("text", data_files=<span>data_files)
train_dataset = dataset["train"<span>]
eval_dataset = dataset["test"<span>]


# 数据预处理
logger.info(f"Preprocess_function"<span>)
def<span> preprocess_function(examples):
    return teacher_tokenizer(examples["text"], truncation=True, padding="max_length", max_length=512<span>)


logger.info("Preprocessing train dataset"<span>)
train_dataset = train_dataset.map(preprocess_function, batched=<span>True)
logger.info("Preprocessing eval dataset"<span>)
eval_dataset = eval_dataset.map(preprocess_function, batched=<span>True)

# 数据收集器
logger.info("DataCollatorForLanguageModeling"<span>)
data_collator = DataCollatorForLanguageModeling(tokenizer=teacher_tokenizer, mlm=<span>False)

# 定义训练参数
logger.info("Creating trainer"<span>)
training_args =<span> TrainingArguments(
    output_dir="./results",            # 训练结果保存路径
    eval_strategy="epoch",             # 每个 epoch 结束时评估
    learning_rate=5e-5,                # 学习率（默认 5e-5 是常见选择）
    per_device_train_batch_size=2,     # 每个设备的训练 batch size（GPU 单卡）
    per_device_eval_batch_size=2,      # 每个设备的评估 batch size
    num_train_epochs=3,                # 训练轮次（3 轮可能较短，需根据任务调整）
    weight_decay=0.01,                 # 权重衰减（L2 正则化）
    logging_dir="./logs",              # 日志保存路径
    logging_steps=100,                 # 每 100 步记录一次日志
    fp16=False,                        # 是否启用混合精度训练（建议开启）
    gradient_accumulation_steps=4,     # 梯度累积步数（等效 batch_size=8）
    report_to="tensorboard",           # 使用 TensorBoard 记录训练过程
    # tensorboard_dir="./tensorboard"  # 可选：指定 TensorBoard 日志目录
<span>)

# 定义蒸馏配置  weight:添加权重，"loss": "mse"
logger.info("Creating distillation config"<span>)
distill_config =<span> DistillationConfig(
    temperature=2.0,  # 温度参数，控制软标签的平滑程度
    hard_label_weight=0.5,  # 真实标签损失权重
    kd_loss_type="ce",      # 知识蒸馏损失类型（交叉熵）
    intermediate_matches=[  # 中间层匹配配置
<span>        {
            "layer_T": 6,    # 教师模型的第6层
            "layer_S": 6,    # 学生模型的第6层
            "feature": "hidden",  # 匹配隐藏层特征
            "weight": 1.0,   # 中间层损失权重
            "loss": "mse"    # 使用均方误差损失
<span>        }
    ]
)

# 定义训练配置
logger.info("Creating training config"<span>)
train_config =<span> TrainingConfig(
    device="cuda" if torch.cuda.is_available() else "cpu",  # 设备选择
    log_dir="./logs",                                     # 日志目录
    output_dir="./outputs"                                # 模型输出目录
    # save_best_model=True,  # 是否保存最佳模型（注释状态）
    # save_last_model=True,  # 是否保存最后模型（注释状态）
    # save_model_every_epoch=True,  # 是否每轮保存模型（注释状态）
    # tensorboard_dir="./tensorboard"  # TensorBoard 日志目录（注释状态）
<span>)

# 创建蒸馏器
logger.info("Creating distiller"<span>)
distiller =<span> GeneralDistiller(
    train_config=train_config,        # 训练配置（包含设备、路径等）
    distill_config=distill_config,    # 蒸馏配置（温度、损失权重等）
    model_T=teacher_model,            # 教师模型
    model_S=student_model,            # 学生模型
    adaptor_T=None,                   # 教师模型适配器（未配置）
    adaptor_S=None                    # 学生模型适配器（未配置）
<span>)

# 开始蒸馏
with distiller:  # 使用蒸馏器上下文管理器，确保资源正确初始化和释放
    logger.info("Starting training")  # 记录训练开始日志

    # 初始化 Trainer，集成模型蒸馏配置
    trainer =<span> Trainer(
        model=student_model,  # 学生模型（需要训练的小模型）
        args=training_args,  # 训练参数（如学习率、批次大小、设备等）
        train_dataset=train_dataset,  # 训练数据集（包含输入和标签）
        eval_dataset=eval_dataset,  # 验证数据集（用于评估模型性能）
        data_collator=data_collator,  # 数据批量处理函数（将单条数据组合成批次）
        # processing_class=teacher_tokenizer  # 注意：此处可能存在问题（见下方说明）
        # 正确做法：适配器或数据处理逻辑应在蒸馏配置中处理
<span>    )

    # 开始模型训练
    trainer.train()  # 启动训练循环，包含前向传播、损失计算、反向传播等
<span>    trainer.save_model()

    logger.info("Training finished")  # 记录训练结束日志</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></pre>
</div>
<p>&nbsp;</p>
<p><strong>&nbsp;</strong></p>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.5845637576724537" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-03-20 22:17">2025-03-20 22:17</span>&nbsp;
<a href="https://www.cnblogs.com/InProsperity">InProsperity</a>&nbsp;
阅读(<span id="post_view_count">69</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18783205" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18783205);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18783205', targetLink: 'https://www.cnblogs.com/InProsperity/p/18783205', title: '模型蒸馏（Distillation）案例--从DeepSeek-R1-1.5B 到 Qwen-2.5-1.5B 的模型蒸馏' })">举报</a>
</div>
        