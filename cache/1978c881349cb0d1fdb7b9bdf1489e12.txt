
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/qizhou/p/18916588" title="发布于 2025-06-11 01:25">
    <span role="heading" aria-level="2">Benchmark论文解读：Evaluating the Ripple Effects of Knowledge Editing in Language Models</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body blogpost-body-html">
<p data-pm-slice="0 0 []">  论文发表于自然语言处理顶刊TACL-2024（<a href="https://aclanthology.org/2024.tacl-1.16/" target="_blank" rel="noopener nofollow">原文链接</a>）。目前模型编辑方法的评估主要集中在测试单个事实是否被成功注入，以及模型对其它事实的预测是否没有改变。作者认为这样的评估模式有限，因为注入一个事实会产生涟漪效应，模型应该同步更新一系列的额外事实。比如当注入：z是e的母亲时。模型应该同步更新：z的孩子是e的兄弟姐妹。为了解决这个问题：</p>
<p>  1、提出一套新的评价标准，考虑模型编辑对涟漪效应的影响。</p>
<p>  2、基于知识图构建了数据集RIPPLEDITS，用于捕捉各种类型的涟漪效应。</p>
<p>  3、在RIPPLEDITS上评估了之前的编辑方法，发现简单的in-context编辑baseline获得了最好的分数，为模型编辑提供了一个有前途的研究方向。</p>
<p>  阅读本文请同时参考原始论文图表。</p>
<h1>方法</h1>
<h2>评价指标</h2>
<p>  如图1所示，当将埃菲尔铁塔所在城市修改为伦敦时，模型不但要回答出埃菲尔铁塔所在城市是伦敦，并且无关事实对的回答不能变动，还要回答出：1、所在国家是英国而不是法国。2、对应的协调世界时是UTC+0。也就是一系列受到这个编辑影响的事实都要作相应的修改，称为这些事实为涟漪效应 (Ripple Effects)。</p>
<p>  对于一个给定模型，假设它学习到的知识可以表示为知识图谱的形式<span class="math">$\mathcal{K}=\{(e_i,r_i,o_i)\}_{i=1}^N$</span>。则对于该模型的一个知识编辑<span class="math">$\delta:(e,r,o)\to (e,r,o^*)$</span>，定义其涟漪效应为受到<span class="math">$\delta$</span>影响的事实集合<span class="math">$\mathcal{R}(\delta)$</span>，并称集合大小<span class="math">$|\mathcal{R}(\delta)|$</span>为这个编辑的严重程度。</p>
<p>  由于涟漪效应跨度可能十分大，本文主要关注与编辑事实的实体<span class="math">$e,o$</span>距离两跳以内的事实。为了评估模型的涟漪效应，提出了六个具体的评估标准。以修改事实(Prince, sibling, Nicholas Carminowe)为例，即修改后的模型关于问题：The sibling of Prince are …，有答案Nicholas Carminowe。六个评估标准如图2所示：</p>
<p>  A、逻辑泛化 (Logical generalization, LG)：测试模型是否回忆事实<span class="math">$(x,r',z)$</span>，其中<span class="math">$x\in \{e,o,o^*\}$</span>，<span class="math">$r'$</span>是与<span class="math">$r$</span>语义相关的关系。A中举了一个与原始提问对称的例子，即测试事实<span class="math">$(o^*,r,e)$</span>。</p>
<p>  B、组合I (Compositionality I, CI)：通过链接<span class="math">$(e,r,o^*)$</span>与<span class="math">$(o^*,r',z)$</span>，测试模型是否回忆事实<span class="math">$(e,r'',z)$</span>。其中模型在编辑之前就已知事实<span class="math">$(o^*,r',z)$</span>，关系<span class="math">$r''$</span>为<span class="math">$r,r'$</span>的组合，从而通过关系组合跳过<span class="math">$o^*$</span>。</p>
<p>  C、组合II (Compositionality II, CII)：通过链接<span class="math">$(e',r',e)$</span>与<span class="math">$(e,r,o^*)$</span>，测试模型是否回忆事实<span class="math">$(e',r'',o^*)$</span>。其中模型在编辑之前就已知事实<span class="math">$(e',r',e)$</span>，关系<span class="math">$r''$</span>为<span class="math">$r',r$</span>的组合，从而通过关系组合跳过<span class="math">$e$</span>。</p>
<p>  D、主体别名 (Subject Aliasing, SA)：测试模型是否回忆事实<span class="math">$(e',r,o^*)$</span>，其中<span class="math">$e'$</span>是<span class="math">$e$</span>的别名。</p>
<p>  E、遗忘度 (Forgetfulness, FN)：对于1-N的关系<span class="math">$r$</span>，测试模型是否回忆<span class="math">$(e,r,o')$</span>。其中<span class="math">$(e,r,o')$</span>是模型编辑前已知的事实。</p>
<p>  F、关系特异性 (Relation Specificity, RS)：测试模型是否回忆与编辑事实无关的事实。</p>
<h2>数据集构建</h2>
<p>  本文使用知识图构建基于以上评价指标的数据集RIPPLEDITS：</p>
<p>  1、事实三元组的收集：从WIKIDATA（由事实三元组构成的关系知识库）中收集待编辑的事实三元组，依据三个原则：最近(Recent)、随机(Random)、流行(Popular)。</p>
<p>  2、事实三元组的修改：对于Recent类型的三元组，模型训练时没有涉及，因此可以直接使用。对于Random和Popular类型的三元组，将相应的三元组事实修改为反事实。</p>
<p>  3、关于以上得到的每个待编辑事实三元组，收集六个评估指标对应的测试三元组。</p>
<p>  4、将所有事实三元组用模板转换为自然语言。</p>
<p>  统计数据如表1所示和图4所示。</p>
<h1>实验</h1>
<p>  图5：in-context编辑 (ICE) 的例子。</p>
<p>  表3/4/5：各编辑方法在各模型上以及不同数据类别上的编辑结果。可以看出In-context编辑的综合效果最好。</p>
<p>  表6：SOTA方法编辑GPT-2在4个评价指标上的平均结果。可以看出这些方法在这些涟漪效应上效果很差。</p>
<p>  图6：ROME在不同参数的模型上编辑的准确率变化图。可以看出，模型参数量越大，ROME准确率越高。</p>
<p>  图7：ROME、MEMIT、MEND分别使用三类数据编辑GPT-2在6个评价指标上的平均结果。</p>
<h1>总结</h1>
<p>  1、本文在MQUAKE和浙大综述之后，2023/7/24发布arxiv。</p>
<p>  2、本文所讨论的涟漪效应和浙大综述论文<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F6862026%2Fitems%2F6MSRB6CL%22%5D%2C%22itemData%22%3A%7B%22id%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F6862026%2Fitems%2F6MSRB6CL%22%2C%22type%22%3A%22article%22%2C%22abstract%22%3A%22Recent%20advancements%20in%20deep%20learning%20have%20precipitated%20the%20emergence%20of%20large%20language%20models%20(LLMs)%20which%20exhibit%20an%20impressive%20aptitude%20for%20understanding%20and%20producing%20text%20akin%20to%20human%20language.%20Despite%20the%20ability%20to%20train%20highly%20capable%20LLMs%2C%20the%20methodology%20for%20maintaining%20their%20relevancy%20and%20rectifying%20errors%20remains%20elusive.%20To%20that%20end%2C%20the%20past%20few%20years%20have%20witnessed%20a%20surge%20in%20techniques%20for%20editing%20LLMs%2C%20the%20objective%20of%20which%20is%20to%20alter%20the%20behavior%20of%20LLMs%20within%20a%20speci%EF%AC%81c%20domain%20without%20negatively%20impacting%20performance%20across%20other%20inputs.%20This%20paper%20embarks%20on%20a%20deep%20exploration%20of%20the%20problems%2C%20methods%2C%20and%20opportunities%20relating%20to%20model%20editing%20for%20LLMs.%20In%20particular%2C%20we%20provide%20an%20exhaustive%20overview%20of%20the%20task%20de%EF%AC%81nition%20and%20challenges%20associated%20with%20model%20editing%2C%20along%20with%20an%20in-depth%20empirical%20analysis%20of%20the%20most%20progressive%20methods%20currently%20at%20our%20disposal.%20We%20also%20build%20a%20new%20benchmark%20dataset%20to%20facilitate%20a%20more%20robust%20evaluation%20and%20pinpoint%20enduring%20issues%20intrinsic%20to%20existing%20techniques.%20Our%20objective%20is%20to%20provide%20valuable%20insights%20into%20the%20effectiveness%20and%20feasibility%20of%20each%20model%20editing%20technique%2C%20thereby%20assisting%20the%20research%20community%20in%20making%20informed%20decisions%20when%20choosing%20the%20most%20appropriate%20method%20for%20a%20speci%EF%AC%81c%20task%20or%20context1.%22%2C%22language%22%3A%22en%22%2C%22note%22%3A%22arXiv%3A2305.13172%20%5Bcs%5D%22%2C%22number%22%3A%22arXiv%3A2305.13172%22%2C%22publisher%22%3A%22arXiv%22%2C%22source%22%3A%22arXiv.org%22%2C%22title%22%3A%22Editing%20Large%20Language%20Models%3A%20Problems%2C%20Methods%2C%20and%20Opportunities%22%2C%22title-short%22%3A%22Editing%20Large%20Language%20Models%22%2C%22URL%22%3A%22http%3A%2F%2Farxiv.org%2Fabs%2F2305.13172%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Yao%22%2C%22given%22%3A%22Yunzhi%22%7D%2C%7B%22family%22%3A%22Wang%22%2C%22given%22%3A%22Peng%22%7D%2C%7B%22family%22%3A%22Tian%22%2C%22given%22%3A%22Bozhong%22%7D%2C%7B%22family%22%3A%22Cheng%22%2C%22given%22%3A%22Siyuan%22%7D%2C%7B%22family%22%3A%22Li%22%2C%22given%22%3A%22Zhoubo%22%7D%2C%7B%22family%22%3A%22Deng%22%2C%22given%22%3A%22Shumin%22%7D%2C%7B%22family%22%3A%22Chen%22%2C%22given%22%3A%22Huajun%22%7D%2C%7B%22family%22%3A%22Zhang%22%2C%22given%22%3A%22Ningyu%22%7D%5D%2C%22accessed%22%3A%7B%22date-parts%22%3A%5B%5B%222023%22%2C7%2C26%5D%5D%7D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222023%22%2C5%2C22%5D%5D%7D%7D%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Yao 等, 2023</span>)</span>中提到的可移植性 (Portability) 类似，但本文进一步细化为6个类别，综述中仅主要考虑了本文的2效应。</p>
<p>&nbsp;</p>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0" data-date-updated="2025-06-11 01:25">2025-06-11 01:25</span>&nbsp;
<a href="https://www.cnblogs.com/qizhou">颀周</a>&nbsp;
阅读(<span id="post_view_count">1</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18916588);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18916588', targetLink: 'https://www.cnblogs.com/qizhou/p/18916588', title: 'Benchmark论文解读：Evaluating the Ripple Effects of Knowledge Editing in Language Models' })">举报</a>
</div>
        