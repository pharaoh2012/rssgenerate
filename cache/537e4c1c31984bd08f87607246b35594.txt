
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/cjdty/p/18659438" title="发布于 2025-01-08 11:56">
    <span role="heading" aria-level="2">[软件工具使用记录] windows离线ollama部署本地模型并配置continue实现离线代码补全</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p>qwen2.5coder发布之后，觉得差不多可以实现离线模型辅助编程了，所以尝试在公司内网部署模型，配合vsocde插件continue实现代码提示、聊天功能。</p>
<p>目前使用qwen2.5coder的32b模型，体验上和gpt-4o差不多（都稀碎），适用于编写脚本，查一些简单问题，例如<code>flask如何把变量传到前端</code>，准确率还可以，但是补全功能稀碎。</p>
<p>硬件如下：</p>
<table>
<thead>
<tr>
<th>cpu</th>
<th>gpu</th>
<th>内存</th>
</tr>
</thead>
<tbody>
<tr>
<td>AMD Ryzen 9 5950X 16核</td>
<td>AMD Radeon TX 6900XT（需要安装最新驱动）/<em>16G显存直接吃满</em></td>
<td>64G 2600Mhz/<em>实际吃30G内存</em></td>
</tr>
</tbody>
</table>
<p>跑起来不算快，和我阅读速度差不多，对这套硬件来说挺吃力的。GPU没怎么跑，似乎主要是cpu在发力吃到60%占用率</p>
<h1 id="部署ollama">部署ollama</h1>
<h2 id="安装ollama客户端--选择模型">安装ollama客户端 &amp;&amp; 选择模型</h2>
<p>首先去<a href="https://ollama.com/download/windows" target="_blank" rel="noopener nofollow">Download Ollama on Windows</a>下载ollama的windows版本，安装包非常大，基本上700-800M</p>
<p>在有网络的电脑上安装，然后在<a href="https://ollama.com/search" target="_blank" rel="noopener nofollow">Ollama</a>这里找到需要的模型，例如这里我选择qwen2.5code的0.5b模型</p>
<p><img src="https://img2024.cnblogs.com/blog/1236187/202501/1236187-20250108114513142-1109514423.png" alt="image" loading="lazy"></p>
<p>点击第二个箭头<code>Tags</code>可以选择不同的量化版本，然后复制第三个箭头的指令</p>
<p>按下<code>Win+R</code>快捷键，运行<code>cmd</code>，执行复制的命令，比如这里是<code>ollama run qwen2.5-coder:0.5b</code></p>
<p>没有魔法的情况下可能会失败，一般情况下多试几次，最差可能需要几十次才能开始下载</p>
<h2 id="找到模型文件及modelfile内容">找到模型文件及Modelfile内容</h2>
<p><img src="https://img2024.cnblogs.com/blog/1236187/202501/1236187-20250108114524272-1336577142.png" alt="image" loading="lazy"></p>
<p>搜索pull的时候的哈希字符，可以找到模型位置，一般在<code>C:\Users\Administrator\.ollama\models\blobs</code></p>
<p>按照时间排序，找到最大的那个文件，就是gguf格式的模型，复制出来，改名为<code>qwen2.5-coder0.5b.gguf</code></p>
<p>在命令行执行形如<code>ollama show qwen2.5-coder:0.5b --modelfile</code>的指令，可以得到模型的<code>Modelfile</code>文件内容，保存为<code>Modelfile</code>文件</p>
<p>现在有以下两个文件</p>
<p><img src="https://img2024.cnblogs.com/blog/1236187/202501/1236187-20250108114544521-2054625164.png" alt="image" loading="lazy"></p>
<p>其中，文件内容是默认提示词模板，可参考<a href="https://ollama.readthedocs.io/modelfile/" target="_blank" rel="noopener nofollow">模型文件参考 - Ollama 中文文档</a>进行修改，例如可以实现让<code>llama3.3</code>优先使用中文，这个可以通过在其中加入<code>请优先使用简体中文回复</code>，这样的字符实现，最好使用翻译软件翻译成英文再放进去（比如插入到第13行）</p>
<ul>
<li><strong>修改第五行的FROM，将模型路径修改为模型的真实路径，例如这里是<code>./qwen2.5-coder0.5b.gguf</code></strong></li>
</ul>
<h2 id="内网部署ollama">内网部署ollama</h2>
<ul>
<li>在没有网络的内网电脑中安装第一步下载的ollama安装包</li>
<li>复制上面准备的两个文件到内网</li>
</ul>
<p>在两个文件所在目录的地址栏输入<code>cmd</code>，按下回车</p>
<p><img src="https://img2024.cnblogs.com/blog/1236187/202501/1236187-20250108114556001-2033817167.png" alt="image" loading="lazy"></p>
<p>命令行中输入<code>ollama create qwen2.5-coder0.5b -f Modelfile</code>，其中create后面是你自定义的模型名字（推荐和外网保持一样）</p>
<p>这样就导入进来了，接下来的使用和外网一模一样，输入<code>ollama list</code>命令可以看到导入的模型</p>
<p><strong>默认情况下<code>ollama</code>会开机启动，如果没有启动，手动执行就行，右下角的托盘图表中应该有它</strong></p>
<h1 id="配置continue">配置continue</h1>
<h2 id="本地使用">本地使用</h2>
<p><a href="https://github.com/continuedev/continue/releases" target="_blank" rel="noopener nofollow">Releases · continuedev/continue</a>这里下载到最新的continue插件，复制到内网，在vscode中安装，可参考<a href="https://blog.csdn.net/fightsyj/article/details/111315376" target="_blank" rel="noopener nofollow">VS Code 安装 VSIX 插件_.vsix-CSDN博客</a></p>
<p><img src="https://img2024.cnblogs.com/blog/1236187/202501/1236187-20250108114612867-1841915118.gif" alt="image" loading="lazy"></p>
<p>现在，就可以使用模型了</p>
<h2 id="局域网共享">局域网共享</h2>
<p>如果项目组中只有一台电脑能运行模型，别的性能不够，需要局域网访问ollama，那么可以按照如下方式调整</p>
<h3 id="ollama">ollama</h3>
<p>默认它的服务监听<code>127.0.0.1:11434</code>端口，这会导致局域网其他机器访问不到，可以参考<a href="https://github.com/ollama/ollama/issues/703" target="_blank" rel="noopener nofollow">Allow listening on all local interfaces · Issue #703 · ollama/ollama</a>实现监听所有端口</p>
<p>简单来说，就是设定环境变量<code>OLLAMA_HOST=0.0.0.0</code>，windows上也是一样的，如下</p>
<p><img src="https://img2024.cnblogs.com/blog/1236187/202501/1236187-20250108114626408-2121814357.gif" alt="image" loading="lazy"></p>
<p>然后重启ollama即可，通过<code>netstat -ano | findstr 11434</code>查看是否监听了0.0.0.0</p>
<h3 id="continue">continue</h3>
<p>可参考：<a href="https://github.com/continuedev/continue/issues/1175#issuecomment-2081651169" target="_blank" rel="noopener nofollow">https://github.com/continuedev/continue/issues/1175#issuecomment-2081651169</a></p>
<p>简单来说，在远程主机上，把设置中的以下内容改为指定内容即可</p>
<p><img src="https://img2024.cnblogs.com/blog/1236187/202501/1236187-20250108114635814-24868120.png" alt="image" loading="lazy"></p>
<pre><code class="language-json">    {
      "model": "AUTODETECT",
      "title": "Ollama (Remote)",
      "completionOptions": {},
      "apiBase": "http://192.168.1.100:11434",
      "provider": "ollama"
    }
</code></pre>
<p>其中apiBase就是部署了ollama的机器</p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.6793585409085648" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-01-08 11:57">2025-01-08 11:56</span>&nbsp;
<a href="https://www.cnblogs.com/cjdty">Startu</a>&nbsp;
阅读(<span id="post_view_count">244</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18659438" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18659438);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18659438', targetLink: 'https://www.cnblogs.com/cjdty/p/18659438', title: '[软件工具使用记录] windows离线ollama部署本地模型并配置continue实现离线代码补全' })">举报</a>
</div>
        