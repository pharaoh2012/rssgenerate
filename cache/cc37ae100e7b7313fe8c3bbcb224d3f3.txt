
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/powertoolsteam/p/18976279" title="发布于 2025-07-10 09:54">
    <span role="heading" aria-level="2">通过 .NET Aspire 使用本地 AI 模型</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h2 id="引言">引言</h2>
<p>在当今快速发展的 AI 领域，开发人员经常需要在本地环境中实验和测试 AI 模型，然后再将其部署到云端。使用本地 AI 模型不仅能够节省云资源成本，还能提供更快的迭代速度和更好的隐私保护。本文将介绍如何利用 .NET Aspire 框架结合 Ollama 在本地运行 AI 模型，并通过 Microsoft.Extensions.AI 抽象层实现从本地开发到云部署的无缝过渡。</p>
<h2 id="正文内容">正文内容</h2>
<h3 id="在-net-aspire-中设置-ollama">在 .NET Aspire 中设置 Ollama</h3>
<p>Ollama 是一个出色的工具，它允许开发者在本地运行大型语言模型。要在 .NET Aspire 应用程序中使用 Ollama，最简便的方法是使用 .NET Aspire 社区工具包中的 Ollama 托管集成。</p>
<p>首先，我们需要通过 NuGet 安装 Ollama 托管集成包。可以通过以下命令行将其添加到应用程序主机项目中：</p>
<pre><code class="language-bash">dotnet add package CommunityToolkit.Aspire.Hosting.Ollama
</code></pre>
<p>安装完成后，我们可以在 Program.cs 文件中配置 Ollama 托管集成。以下是一个典型的配置示例：</p>
<pre><code class="language-csharp">var ollama = builder.AddOllama("ollama")
                   .WithDataVolume()
                   .WithOpenWebUI();
</code></pre>
<p>这段代码使用了 <code>AddOllama</code> 扩展方法将容器添加到应用程序主机。<code>WithDataVolume()</code> 方法确保了模型数据在容器重启后仍然保留，避免了每次启动时重新下载大量数据的麻烦。而 <code>WithOpenWebUI()</code> 则添加了一个网页界面，让我们可以在应用程序之外与模型进行交互。</p>
<h3 id="运行本地-ai-模型">运行本地 AI 模型</h3>
<p>配置好 Ollama 服务器后，我们需要为其添加具体的 AI 模型。Ollama 提供了 <code>AddModel</code> 方法来实现这一功能。例如，要添加 Llama 3.2 模型，可以使用以下代码：</p>
<pre><code class="language-csharp">var chat = ollama.AddModel("chat", "llama3.2");
</code></pre>
<p>如果需要指定模型的特定版本或标签，可以在方法中添加相应参数。例如，<code>ollama.AddModel("chat", "llama3.2:1b")</code> 将选择 Llama 3.2 模型的 1b 版本。如果所需模型不在 Ollama 库中，还可以使用 <code>AddHuggingFaceModel</code> 方法从 Hugging Face 模型中心添加模型。</p>
<p>添加模型后，我们需要将其作为资源关联到应用程序中的其他服务：</p>
<pre><code class="language-csharp">builder.AddProject&lt;Projects.MyApi&gt;("api")
       .WithReference(chat);
</code></pre>
<p>运行应用程序主机项目时，Ollama 服务器会自动启动并下载指定的模型。需要注意的是，模型下载可能需要较长时间，在此期间不应停止应用程序主机。如果希望确保依赖模型的资源等待下载完成后再启动，可以使用 <code>WaitFor</code> 方法：</p>
<pre><code class="language-csharp">builder.AddProject&lt;Projects.MyApi&gt;("api")
       .WithReference(chat)
       .WaitFor(chat);
</code></pre>
<p>在控制面板中，我们可以看到模型下载的状态。Ollama 服务器会显示为运行中但非正常状态，直到模型下载完成。同时，依赖该模型的 API 资源也会保持等待状态。</p>
<p><img src="https://devblogs.microsoft.com/dotnet-ch/wp-content/uploads/sites/75/2025/01/downloading-models-300x118.png" alt="Image downloading models" loading="lazy"></p>
<h3 id="在应用程序中使用模型">在应用程序中使用模型</h3>
<p>配置好 API 项目与 chat 模型的关联后，我们可以使用 OllamaSharp 库与 Ollama 服务器交互。首先需要安装 .NET Aspire 社区工具包中的 OllamaSharp 集成：</p>
<pre><code class="language-bash">dotnet add package CommunityToolkit.Aspire.OllamaSharp
</code></pre>
<p>这个集成允许我们将 OllamaSharp 客户端注册为 Microsoft.Extensions.AI 包中的 <code>IChatClient</code> 或 <code>IEmbeddingsGenerator</code> 服务。这种抽象设计使得我们可以轻松地从本地 Ollama 服务器切换到云服务（如 Azure OpenAI）而无需修改客户端代码：</p>
<pre><code class="language-csharp">builder.AddOllamaSharpChatClient("chat");
</code></pre>
<p>对于需要使用嵌入模型的场景，可以使用 <code>AddOllamaSharpEmbeddingsGenerator</code> 方法注册 <code>IEmbeddingsGenerator</code> 服务。</p>
<p>为了充分利用 Microsoft.Extensions.AI 的功能管道，我们可以将服务提供给 <code>ChatClientBuilder</code>：</p>
<pre><code class="language-csharp">builder.AddKeyedOllamaSharpChatClient("chat");
builder.Services.AddChatClient(sp =&gt; sp.GetRequiredKeyedService("chat"))
                .UseFunctionInvocation()
                .UseOpenTelemetry(configure: t =&gt; t.EnableSensitiveData = true)
                .UseLogging();
</code></pre>
<p>最后，我们可以将 <code>IChatClient</code> 注入到路由处理程序中，实现与模型的交互：</p>
<pre><code class="language-csharp">app.MapPost("/chat", async (IChatClient chatClient, string question) =&gt;
{
    var response = await chatClient.CompleteAsync(question);
    return response.Message;
});
</code></pre>
<h3 id="云托管模型支持">云托管模型支持</h3>
<p>虽然 Ollama 非常适合本地开发，但在生产环境中，我们可能需要使用云端的 AI 服务，如 Azure OpenAI。为此，我们可以修改 API 项目的配置，使其在云端运行时自动切换服务实现：</p>
<pre><code class="language-csharp">if (builder.Environment.IsDevelopment())
{
    builder.AddKeyedOllamaSharpChatClient("chat");
}
else
{
    builder.AddKeyedAzureOpenAIClient("chat");
}

builder.Services.AddChatClient(sp =&gt; sp.GetRequiredKeyedService("chat"))
                .UseFunctionInvocation()
                .UseOpenTelemetry(configure: t =&gt; t.EnableSensitiveData = true)
                .UseLogging();
</code></pre>
<p>这种设计模式充分体现了 Microsoft.Extensions.AI 抽象层的价值，它允许我们在不改变业务逻辑代码的情况下，灵活切换底层 AI 服务的实现方式。</p>
<h2 id="结论">结论</h2>
<p>本文详细介绍了如何利用 .NET Aspire 框架在本地环境中设置和使用 AI 模型。通过简单的几行代码，我们就能配置 Ollama 服务器，指定所需的 AI 模型，并将其集成到应用程序中。更重要的是，借助 Microsoft.Extensions.AI 的抽象能力，我们可以轻松实现从本地开发到云部署的平滑过渡。</p>
<p>这种开发模式为 AI 应用开发提供了极大的灵活性和便利性。开发者可以在本地环境中快速迭代和测试模型，待功能成熟后再迁移到云端，既保证了开发效率，又兼顾了生产环境的性能和稳定性。对于 .NET 开发者而言，.NET Aspire 结合 Ollama 的方案提供了一条高效、便捷的 AI 应用开发路径。</p>
<p><a href="https://www.cnblogs.com/powertoolsteam/p/18970200" target="_blank">.NET AI 模板</a></p>

</div>
<div id="MySignature" role="contentinfo">
    <hr>
<br>
<p style="font-size: 16px; font-family: 微软雅黑, 黑体, Arial; color: #000">本文是由葡萄城技术开发团队发布，转载请注明出处：<a href="https://www.grapecity.com.cn/" target="_blank">葡萄城官网</a></p>
<!--p style="font-size: 16px; font-family: 微软雅黑, 黑体, Arial; color: #000">了解企业级低代码开发平台，请前往<a href="https://www.grapecity.com.cn/solutions/huozige" target="_blank">活字格</a>
</p><p style="font-size: 16px; font-family: 微软雅黑, 黑体, Arial; color: #000">了解可嵌入您系统的在线 Excel，请前往<a href="https://www.grapecity.com.cn/developer/spreadjs" target="_blank">SpreadJS纯前端表格控件</a></p>
<p style="font-size: 16px; font-family: 微软雅黑, 黑体, Arial; color: #000">了解嵌入式的商业智能和报表软件，请前往<a href="https://www.grapecity.com.cn/solutions/wyn" target="_blank">Wyn Enterprise
</a></p-->

<br>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0" data-date-updated="2025-07-10 09:55">2025-07-10 09:54</span>&nbsp;
<a href="https://www.cnblogs.com/powertoolsteam">葡萄城技术团队</a>&nbsp;
阅读(<span id="post_view_count">106</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18976279);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18976279', targetLink: 'https://www.cnblogs.com/powertoolsteam/p/18976279', title: '通过 .NET Aspire 使用本地 AI 模型' })">举报</a>
</div>
        