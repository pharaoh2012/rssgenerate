
        <div class="postTitle">
            <h1><a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/chenyishi/p/18949397" title="å‘å¸ƒäº 2025-06-26 10:27">
    <span role="heading" aria-level="2">pytorchå…¥é—¨ - GoogLeNetç¥ç»ç½‘ç»œ</span>
    

</a>
</h1>
        </div>
        <div id="cnblogs_post_body" class="blogpost-body blogpost-body-html">
<p>GoogLeNet æ˜¯ Google åœ¨ 2014 å¹´ ILSVRCï¼ˆImageNet Large Scale Visual Recognition Challengeï¼‰æ¯”èµ›ä¸­æå‡ºçš„ä¸€ç§æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œå…¶å…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº† Inception æ¨¡å—ï¼Œå¤§å¤§æé«˜äº†ç½‘ç»œçš„å‚æ•°åˆ©ç”¨ç‡ä¸è®¡ç®—æ•ˆç‡ã€‚</p>
<p>æœ¬æ–‡å°†é€šè¿‡å®Œæ•´çš„ PyTorch å®ç°ï¼Œä»èƒŒæ™¯ã€ç»“æ„ã€å‚æ•°è®¡ç®—ã€æºç è®²è§£å››ä¸ªæ–¹é¢ç³»ç»Ÿé˜è¿° GoogLeNetï¼Œå¹¶é™„å¸¦å¯è¿è¡Œä»£ç ä¸æ³¨é‡Šã€‚</p>
<h4>ğŸ“Œ GoogLeNet èƒŒæ™¯ç®€ä»‹</h4>
<p>GoogLeNet å‡ºè‡ªè®ºæ–‡ã€ŠGoing Deeper with Convolutionsã€‹ï¼Œæ˜¯ ILSVRC-2014 å† å†›æ¨¡å‹ï¼Œå‡†ç¡®ç‡è¿œè¶…åŒæœŸç½‘ç»œå¦‚ AlexNet ä¸ VGGã€‚</p>
<p>è¯¥ç½‘ç»œæœ€æ ¸å¿ƒçš„åˆ›æ–°æ˜¯ <strong>Inception æ¨¡å—</strong>ï¼Œå®ƒå°†ä¸åŒå¤§å°çš„å·ç§¯æ ¸ï¼ˆå¦‚ 1x1ã€3x3ã€5x5ï¼‰åŠæ± åŒ–æ“ä½œå¹¶è¡Œç»„åˆï¼Œä½¿å¾—ç½‘ç»œèƒ½æå–å¤šå°ºåº¦ç‰¹å¾ï¼ŒåŒæ—¶å¤§å¹…å‡å°‘å‚æ•°é‡ï¼ˆå°¤å…¶æ˜¯ 5x5 å·ç§¯å‰çš„é™ç»´ 1x1 å·ç§¯ï¼‰ã€‚</p>
<p><img src="https://img2024.cnblogs.com/blog/1033233/202506/1033233-20250626102147667-760100642.png" alt="" loading="lazy"></p>
<h4>&nbsp;ğŸ“Œ GoogLeNet ç½‘ç»œç»“æ„è§£æ</h4>
<p>GoogLeNet å¹¶ä¸æ˜¯å•çº¯çš„å±‚å å·ç§¯+æ± åŒ–ï¼Œå®ƒå¼•å…¥çš„ <strong>Inception æ¨¡å—</strong> ç»“æ„å¦‚ä¸‹ï¼š</p>
<ul>
<li>
<p><strong>1x1 å·ç§¯åˆ†æ”¯</strong>ï¼šç›´æ¥è¿›è¡Œ 1x1 å·ç§¯ï¼›</p>
</li>
<li>
<p><strong>3x3 å·ç§¯åˆ†æ”¯</strong>ï¼š1x1 å·ç§¯é™ç»´ â†’ 3x3 å·ç§¯ï¼›</p>
</li>
<li>
<p><strong>5x5 å·ç§¯åˆ†æ”¯</strong>ï¼š1x1 å·ç§¯é™ç»´ â†’ 5x5 å·ç§¯ï¼›</p>
</li>
<li>
<p><strong>æ± åŒ–åˆ†æ”¯</strong>ï¼š3x3 æœ€å¤§æ± åŒ– â†’ 1x1 å·ç§¯ã€‚</p>
</li>
</ul>
<p>æ¯ä¸ª Inception çš„è¾“å‡ºä¼šåœ¨é€šé“ç»´åº¦ä¸Šæ‹¼æ¥ï¼Œå½¢æˆæ›´ä¸°å¯Œçš„ç‰¹å¾è¡¨è¾¾ã€‚</p>
<p>æ•´ç½‘åŒ…æ‹¬å¦‚ä¸‹æ¨¡å—ç»„åˆï¼š</p>
<ol>
<li>
<p>å·ç§¯+æ± åŒ–ï¼ˆb1ã€b2ï¼‰</p>
</li>
<li>
<p>å¤šç»„ Inception + æ± åŒ–ï¼ˆb3ã€b4ã€b5ï¼‰</p>
</li>
<li>
<p>è‡ªé€‚åº”å¹³å‡æ± åŒ– â†’ å…¨è¿æ¥å±‚è¾“å‡ºåˆ†ç±»ç»“æœï¼ˆb6ï¼‰</p>
</li>
</ol>
<h4>ğŸ“Œ GoogLeNet æ¯å±‚å‚æ•°è®¡ç®—åˆ†æï¼ˆä»¥ b1 ä¸ºä¾‹ï¼‰</h4>
<p>ä»¥ç¬¬ä¸€å±‚ä¸ºä¾‹ï¼š</p>
<pre><code class="language-python">nn.Conv2d(1, 64, 7, 2, 3)
</code></pre>
<ul>
<li>
<p>è¾“å…¥é€šé“æ•°ï¼š1ï¼ˆç°åº¦å›¾ï¼‰</p>
</li>
<li>
<p>è¾“å‡ºé€šé“æ•°ï¼š64</p>
</li>
<li>
<p>å·ç§¯æ ¸å¤§å°ï¼š7x7</p>
</li>
<li>
<p>æ­¥å¹…ï¼š2</p>
</li>
<li>
<p>å¡«å……ï¼š3</p>
</li>
</ul>
<p>è¾“å‡ºå°ºå¯¸ä¸ºï¼š</p>
<pre><code>H_out = floor((224 + 2Ã—3 - 7)/2) + 1 = 112
W_out = 112
è¾“å‡ºå¼ é‡å°ºå¯¸ = [64, 112, 112]
</code></pre>
<p>æ± åŒ–å±‚ï¼š</p>
<pre><code class="language-python">nn.MaxPool2d(3, 2, 1)
</code></pre>
<p>è¾“å‡ºå°ºå¯¸å˜ä¸ºï¼š</p>
<pre><code>H_out = floor((112 + 2Ã—1 - 3)/2) + 1 = 56
W_out = 56
è¾“å‡ºå¼ é‡å°ºå¯¸ = [64, 56, 56]
</code></pre>
<p>ç±»ä¼¼æ–¹æ³•å¯è®¡ç®—å„å±‚å½¢çŠ¶ï¼Œå°¤å…¶è¦æ³¨æ„æ¯ä¸ª Inception çš„è¾“å‡ºé€šé“æ•°æ˜¯å››ä¸ªåˆ†æ”¯é€šé“æ•°çš„ <strong>åŠ å’Œ</strong>ã€‚</p>
<h4>ğŸ“Œ GoogLeNet æ¨¡å‹å®Œæ•´æºç è§£é‡Šï¼ˆå«æ³¨é‡Šï¼‰</h4>
<h5>model.py - GoogLeNet æ¨¡å‹å®šä¹‰</h5>
<pre><code class="language-python">import torch  # å¯¼å…¥PyTorchä¸»åº“
from torch import nn  # å¯¼å…¥ç¥ç»ç½‘ç»œæ¨¡å—
from torchsummary import summary  # å¯¼å…¥æ¨¡å‹ç»“æ„æ‘˜è¦å·¥å…·

# å®šä¹‰Inceptionæ¨¡å—
class Inception(nn.Module):
    def __init__(
        self, in_channels, out1x1, out3x3red, out3x3, out5x5red, out5x5, pool_proj
    ):
        super(Inception, self).__init__()  # è°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°
        self.ReLu = nn.ReLU()  # å®šä¹‰ReLUæ¿€æ´»å‡½æ•°

        self.branch1x1 = nn.Conv2d(in_channels, out1x1, kernel_size=1)  # 1x1å·ç§¯åˆ†æ”¯

        self.branch3x3 = nn.Sequential(
            nn.Conv2d(in_channels, out3x3red, kernel_size=1),  # 1x1å·ç§¯é™ç»´
            nn.ReLU(),  # æ¿€æ´»
            nn.Conv2d(out3x3red, out3x3, kernel_size=3, padding=1),  # 3x3å·ç§¯
        )

        self.branch5x5 = nn.Sequential(
            nn.Conv2d(in_channels, out5x5red, kernel_size=1),  # 1x1å·ç§¯é™ç»´
            nn.ReLU(),  # æ¿€æ´»
            nn.Conv2d(out5x5red, out5x5, kernel_size=5, padding=2),  # 5x5å·ç§¯
        )

        self.branch_pool = nn.Sequential(
            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),  # 3x3æœ€å¤§æ± åŒ–
            nn.Conv2d(in_channels, pool_proj, kernel_size=1),  # 1x1å·ç§¯
        )

    def forward(self, x):
        p1 = self.ReLu(self.branch1x1(x))  # 1x1å·ç§¯åˆ†æ”¯å‰å‘ä¼ æ’­
        p2 = self.ReLu(self.branch3x3(x))  # 3x3å·ç§¯åˆ†æ”¯å‰å‘ä¼ æ’­
        p3 = self.ReLu(self.branch5x5(x))  # 5x5å·ç§¯åˆ†æ”¯å‰å‘ä¼ æ’­
        p4 = self.ReLu(self.branch_pool(x))  # æ± åŒ–åˆ†æ”¯å‰å‘ä¼ æ’­
        outputs = [p1, p2, p3, p4]  # åˆå¹¶æ‰€æœ‰åˆ†æ”¯è¾“å‡º
        return torch.cat(outputs, 1)  # åœ¨é€šé“ç»´åº¦æ‹¼æ¥

# å®šä¹‰GoogLeNetä¸»ç»“æ„
class GoogLeNet(nn.Module):
    def __init__(self, num_classes=10):
        super(GoogLeNet, self).__init__()  # è°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°

        self.b1 = nn.Sequential(
            nn.Conv2d(1, 64, 7, 2, 3),  # 7x7å·ç§¯ï¼Œæ­¥é•¿2ï¼Œå¡«å……3
            nn.ReLU(),  # æ¿€æ´»
            nn.MaxPool2d(3, 2, 1)  # 3x3æœ€å¤§æ± åŒ–ï¼Œæ­¥é•¿2ï¼Œå¡«å……1
        )

        self.b2 = nn.Sequential(
            nn.Conv2d(64, 64, 1),  # 1x1å·ç§¯
            nn.ReLU(),  # æ¿€æ´»
            nn.Conv2d(64, 192, 3, 1, 1),  # 3x3å·ç§¯
            nn.ReLU(),  # æ¿€æ´»
            nn.MaxPool2d(3, 2, 1),  # 3x3æœ€å¤§æ± åŒ–
        )

        self.b3 = nn.Sequential(
            Inception(192, 64, 96, 128, 16, 32, 32),  # ç¬¬ä¸€ä¸ªInceptionæ¨¡å—
            Inception(256, 128, 128, 192, 32, 96, 64),  # ç¬¬äºŒä¸ªInceptionæ¨¡å—
            nn.MaxPool2d(3, 2, 1),  # æ± åŒ–
        )

        self.b4 = nn.Sequential(
            Inception(480, 192, 96, 208, 16, 48, 64),  # å¤šä¸ªInceptionæ¨¡å—
            Inception(512, 160, 112, 224, 24, 64, 64),
            Inception(512, 128, 128, 256, 24, 64, 64),
            Inception(512, 112, 128, 288, 32, 64, 64),
            Inception(528, 256, 160, 320, 32, 128, 128),
            nn.MaxPool2d(3, 2, 1),  # æ± åŒ–
        )

        self.b5 = nn.Sequential(
            Inception(832, 256, 160, 320, 32, 128, 128),  # å€’æ•°ç¬¬äºŒä¸ªInception
            Inception(832, 384, 192, 384, 48, 128, 128),  # æœ€åä¸€ä¸ªInception
        )

        self.b6 = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),  # è‡ªé€‚åº”å¹³å‡æ± åŒ–åˆ°1x1
            nn.Flatten(),  # å±•å¹³
            nn.Linear(1024, num_classes),  # å…¨è¿æ¥è¾“å‡º
        )

        # æƒé‡åˆå§‹åŒ–
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode="fan_out", nonlinearity="relu")  # Heåˆå§‹åŒ–
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)  # åç½®åˆå§‹åŒ–ä¸º0
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)  # æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)  # åç½®åˆå§‹åŒ–ä¸º0

    def forward(self, x):
        x = self.b1(x)  # ç¬¬ä¸€é˜¶æ®µ
        x = self.b2(x)  # ç¬¬äºŒé˜¶æ®µ
        x = self.b3(x)  # ç¬¬ä¸‰é˜¶æ®µ
        x = self.b4(x)  # ç¬¬å››é˜¶æ®µ
        x = self.b5(x)  # ç¬¬äº”é˜¶æ®µ
        x = self.b6(x)  # åˆ†ç±»é˜¶æ®µ
        return x  # è¿”å›è¾“å‡º

# æµ‹è¯•æ¨¡å‹ç»“æ„
if __name__ == "__main__":
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # é€‰æ‹©è®¾å¤‡
    model = GoogLeNet().to(device=device)  # å®ä¾‹åŒ–æ¨¡å‹å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
    print(summary(model, (1, 224, 224)))  # æ‰“å°æ¨¡å‹ç»“æ„æ‘˜è¦</code></pre>
<h4>ğŸ“Œ æ¨¡å‹è®­ç»ƒä»£ç è¯¦è§£ï¼ˆtrain.pyï¼‰</h4>
<pre><code class="language-python">import os  # æ“ä½œç³»ç»Ÿç›¸å…³
import sys  # Pythonè¿è¡Œç¯å¢ƒç›¸å…³

sys.path.append(os.getcwd())  # æ·»åŠ å½“å‰ç›®å½•åˆ°æ¨¡å—æœç´¢è·¯å¾„

import time  # è®¡æ—¶
from torchvision.datasets import FashionMNIST  # FashionMNISTæ•°æ®é›†
from torchvision import transforms  # æ•°æ®é¢„å¤„ç†
from torch.utils.data import (
    DataLoader,  # æ‰¹é‡æ•°æ®åŠ è½½
    random_split,  # æ•°æ®é›†åˆ’åˆ†
)
import numpy as np  # æ•°å€¼è®¡ç®—
import matplotlib.pyplot as plt  # ç»˜å›¾
import torch  # PyTorchä¸»åº“
from torch import nn, optim  # ç¥ç»ç½‘ç»œå’Œä¼˜åŒ–å™¨
import copy  # æ·±æ‹·è´
import pandas as pd  # æ•°æ®å¤„ç†

from GoogLeNet_model.model import GoogLeNet  # å¯¼å…¥GoogLeNetæ¨¡å‹

batch_size = 32  # æ‰¹é‡å¤§å°
</code></pre>
<h5>åŠ è½½è®­ç»ƒé›†ä¸éªŒè¯é›†</h5>
<pre><code class="language-python">def train_val_date_load():  # åŠ è½½è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_dataset = FashionMNIST(
        root="./data",  # æ•°æ®è·¯å¾„
        train=True,  # åŠ è½½è®­ç»ƒé›†
        download=True,  # è‡ªåŠ¨ä¸‹è½½
        transform=transforms.Compose(
            [
                transforms.Resize(size=224),  # ç¼©æ”¾åˆ°224x224
                transforms.ToTensor(),  # è½¬ä¸ºTensor
            ]
        ),
    )

    train_date, val_data = random_split(
        train_dataset,
        [
            int(len(train_dataset) * 0.8),  # 80%è®­ç»ƒé›†
            len(train_dataset) - int(len(train_dataset) * 0.8),  # 20%éªŒè¯é›†
        ],
    )

    train_loader = DataLoader(
        dataset=train_date,
        batch_size=batch_size,
        shuffle=True,
        num_workers=1,  # è®­ç»ƒé›†åŠ è½½å™¨
    )

    val_loader = DataLoader(
        dataset=val_data,
        batch_size=batch_size,
        shuffle=True,
        num_workers=1,  # éªŒè¯é›†åŠ è½½å™¨
    )

    return train_loader, val_loader  # è¿”å›åŠ è½½å™¨
</code></pre>
<h5>è®­ç»ƒä¸»æµç¨‹</h5>
<pre><code class="language-python">def train_model_process(model, train_loader, val_loader, epochs=10):  # è®­ç»ƒè¿‡ç¨‹
    device = "cuda" if torch.cuda.is_available() else "cpu"  # è®¾å¤‡é€‰æ‹©
    optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adamä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()  # äº¤å‰ç†µæŸå¤±
    model.to(device)  # æ¨¡å‹è½¬åˆ°è®¾å¤‡

    best_model_wts = copy.deepcopy(model.state_dict())  # æœ€ä½³æ¨¡å‹å‚æ•°
    best_acc = 0.0  # æœ€ä½³å‡†ç¡®ç‡
    train_loss_all = []  # è®­ç»ƒæŸå¤±
    val_loss_all = []  # éªŒè¯æŸå¤±
    train_acc_all = []  # è®­ç»ƒå‡†ç¡®ç‡
    val_acc_all = []  # éªŒè¯å‡†ç¡®ç‡

    since = time.time()  # è®­ç»ƒå¼€å§‹æ—¶é—´

    for epoch in range(epochs):  # è½®æ¬¡å¾ªç¯
        print(f"Epoch {epoch + 1}/{epochs}")  # å½“å‰è½®æ¬¡

        train_loss = 0.0  # æœ¬è½®è®­ç»ƒæŸå¤±
        train_correct = 0  # æœ¬è½®è®­ç»ƒæ­£ç¡®æ•°

        val_loss = 0.0  # æœ¬è½®éªŒè¯æŸå¤±
        val_correct = 0  # æœ¬è½®éªŒè¯æ­£ç¡®æ•°

        train_num = 0  # æœ¬è½®è®­ç»ƒæ ·æœ¬æ•°
        val_num = 0  # æœ¬è½®éªŒè¯æ ·æœ¬æ•°

        for step, (images, labels) in enumerate(train_loader):  # è®­ç»ƒé›†å¾ªç¯
            images = images.to(device)  # å›¾ç‰‡è½¬è®¾å¤‡
            labels = labels.to(device)  # æ ‡ç­¾è½¬è®¾å¤‡

            model.train()  # è®­ç»ƒæ¨¡å¼

            outputs = model(images)  # å‰å‘ä¼ æ’­
            pre_lab = torch.argmax(outputs, dim=1)  # é¢„æµ‹æ ‡ç­¾
            loss = criterion(outputs, labels)  # æŸå¤±è®¡ç®—

            optimizer.zero_grad()  # æ¢¯åº¦æ¸…é›¶
            loss.backward()  # åå‘ä¼ æ’­
            optimizer.step()  # å‚æ•°æ›´æ–°

            train_loss += loss.item() * images.size(0)  # ç´¯è®¡æŸå¤±
            train_correct += torch.sum(pre_lab == labels.data)  # ç´¯è®¡æ­£ç¡®æ•°
            train_num += labels.size(0)  # ç´¯è®¡æ ·æœ¬æ•°

            print(
                "Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Acc:{:.4f}".format(
                    epoch + 1,
                    epochs,
                    step + 1,
                    len(train_loader),
                    loss.item(),
                    torch.sum((pre_lab == labels.data) / batch_size),
                )
            )

        for step, (images, labels) in enumerate(val_loader):  # éªŒè¯é›†å¾ªç¯
            images = images.to(device)
            labels = labels.to(device)
            model.eval()  # è¯„ä¼°æ¨¡å¼

            with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦
                outputs = model(images)
                pre_lab = torch.argmax(outputs, dim=1)
                loss = criterion(outputs, labels)

                val_loss += loss.item() * images.size(0)
                val_correct += torch.sum(pre_lab == labels.data)
                val_num += labels.size(0)

                print(
                    "Epoch [{}/{}], Step [{}/{}], Val Loss: {:.4f}, Acc:{:.4f}".format(
                        epoch + 1,
                        epochs,
                        step + 1,
                        len(val_loader),
                        loss.item(),
                        torch.sum((pre_lab == labels.data) / batch_size),
                    )
                )

        # è®°å½•æ¯è½®è®­ç»ƒå’ŒéªŒè¯çš„æŸå¤±ä¸å‡†ç¡®ç‡
        train_loss_all.append(train_loss / train_num)
        val_loss_all.append(val_loss / val_num)
        train_acc = train_correct.double() / train_num
        val_acc = val_correct.double() / val_num
        train_acc_all.append(train_acc.item())
        val_acc_all.append(val_acc.item())

        print(
            f"Train Loss: {train_loss / train_num:.4f}, Train Acc: {train_acc:.4f}, "
            f"Val Loss: {val_loss / val_num:.4f}, Val Acc: {val_acc:.4f}"
        )

        if val_acc_all[-1] &gt; best_acc:  # æ›´æ–°æœ€ä½³æ¨¡å‹
            best_acc = val_acc_all[-1]
            best_model_wts = copy.deepcopy(model.state_dict())

    time_elapsed = time.time() - since  # æ€»è€—æ—¶
    print(
        f"Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\n"
        f"Best val Acc: {best_acc:.4f}"
    )

    torch.save(model.state_dict(), "./models/google_net_best_model.pth")  # ä¿å­˜æ¨¡å‹
    train_process = pd.DataFrame(
        data={
            "epoch": range(1, epochs + 1),  # è½®æ¬¡
            "train_loss_all": train_loss_all,  # è®­ç»ƒæŸå¤±
            "val_loss_all": val_loss_all,  # éªŒè¯æŸå¤±
            "train_acc_all": train_acc_all,  # è®­ç»ƒå‡†ç¡®ç‡
            "val_acc_all": val_acc_all,  # éªŒè¯å‡†ç¡®ç‡
        }
    )

    return train_process  # è¿”å›è®­ç»ƒè¿‡ç¨‹
</code></pre>
<h5>è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–</h5>
<pre><code class="language-python">def matplot_acc_loss(train_process):  # ç»˜åˆ¶æ›²çº¿
    plt.figure(figsize=(12, 5))  # ç”»å¸ƒå¤§å°

    plt.subplot(1, 2, 1)  # ç¬¬1ä¸ªå­å›¾
    plt.plot(
        train_process["epoch"], train_process["train_loss_all"], label="Train Loss"
    )
    plt.plot(
        train_process["epoch"], train_process["val_loss_all"], label="Val Loss"
    )
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title("Loss vs Epoch")
    plt.legend()

    plt.subplot(1, 2, 2)  # ç¬¬2ä¸ªå­å›¾
    plt.plot(
        train_process["epoch"], train_process["train_acc_all"], label="Train Acc"
    )
    plt.plot(
        train_process["epoch"], train_process["val_acc_all"], label="Val Acc"
    )
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.title("Accuracy vs Epoch")
    plt.legend()

    plt.tight_layout()  # è‡ªåŠ¨è°ƒæ•´
    plt.ion()  # äº¤äº’æ¨¡å¼
    plt.show()  # æ˜¾ç¤ºå›¾åƒ
    plt.savefig("./models/google_net_output.png")  # ä¿å­˜å›¾ç‰‡
</code></pre>
<h5>è®­ç»ƒä¸»å…¥å£</h5>
<pre><code class="language-python">if __name__ == "__main__":  # ä¸»ç¨‹åºå…¥å£
    traindatam, valdata = train_val_date_load()  # åŠ è½½æ•°æ®
    result = train_model_process(
        GoogLeNet(), traindatam, valdata, 10
    )  # è®­ç»ƒæ¨¡å‹
    matplot_acc_loss(result)  # ç»˜åˆ¶æ›²çº¿
</code></pre>
<h4>ğŸ“Œ æ¨¡å‹æµ‹è¯•ä»£ç è¯¦è§£ï¼ˆtest.pyï¼‰</h4>
<pre><code class="language-python">import os  # æ“ä½œç³»ç»Ÿç›¸å…³
import sys  # Pythonè¿è¡Œç¯å¢ƒç›¸å…³

sys.path.append(os.getcwd())  # æ·»åŠ å½“å‰ç›®å½•åˆ°æ¨¡å—æœç´¢è·¯å¾„
import torch  # PyTorchä¸»åº“
from torch.utils.data import (
    DataLoader,  # æ‰¹é‡æ•°æ®åŠ è½½
    random_split,  # æ•°æ®é›†åˆ’åˆ†ï¼ˆæœªç”¨åˆ°ï¼‰
)
from torchvision import datasets, transforms  # torchvisionæ•°æ®é›†å’Œå˜æ¢
from torchvision.datasets import FashionMNIST  # FashionMNISTæ•°æ®é›†
from GoogLeNet_model.model import GoogLeNet  # å¯¼å…¥GoogLeNetæ¨¡å‹
</code></pre>
<h5>åŠ è½½æµ‹è¯•é›†</h5>
<pre><code class="language-python">def test_data_load():  # åŠ è½½æµ‹è¯•æ•°æ®
    test_dataset = FashionMNIST(
        root="./data",  # æ•°æ®è·¯å¾„
        train=False,  # åŠ è½½æµ‹è¯•é›†
        download=True,  # è‡ªåŠ¨ä¸‹è½½
        transform=transforms.Compose(
            [
                transforms.Resize(size=224),  # ç¼©æ”¾åˆ°224x224
                transforms.ToTensor(),  # è½¬ä¸ºTensor
            ]
        ),
    )

    test_loader = DataLoader(
        dataset=test_dataset,
        batch_size=128,
        shuffle=True,
        num_workers=1,  # æµ‹è¯•é›†åŠ è½½å™¨
    )

    return test_loader  # è¿”å›åŠ è½½å™¨

print(test_data_load())  # æ‰“å°æµ‹è¯•é›†åŠ è½½å™¨ï¼ˆè°ƒè¯•ï¼‰
</code></pre>
<h5>æµ‹è¯•è¿‡ç¨‹</h5>
<pre><code class="language-python">def test_model_process(model, test_loader):  # æµ‹è¯•è¿‡ç¨‹
    device = "cuda" if torch.cuda.is_available() else "cpu"  # è®¾å¤‡é€‰æ‹©
    model.to(device)  # æ¨¡å‹è½¬åˆ°è®¾å¤‡
    model.eval()  # è¯„ä¼°æ¨¡å¼

    correct = 0  # æ­£ç¡®æ•°
    total = 0  # æ€»æ•°

    with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦
        for images, labels in test_loader:  # æµ‹è¯•é›†å¾ªç¯
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)  # å‰å‘ä¼ æ’­
            _, predicted = torch.max(outputs, 1)  # é¢„æµ‹æ ‡ç­¾
            total += labels.size(0)  # ç´¯è®¡æ€»æ•°
            correct += torch.sum(predicted == labels.data)  # ç´¯è®¡æ­£ç¡®æ•°

    accuracy = correct / total * 100  # å‡†ç¡®ç‡ï¼ˆç™¾åˆ†æ¯”ï¼‰
    print(f"Test Accuracy: {accuracy:.2f}%")  # æ‰“å°å‡†ç¡®ç‡
</code></pre>
<h5>ä¸»å…¥å£æµ‹è¯•</h5>
<pre><code class="language-python">if __name__ == "__main__":  # ä¸»ç¨‹åºå…¥å£
    test_loader = test_data_load()  # åŠ è½½æµ‹è¯•é›†
    model = GoogLeNet()  # å®ä¾‹åŒ–æ¨¡å‹
    model.load_state_dict(
        torch.load("./models/google_net_best_model.pth")
    )  # åŠ è½½æ¨¡å‹å‚æ•°
    test_model_process(model, test_loader)  # æµ‹è¯•æ¨¡å‹</code></pre>
<h4>âœ… æ€»ç»“</h4>
<p>æœ¬æ–‡å®Œæ•´åœ°å®ç°å¹¶è§£é‡Šäº† GoogLeNet æ¨¡å‹çš„æ¶æ„ã€å‚æ•°è®¡ç®—ä¸ PyTorch è®­ç»ƒ/æµ‹è¯•æµç¨‹ï¼š</p>
<ul>
<li>
<p>ç»“æ„åˆ›æ–°åœ¨äº <strong>Inception æ¨¡å—</strong> çš„å¤šè·¯å¾„è®¾è®¡ï¼›</p>
</li>
<li>
<p>æ¨¡å‹å‚æ•°é«˜æ•ˆï¼Œè®­ç»ƒç¨³å®šï¼›</p>
</li>
<li>
<p>é€‚é…ä»»ä½•å›¾åƒåˆ†ç±»ä»»åŠ¡ï¼Œä»£ç å¯å¤ç”¨æ€§å¼ºã€‚</p>
</li>
</ul>
</div>
<div class="clear"></div>

        <div class="postDesc">posted on 
<span id="post-date" data-last-update-days="0" data-date-updated="2025-06-26 10:28">2025-06-26 10:27</span>&nbsp;
<a href="https://www.cnblogs.com/chenyishi">chesterÂ·chen</a>&nbsp;
é˜…è¯»(<span id="post_view_count">0</span>)&nbsp;
è¯„è®º(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18949397);return false;">æ”¶è—</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18949397', targetLink: 'https://www.cnblogs.com/chenyishi/p/18949397', title: 'pytorchå…¥é—¨ - GoogLeNetç¥ç»ç½‘ç»œ' })">ä¸¾æŠ¥</a>
</div>
    