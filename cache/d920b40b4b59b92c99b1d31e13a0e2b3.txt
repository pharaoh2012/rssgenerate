
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/deali/p/18682643" title="å‘å¸ƒäº 2025-01-20 23:00">
    <span role="heading" aria-level="2">ç¼–å†™çˆ¬è™«ä¸‹è½½å…¬ä¼—å·ä¸Šå¥½çœ‹çš„å£çº¸</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h2 id="å‰è¨€">å‰è¨€</h2>
<p>å¾ˆå¤šå¹´å‰æˆ‘è¿˜åœ¨å¤§å­¦çš„æ—¶å€™ï¼Œæ›¾ç»å†™è¿‡ä¸€ç¯‡ç±»ä¼¼çš„æ–‡ç« ï¼Œä¸è¿‡å½“æ—¶æ˜¯é‡‡é›†æŸæ¸¸æˆå®˜ç½‘ä¸Šå¥½çœ‹çš„å£çº¸ã€‚</p>
<p>æœ€è¿‘å¾®ä¿¡å…¬ä¼—å·æ€»æ˜¯ç»™æˆ‘æ¨èå„ç§å£çº¸ï¼Œé‡Œé¢æœ‰ä¸å°‘å¥½çœ‹çš„ï¼Œä¸è¿‡ä¸€å¼ å¼ ä¿å­˜å¤ªéº»çƒ¦äº†ï¼Œç´¢æ€§å†™ä¸ªçˆ¬è™«è‡ªåŠ¨ä¸‹è½½ã€‚</p>
<h2 id="è¿™ä¸ªçˆ¬è™«çš„åŠŸèƒ½ç‚¹">è¿™ä¸ªçˆ¬è™«çš„åŠŸèƒ½ç‚¹</h2>
<p>ç®€å•åˆ—ä¸€ä¸‹è¿™æ¬¡é¡¹ç›®æ¶‰åŠåˆ°çš„åŠŸèƒ½ç‚¹ï¼Œä¸è¿‡å¹¶ä¸ä¼šæ¯ä¸ªéƒ½å†™åœ¨æœ¬æ–‡é‡Œï¼Œä¸»è¦è¿˜æ˜¯çˆ¬è™«éƒ¨åˆ†ã€‚</p>
<p>å…¶ä»–åŠŸèƒ½å¦‚æœæœ‰åŒå­¦æ„Ÿå…´è¶£ï¼Œåç»­æˆ‘å†åˆ†äº«ã€‚</p>
<ul>
<li>è·å–æŒ‡å®šå…¬ä¼—å·çš„æ‰€æœ‰æ–‡ç« </li>
<li>ä¸‹è½½æ–‡ç« é‡Œç¬¦åˆè§„åˆ™çš„å£çº¸</li>
<li>è¿‡æ»¤æ— å…³å›¾ç‰‡ï¼Œå¦‚å¼•å¯¼å…³æ³¨å°å›¾æ ‡</li>
<li>æ•°æ®æŒä¹…åŒ–ï¼ˆè¯•ç”¨å¼‚æ­¥ORMå’Œè½»é‡çº§NoSQLï¼‰</li>
<li>å›¾ç‰‡åˆ†æï¼ˆå°ºå¯¸ä¿¡æ¯ã€æ„ŸçŸ¥å“ˆå¸Œã€æ–‡ä»¶MD5ï¼‰</li>
<li>æ‰€æœ‰è¿è¡Œè¿‡ç¨‹éƒ½æœ‰è¿›åº¦æ¡å±•ç¤ºï¼Œéå¸¸å‹å¥½</li>
</ul>
<h2 id="çˆ¬è™«ç›¸å…³æ–‡ç« ">çˆ¬è™«ç›¸å…³æ–‡ç« </h2>
<p>è¿™å‡ å¹´æˆ‘å†™è¿‡ä¸å°‘è·Ÿçˆ¬è™«æœ‰å…³çš„æ–‡ç« ï¼Œ</p>
<ul>
<li><a href="https://www.cnblogs.com/deali/p/8080599.html" target="_blank">ç¼–å†™çˆ¬è™«è‡ªåŠ¨ä¸‹è½½æŸæ¸¸æˆå®˜ç½‘ä¸Šå¥½çœ‹çš„å£çº¸</a></li>
<li><a href="https://www.cnblogs.com/deali/p/14329032.html" target="_blank">Seleniumçˆ¬è™«å®æˆ˜ï¼šæˆªå–ç½‘é¡µä¸Šçš„å›¾ç‰‡</a></li>
<li><a href="https://www.cnblogs.com/deali/p/14367106.html" target="_blank">Seleniumçˆ¬è™«å®è·µï¼ˆè¸©å‘è®°å½•ï¼‰ä¹‹ajaxè¯·æ±‚æŠ“åŒ…ã€æµè§ˆå™¨é€€å‡º</a></li>
<li><a href="https://www.cnblogs.com/deali/p/15890678.html" target="_blank">çˆ¬è™«ç¬”è®°ï¼šæé«˜æ•°æ®é‡‡é›†æ•ˆç‡ï¼ä»£ç†æ± å’Œçº¿ç¨‹æ± çš„ä½¿ç”¨</a></li>
<li><a href="https://www.cnblogs.com/deali/p/17061678.html" target="_blank">C#çˆ¬è™«å¼€å‘å°ç»“</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/108028434" target="_blank" rel="noopener nofollow">æŠŠçˆ¬è™«æ”¾åˆ°æ‰‹æœºä¸Šè·‘ï¼Flutterçˆ¬è™«æ¡†æ¶åˆæ¢~</a></li>
</ul>
<h2 id="é¡¹ç›®ç»“æ„">é¡¹ç›®ç»“æ„</h2>
<p>ä¾ç„¶æ˜¯ä½¿ç”¨ pdm è¿™ä¸ªå·¥å…·æ¥ä½œä¸ºä¾èµ–ç®¡ç†ã€‚</p>
<p>æœ¬é¡¹ç›®ç”¨åˆ°çš„ä¾èµ–æœ‰è¿™äº›</p>
<pre><code class="language-toml">dependencies = [
    "requests&gt;=2.32.3",
    "bs4&gt;=0.0.2",
    "loguru&gt;=0.7.3",
    "tqdm&gt;=4.67.1",
    "tinydb&gt;=4.8.2",
    "pony&gt;=0.7.19",
    "tortoise-orm[aiosqlite]&gt;=0.23.0",
    "orjson&gt;=3.10.14",
    "aerich[toml]&gt;=0.8.1",
    "pillow&gt;=11.1.0",
    "imagehash&gt;=4.3.1",
]
</code></pre>
<p>è¿˜æœ‰ä¸€ä¸ªdevä¾èµ–ï¼Œç”¨æ¥è§‚æµ‹æ•°æ®åº“ï¼ˆè¯•ç”¨äº†è½»é‡çº§NoSQLï¼Œæ²¡æœ‰å¯è§†åŒ–çš„æ–¹æ³•ï¼‰</p>
<pre><code class="language-toml">[dependency-groups]
dev = [
    "jupyterlab&gt;=4.3.4",
]
</code></pre>
<h2 id="æ•°æ®æŒä¹…åŒ–">æ•°æ®æŒä¹…åŒ–</h2>
<p>æ¯æ¬¡è¿™ç§é¡¹ç›®æˆ‘éƒ½ä¼šè¯•ç”¨ä¸åŒçš„æ•°æ®æŒä¹…åŒ–æ–¹æ¡ˆ</p>
<p>å¯¹äºå…³ç³»å‹æ•°æ®åº“ï¼Œæˆ‘ä¸Šä¸€æ¬¡æ˜¯ç”¨äº†peeweeè¿™ä¸ªORM</p>
<p>åé¢å‘ç°ä¸»è¦é—®é¢˜æ˜¯ä¸æ”¯æŒè‡ªåŠ¨è¿ç§»ï¼ˆä¹Ÿè®¸ç°åœ¨å·²ç»æ”¯æŒäº†ï¼Œä½†æˆ‘ä½¿ç”¨æ—¶æ˜¯å‡ å¹´å‰äº†ï¼‰</p>
<p>å…¶ä»–è¿˜è¡Œï¼Œå‡‘åˆç”¨ã€‚</p>
<p>è¿™æ¬¡æˆ‘ä¸€å¼€å§‹å¹¶æ²¡æœ‰åšæŒä¹…åŒ–ï¼Œä½†å‡ æ¬¡å…³æœºå¯¼è‡´è¿›åº¦ä¸¢å¤±ï¼Œè¦å†™ä¸€å †è§„åˆ™å»åŒ¹é…ï¼Œå®åœ¨æ˜¯éº»çƒ¦ã€‚</p>
<p>åé¢ç›´æ¥å…¨éƒ¨é‡æ„äº†ã€‚</p>
<p>æˆ‘å…ˆåå°è¯•äº† tinydbï¼ˆå•æ–‡ä»¶æ–‡æ¡£å‹NoSQLï¼‰ã€ponyï¼ˆå…³ç³»å‹ORMï¼‰ã€tortoise-orm</p>
<p>æœ€ç»ˆé€‰æ‹©äº† tortoise-ormï¼ŒåŸå› æ˜¯è¯­æ³•å’ŒDjango ORMå¾ˆåƒï¼Œä¸æƒ³èµ°å‡ºèˆ’é€‚åœˆäº†ã€‚</p>
<h3 id="æ¨¡å‹å®šä¹‰">æ¨¡å‹å®šä¹‰</h3>
<pre><code class="language-python">from tortoise.models import Model
from tortoise import fields


class Article(Model):
    id = fields.IntField(primary_key=True)
    raw_id = fields.TextField()
    title = fields.TextField()
    url = fields.TextField()
    created_at = fields.DatetimeField()
    updated_at = fields.DatetimeField()
    html = fields.TextField()
    raw_json = fields.JSONField()

    def __str__(self):
        return self.title


class Image(Model):
    id = fields.IntField(primary_key=True)
    article = fields.ForeignKeyField('models.Article', related_name='images')
    url = fields.TextField()
    is_downloaded = fields.BooleanField(default=False)
    downloaded_at = fields.DatetimeField(null=True)
    local_file = fields.TextField(null=True)
    size = fields.IntField(null=True, description='unit: bytes')
    width = fields.IntField(null=True)
    height = fields.IntField(null=True)
    image_hash = fields.TextField(null=True)
    md5_hash = fields.TextField(null=True)

    def __str__(self):
        return self.url
</code></pre>
<p>è¿™ä¿©æ¨¡å‹èƒ½æ»¡è¶³æœ¬é¡¹ç›®çš„æ‰€æœ‰éœ€æ±‚äº†ï¼Œç”šè‡³è¿˜èƒ½è¿›ä¸€æ­¥å®ç°åç»­åŠŸèƒ½ï¼Œå¦‚ï¼šç›¸ä¼¼å›¾ç‰‡è¯†åˆ«ã€å›¾ç‰‡åˆ†ç±»ç­‰ã€‚</p>
<h2 id="è·å–æŒ‡å®šå…¬ä¼—å·çš„æ‰€æœ‰æ–‡ç« ">è·å–æŒ‡å®šå…¬ä¼—å·çš„æ‰€æœ‰æ–‡ç« </h2>
<p>è¿™ç§æ–¹æ³•éœ€è¦æœ‰ä¸€ä¸ªå…¬ä¼—å·ã€‚</p>
<p>é€šè¿‡å…¬ä¼—å·é‡Œæ·»åŠ ã€Œè¶…é“¾æ¥ã€çš„åŠŸèƒ½æ¥è·å–æ–‡ç« åˆ—è¡¨ã€‚</p>
<p>å…·ä½“æ“ä½œè§å‚è€ƒèµ„æ–™ã€‚</p>
<h3 id="å‡†å¤‡å·¥ä½œ">å‡†å¤‡å·¥ä½œ</h3>
<p>è¿™é‡Œåªæå‡ ä¸ªå…³é”®ç‚¹ï¼Œè¿›å…¥è¶…é“¾æ¥èœå•åï¼ŒæŒ‰F12æŠ“åŒ…</p>
<p>ä¸»è¦çœ‹ <code>/cgi-bin/appmsg</code> è¿™ä¸ªæ¥å£ï¼Œéœ€è¦æå–å…¶ä¸­çš„</p>
<ul>
<li>Cookie</li>
<li>token</li>
<li>fakeid - å…¬ä¼—å·IDï¼Œbase64ç¼–ç </li>
</ul>
<p>å‰ä¸¤ä¸ªæ¯æ¬¡ç™»å½•éƒ½ä¸ä¸€æ ·ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨ selenium æ­é…æœ¬åœ°ä»£ç†æ¥æŠ“åŒ…è‡ªåŠ¨æ›´æ–°ï¼Œè¯¦æƒ…å‚è€ƒæˆ‘ä¹‹å‰å†™è¿‡çš„æ–‡ç« : <a href="https://www.cnblogs.com/deali/p/14367106.html" target="_blank">Seleniumçˆ¬è™«å®è·µï¼ˆè¸©å‘è®°å½•ï¼‰ä¹‹ajaxè¯·æ±‚æŠ“åŒ…ã€æµè§ˆå™¨é€€å‡º</a></p>
<h3 id="ä»£ç å®ç°">ä»£ç å®ç°</h3>
<p>æˆ‘å°†æ“ä½œå°è£…ä¸º class</p>
<pre><code class="language-python">class ArticleCrawler:
    def __init__(self):
        self.url = "æ¥å£åœ°å€ï¼Œæ ¹æ®æŠ“åŒ…åœ°å€æ¥"
        self.cookie = ""
        self.headers = {
            "Cookie": self.cookie,
            "User-Agent": "å¡«å†™åˆé€‚çš„UA",
        }
        self.payload_data = {} # æ ¹æ®å®é™…æŠ“åŒ…æ‹¿åˆ°çš„æ•°æ®æ¥
        self.session = requests.Session()
        self.session.headers.update(self.headers)

    def fetch_html(self, url):
        """è·å–æ–‡ç«  HTML"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            return response.text
        except Exception as e:
            logger.error(f"Failed to fetch HTML for {url}: {e}")
            return None

    @property
    def total_count(self):
        """è·å–æ–‡ç« æ€»æ•°"""
        content_json = self.session.get(self.url, params=self.payload_data).json()
        try:
            count = int(content_json["app_msg_cnt"])
            return count
        except Exception as e:
            logger.error(e)
            logger.warning(f'response json: {content_json}')

        return None

    async def crawl_list(self, count, per_page=5):
        """è·å–æ–‡ç« åˆ—è¡¨å¹¶å­˜å…¥æ•°æ®åº“"""
        logger.info(f'æ­£åœ¨è·å–æ–‡ç« åˆ—è¡¨ï¼Œtotal count: {count}')

        created_articles = []

        page = int(math.ceil(count / per_page))
        for i in tqdm(range(page), ncols=100, desc="è·å–æ–‡ç« åˆ—è¡¨"):
            payload = self.payload_data.copy()
            payload["begin"] = str(i * per_page)
            resp_json = self.session.get(self.url, params=payload).json()
            articles = resp_json["app_msg_list"]

            # å­˜å…¥
            for item in articles:
                # æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨ï¼Œé¿å…é‡å¤æ’å…¥
                if await Article.filter(raw_id=item['aid']).exists():
                    continue

                created_item = await Article.create(
                    raw_id=item['aid'],
                    title=item['title'],
                    url=item['link'],
                    created_at=datetime.fromtimestamp(item["create_time"]),
                    updated_at=datetime.fromtimestamp(item["update_time"]),
                    html='',
                    raw_json=item,
                )
                created_articles.append(created_item)

            time.sleep(random.uniform(3, 6))

        logger.info(f'created articles: {len(created_articles)}')

    async def crawl_all_list(self):
        return self.crawl_list(self.total_count)

    async def crawl_articles(self, fake=False):
        # è¿™é‡Œæ ¹æ®å®é™…æƒ…å†µï¼Œç­›é€‰å‡ºå£çº¸æ–‡ç« 
        qs = (
            Article.filter(title__icontains='å£çº¸')
            .filter(Q(html='') | Q(html__isnull=True))
        )

        count = await qs.count()

        logger.info(f'ç¬¦åˆæ¡ä»¶çš„æ²¡æœ‰HTMLçš„æ–‡ç« æ•°é‡: {count}')

        if fake: return

        with tqdm(
                total=count,
                ncols=100,
                desc="â¬‡ Downloading articles",
                # å¯é€‰é¢œè‰² [hex (#00ff00), BLACK, RED, GREEN, YELLOW, BLUE, MAGENTA, CYAN, WHITE]
                colour='green',
                unit="page",
                bar_format='{l_bar}{bar} | {n_fmt}/{total_fmt} pages [{rate_fmt}]',
        ) as pbar:
            async for article in qs:
                article: Article
                article.html = self.fetch_html(article.url)
                await article.save()
                pbar.update(1)
                time.sleep(random.uniform(2, 5))
</code></pre>
<h3 id="è¿™æ®µä»£ç åšäº†å•¥">è¿™æ®µä»£ç åšäº†å•¥ï¼Ÿ</h3>
<p>åº”è¯¥è¯´æ˜¯è¿™ä¸ªç±»æœ‰ä»€ä¹ˆåŠŸèƒ½ã€‚</p>
<ul>
<li>è·å–æŒ‡å®šå…¬ä¼—å·çš„æ–‡ç« æ€»æ•°</li>
<li>å¾ªç¯æŒ‰é¡µè·å–å…¬ä¼—å·çš„æ–‡ç« ï¼ŒåŒ…æ‹¬æ–‡ç« æ ‡é¢˜ã€åœ°å€ã€å†…å®¹</li>
<li>å°†æ–‡ç« å­˜å…¥æ•°æ®åº“</li>
</ul>
<h3 id="ä»£ç è§£æ">ä»£ç è§£æ</h3>
<p>å…¶ä¸­å…³é”®å°±æ˜¯ <code>crawl_list</code> æ–¹æ³•</p>
<p>å…¶å®ä»£ç æ˜¯æ¯”è¾ƒç²—ç³™çš„ï¼Œæ²¡æœ‰é”™è¯¯å¤„ç†ï¼Œè€Œä¸”æ¯ä¸ªå¾ªç¯é‡Œéƒ½ä¼šå»è®¿é—®æ•°æ®åº“ï¼Œæ€§èƒ½è‚¯å®šæ˜¯ä¸å’‹æ ·çš„ã€‚</p>
<p>æ­£ç¡®çš„åšæ³•æ˜¯å…ˆæŠŠæ•°æ®åº“é‡Œå·²æœ‰çš„æ–‡ç« IDè¯»å–å‡ºæ¥ï¼Œç„¶åå°±ä¸ä¼šæ¯æ¬¡å¾ªç¯éƒ½æŸ¥è¯¢æ•°æ®åº“äº†ã€‚</p>
<p>ä¸è¿‡æ˜¯ç®€å•çš„çˆ¬è™«å°±æ²¡å»ä¼˜åŒ–äº†ã€‚</p>
<p>ç„¶åæ¯æ¬¡å¾ªç¯ä½¿ç”¨ <code>time.sleep(random.uniform(3, 6))</code> éšæœºæš‚åœä¸€æ®µæ—¶é—´ã€‚</p>
<h3 id="è¿›åº¦æ¡">è¿›åº¦æ¡</h3>
<p>è¿™é‡Œä½¿ç”¨äº† tqdm åº“æ¥å®ç°è¿›åº¦æ¡ï¼ˆpython ç”Ÿæ€ä¼¼ä¹æœ‰æ›´ç®€å•çš„è¿›åº¦æ¡åº“ï¼Œæˆ‘ä¹‹å‰ç”¨è¿‡ï¼Œä¸è¿‡å¤§å¤šæ˜¯åŸºäº tqdm å°è£…çš„ï¼‰</p>
<p><code>bar_format</code> å‚æ•°ç”¨æ³•ï¼šä½¿ç”¨ bar_format æ¥è‡ªå®šä¹‰è¿›åº¦æ¡çš„æ ¼å¼ï¼Œå¯ä»¥æ˜¾ç¤ºå·²å¤„ç†æ–‡ä»¶æ•°é‡ã€æ€»æ–‡ä»¶æ•°é‡ã€å¤„ç†é€Ÿåº¦ç­‰ã€‚</p>
<ul>
<li>{l_bar} æ˜¯è¿›åº¦æ¡çš„å·¦ä¾§éƒ¨åˆ†ï¼ŒåŒ…å«æè¿°å’Œç™¾åˆ†æ¯”ã€‚</li>
<li>{bar} æ˜¯å®é™…çš„è¿›åº¦æ¡ã€‚</li>
<li>{n_fmt}/{total_fmt} æ˜¾ç¤ºå½“å‰è¿›åº¦å’Œæ€»æ•°ã€‚</li>
<li>{rate_fmt} æ˜¾ç¤ºå¤„ç†é€Ÿç‡ã€‚</li>
</ul>
<h2 id="è§£æç½‘é¡µ">è§£æç½‘é¡µ</h2>
<p>å‰é¢åªæ˜¯æŠŠæ–‡ç« çš„ HTML ä¸‹è½½ä¸‹æ¥ï¼Œè¿˜å¾—ä»ç½‘é¡µé‡Œæå–å‡ºå›¾ç‰‡åœ°å€ã€‚</p>
<p>è¿™æ—¶å€™å°±éœ€è¦å†™ä¸€ä¸ªè§£æçš„æ–¹æ³•äº†</p>
<pre><code class="language-python">def parse_html(html: str) -&gt; list:
    soup = BeautifulSoup(html, 'html.parser')
    img_elements = soup.select('img.wxw-img')

    images = []

    for img_element in img_elements:
        img_url = img_element['data-src']
        images.append(img_url)

    return images
</code></pre>
<p>ç®€å•ä½¿ç”¨ css selector æ¥æå–å›¾ç‰‡</p>
<h2 id="æå–å›¾ç‰‡">æå–å›¾ç‰‡</h2>
<p>è¿˜è®°å¾—æ¨¡å‹æœ‰ä¸ª Image å§ï¼Ÿ</p>
<p>åˆ°ç›®å‰ä¸ºæ­¢è¿˜æ²¡ç”¨ä¸Šã€‚</p>
<p>è¿™ä¸€å°èŠ‚å°±æ¥æå–å¹¶å­˜å…¥æ•°æ®åº“</p>
<pre><code class="language-python">async def extract_images_from_articles():
	# æ ¹æ®å®é™…æƒ…å†µå†™æŸ¥è¯¢
    qs = (
        Article.filter(title__icontains='å£çº¸')
        .exclude(Q(html='') | Q(html__isnull=True))
    )

    article_count = await qs.count()

    with tqdm(
            total=article_count,
            ncols=100,
            desc="â¬‡ extract images from articles",
            colour='green',
            unit="article",
            bar_format='{l_bar}{bar} | {n_fmt}/{total_fmt} articles [{rate_fmt}]',
    ) as pbar:
        async for article in qs:
            article: Article
            images = parse_html(article.html)
            for img_url in images:
                if await Image.filter(url=img_url).exists():
                    continue

                await Image.create(
                    article=article,
                    url=img_url,
                )

            pbar.update(1)

    logger.info(f'article count: {article_count}, image count: {await Image.all().count()}')
</code></pre>
<p>è¿™ä¸ªæ–¹æ³•å…ˆæŠŠæ•°æ®åº“é‡Œçš„æ–‡ç« è¯»å–å‡ºæ¥ï¼Œç„¶åä»æ–‡ç« çš„ HTML é‡Œæå–å‡ºå›¾ç‰‡ï¼Œæœ€åæŠŠæ‰€æœ‰å›¾ç‰‡å­˜å…¥æ•°æ®åº“ã€‚</p>
<p>è¿™é‡Œä»£ç åŒæ ·å­˜åœ¨å¾ªç¯é‡Œåå¤æŸ¥è¯¢æ•°æ®åº“çš„é—®é¢˜ï¼Œä¸è¿‡æˆ‘æ‡’å¾—ä¼˜åŒ–äº†â€¦</p>
<h2 id="ä¸‹è½½å›¾ç‰‡">ä¸‹è½½å›¾ç‰‡</h2>
<p>ç±»ä¼¼çš„ï¼Œæˆ‘ç¼–å†™äº† ImageCrawler ç±»</p>
<pre><code class="language-python">class ImageCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(headers)
        self.images_dir = os.path.join('output', 'images')
        os.makedirs(self.images_dir, exist_ok=True)

    def download_image(self, url):
        img_path = os.path.join(self.images_dir, f'{time.time()}.{extract_image_format_re(url)}')
        img_fullpath = os.path.join(os.getcwd(), img_path)

        try:
            response = self.session.get(url)
            with open(img_fullpath, 'wb') as f:
                f.write(response.content)

            return img_path
        except Exception as e:
            logger.error(e)

        return None
</code></pre>
<p>è¿™ä¸ªä»£ç å°±ç®€å•å¤šäº†ï¼Œå°±å•çº¯ä¸‹è½½å›¾ç‰‡ã€‚</p>
<p>å›¾ç‰‡çš„æ–‡ä»¶åæˆ‘ä½¿ç”¨äº†æ—¶é—´æˆ³ã€‚</p>
<p>ä¸è¿‡è¦å®é™…æŠŠå›¾ç‰‡é‡‡é›†ä¸‹æ¥ï¼Œè¿˜æ²¡é‚£ä¹ˆç®€å•ã€‚</p>
<p>æ¥ä¸‹æ¥å†™ä¸€ä¸ªä¸‹è½½å›¾ç‰‡çš„æ–¹æ³•</p>
<pre><code class="language-python">async def download_images():
    images = await Image.filter(is_downloaded=False)

    if not images:
        logger.info(f'no images to download')
        return

    c = ImageCrawler()

    with tqdm(
            total=len(images),
            ncols=100,
            desc="â¬‡ download images",
            colour='green',
            unit="image",
            bar_format='{l_bar}{bar} | {n_fmt}/{total_fmt} images [{rate_fmt}]',
    ) as pbar:
        for image in images:
            image: Image
            img_path = c.download_image(image.url)
            if not img_path:
                continue

            image.is_downloaded = True
            image.local_file = img_path
            await image.save()

            pbar.update(1)
            time.sleep(random.uniform(1, 3))
</code></pre>
<p>ç­›é€‰æœªä¸‹è½½çš„å›¾ç‰‡ï¼Œä¸‹è½½ä¹‹åæ›´æ–°æ•°æ®åº“ï¼ŒæŠŠå›¾ç‰‡çš„ä¸‹è½½è·¯å¾„å­˜è¿›å»ã€‚</p>
<h2 id="æŠŠç¨‹åºè¿è¡Œèµ·æ¥">æŠŠç¨‹åºè¿è¡Œèµ·æ¥</h2>
<p>æœ€åéœ€è¦æŠŠç¨‹åºçš„å„éƒ¨åˆ†åƒç³–è‘«èŠ¦ä¸€æ ·ä¸²èµ·æ¥ã€‚</p>
<p>è¿™æ¬¡ç”¨åˆ°äº†å¼‚æ­¥ï¼Œæ‰€æœ‰ä¼šæœ‰äº›ä¸ä¸€æ ·</p>
<pre><code class="language-python">async def main():
    await init()
    await extract_images_from_articles()
    await download_images()
</code></pre>
<p>æœ€ååœ¨ç¨‹åºå…¥å£è°ƒç”¨</p>
<pre><code class="language-python">if __name__ == '__main__':
    run_async(main())
</code></pre>
<p>run_async æ–¹æ³•æ˜¯ tortoise-orm æä¾›çš„ï¼Œå¯ä»¥ç­‰å¾…å¼‚æ­¥æ–¹æ³•è¿è¡Œå®Œæˆï¼Œå¹¶å›æ”¶æ•°æ®åº“è¿æ¥ã€‚</p>
<h2 id="å¼€å‘è®°å½•">å¼€å‘è®°å½•</h2>
<p>æˆ‘å°† git æäº¤è®°å½•å¯¼å‡ºä¹‹åç®€å•æ•´ç†ä¸‹ï¼Œå½¢æˆè¿™ä¸ªå¼€å‘è®°å½•è¡¨æ ¼ã€‚</p>
<table>
<thead>
<tr>
<th>Date &amp; Time</th>
<th>Message</th>
</tr>
</thead>
<tbody>
<tr>
<td>2025-01-18 19:02:21</td>
<td>ğŸ¹image_crawlerå°ä¿®æ”¹</td>
</tr>
<tr>
<td>2025-01-18 18:09:11</td>
<td>ğŸ¹æ›´æ–°äº†cookieï¼›crawl_articlesæ–¹æ³•å¢åŠ fakeåŠŸèƒ½ï¼›crawl_listæ–¹æ³•å®Œæˆä¹‹åä¼šæ˜¾ç¤ºæ›´æ–°äº†å¤šå°‘æ–‡ç« </td>
</tr>
<tr>
<td>2025-01-12 15:48:15</td>
<td>ğŸ¥¤hash_sizeæ”¹æˆäº†32ï¼Œæ„Ÿè§‰é€Ÿåº¦æ²¡å¤šå¤§å˜åŒ–</td>
</tr>
<tr>
<td>2025-01-12 15:13:06</td>
<td>ğŸŸåŠ ä¸Šäº†å¤šç§å“ˆå¸Œç®—æ³•æ”¯æŒ</td>
</tr>
<tr>
<td>2025-01-12 15:00:43</td>
<td>ğŸ•å›¾ç‰‡åˆ†æè„šæœ¬æå®šï¼Œç°åœ¨å›¾ç‰‡ä¿¡æ¯å®Œæ•´å¡«å……å¥½äº†</td>
</tr>
<tr>
<td>2025-01-11 23:41:14</td>
<td>ğŸŒ­ä¿®å¤äº†ä¸ªbugï¼Œä»Šæ™šå¯ä»¥æŒ‚ç€ä¸€ç›´ä¸‹è½½äº†</td>
</tr>
<tr>
<td>2025-01-11 23:36:46</td>
<td>ğŸ•å®Œæˆäº†ä¸‹è½½å›¾ç‰‡çš„é€»è¾‘ï¼ˆæœªæµ‹è¯•ï¼‰ï¼›åŠ å…¥pillowå’Œimagehashåº“ï¼Œåç»­å†åšå›¾ç‰‡çš„è¯†åˆ«åŠŸèƒ½ï¼Œå…ˆä¸‹è½½å§ã€‚</td>
</tr>
<tr>
<td>2025-01-11 23:25:26</td>
<td>ğŸ¥“å›¾ç‰‡çˆ¬è™«åˆæ­¥é‡æ„ï¼ŒæŠŠå›¾ç‰‡é“¾æ¥ä»æ–‡ç« htmlé‡Œæå–å‡ºæ¥äº†ï¼›æƒ³è¦ä½¿ç”¨aerichåšmigrationï¼Œè¿˜æ²¡å®Œæˆ</td>
</tr>
<tr>
<td>2025-01-11 22:27:04</td>
<td>ğŸ”åˆå®Œæˆä¸€ä¸ªåŠŸèƒ½ï¼šé‡‡é›†æ–‡ç« çš„HTMLå¹¶å­˜å…¥æ•°æ®åº“</td>
</tr>
<tr>
<td>2025-01-11 21:19:19</td>
<td>ğŸ¥ªæˆåŠŸæŠŠarticle_crawleræ”¹é€ ä¸ºä½¿ç”¨tortoise-orm</td>
</tr>
</tbody>
</table>
<h3 id="å¦‚ä½•å¯¼å‡ºè¿™æ ·çš„è®°å½•">å¦‚ä½•å¯¼å‡ºè¿™æ ·çš„è®°å½•ï¼Ÿ</h3>
<p>ä½¿ç”¨ git å‘½ä»¤å¯¼å‡ºæäº¤è®°å½•</p>
<pre><code class="language-bash">git log --pretty=format:"- %s (%ad)" --date=iso
</code></pre>
<p>è¿™é‡Œä½¿ç”¨äº† markdown çš„åˆ—è¡¨æ ¼å¼</p>
<p>ç”Ÿæˆä¹‹åå†æ ¹æ®éœ€æ±‚è°ƒæ•´ä¸ºè¡¨æ ¼å³å¯ã€‚</p>
<h2 id="å°ç»“">å°ç»“</h2>
<p>çˆ¬è™«æ²¡ä»€ä¹ˆå¥½è¯´çš„ï¼Œè¿™ç§ç®€å•çš„ç›´æ¥ä¿¡æ‰‹æ‹ˆæ¥ï¼Œä¸æ˜¯æˆ‘å¹ï¼Œä»€ä¹ˆè¯­è¨€éƒ½æ˜¯éšä¾¿å†™ï¼Œæ¯•ç«Ÿçˆ¬è™«ä¹Ÿæ˜¯å¾ˆå¤šç¨‹åºè¯¾ç¨‹å…¥é—¨çº§åˆ«çš„å†…å®¹ï¼Œå®åœ¨æ²¡å•¥éš¾åº¦ï¼Œæœ‰æ„æ€çš„åœ¨äºæ¯æ¬¡å†™çˆ¬è™«éƒ½æ­é…ä¸€äº›æ–°çš„ä¸œè¥¿æ¥å°è¯•ï¼Œæˆ–è€…ç”¨ä¸åŒçš„æŠ€æœ¯æ ˆç”šè‡³è®¾å¤‡æ¥å°è¯•çˆ¬è™«ï¼ˆåƒæˆ‘ä¹‹å‰æŠŠçˆ¬è™«æ”¾åˆ°æ‰‹æœºä¸Šè·‘ä¸€æ ·ï¼‰ï¼Œä¹Ÿè®¸å°†æ¥å¯ä»¥æŠŠçˆ¬è™«æ”¾åˆ°å•ç‰‡æœºä¸Šè¿è¡Œï¼Ÿï¼ˆä¼¼ä¹ä¸å¤ªå¯è¡Œï¼Œå†…å­˜å’Œå­˜å‚¨ç©ºé—´éƒ½å¤ªå°äº†ï¼Œæ ‘è“æ´¾å€’æ˜¯å¯ä»¥ï¼Œä½†è¿™ç®—æ˜¯ä¸ªå°æœåŠ¡å™¨ã€‚ï¼‰</p>
<blockquote>
<p>PSï¼šæˆ‘åº”è¯¥è¯•è¯•ç”¨ Rust å†™çˆ¬è™«</p>
</blockquote>
<p>ç»§ channels åï¼Œæˆ‘åˆç»§ç»­å†™å¼‚æ­¥ python ä»£ç äº†ï¼Œå¼‚æ­¥ç¡®å®å¥½ï¼Œå¯æƒœ python çš„å¼‚æ­¥å‘å±•å¾—æ¯”è¾ƒæ™šï¼Œç”Ÿæ€è¿˜ä¸å®Œå–„ã€‚</p>
<p>çœŸå¸Œæœ› Django ORM æ—©æ—¥æ”¯æŒå¼‚æ­¥å•Šï¼Œè¿™æ ·å°±èƒ½æ„‰å¿«çš„å’Œ channels æ­é…ä½¿ç”¨äº†â€¦</p>

</div>
<div id="MySignature" role="contentinfo">
    å¾®ä¿¡å…¬ä¼—å·ï¼šã€Œç¨‹åºè®¾è®¡å®éªŒå®¤ã€
ä¸“æ³¨äºäº’è”ç½‘çƒ­é—¨æ–°æŠ€æœ¯æ¢ç´¢ä¸å›¢é˜Ÿæ•æ·å¼€å‘å®è·µï¼ŒåŒ…æ‹¬æ¶æ„è®¾è®¡ã€æœºå™¨å­¦ä¹ ä¸æ•°æ®åˆ†æç®—æ³•ã€ç§»åŠ¨ç«¯å¼€å‘ã€Linuxã€Webå‰åç«¯å¼€å‘ç­‰ï¼Œæ¬¢è¿ä¸€èµ·æ¢è®¨æŠ€æœ¯ï¼Œåˆ†äº«å­¦ä¹ å®è·µç»éªŒã€‚
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.2207341652638889" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-01-20 23:01">2025-01-20 23:00</span>&nbsp;
<a href="https://www.cnblogs.com/deali">ç¨‹åºè®¾è®¡å®éªŒå®¤</a>&nbsp;
é˜…è¯»(<span id="post_view_count">14</span>)&nbsp;
è¯„è®º(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18682643" rel="nofollow">ç¼–è¾‘</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18682643);return false;">æ”¶è—</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18682643', targetLink: 'https://www.cnblogs.com/deali/p/18682643', title: 'ç¼–å†™çˆ¬è™«ä¸‹è½½å…¬ä¼—å·ä¸Šå¥½çœ‹çš„å£çº¸' })">ä¸¾æŠ¥</a>
</div>
        