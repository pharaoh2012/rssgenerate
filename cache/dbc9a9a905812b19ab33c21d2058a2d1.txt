
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/Big-Yellow/p/18746232" title="发布于 2025-03-02 15:07">
    <span role="heading" aria-level="2">常见的各类LLM基座模型（GPT、DeepSeek、Qwen等）模型解析以及对比</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p>From： <a href="https://www.big-yellow-j.top/posts/2025/02/15/LLM.html" target="_blank" rel="noopener nofollow">https://www.big-yellow-j.top/posts/2025/02/15/LLM.html</a></p>
<h2 id="各类llm模型技术汇总">各类LLM模型技术汇总</h2>
<p><img src="https://img2024.cnblogs.com/blog/3395559/202503/3395559-20250302150624505-232183049.png" alt="" loading="lazy"></p>
<p>只去对比整体框架，对所采用的激活函数，归一化处理，位置编码等参考：<br>
<strong>1、位置编码</strong>：<a href="https://www.big-yellow-j.top/posts/2025/02/03/pos-embedding.html" target="_blank" rel="noopener nofollow">https://www.big-yellow-j.top/posts/2025/02/03/pos-embedding.html</a><br>
<strong>2、归一化处理</strong>：<a href="https://www.big-yellow-j.top/posts/2025/01/05/dl-norm.html" target="_blank" rel="noopener nofollow">https://www.big-yellow-j.top/posts/2025/01/05/dl-norm.html</a><br>
<strong>3、分布式训练</strong>：<a href="https://www.big-yellow-j.top/posts/2025/01/03/DistributeTraining.html" target="_blank" rel="noopener nofollow">https://www.big-yellow-j.top/posts/2025/01/03/DistributeTraining.html</a></p>
<h2 id="gpt系列">GPT系列</h2>
<h3 id="1gpt-v1">1.GPT v1</h3>
<p>对于大部分的深度学习任务，需要大量的<strong>标记数据</strong>（labeled data），但是如果使用大量的标记数据就会导致一个问题：构建得到的模型缺少适用性（可以理解为模型的泛化性能可能不佳）。那么就尝试使用<strong>非标记的数据</strong>（unlabelled data）但是这样一来又会有一个新的问题：时间消费大（time-consuming and expensive）。所以目前学者提出：使用预训练的词嵌入来提高任务性能。使用 <em>未标注的文本信息</em>（word-level information from unlabelled text）可能会：1、不清楚那种<strong>优化目标</strong>（optimization objective）在学习对迁移有用的文本表示时最有效；2、如何将这些学习到的表征有效的迁移到<strong>目标任务</strong>（target task）中。<br>
作者提出：1、<strong>无监督的预训练</strong>（unsupervised pre-training）；2、<strong>监督的微调</strong>（supervised fine-tuning）</p>
<blockquote>
<p>1、<strong>Unsupervised pre-training</strong></p>
</blockquote>
<p>给定一些列的的<strong>无标签</strong>的 <strong>token</strong>：<span class="math inline">\(U={u_1,...,u_n}\)</span>，构建自回归的模型：</p>
<p></p><div class="math display">\[L_1(U)= \sum_{i}logP(u_i|u_{i-k},...,u_{i-1};\theta)
\]</div><p></p><p>其中 <span class="math inline">\(\theta\)</span>为模型的参数。作者在模型中使用 <strong>Transforme</strong>作为 <strong>decoder</strong>，在最后的模型上作者构建得到为：</p>
<p></p><div class="math display">\[h_0= UW_e+W_p \\
h_l = transformer\_block(h_{l-1})\forall i \in [1,n]\\
P(u)=softmax(h_nW_e^T)
\]</div><p></p><p>其中<span class="math inline">\(n\)</span>代表神经网路层的数目，<span class="math inline">\(W_e\)</span>代表 <em>token embedding matrix</em>，<span class="math inline">\(W_p\)</span>代表 <em>position embedding matrix</em>。对于无监督下的预训练：通过构建的数据集，去对模型的参数进行训练，得到模型的参数。</p>
<blockquote>
<p>2、<strong>Supervised fine-tunning</strong></p>
</blockquote>
<p>作者在此部分提到：通过第一步得到的模型参数去对监督任务进行训练（采用的模型结构是没有变化的）。给定标签数据集<span class="math inline">\(C\)</span>，给定输入：<span class="math inline">\({x^1,...,x^m }\)</span>以及其标签<span class="math inline">\(y\)</span>。将数据投入到预训练得到的模型参数里面得到：<span class="math inline">\(h_l^m\)</span>，然后添加一个线性输出层（参数为：<span class="math inline">\(W_y\)</span>）去对<span class="math inline">\(y\)</span>进行预测。</p>
<p></p><div class="math display">\[P(y|x^1,...,x^m)=softmax(h_l^wW_y)
\]</div><p></p><blockquote>
<p>对于上述两部分步骤直观上理解：人首先从外界获取大量信息：网络，书本等，把这些信息了解之后，然后去写作文或者去回答问题。</p>
</blockquote>
<p>模型结构：</p>
<p><img src="https://img2024.cnblogs.com/blog/3395559/202503/3395559-20250302150624019-1251600770.png" alt="" loading="lazy"></p>
<h3 id="2gpt-v2">2.GPT v2</h3>
<p><strong>GPT v2</strong>区别前一个模型，区别在于将<strong>layer-norm</strong> 位置替换到每一个残差连接块的里面，也就是说在数据输入到 <strong>Multi-Head-Attention</strong> 以及 <strong>Feed-Forward</strong> 之前提前通过一层标准化处理。</p>
<h3 id="3gpt-v3">3.GPT v3</h3>
<h2 id="deepseek系列">DeepSeek系列</h2>
<p>主要介绍<strong>DeepSeek v3</strong>（简称<strong>DS</strong>）各类技术细节，对于<strong>DS</strong>在模型结构上和之前迭代版本的 <strong>DS-2</strong>无太大区别，还是使用混合专家模型，只是补充一个辅助损失去平衡不同专家之间的不均衡问题。</p>
<p><img src="https://img2024.cnblogs.com/blog/3395559/202503/3395559-20250302150624467-204721401.png" alt="" loading="lazy"></p>
<blockquote>
<p>左侧结构和 <strong>GPT-2</strong>结构类似</p>
</blockquote>
<p>在结构上<strong>DS</strong>主要的创新点在于：1、<a href="https://www.big-yellow-j.top/posts/2025/01/29/Attention.html" target="_blank" rel="noopener nofollow">Multi-Head Latent Attention</a>；2、<a href="https://www.big-yellow-j.top/posts/2025/MoE-KV-cache.html" target="_blank" rel="noopener nofollow">DeepSeekMoE</a>。前者为优化 <strong>KV-cache</strong> 操作，通过一个低秩的<span class="math inline">\(c_r^{KV}\)</span>代替原本占用较高的QV的值（首先通过降维方式降低原本维度，这样一来在显存占用上就会降低，而后通过升维方式，恢复到原本的维度），后者为混合专家模型，不过区别于常用的<code>MoE</code>方法，在<strong>DS</strong>中将专家模型分为两类：1、<strong>Routed Expert</strong>；2、<strong>Shared Expert</strong>，前者<strong>直接</strong>将隐藏层的输入进行传入，后者则是通过门控网络<strong>筛选</strong>而后隐藏层的输入进行传入。</p>
<p>除此之外，在<strong>DS</strong>中使用<strong>Multi-Token Prediction</strong>（MTP:<a href="https://arxiv.org/pdf/2404.19737%EF%BC%89%E6%8A%80%E6%9C%AF" target="_blank" rel="noopener nofollow">https://arxiv.org/pdf/2404.19737）技术</a></p>
<p><img src="https://img2024.cnblogs.com/blog/3395559/202503/3395559-20250302150624377-592765327.png" alt="" loading="lazy"></p>
<p>在<strong>DS</strong>中一个很耀眼的功能就是：<strong>DeepSeek-R1</strong>（一种思维链技术：<strong>CoT</strong>:<strong>Chain of Thought</strong>，在GPT-o1中也使用到这种技术）结合论文：<a href="https://arxiv.org/pdf/2201.11903%EF%BC%9Bhttps://arxiv.org/pdf/2501.12948%EF%BC%9B%E4%B8%AD%E5%AF%B9" target="_blank" rel="noopener nofollow">https://arxiv.org/pdf/2201.11903；https://arxiv.org/pdf/2501.12948；中对</a> <strong>CoT</strong>技术的描述，可以简单的理解为：让LLM可以自主去思考问题，比如在<a href="https://arxiv.org/pdf/2201.11903" target="_blank" rel="noopener nofollow">论文</a>中对 <strong>CoT</strong>技术的描述。</p>
<p><img src="https://img2024.cnblogs.com/blog/3395559/202503/3395559-20250302150624365-1138767995.png" alt="" loading="lazy"></p>
<p>相较之直接让GPT输出答案，区别在于还要他给出推理过程。结合在 <a href="https://arxiv.org/pdf/2501.12948" target="_blank" rel="noopener nofollow"><strong>DS-R1</strong></a>中的描述对于 <strong>DS-R1</strong>整体过程理解如下：</p>
<p><img src="https://img2024.cnblogs.com/blog/3395559/202503/3395559-20250302150623150-1307590203.png" alt="" loading="lazy"></p>
<p>非常明显的一个强化学习过程，在论文里面提到的使用 <strong>Group Relative Policy Optimization</strong>（GRPO）策略进行优化</p>
<p><img src="https://img2024.cnblogs.com/blog/3395559/202503/3395559-20250302150623760-1275157116.png" alt="" loading="lazy"></p>
<p>在 <strong>DS-R1</strong>中作者提到的使用的模板</p>
<p><img src="https://img2024.cnblogs.com/blog/3395559/202503/3395559-20250302150624303-1468184141.png" alt="" loading="lazy"></p>
<p>对于上述优化过程<a href="https://medium.com/@sahin.samia/the-math-behind-deepseek-a-deep-dive-into-group-relative-policy-optimization-grpo-8a75007491ba" target="_blank" rel="noopener nofollow">理解</a>：比如说对于一个数学问题：<span class="math inline">\(8+5=?\)</span>，这就是上面公式中所提到的question <span class="math inline">\(q\)</span>，按照上面的描述，将会生成一系列的输出：<span class="math inline">\({o_1,...,o_G}\)</span><br>
<strong>Step-1</strong>：生成若干的回答。<span class="math inline">\({o_1,...,o_G}\)</span><br>
<strong>Step-2</strong>：对于生成的回答进行评分。<span class="math inline">\({r_1,...,r_G}\)</span>，而后计算<span class="math inline">\(A_i=\frac{r_i- \text{mean}({r_1,...,r_G})}{\text{std}({r_,...,r_G})}\)</span><br>
<strong>Step-3</strong>：使用裁剪更新策略：<span class="math inline">\(\text{clip}(\frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{old}(o_i|q)}},1-\epsilon,\epsilon)\)</span>比如说：如果新策略开始给o1分配过高的概率，裁剪机制确保不会过度强调这个响应。这种方式保证了即使在像推理这样复杂的任务中，策略优化也能保持稳定和可靠。通过clip函数将内部值限定在<span class="math inline">\((1-\epsilon, 1+\epsilon)\)</span>之间<br>
<strong>Step-4</strong>：通过KL散度（用来度量两个概率分布相似度的指标）惩罚偏差</p>
<hr>
<p><strong>PPO</strong>和 <strong>GRPO</strong></p>
<p><img src="https://img2024.cnblogs.com/blog/3395559/202503/3395559-20250302150624431-669026264.png" alt="" loading="lazy"></p>
<p>上面提到的几个模型：<br>
1、<strong>Policy Model</strong>：我们需要优化的模型<br>
2、<strong>Value Model</strong>：估计状态的价值，帮助指导策略优化<br>
3、<strong>Reference Model</strong>：提供历史策略的参考，确保优化过程中策略变化不过度<br>
4、<strong>Reward Model</strong>：定义奖励信号，用于强化学习中的奖励反馈</p>
<p><strong>GRPO</strong>实现，参考<a href="https://mp.weixin.qq.com/s/BYPKP5oXg1V4C_vg0VFGhw" target="_blank" rel="noopener nofollow"><strong>腾讯</strong></a>以及 <a href="https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb" target="_blank" rel="noopener nofollow">Github</a>上实现代码，对于复现可以直接用Huggingface中的 <a href="https://huggingface.co/docs/trl/main/en/grpo_trainer" target="_blank" rel="noopener nofollow"><strong>trl</strong></a>来进行复现</p>
<hr>
<h2 id="llama系列">LLama系列</h2>
<h3 id="1llama-v1">1.LLama v1</h3>
<p>LLaMA 所采用的 Transformer 结构和细节，与标准的 Transformer 架构不同的地方包括采用了前置层归一化（Pre-normalization）并使用 <strong>RMSNorm 归一化函数 （Normalizing Function）</strong>、激活函数更换为 SwiGLU，并使用了旋转位置嵌入（RoP），整体 Transformer 架构与 GPT-2 类似</p>
<h3 id="2llama-v2">2.LLama v2</h3>
<p>区别于上一代的LLama v1改进如下几点（主要参考<a href="https://arxiv.org/pdf/2307.09288" target="_blank" rel="noopener nofollow">论文</a>中的A.2.1中的描述）：<br>
<strong>1、序列长度</strong>：由原来的2048 tokens变化为4096 tokens<br>
<strong>2、使用GQA</strong>：通过使用KV-cache可以加快模型生成速度，但是也会造成过大的显存占用，因此<code>LLama v2</code>使用<code>GQA</code>来减少这个过程中的显存占用。</p>
<blockquote>
<p><strong>GQA原理</strong>：<a href="https://www.big-yellow-j.top/posts/2025/01/29/Attention.html" target="_blank" rel="noopener nofollow">https://www.big-yellow-j.top/posts/2025/01/29/Attention.html</a></p>
</blockquote>
<h3 id="3llama-v3">3.LLama v3</h3>
<p>模型参数细节：</p>
<p><img src="https://img2024.cnblogs.com/blog/3395559/202503/3395559-20250302150624249-1541309833.png" alt="" loading="lazy"></p>
<h2 id="bert">BERT</h2>
<p>预训练阶段任务：<br>
1、<strong>Masked LM(MLM)</strong></p>
<p>MLM是一种预训练任务，通过随机掩蔽输入序列中的部分词元，模型根据上下文预测被掩蔽的词元，从而学习双向语言表示。在模型中作者按照：80：10：10的比例进行处理（80：将词元替换为[MASK]；10：将词元替换为词汇表中随机选取的其他词。；10：保持原词元不变）<br>
<strong>例子：</strong><br>
输入：“今天天气真好，我打算去[MASK]。”<br>
模型的任务是根据上下文预测“[MASK]”应该是“公园”。</p>
<p>2、<strong>Next Sentence Prediction(NSP)</strong></p>
<p>NSP是一种预训练任务，模型接收两个句子并预测第二个句子是否是第一个句子的后续。该任务帮助模型理解句子间的逻辑关系。<br>
例子：<br>
句子对：<br>
句子1：“我喜欢去公园散步。”<br>
句子2：“今天下午我会去跑步。”<br>
模型的任务是判断第二个句子是否是第一个句子的自然延续，答案是“是”。</p>
<ul>
<li><strong>缺点</strong><br>
1、BERT neglects dependency between the masked positions and suffers from a <strong>pretrain-finetune discrepancy</strong>（忽略了屏蔽位置之间的依赖性，并遭受预训练微调差异的影响）</li>
</ul>
<blockquote>
<p>这是因为在 BERT模型中，在预训练阶段会添加 [MASK]，但是在 下游任务(downsteram tasks)中并不会使用 <strong>[MASK]</strong></p>
</blockquote>
<h2 id="huggingface-trl使用">HuggingFace-trl使用</h2>
<p>参考：<br>
1、<a href="https://blog.csdn.net/qq_38961840/article/details/145387854" target="_blank" rel="noopener nofollow">https://blog.csdn.net/qq_38961840/article/details/145387854</a><br>
2、<a href="https://huggingface.co/docs/trl/main/en/grpo_trainer" target="_blank" rel="noopener nofollow">https://huggingface.co/docs/trl/main/en/grpo_trainer</a><br>
3、<a href="https://mp.weixin.qq.com/s/BYPKP5oXg1V4C_vg0VFGhw" target="_blank" rel="noopener nofollow">https://mp.weixin.qq.com/s/BYPKP5oXg1V4C_vg0VFGhw</a></p>
<h2 id="参考">参考</h2>
<p>1、<a href="https://arxiv.org/pdf/2307.06435" target="_blank" rel="noopener nofollow">A Comprehensive Overview of Large Language Models</a><br>
2、<a href="https://arxiv.org/abs/2302.13971" target="_blank" rel="noopener nofollow">LLaMA: Open and Efficient Foundation Language Models</a><br>
3、<a href="https://arxiv.org/pdf/1810.04805" target="_blank" rel="noopener nofollow">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a><br>
4、<a href="https://arxiv.org/abs/2307.09288" target="_blank" rel="noopener nofollow">Llama 2: Open Foundation and Fine-Tuned Chat Models</a><br>
5、<a href="https://arxiv.org/pdf/2407.21783v3" target="_blank" rel="noopener nofollow">The Llama 3 Herd of Models</a><br>
6、<a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank" rel="noopener nofollow">Language Models are Unsupervised Multitask Learners</a><br>
7、<a href="https://arxiv.org/pdf/2404.19737" target="_blank" rel="noopener nofollow">Better &amp; Faster Large Language Models via Multi-token Prediction</a><br>
8、<a href="https://arxiv.org/pdf/2501.12948" target="_blank" rel="noopener nofollow">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</a><br>
11、<a href="https://arxiv.org/pdf/2201.11903" target="_blank" rel="noopener nofollow">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a><br>
12、<a href="https://medium.com/@sahin.samia/the-math-behind-deepseek-a-deep-dive-into-group-relative-policy-optimization-grpo-8a75007491ba" target="_blank" rel="noopener nofollow">The Math Behind DeepSeek: A Deep Dive into Group Relative Policy Optimization (GRPO)</a><br>
13、<a href="https://www.youtube.com/watch?v=Yi1UCrAsf4o" target="_blank" rel="noopener nofollow">https://www.youtube.com/watch?v=Yi1UCrAsf4o</a><br>
14、<a href="https://arxiv.org/pdf/1910.10683" target="_blank" rel="noopener nofollow">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a><br>
15、<a href="https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb" target="_blank" rel="noopener nofollow">https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb</a><br>
16、<a href="https://mp.weixin.qq.com/s/BYPKP5oXg1V4C_vg0VFGhw" target="_blank" rel="noopener nofollow">https://mp.weixin.qq.com/s/BYPKP5oXg1V4C_vg0VFGhw</a><br>
17、<a href="https://huggingface.co/docs/trl/main/en/grpo_trainer" target="_blank" rel="noopener nofollow">https://huggingface.co/docs/trl/main/en/grpo_trainer</a></p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.048355603194444444" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-03-02 15:08">2025-03-02 15:07</span>&nbsp;
<a href="https://www.cnblogs.com/Big-Yellow">Big-Yellow-J</a>&nbsp;
阅读(<span id="post_view_count">20</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18746232" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18746232);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18746232', targetLink: 'https://www.cnblogs.com/Big-Yellow/p/18746232', title: '常见的各类LLM基座模型（GPT、DeepSeek、Qwen等）模型解析以及对比' })">举报</a>
</div>
        