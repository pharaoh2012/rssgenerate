
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/SuahiStudy/p/18816013" title="发布于 2025-04-09 09:57">
    <span role="heading" aria-level="2">【深度学习】从VAE到GAN漫谈</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h2 id="正文">正文</h2>
<h3 id="从ae说起">从AE说起</h3>
<blockquote>
<p>AE是一个特征提取模型，通过编解码的形式重构输入，完成低维特征表示工作</p>
</blockquote>
<h4 id="推导">推导</h4>
<ol>
<li>存在一个输入<span class="math inline">\(x\)</span>，构造AE编码器<span class="math inline">\(p_\theta(x)\)</span>，得到离散低维特征<span class="math inline">\(z\)</span>；</li>
<li>通过AE解码器<span class="math inline">\(q_\phi(z)\)</span>，重构回<span class="math inline">\(\hat{x}\)</span>；</li>
<li>通过正则项<span class="math inline">\(\Vert x-\hat{x} \Vert_2\)</span>构造损失函数实现。</li>
</ol>
<p></p><div class="math display">\[\begin{aligned}
z &amp;= p_\theta(x) \\
\hat{x} &amp;= q_\phi(z) \\
\mathcal{L} &amp;= \frac{1}{2}\Vert x-\hat{x} \Vert_2
\end{aligned}
\]</div><p></p><h4 id="缺点">缺点</h4>
<p><img src="https://suahi-1311668441.cos.ap-shanghai.myqcloud.com/2024/202503121452772.png" alt="image.png" loading="lazy"></p>
<p>AE的<span class="math inline">\(x-&gt;z-&gt;\hat{x}\)</span>是一一映射确定的关系，这就意味着作为一个生成模型他是不够格的，想象一下我有一批样本<span class="math inline">\(\{x_1,x_2,\dots,x_n\}\)</span>，对其进行AE训练可得对应的低维特征<span class="math inline">\(\{z_1,z_2,\dots,z_n\}\)</span>，当作为生成模型时我只能从有限离散的低维特征簇中随机选择现有的特征<span class="math inline">\(z_i\)</span>输入我的解码器<span class="math inline">\(\hat{x_i}=q_\phi(z_i)\)</span>，得到原先样本中已有的一张图、一段信号等。</p>
<p>而当我们想从，举例<span class="math inline">\(z_1、z_2\)</span>之间取一个<span class="math inline">\(z_{1.5}\)</span>（不太严谨的说法，主要想表面取一个低维特征簇之外的特征）时，由于没有受过训练，得到的一定是一个很诡异的东西。</p>
<h3 id="vae的诞生">VAE的诞生</h3>
<blockquote>
<p>VAE与AE最大的不同在于，提出潜变量z的概念，将潜变量z由离散的分布-&gt;连续的分布，这使得生成模型得以实现，由于z可以从连续分布中任意采样，大大的增大了作为生成模型的性能。</p>
</blockquote>
<h4 id="1-加点噪声吧">1. 加点噪声吧</h4>
<p>原先在AE中，所有的<span class="math inline">\(z_i\)</span>可以理解为由单独一个编码器<span class="math inline">\(\mu(x)\)</span>得到的均值<span class="math inline">\(\mu\)</span>，可知这个<span class="math inline">\(\mu\)</span>是离散且服从狄拉克分布的。因此在<span class="math inline">\(\mu\)</span>周围采样，甚至距离<span class="math inline">\(\mu_i\)</span>特别接近也只能生成近似于<span class="math inline">\(x_i\)</span>的图像。</p>
<p>![[AE特征分布]]</p>
<p>在AE中看起来是没办法提取一个<span class="math inline">\(z_{n+1}\)</span>去生成了，那如果加点噪声呢？额外添加一个编码器<span class="math inline">\(\sigma(x)\)</span>作为我的噪声，让我的<span class="math inline">\(z_i\)</span>增加不确定性，每次都是从一个<span class="math inline">\(p(z_i)\)</span>分布中采样得到的，那么我在这些分布的交叉处就可以获得保留各自输入样本特征的图像啦！</p>
<p>![[CVAE特征分布]]</p>
<blockquote>
<p>[!NOTE]</p>
<p>实际上，这是CVAE的特征分布示意图，每种类图像（以MNIST为例）的<span class="math inline">\(z_i\)</span>潜变量分布都对应有一个<span class="math inline">\(\mu_Y\)</span>均值、1为方差的高斯分布</p>
</blockquote>
<p>上述描述与真实的VAE还存在一些不同：</p>
<ol>
<li>VAE的<span class="math inline">\(z\)</span>分布我们希望它光滑且连续，这样我们才能从中任意采样确保能得到有意义的图像，所以在后文提到使用正则化项KL散度限制所有的<span class="math inline">\(z\)</span>分布-&gt;标准高斯分布；</li>
<li>z的采样具有随机性，因此是不可梯度回传的，我们使用一种重参数化技巧，把从<span class="math inline">\(N(\mu,\sigma)\)</span>随机采样转变为随机生成一个<span class="math inline">\(\epsilon \sim N(0,1)\)</span>噪音，<span class="math inline">\(z=\mu + \epsilon * \sigma\)</span>；</li>
</ol>
<p>把<span class="math inline">\(\{z_1,z_2,\dots,z_n\}\)</span>强制约束到一个分布<span class="math inline">\(q(z)\)</span>（例如正态分布），那么我就可以通过<span class="math inline">\(z\sim q(z)\)</span>的方式从这个光滑连续的分布中采样并生成图形了。</p>
<blockquote>
<p>[!question] 为什么AE中也可以采样，但是VAE采样可以生成有意义图形？</p>
<ol>
<li>AE的<span class="math inline">\(z\)</span>与<span class="math inline">\(x\)</span>之间是一一对应的，没有经过训练的<span class="math inline">\(z_i\)</span>没有对应的物理含义；</li>
<li>VAE中每次训练都强制性把<span class="math inline">\(q(z|x)\)</span>后验分布约束为一个<span class="math inline">\(N(0,I)\)</span>分布，这样每次训练都经历这一步，后续我从<span class="math inline">\(N(0,I)\)</span>分布中就算采样得到的<span class="math inline">\(Z\)</span>并没有在训练中出现过，也能实现保留部分特征的效果；</li>
<li>从数学的角度而言，每次训练过程中是从如下分布取到的，而正则化项迫使<span class="math inline">\(\mu(x)-&gt;0\)</span>、<span class="math inline">\(\sigma(x)-&gt;1\)</span>形成一个标准的正态分布；然而重构损失项也会为了尽可能高的精度去抹平噪声项<span class="math inline">\(\sigma(x)\)</span>，并且使得<span class="math inline">\(\mu(x)\)</span>还原回原先AE中离散的位置。这种天然的<strong>对抗</strong>其实也含有GAN中对抗的哲学，并且最终形态也可知<span class="math inline">\(q(z|x)\)</span>一定不是一个标准正态分布，而是保留了其趋向于表征其特征的<span class="math inline">\(\mu(x)\)</span>的趋势，因此这也可以解释为什么VAE的潜变量空间会呈现一定的分块或者说聚类性。<p></p><div class="math display">\[z \sim q(z|x) = N(\mu(x);\sigma(x)I)
\]</div><p></p></li>
</ol>
</blockquote>
<p>现在还存在一个问题，就是<span class="math inline">\(z\)</span>和<span class="math inline">\(x\)</span>不匹配，如果<span class="math inline">\([\mu, \sigma] \sim [\mu(X),\sigma(X)]\)</span>，那么我采样出来的<span class="math inline">\(z\)</span>到底是对应了哪个<span class="math inline">\(x\)</span>呢？我解码后的<span class="math inline">\(\hat{x}\)</span>又该和哪个<span class="math inline">\(x\)</span>去做对比呢？所以我们提出<span class="math inline">\(z \sim q(z|x)\)</span>：</p>
<ol>
<li><span class="math inline">\(z_i \sim q(z|x_i)\)</span></li>
<li><span class="math inline">\(\hat{x_i}=decoder(z_i)\)</span></li>
<li><span class="math inline">\(\mathcal{L}=D(x_i, \hat{x_i})\)</span></li>
</ol>
<p>并且当<span class="math inline">\(q(z|x) \sim N(0,I)\)</span>后，<span class="math inline">\(q(z)\)</span>也自然的服从标准正态分布</p>
<p></p><div class="math display">\[q(z)=\int q(x,z)dx=\int q(z|x)q(x)dx=q(z|x)\int q(x)dx=q(z|x) \sim N(0,1)
\]</div><p></p><p>太好了，这下我们有救了。</p>
<h4 id="2-无中生有">2. 无中生有</h4>
<p>现有一批样本<span class="math inline">\(\{x_1,x_2,\dots,x_n\}\)</span>，根据贝叶斯学派的观点，一定是服从一个证据分布<span class="math inline">\(\tilde{p}(x)\)</span></p>
<p><img src="https://suahi-1311668441.cos.ap-shanghai.myqcloud.com/2024/202503121614819.png" alt="image.png" loading="lazy"></p>
<p>通常我们可以假设存在一个潜变量<span class="math inline">\(z\)</span>，这个<span class="math inline">\(z\)</span>是物理空间观测到的<span class="math inline">\(\{x_1,x_2,\dots,x_n\}\)</span>样本的潜在表达，并且根据第一节的内容我们知道，这个<span class="math inline">\(z\)</span>最好是从<span class="math inline">\(p(z|x)\)</span>分布中采样的。</p>
<p>那么只要我们知道<span class="math inline">\(p(z|x)\)</span>后，可以得到一一对应的<span class="math inline">\(x\)</span>与<span class="math inline">\(z\)</span>的关系，并且可以开展后续的<span class="math inline">\(z\)</span>采样和解码工作，然而如下式，对<span class="math inline">\(p(z|x)\)</span>的直接求解是不现实的。</p>
<p></p><div class="math display">\[\begin{aligned}
p(z|x) = \frac{p(z,x)}{\tilde{p}(x)}= \frac{p(x|z)p(z)}{\int_z p(x,z)dz}= \frac{p(x|z)p(z)}{\int_z p(x|z)p(z)dz}
\end{aligned}
\]</div><p></p><blockquote>
<p>[!NOTE]</p>
<p>主要是在分母中涉及到对z求边缘积分，我的理解是z是<strong>没有边际</strong>的很难求解</p>
</blockquote>
<p>那么转换一下思路，不能求解<span class="math inline">\(p(z|x)\)</span>，我还不能拟合一个和<span class="math inline">\(p(z|x)\)</span>近似的分布吗？答案是当然可以，不妨假设一个分布<span class="math inline">\(q(z|x)\)</span>。让这二者逼近的同时，保留<span class="math inline">\(q(z|x)\)</span>的高斯分布特性，那么届时也可完成<span class="math inline">\(z\)</span>的采样、解码等后续操作。</p>
<p>这里参考苏剑林苏神的方法，让<span class="math inline">\(q(x,z)\)</span>与<span class="math inline">\(p(x,z)\)</span>逼近，由下式可知，具有更高的证据下界，是一种更强限制性、更严谨的做法。</p>
<p></p><div class="math display">\[\begin{aligned}
KL(p(x,z)||q(x,z))&amp;=\iint p(x,z)ln\frac{p(x,z)}{q(x,z)}dxdz \\
&amp;=\iint p(x,z)ln\frac{\tilde{p}(x)p(z|x)}{q(x)q(z|x)}dxdz \\
&amp;=\iint \tilde{p}(x)p(z|x)(ln\frac{\tilde{p}(x)}{q(x)}+ln\frac{p(z|x)}{q(z|x)})dxdz \\
&amp;=\iint \tilde{p}(x)p(z|x)ln\frac{\tilde{p}(x)}{q(x)}dxdz+\iint \tilde{p}(x)p(z|x)ln\frac{p(z|x)}{q(z|x)})dxdz \\
&amp;=\int_x \tilde{p}(x)ln\frac{\tilde{p}(x)}{q(x)}[\int_z p(z|x)dz]dx+\int_x \tilde{p}(x)[\int_z p(z|x)ln\frac{p(z|x)}{q(z|x)}dz]dx \\
&amp;=\int_x \tilde{p}(x)ln\frac{\tilde{p}(x)}{q(x)}dx+\int_x \tilde{p}(x) KL(p(z|x)||q(z|x))dx \\
&amp;=KL(\tilde{p}(x)||q(x)) + \int_x \tilde{p}(x) KL(p(z|x)||q(z|x))dx \\
\end{aligned}
\]</div><p></p><h4 id="3-神经逼近">3. 神经逼近</h4>
<p>通过散度值，我们可以定义一个优化目标，即<span class="math inline">\(KL(p(x,z)||q(x,z))\)</span>越接近于0越好，其中<span class="math inline">\(q(x,z)\)</span>是我们假设存在且可控的一个分布，<span class="math inline">\(q(x,z)=q(z|x)q(x)=q(x|z)q(z)\)</span>。</p>
<p>在后续公式推导前，我还是要明确所有符号的意义：</p>
<ul>
<li><span class="math inline">\(q(x,z)\)</span>：假设存在的一个联合概率密度函数；</li>
<li><span class="math inline">\(q(x|z)\)</span>：假设存在的似然分布；</li>
<li><span class="math inline">\(q(z|x)\)</span>：假设存在的后验条件概率分布；</li>
<li><span class="math inline">\(q(z)\)</span>：假设存在的先验分布；</li>
<li>反之，关于<span class="math inline">\(p\)</span>符合的都是证据分布<span class="math inline">\(\tilde{p}(x)\)</span>潜变量空间中真实存在的。</li>
</ul>
<p>开始吧。</p>
<p></p><div class="math display">\[\begin{aligned}
KL(p(x,z)||q(x,z))&amp;=\iint p(x,z)ln\frac{p(x,z)}{q(x,z)}dxdz \\
&amp;=\int_x \tilde{p}(x)[\int_zp(z|x)ln\frac{p(x,z)}{q(x,z)}dz]dx \\
&amp;=\mathbb{E}_{x \sim \tilde{p}(x)}[\int_zp(z|x)ln\frac{\tilde{p}(x)p(z|x))}{q(x,z)}dz] \\
&amp;=\mathbb{E}_{x \sim \tilde{p}(x)}[\int_zp(z|x)ln \tilde{p}(x)dz + \int_zp(z|x)ln\frac{p(z|x)}{q(x,z)})]
\end{aligned} \\
\]</div><p></p><p>其中</p>
<p></p><div class="math display">\[\mathbb{E}_{x \sim \tilde{p}(x)}[\int_zp(z|x)ln \tilde{p}(x)dz]=\mathbb{E}_{x \sim \tilde{p}(x)}[ln \tilde{p}(x)]
\]</div><p></p><p>是真实存在的证据分布<span class="math inline">\(\tilde{p}(x)\)</span>的信息熵，记作常数<span class="math inline">\(C\)</span></p>
<p>因此<span class="math inline">\(KL(p(x,z)||q(x,z))=C+\mathbb{E}_{x \sim \tilde{p}(x)}[\int_zp(z|x)ln\frac{p(z|x)}{q(x,z)})dz]\)</span>，因此最小化<span class="math inline">\(KL(p(x,z)||q(x,z))\)</span>相当于最小化后面那项</p>
<p></p><div class="math display">\[\mathcal{L}=KL(p(x,z)||q(x,z))-C=\int_zp(z|x)ln\frac{p(z|x)}{q(x,z)})dz
\]</div><p></p><p>并且我们可以说<span class="math inline">\(\mathcal{L}\)</span>存在下界<span class="math inline">\(-C\)</span>。</p>
<p>接着来</p>
<p></p><div class="math display">\[\begin{aligned}
\mathcal{L}&amp;=\mathbb{E}_{x \sim \tilde{p}(x)}[\int_zp(z|x)ln\frac{p(z|x)}{q(x,z)})dz] \\
&amp;=\mathbb{E}_{x \sim \tilde{p}(x)}[\int_zp(z|x)ln\frac{p(z|x)}{q(x|z)q(z))})dz] \\
&amp;=\mathbb{E}_{x \sim \tilde{p}(x)}[\int_zp(z|x)ln\frac{p(z|x)}{q(z)}dz-\int_zp(z|x)lnq(x|z)dz] \\
&amp;=\mathbb{E}_{x \sim \tilde{p}(x)}[KL(p(z|x)||q(z))-\mathbb{E}_{z \sim p(z|x)}[lnq(x|z)]]
\end{aligned} 
\]</div><p></p><p>得到原作者论文中的损失函数式子。</p>
<h4 id="4-直观的物理意义">4. 直观的物理意义</h4>
<p>首先要牢记一点，最小化<span class="math inline">\(\mathcal{L}\)</span>。</p>
<ul>
<li>第一项：<span class="math inline">\(-\mathbb{E}_{z \sim p(z|x)}[lnq(x|z)]\)</span></li>
</ul>
<p>KL项已经存在下界0了，所以应该是最大化<span class="math inline">\(\mathbb{E}_{z \sim p(z|x)}[lnq(x|z)]\)</span>，怎么理解呢？</p>
<p>官方名字应该叫做，当<span class="math inline">\(z\)</span>采样于<span class="math inline">\(p(z|x)\)</span>分布时，最大对数似然函数的期望</p>
<p>可以理解为：</p>
<ol>
<li><span class="math inline">\(z\)</span>目前已经完成从<span class="math inline">\(p(z|x)\)</span>分布中采样得到了；</li>
<li>这个条件概率分布<span class="math inline">\(q(x|z)\)</span>理解为固定<span class="math inline">\(z\)</span>，对<span class="math inline">\(q(x)\)</span>分布进行积分；</li>
<li><span class="math inline">\(\mathbb{E}_{z \sim p(z|x)}[lnq(x|z)] \approx \frac{1}{k}\sum_{i=1}^k{lnq(x_i|z)}\)</span>，以离散的形式看会更清晰，固定<span class="math inline">\(z\)</span>后生成<span class="math inline">\(x_i\)</span>的期望；</li>
</ol>
<p>这项越大，表示重构的<span class="math inline">\(\hat{x}\)</span>越接近于<span class="math inline">\(x\)</span>。</p>
<ul>
<li>第二项：<span class="math inline">\(KL(p(z|x)||q(z))\)</span></li>
</ul>
<p>这个就不用多说了，KL代表两个分布之间的近似程度，一般我们会假设<span class="math inline">\(q(z) \sim N(0,I)\)</span>，所以我们是对<span class="math inline">\(p(z|x)\)</span>这一项进行了约束，让其接近于标准正态分布。这样有两个好处：</p>
<ol>
<li>方便后续作为生成模型采样；</li>
<li>强制性的添加噪声，提高encoder的噪声鲁棒性，以及防止退化为一个自编码器模型；</li>
</ol>
<h3 id="gan暴力解决">GAN暴力解决</h3>
<h4 id="0-前言">0. 前言</h4>
<p>关于上一节中VAE的一些概念，再进行补充：</p>
<p></p><div class="math display">\[\begin{aligned}
p(x)&amp;=\int_z p(x,z)dz=\int_zp(x|z)p(z)dz \\
p(z) &amp;= \int_xp(x,z)dx=\int_xp(z|x)\tilde{p}(x)dx
\end{aligned}
\]</div><p></p><p>上述两个公式分别还原了在VAE中编码器与解码器的工作。</p>
<p>如前面节所述，后验分布<span class="math inline">\(p_\theta(z|x)\)</span>较难计算得到，所以需要用一个神经网络<span class="math inline">\(q_\phi(z|x)\)</span>进行拟合；</p>
<p>之所以不使用一个<span class="math inline">\(q_\phi(x|z)\)</span>对似然分布<span class="math inline">\(p_\theta(x|z)\)</span>进行拟合主要是由于：</p>
<p>![[002 VAE详细文档#一些补充—对变分推断的一些思考]]</p>
<h4 id="1-背景">1. 背景</h4>
<p>一道面试题：</p>
<blockquote>
<p>现有生成服从均匀分布的一个随机数生成程序，那如何据此去做一个生成<strong>符合正态分布的随机数</strong>据呢？即，如何从均匀分布X<sub>U[0,1]转化为正态分布Y</sub>N(0,1)？</p>
</blockquote>
<p></p><div class="math display">\[Y=f(X)
\]</div><p></p><p>是否可以寻找一个映射关系，让<span class="math inline">\(X\)</span>与<span class="math inline">\(Y\)</span>实现一一对应，通过这个映射使得均匀分布映射为正态分布？</p>
<p>那么这个映射关系一定使得<span class="math inline">\(X \sim U[0,1]\)</span>和<span class="math inline">\(Y \sim N(0,1)\)</span>都符合<span class="math inline">\([x,x+dx]\)</span>区间的概率与<span class="math inline">\([y,y+dy]\)</span>区间的概率相同，即：</p>
<p></p><div class="math display">\[\int_0^x \rho(t)dt=\int_{-\infty}^y \phi(t)dt=\int_{-\infty}^y\frac{1}{\sqrt{2\pi}}exp(-\frac{t^2}{2})dt=\Phi(y)
\]</div><p></p><blockquote>
<p>[!NOTE]</p>
<p>为什么说[x, x+dx]区间的概率和[y, y+dy]区间的概率相同？</p>
<ol>
<li>上面的区间概率实际上只是连续概率密度函数中，计算<strong>一个点对应的概率值</strong>，针对<span class="math inline">\(X\)</span>的样本记作<span class="math inline">\(\rho(x)\)</span>，针对<span class="math inline">\(Y\)</span>的样本记作<span class="math inline">\(\phi(y)\)</span>；</li>
<li>倒推来说，我们希望最后服从均匀分布的随机数程序生成一个数，例如<code>0.7</code>，我们希望它能经过映射关系<code>f</code>后转化到对应的正态分布数据<code>0.5244</code>（实际上我们希望一个服从正态分布的随机数程序可以直接生成这个数），所以换言之，<strong>我们希望均匀分布随机数程序生成<code>0.7</code>的概率和正态分布随机数程序生成<code>0.5244</code>的概率是相同的</strong>，<strong>是一种一一对应的映射关系</strong>；</li>
</ol>
</blockquote>
<p>其中<span class="math inline">\(\Phi(y)\)</span>是指标准正态分布的累计分布函数（Cumulative Distribution Function)</p>
<p>因此，<span class="math inline">\(y\)</span>其实可以由CDF的逆函数表达：</p>
<p></p><div class="math display">\[y=\Phi^{-1}(\int_0^x\rho(t)dt)
\]</div><p></p><p>但是但是，CDF是没有一个<strong>显式的函数表达式</strong>的！因此，求逆显然也是不可行的。</p>
<p>举这个例子只是想说，从均匀分布-&gt;标准正态分布的过程已是如此艰难，更不用说一些我们无法说出分布的离散例子了。</p>
<h4 id="2-神经推导">2. 神经推导</h4>
<p>那咋办呢？</p>
<p>简单，上神经网络呗，特别要记住遇到<span class="math inline">\(f(*)\)</span>就要想到神经网络，有点类似于当年米开朗基罗那种</p>
<blockquote>
<p>给我一个支点，我就可以翘起一个地球</p>
</blockquote>
<p>的豪迈，基本上一个<strong>显式或隐式</strong>的函数都可以通过神经网络拟合得到。</p>
<p>既然具备了利器，那我们收敛回现实，讨论一下比正态分布更复杂的分布，并且这类分布通常是由<span class="math inline">\(Z=\{z_1,z_2,\dots,z_n\}\)</span>这些样本来描述的（比如我们只能给出一批图片的样本，而无法告诉我们的网络这批图片遵从什么分布，又或者说我知道图片遵从什么分布了还需要你生成模型干啥？），我们的GAN所做的事情就<strong>是<code>把噪声（标准正态分布）映射到这堆数据背后的分布中去</code>。</strong></p>
<p>来吧，拟合</p>
<p></p><div class="math display">\[Y=G(X;\theta)
\]</div><p></p><p>特别用了<span class="math inline">\(G\)</span>来表示<code>Generative</code>，生成模型。通过把噪声<span class="math inline">\(X\)</span>输入到这个<span class="math inline">\(G\)</span>中，得到一个希望逼近于真实图片分布的一个<span class="math inline">\(Y\)</span>分布。</p>
<blockquote>
<p>[!NOTE]</p>
<p>重点注意这里，<code>X</code>和<code>Y</code>都大写表示分布，如上文所述是<strong>分布到分布的映射</strong>。</p>
</blockquote>
<p>通常来说，两个分布的<strong>接近程度</strong>我们可以使用</p>
<p></p><div class="math display">\[\begin{aligned}
KL(p_1(x)||p_2(x))&amp;=\int p_1(x)log\frac{p_1(x)}{p_2(x)}dx\\
JS(p_1(x),p_2(x))&amp;=\frac{1}{2}KL(p_1(x)||p_2(x))+\frac{1}{2}KL(p_2(x)||p_1(x))
\end{aligned}
\]</div><p></p><p>上述二者的区别在于<code>KL</code>距离是不对称的，即<span class="math inline">\(KL(p_1(x)||p_2(x))\)</span>和<span class="math inline">\(KL(p_2(x)||p_1(x))\)</span>是<strong>不相等的</strong>。但是<code>JS</code>距离是对称的。</p>
<p>anyway，现在没有概率分布，就勉强用<strong>频率</strong>来<strong>代替</strong>一下，先看看能不能计算距离再说。</p>
<p></p><div class="math display">\[\begin{aligned}
p_z(I_i) &amp;=\frac{1}{N} \sum_{j=1}^N \#(z_j \in I_i) \\
p_y(I_i) &amp;=\frac{1}{M} \sum_{j=1}^M \#(y_j \in I_i)
\end{aligned}
\]</div><p></p><blockquote>
<p>[!NOTE] Example</p>
<p>我现在有<span class="math inline">\(\{1.2, 1.5, 2.3, 3.1, 3.6, 4.0, 4.2, 5.7, 6.1, 6.8\}\)</span>这些数据，我需要评估这些数据背后的分布，使用上述公式整体步骤如下：</p>
<ol>
<li>划分区间，<span class="math inline">\(I_1=[1,3)、I_2=[3,5)、I_3=[5,7)\)</span>；</li>
<li>统计落在每个区间内的个数，<span class="math inline">\(C(I_1)=3、C(I_2)=4、C(I_3)=3\)</span>；</li>
<li>计算<span class="math inline">\(p(I_1)=\frac{3}{10}=0.3、p(I_2)=\frac{4}{10}=0.4、p(I_3)=\frac{3}{10}=0.3\)</span>；</li>
</ol>
</blockquote>
<p>注意到<span class="math inline">\(y\)</span>是由<span class="math inline">\(G(X;\theta)\)</span>生成的，所以应该使用<span class="math inline">\(p_y(I_i;\theta)\)</span>表示，那么我们可以通过构建：</p>
<p></p><div class="math display">\[\begin{aligned}
\mathcal{L}&amp;=JS(p_y(I_i;\theta),p_z(I_i)) \\
\theta &amp;= \mathop{\arg \min} \limits_{\theta}\mathcal{L}
\end{aligned}
\]</div><p></p><p>得到最终的生成器网络参数<span class="math inline">\(\theta\)</span>。</p>
<p>但是但是但是，对于<strong>多元变量的概率分布</strong>而言。计算复杂度是巨大的。假设一张<code>MNIST</code>的手写数据集照，它的维度是<code>28*28=784</code>的，用上述方法统计它的频率，那划分的区间就是：</p>
<p></p><div class="math display">\[\mathop{\{0,1\},\{0,1\},\dots, \{0,1\}}\limits_{782}
\]</div><p></p><p><span class="math inline">\(2^{784} \approx 10^{236}\)</span>量级！</p>
<p>还是换个方法吧...</p>
<h4 id="3-神经距离">3. 神经距离</h4>
<p>说白了，距离不就是</p>
<p></p><div class="math display">\[\mathcal{L}=f(\{y_i\}_{i=1}^M, \{z_i\}_{i=1}^N)
\]</div><p></p><p>只不过也是一个映射关系，也是一个函数罢了。</p>
<p>对了，上神经网络吧，我让<strong>神经网络帮我打分</strong>，帮我看他们的<strong>分布差距</strong>！</p>
<p></p><div class="math display">\[\mathcal{L}=f(\{y_i\}_{i=1}^M, \{z_i\}_{i=1}^N; \Theta)
\]</div><p></p><blockquote>
<p>[!NOTE]</p>
<p>GAN的高明之处就在于，连<strong>距离都是暴力求解</strong>的，相比VAE使用KL散度去衡量<span class="math inline">\(q_\phi(z|x)\)</span>和<span class="math inline">\(p_\theta(z|x)\)</span>之间的距离，VAE简直太文艺了。尤其注意一点，我这里也<strong>不使用</strong><span class="math inline">\(p_y(I_i)\)</span><strong>频率</strong>去<strong>估计</strong>给出样本的<strong>分布</strong>了，把这步变换直接放入神经网络自己算去吧。</p>
</blockquote>
<p>但是这种算法，我应该保障：<code>输入顺序的改变（shuffle）不改变数据的分布</code>，所以需要进行一步<code>无序化</code>的操作，这类操作在<strong>计算离散的样本之间的分布距离</strong>（尤其是在<strong>聚类</strong>任务中）经常用到：</p>
<p></p><div class="math display">\[\mathcal{L}=\frac{1}{M!}\sum^{对所有y_1,\dots,y_M可能的排列求和}_{j=1}D(\{y_i\}_{i=1}^M, \{z_i\}_{i=1}^N; \Theta)
\]</div><p></p><p>若<span class="math inline">\(D(y_i,\Theta)\)</span>表示<span class="math inline">\(y_i\)</span>到聚类中心<span class="math inline">\(\Theta\)</span>的欧氏距离，则上述<span class="math inline">\(\mathcal{L}\)</span>表示簇内平均距离；<br>
若<span class="math inline">\(D(y_i,\Theta)\)</span>表示<span class="math inline">\(y_i\)</span>与理论分布<span class="math inline">\(q_{\Theta}(y)\)</span>的负对数似然，则上述<span class="math inline">\(\mathcal{L}\)</span>表示近似KL散度；<br>
上述是<strong>大数定理</strong>的体现，宗旨是<strong>数量足够多的时候，个体差异的均值可以表征整体差异状态</strong>。</p>
<p></p><div class="math display">\[\begin{aligned}
D(y_i,\Theta)&amp;=-log q_{\Theta}(y_i)，表示与理论分布的负对数似然\\
\{y_i\}_{i=1}^M &amp;\sim P_{emp}，表示离散样本假设服从一个经验分布，Q_{\Theta}表示理论分布\\
KL(P_{emp}||Q_{\Theta})&amp;=\int P_{emp}log\frac{P_{emp}}{Q_{\Theta}}=\int P_{emp}logP_{emp}-\int P_{emp}logQ_{\Theta}\\
&amp;=H(P_{emp})-\mathbb{E}_{y \sim P_{emp}}[logq_{\Theta}(y)] \\
&amp;\approx -\sum_{i=1}^M\frac{1}{M}log\frac{1}{M}-\frac{1}{M}\sum_{i=1}^Mlogq_{\Theta}(y_i)\\
&amp;=logM+\frac{1}{M}\sum_{i=1}^MD(y_i,\Theta)\\
&amp;\approx \frac{1}{M}\sum D(y_i,\Theta),M\rightarrow\infty

\end{aligned}
\]</div><p></p><blockquote>
<p>[!NOTE] 感性认知</p>
<p>一、<span class="math inline">\(-logq_{\Theta}(y_i)\)</span>表示样本在理论分布<span class="math inline">\(Q\)</span>下<strong>的<code>不匹配程度</code>或者说<code>不符合预期程度</code></strong>，具体来说：</p>
<ol>
<li>如果<span class="math inline">\(q_{\Theta}(y_i)\)</span>越大，表示<span class="math inline">\(y_i\)</span>满足这个分布的可能性越高，但是<span class="math inline">\(-logq_{\Theta}(y_i)\)</span>会越小， 表示<code>符合预期</code>，表示<code>意外程度低</code>；</li>
<li>反之，如果<span class="math inline">\(q_{\Theta}(y_i)\)</span>越小，表示<span class="math inline">\(y_i\)</span>满足这个分布的可能性越低，但是<span class="math inline">\(-logq_{\Theta}(y_i)\)</span>会越大， 表示<code>不符合预期</code>，表示`意外程度高；</li>
</ol>
<p>二、交叉熵<span class="math inline">\(H(P,Q)=-\int P_{emp}logQ_{\Theta}=-\mathbb{E}_{y \sim P_{emp}}[logq_{\Theta}(y)]\approx\frac{1}{M}\sum_{i=1}^Mlog-q_{\Theta}(y_i)\)</span>，其实<strong>表征了所有样本的负对数似然的平均值</strong>，表示<code>用分布Q编码真实分布P的样本所需要的平均信息量</code>；</p>
<p>三、KL散度<span class="math inline">\(KL(P||Q)=H(P,Q)-H(P)\)</span>，本质上是，去除<code>P</code>本身的不确定性（熵的定义）后，Q对于P的额外信息损失，是<strong>衡量分布差异的度量</strong>（因此也可以理解<span class="math inline">\(KL(P||Q)\)</span>和<span class="math inline">\(KL(Q||P)\)</span>不一样啦~）</p>
</blockquote>
<p>证明到此结束，我想说的是，在<strong>数据量够多</strong>的情况下，直接使用</p>
<p></p><div class="math display">\[\mathcal{L}=\frac{1}{M}\sum D(\{y_i\}_{i=1}^M, \{z_i\}_{i=1}^N; \Theta)
\]</div><p></p><p>来估计两个离散分布之间的距离是符合大数定理的，可以这么搞。</p>
<h4 id="4-gan就完了">4. GAN就完了</h4>
<p>由于<span class="math inline">\(\{z_i\}_{i=1}^M\)</span>是固定的，变数都在于由<span class="math inline">\(\theta\)</span>生成的<span class="math inline">\(\{y_i\}_{i=1}^M\)</span>序列上，所以先简写为</p>
<p></p><div class="math display">\[\mathcal{L}=\frac{1}{M}\sum_{i=1}^M D(y_i; \Theta)
\]</div><p></p><p>再回顾一下<span class="math inline">\(D(y_i; \Theta)\)</span>和<span class="math inline">\(Y=G(X;\theta)\)</span>的定义：</p>
<ol>
<li><span class="math inline">\(Y=G(X;\theta)\)</span>是指从指定分布<span class="math inline">\(X\)</span>中推导目标<span class="math inline">\(Y\)</span>的过程；</li>
<li><span class="math inline">\(D(y_i; \Theta)\)</span>是指生成的<span class="math inline">\(Y\)</span>与已有样本<span class="math inline">\(Z\)</span>之间的距离，由于样本都是离散的，所以通过大量样本的平均距离代替分布之间的距离。</li>
</ol>
<p>因此，<span class="math inline">\(\mathcal{L}\)</span>就是表征两个分布之间的差异程度，<strong>可以想象为<code>Entrophy</code>的打分机制，分数越高越表示二者之间的差距越大【1：假】，分数越低表示二者之间的差距越小【0：真】</strong>。</p>
<blockquote>
<p>[!NOTE]</p>
<p>值得注意的是，1和0并不总代表着真和假，它的<strong>含义是人为赋予</strong>的。这点在代码中的损失函数构建中也可见一斑（无论是否加<code>负号</code>，都是成立的）。</p>
</blockquote>
<p>根据GAN中生成器<code>G</code>、判别器<code>D</code>的概念，<code>G</code>希望生成得到的样本足够真实，即生成得到的<span class="math inline">\(y_i\)</span>输入<code>D</code>中的分数<span class="math inline">\(\mathcal{L}\)</span>足够小（当等于0时，完全<strong>是真的</strong>）；而<code>D</code>则希望生成得到的<span class="math inline">\(y_i\)</span>输入后得到的分数<span class="math inline">\(\mathcal{L}\)</span>足够大（当等于1时，完全<strong>可以区分</strong>），并且希望真实样本<span class="math inline">\(z_i\)</span>输入得到的分数<span class="math inline">\(\mathcal{L}\)</span>足够小（希望<strong>真的可以被识别为真的</strong>）。<strong>因此这里就存在了<code>GAN</code>中<code>Adversarial</code>对抗的含义</strong>。</p>
<p>并且也引出训练过程：</p>
<p>1.固定生成器<span class="math inline">\(G(X;\theta)\)</span>，生成一批样本<span class="math inline">\(Y\)</span>，得到目标函数：</p>
<p></p><div class="math display">\[\begin{aligned} 
\Theta &amp;=\mathop{\arg \max} \limits_{\Theta}L_1\\
&amp;=\mathop{\arg \max} \limits_{\Theta}\frac{1}{B}\sum_{i=1}^B[D(y_i,\Theta)-D(z_i,\Theta)]
\end{aligned}
\]</div><p></p><blockquote>
<p>[!NOTE]</p>
<p>这里的<code>B</code>指的是批次数<code>batchsize</code>，个人理解这是大数定理影响下的<strong>无奈妥协之举</strong>。</p>
</blockquote>
<p>2.固定判别器<span class="math inline">\(D(Y;\Theta)\)</span>，通过训练<span class="math inline">\(\theta\)</span>使得生成样本越加逼真：</p>
<p></p><div class="math display">\[\begin{aligned} 
\theta &amp;=\mathop{\arg \min} \limits_{\theta}L_2\\
&amp;=\mathop{\arg \min} \limits_{\theta}\frac{1}{B}\sum_{i=1}^B[D(G(x_i,\theta),\Theta)]
\end{aligned}
\]</div><p></p><blockquote>
<p>[!NOTE]</p>
<ol>
<li><span class="math inline">\(L_1\)</span>是<strong>真伪样本分布差（的最大值）</strong>，那么<span class="math inline">\(L_1\)</span>越小表示”伪造“样本质量越好，所以<span class="math inline">\(L_1\)</span>是表征训练程度的指标，<span class="math inline">\(L_1\)</span>越小表示训练的越好；</li>
<li>将<span class="math inline">\(L_1\)</span>作为核心指标的原因也和训练过程有关，由于首先需要生成一批样本，所以如果<strong>生成器太弱<span class="math inline">\(L_1\)</span>过大的话</strong>，<span class="math inline">\(L_2\)</span>会被要求<strong>快速的下降</strong>来使得<strong>惩罚尽可能的小</strong>，但是这也可能会导致生成器生成一些<strong>重复、保底</strong>的图片，对于训练不是很友好；所以一定程度上要求生成器强一些，<span class="math inline">\(L_1\)</span>一开始就比较小，这样才能<strong>倒逼</strong><span class="math inline">\(L_2\)</span>不大且往更精细（更小）的方向去；</li>
</ol>
</blockquote>
<h4 id="5-正则约束wgan">5. 正则约束—WGAN</h4>
<p>如果不给<code>D</code>加上约束，会使得<span class="math inline">\(L_1\)</span>值<span class="math inline">\(\rightarrow\infty\)</span>，因为<span class="math inline">\(D(z_i,\Theta)\)</span>就会变得很小以证明判别器觉得它是真的。</p>
<p>通常来说，可以在最后加上<code>sigmod</code>来使得最终输出约束在<span class="math inline">\([0,1]\)</span>的区间内，但是由于<span class="math inline">\(L_1\)</span>值<span class="math inline">\(\rightarrow\infty\)</span>，会导致那段的梯度特别小，无法实现梯度的更新。（例如：<span class="math inline">\(x=10\)</span>，Sigmoid输出是<span class="math inline">\(0.9999\)</span>，导数是<span class="math inline">\(0.0001\)</span>。那么<span class="math inline">\(-\nabla_{\theta}D(G(z))\)</span>会由于<span class="math inline">\(D(G(z)) \approx 0(1)\)</span>而导致梯度无法有效回传）</p>
<p><code>WGAN</code>从原理出发，通过如下约束：</p>
<blockquote>
<p>距离是为了表明两个对象的差距，而如果对象产生微小的变化，那么距离的波动也不能太大。</p>
</blockquote>
<p></p><div class="math display">\[\begin{aligned}
\Vert D(y_i,\theta)-D(y^{'}_i,\theta)\Vert &amp;\leq C\Vert y_i-y^{'}_i\Vert^\alpha \\
\Vert D(y_i,\theta)-D(y^{'}_i,\theta)\Vert &amp;\leq C\Vert y_i-y^{'}_i\Vert, \alpha=1 \\
\Vert \frac{\partial D(y,\Theta)}{\partial y}\Vert &amp;\leq C
\end{aligned}
\]</div><p></p><p>上述就是常见的<code>Lipschitz（利普西茨）</code>约束，满足这个约束可以整体的稳定性得到提升，不会出现对<span class="math inline">\(y\)</span>的一点变动导致距离极大变化的问题（也是在拆东墙补西墙，弥补由<strong>神经网络度量距离</strong>的问题）</p>
<p></p><div class="math display">\[\begin{aligned}
\Theta&amp;=\mathop{\arg \max}\limits_{\Theta}\frac{1}{B}\sum_{i=1}^B[D(y_i,\Theta)-D(z_i,\Theta)] \\
&amp;=\mathop{\arg \min}\limits_{\Theta}\frac{1}{B}\sum_{i=1}^B[D(z_i,\Theta)-D(y_i,\Theta)]
\end{aligned}
\]</div><p></p><p>可以通过把上述利普西茨约束放在<span class="math inline">\(L_1\)</span>项中：</p>
<p></p><div class="math display">\[\begin{aligned}
\Theta&amp;=\mathop{\arg \max}\limits_{\Theta}\frac{1}{B}\sum_{i=1}^B[D(y_i,\Theta)-D(z_i,\Theta)]+\lambda \max(\Vert \frac{\partial D(y,\Theta)}{\partial y}\Vert,1) \\
\Theta&amp;=\mathop{\arg \max}\limits_{\Theta}\frac{1}{B}\sum_{i=1}^B[D(y_i,\Theta)-D(z_i,\Theta)]+\lambda (\Vert \frac{\partial D(y,\Theta)}{\partial y}\Vert-1)^2
\end{aligned}
\]</div><p></p><p>后者是<code>WGAN</code>原作者提出的损失函数，通过一个弱正则项对变动起到约束作用。</p>
<p>具体计算时，偏导数通过离散化计算：</p>
<p></p><div class="math display">\[\begin{aligned}
\Theta&amp;=\mathop{\arg \max}\limits_{\Theta}\frac{1}{B}\sum_{i=1}^B[D(y_i,\Theta)-D(z_i,\Theta)]+\frac{\lambda}{B} \sum_{i=1}^B\max( \frac{\vert D(y_{i,1},\Theta)-D(y_{i,2},\Theta)\vert}{\Vert y_{i,1}-y_{i,2} \Vert},1) \\
\Theta&amp;=\mathop{\arg \max}\limits_{\Theta}\frac{1}{B}\sum_{i=1}^B[D(y_i,\Theta)-D(z_i,\Theta)]+\frac{\lambda}{B} \sum_{i=1}^B( \frac{\vert D(y_{i,1},\Theta)-D(y_{i,2},\Theta)\vert}{\Vert y_{i,1}-y_{i,2} \Vert}-1)^2
\end{aligned}
\]</div><p></p><p>其中<span class="math inline">\(y_{i,j}=\epsilon_{i,j}y_i+(1-\epsilon_{i,j})z_i\)</span>，<span class="math inline">\(\epsilon_{i,j} \sim U[0,1]\)</span>是随机数，<span class="math inline">\(j\)</span>表示插值步数，上述为2。</p>
<blockquote>
<p>[!NOTE]</p>
<p>对<span class="math inline">\(y_i\)</span>和<span class="math inline">\(z_i\)</span>中间进行插值生成，也是考虑到真实样本和生成样本之间的连接区域是<strong>最有可能违反利普西茨约束</strong>，也是<strong>最有意义对其进行约束</strong>的一个范围。</p>
</blockquote>
<p>至此，由<code>VAE</code>-&gt;<code>GAN</code>-&gt;<code>WGAN</code>的序言就结束了。</p>
<p>总体而言，个人理解<code>VAE</code>是一个很优雅，很唯美的哲学对抗，不同于<code>GAN</code>需要先固定生成器优化判别器，再固定判别器优化生成器，直接通过<code>重构</code>＋<code>正则</code>的方式实现了动态对抗；而<code>GAN</code>的意义在于通过神经网络的方式直接把度量的形式都暴力求解出来了，一改往日需要用<code>KL</code>、<code>JS</code>等散度值衡量分布距离的日子；但是<code>GAN</code>由于使用神经网络度量距离会导致违反<code>Lipschitz</code>约束，即对象微小变动导致输入神经网络后距离剧增的现象，因此在判别器<span class="math inline">\(L_1\)</span>中引入正则项，使尽可能的遵循该约束。</p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.3482485294328704" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-04-09 09:58">2025-04-09 09:57</span>&nbsp;
<a href="https://www.cnblogs.com/SuahiStudy">9镑15便士</a>&nbsp;
阅读(<span id="post_view_count">33</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18816013" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18816013);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18816013', targetLink: 'https://www.cnblogs.com/SuahiStudy/p/18816013', title: '【深度学习】从VAE到GAN漫谈' })">举报</a>
</div>
        