
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/xiao987334176/p/18864903" title="发布于 2025-05-08 19:17">
    <span role="heading" aria-level="2">DeepSeek 多模态模型 Janus-Pro 本地部署</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body blogpost-body-html">
<h1>一、概述</h1>
<p>Janus-Pro是DeepSeek最新开源的多模态模型，是一种新颖的自回归框架，统一了多模态理解和生成。通过将视觉编码解耦为独立的路径，同时仍然使用单一的、统一的变压器架构进行处理，该框架解决了先前方法的局限性。这种解耦不仅缓解了视觉编码器在理解和生成中的角色冲突，还增强了框架的灵活性。Janus-Pro 超过了以前的统一模型，并且匹配或超过了特定任务模型的性能。</p>
<p>代码链接：<a href="https://github.com/deepseek-ai/Janus" target="_blank" rel="noopener nofollow">https://github.com/deepseek-ai/Janus</a></p>
<p>模型链接：<a href="https://modelscope.cn/collections/Janus-Pro-0f5e48f6b96047" target="_blank" rel="noopener nofollow">https://modelscope.cn/collections/Janus-Pro-0f5e48f6b96047</a></p>
<p>体验页面：<a href="https://modelscope.cn/studios/AI-ModelScope/Janus-Pro-7B" target="_blank" rel="noopener nofollow">https://modelscope.cn/studios/AI-ModelScope/Janus-Pro-7B</a></p>
<p>&nbsp;</p>
<h1>二、虚拟环境</h1>
<h2>环境说明</h2>
<p>本文使用WSL2运行的ubuntu系统来进行演示，参考链接：https://www.cnblogs.com/xiao987334176/p/18864140</p>
<h2>创建虚拟环境</h2>
<div class="cnblogs_code">
<pre>conda create --name vll-Janus-Pro-7B python=<span style="color: rgba(128, 0, 128, 1)">3.12</span>.<span style="color: rgba(128, 0, 128, 1)">7</span></pre>
</div>
<p>&nbsp;</p>
<p>激活虚拟环境，执行命令：</p>
<div class="cnblogs_code">
<pre>conda activate vll-Janus-Pro-7B</pre>
</div>
<p>&nbsp;</p>
<p>查看CUDA版本，执行命令：</p>
<div class="cnblogs_code">
<pre># <span style="color: rgba(255, 0, 0, 1)">nvcc -</span><span style="color: rgba(0, 0, 0, 1)"><span style="color: rgba(255, 0, 0, 1)">V</span>
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) </span><span style="color: rgba(128, 0, 128, 1)">2005</span>-<span style="color: rgba(128, 0, 128, 1)">2025</span><span style="color: rgba(0, 0, 0, 1)"> NVIDIA Corporation
Built on Wed_Jan_15_19:</span><span style="color: rgba(128, 0, 128, 1)">20</span><span style="color: rgba(0, 0, 0, 1)">:09_PST_2025
Cuda compilation tools, release </span><span style="color: rgba(128, 0, 128, 1)">12.8</span>, V12.<span style="color: rgba(128, 0, 128, 1)">8.61</span><span style="color: rgba(0, 0, 0, 1)">
Build cuda_12.</span><span style="color: rgba(128, 0, 128, 1)">8</span>.r12.<span style="color: rgba(128, 0, 128, 1)">8</span>/compiler.35404655_0</pre>
</div>
<h1>三、安装Janus-Pro</h1>
<p>创建项目目录</p>
<div class="cnblogs_code">
<pre><span style="color: rgba(0, 0, 255, 1)">mkdir</span><span style="color: rgba(0, 0, 0, 1)"> vllm
cd vllm</span></pre>
</div>
<p>&nbsp;</p>
<p>克隆代码</p>
<div class="cnblogs_code">
<pre>git clone https:<span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)">github.com/deepseek-ai/Janus</span></pre>
</div>
<p>&nbsp;</p>
<p>安装依赖包，注意：这里要手动安装pytorch，指定版本。</p>
<div class="cnblogs_code">
<pre>pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128</pre>
</div>
<p>安装其他依赖组件</p>
<div class="cnblogs_code">
<pre>pip3 install transformers attrdict einops timm</pre>
</div>
<p>&nbsp;</p>
<p>下载模型</p>
<p>可以用modelscope下载，安装modelscope，命令如下：</p>
<div class="cnblogs_code">
<pre>pip <span style="color: rgba(0, 0, 255, 1)">install</span><span style="color: rgba(0, 0, 0, 1)"> modelscope

modelscope download </span>--model deepseek-ai/Janus-Pro-7B</pre>
</div>
<p>效果如下：</p>
<div class="cnblogs_code">
<pre># <span style="color: rgba(255, 0, 0, 1)">modelscope download --model deepseek-ai/Janus-Pro-</span><span style="color: rgba(0, 0, 0, 1)"><span style="color: rgba(255, 0, 0, 1)">7B</span>
Downloading Model from https:</span><span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)">www.modelscope.cn to directory: /root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B</span>
Downloading [config.json]: <span style="color: rgba(128, 0, 128, 1)">100</span>%|███████████████████████████████████████████████████| <span style="color: rgba(128, 0, 128, 1)">1</span>.42k/<span style="color: rgba(128, 0, 128, 1)">1</span>.42k [<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>&lt;<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>, <span style="color: rgba(128, 0, 128, 1)">5</span>.29kB/<span style="color: rgba(0, 0, 0, 1)">s]
Downloading [configuration.json]: </span><span style="color: rgba(128, 0, 128, 1)">100</span>%|████████████████████████████████████████████████| <span style="color: rgba(128, 0, 128, 1)">68.0</span>/<span style="color: rgba(128, 0, 128, 1)">68.0</span> [<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>&lt;<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>, 221B/<span style="color: rgba(0, 0, 0, 1)">s]
Downloading [README.md]: </span><span style="color: rgba(128, 0, 128, 1)">100</span>%|█████████████████████████████████████████████████████| <span style="color: rgba(128, 0, 128, 1)">2</span>.49k/<span style="color: rgba(128, 0, 128, 1)">2</span>.49k [<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>&lt;<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>, <span style="color: rgba(128, 0, 128, 1)">7</span>.20kB/<span style="color: rgba(0, 0, 0, 1)">s]
Downloading [processor_config.json]: </span><span style="color: rgba(128, 0, 128, 1)">100</span>%|███████████████████████████████████████████████| <span style="color: rgba(128, 0, 128, 1)">210</span>/<span style="color: rgba(128, 0, 128, 1)">210</span> [<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>&lt;<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>, 590B/<span style="color: rgba(0, 0, 0, 1)">s]
Downloading [janus_pro_teaser1.png]: </span><span style="color: rgba(128, 0, 128, 1)">100</span>%|██████████████████████████████████████████| <span style="color: rgba(128, 0, 128, 1)">95</span>.7k/<span style="color: rgba(128, 0, 128, 1)">95</span>.7k [<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>&lt;<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>, 267kB/<span style="color: rgba(0, 0, 0, 1)">s]
Downloading [preprocessor_config.json]: </span><span style="color: rgba(128, 0, 128, 1)">100</span>%|████████████████████████████████████████████| <span style="color: rgba(128, 0, 128, 1)">346</span>/<span style="color: rgba(128, 0, 128, 1)">346</span> [<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>&lt;<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>, 867B/<span style="color: rgba(0, 0, 0, 1)">s]
Downloading [janus_pro_teaser2.png]: </span><span style="color: rgba(128, 0, 128, 1)">100</span>%|███████████████████████████████████████████| 518k/518k [<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>&lt;<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>, <span style="color: rgba(128, 0, 128, 1)">1</span>.18MB/<span style="color: rgba(0, 0, 0, 1)">s]
Downloading [special_tokens_map.json]: </span><span style="color: rgba(128, 0, 128, 1)">100</span>%|███████████████████████████████████████████| <span style="color: rgba(128, 0, 128, 1)">344</span>/<span style="color: rgba(128, 0, 128, 1)">344</span> [<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>&lt;<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>, <span style="color: rgba(128, 0, 128, 1)">1</span>.50kB/<span style="color: rgba(0, 0, 0, 1)">s]
Downloading [tokenizer_config.json]: </span><span style="color: rgba(128, 0, 128, 1)">100</span>%|███████████████████████████████████████████████| <span style="color: rgba(128, 0, 128, 1)">285</span>/<span style="color: rgba(128, 0, 128, 1)">285</span> [<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>&lt;<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>, 926B/<span style="color: rgba(0, 0, 0, 1)">s]
Downloading [pytorch_model.bin]:   </span><span style="color: rgba(128, 0, 128, 1)">0</span>%|▏                                            | <span style="color: rgba(128, 0, 128, 1)">16.0M</span>/<span style="color: rgba(128, 0, 128, 1)">3</span>.89G [<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>&lt;<span style="color: rgba(128, 0, 128, 1)">03</span>:<span style="color: rgba(128, 0, 128, 1)">55</span>, <span style="color: rgba(128, 0, 128, 1)">17</span>.7MB/<span style="color: rgba(0, 0, 0, 1)">s]
Downloading [tokenizer.json]: </span><span style="color: rgba(128, 0, 128, 1)">100</span>%|████████████████████████████████████████████████| <span style="color: rgba(128, 0, 128, 1)">4.50M</span>/<span style="color: rgba(128, 0, 128, 1)">4.50M</span> [<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>&lt;<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>, <span style="color: rgba(128, 0, 128, 1)">6</span>.55MB/<span style="color: rgba(0, 0, 0, 1)">s]
Processing </span><span style="color: rgba(128, 0, 128, 1)">11</span> items:  <span style="color: rgba(128, 0, 128, 1)">91</span>%|█████████████████████████████████████████████████████▋     | <span style="color: rgba(128, 0, 128, 1)">10.0</span>/<span style="color: rgba(128, 0, 128, 1)">11.0</span> [<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">19</span>&lt;<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>, <span style="color: rgba(128, 0, 128, 1)">14</span>.1it/s]<br>Downloading [pytorch_model.bin]: <span style="color: rgba(128, 0, 128, 1)">100</span>%|█████████████████████████████████████████████| <span style="color: rgba(128, 0, 128, 1)">3</span>.89G/<span style="color: rgba(128, 0, 128, 1)">3</span>.89G [<span style="color: rgba(128, 0, 128, 1)">09</span>:<span style="color: rgba(128, 0, 128, 1)">18</span>&lt;<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>, <span style="color: rgba(128, 0, 128, 1)">7</span>.48MB/<span style="color: rgba(0, 0, 0, 1)">s]
Processing </span><span style="color: rgba(128, 0, 128, 1)">11</span> items: <span style="color: rgba(128, 0, 128, 1)">100</span>%|███████████████████████████████████████████████████████████| <span style="color: rgba(128, 0, 128, 1)">11.0</span>/<span style="color: rgba(128, 0, 128, 1)">11.0</span> [<span style="color: rgba(128, 0, 128, 1)">09</span>:<span style="color: rgba(128, 0, 128, 1)">24</span>&lt;<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>, <span style="color: rgba(128, 0, 128, 1)">51</span>.3s/it]</pre>
</div>
<p>可以看到下载目录为/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B</p>
<p>&nbsp;</p>
<p>把下载的模型移动到vllm目录里面</p>
<div class="cnblogs_code">
<pre><span style="color: rgba(0, 0, 255, 1)">mv</span> /root/.cache/modelscope/hub/models/deepseek-ai /home/xiao/vllm</pre>
</div>
<p>&nbsp;</p>
<h1>四、测试图片理解</h1>
<p>vllm目录有2个文件夹，结构如下：</p>
<div class="cnblogs_code">
<pre><span style="color: rgba(0, 0, 0, 1)"># ll
total </span><span style="color: rgba(128, 0, 128, 1)">20</span><span style="color: rgba(0, 0, 0, 1)">
drwxr</span>-xr-x <span style="color: rgba(128, 0, 128, 1)">4</span> root root <span style="color: rgba(128, 0, 128, 1)">4096</span> May  <span style="color: rgba(128, 0, 128, 1)">8</span> <span style="color: rgba(128, 0, 128, 1)">18</span>:<span style="color: rgba(128, 0, 128, 1)">59</span> ./<span style="color: rgba(0, 0, 0, 1)">
drwxr</span>-x--- <span style="color: rgba(128, 0, 128, 1)">5</span> xiao xiao <span style="color: rgba(128, 0, 128, 1)">4096</span> May  <span style="color: rgba(128, 0, 128, 1)">8</span> <span style="color: rgba(128, 0, 128, 1)">14</span>:<span style="color: rgba(128, 0, 128, 1)">50</span> ../<span style="color: rgba(0, 0, 0, 1)">
drwxr</span>-xr-x <span style="color: rgba(128, 0, 128, 1)">8</span> root root <span style="color: rgba(128, 0, 128, 1)">4096</span> May  <span style="color: rgba(128, 0, 128, 1)">8</span> <span style="color: rgba(128, 0, 128, 1)">18</span>:<span style="color: rgba(128, 0, 128, 1)">59</span> Janus/<span style="color: rgba(0, 0, 0, 1)">
drwxr</span>-xr-x <span style="color: rgba(128, 0, 128, 1)">4</span> root root <span style="color: rgba(128, 0, 128, 1)">4096</span> May  <span style="color: rgba(128, 0, 128, 1)">8</span> <span style="color: rgba(128, 0, 128, 1)">16</span>:<span style="color: rgba(128, 0, 128, 1)">01</span> deepseek-ai/</pre>
</div>
<p>&nbsp;</p>
<p>进入deepseek-ai目录，会看到一个文件夹Janus-Pro-7B</p>
<div class="cnblogs_code">
<pre><span style="color: rgba(0, 0, 0, 1)"># ll
total </span><span style="color: rgba(128, 0, 128, 1)">16</span><span style="color: rgba(0, 0, 0, 1)">
drwxr</span>-xr-x <span style="color: rgba(128, 0, 128, 1)">4</span> root root <span style="color: rgba(128, 0, 128, 1)">4096</span> May  <span style="color: rgba(128, 0, 128, 1)">8</span> <span style="color: rgba(128, 0, 128, 1)">16</span>:<span style="color: rgba(128, 0, 128, 1)">01</span> ./<span style="color: rgba(0, 0, 0, 1)">
drwxr</span>-xr-x <span style="color: rgba(128, 0, 128, 1)">4</span> root root <span style="color: rgba(128, 0, 128, 1)">4096</span> May  <span style="color: rgba(128, 0, 128, 1)">8</span> <span style="color: rgba(128, 0, 128, 1)">18</span>:<span style="color: rgba(128, 0, 128, 1)">59</span> ../<span style="color: rgba(0, 0, 0, 1)">
drwxr</span>-xr-x <span style="color: rgba(128, 0, 128, 1)">2</span> root root <span style="color: rgba(128, 0, 128, 1)">4096</span> May  <span style="color: rgba(128, 0, 128, 1)">7</span> <span style="color: rgba(128, 0, 128, 1)">18</span>:<span style="color: rgba(128, 0, 128, 1)">32</span> Janus-Pro-7B/</pre>
</div>
<p>&nbsp;</p>
<p>返回上一级，在Janus目录，创建image_understanding.py文件，代码如下：</p>
<div class="cnblogs_code">
<pre>import<span> torch
from transformers import<span> AutoModelForCausalLM
from janus.models import<span> MultiModalityCausalLM, VLChatProcessor
from janus.utils.io import<span> load_pil_images

model_path = "../deepseek-ai/Janus-Pro-7B"<span>

image='aa.jpeg'<span>
question='请说明一下这张图片'<span>
vl_chat_processor: VLChatProcessor =<span> VLChatProcessor.from_pretrained(model_path)
tokenizer =<span> vl_chat_processor.tokenizer

vl_gpt: MultiModalityCausalLM =<span> AutoModelForCausalLM.from_pretrained(
    model_path, trust_remote_code=<span>True
)
vl_gpt =<span> vl_gpt.to(torch.bfloat16).cuda().eval()

conversation =<span> [
    {
        "role": "&lt;|User|&gt;"<span>,
        "content": f"&lt;image_placeholder&gt;\n{question}"<span>,
        "images"<span>: [image],
    },
    {"role": "&lt;|Assistant|&gt;", "content": ""<span>},
]

# load images and prepare for inputs
pil_images =<span> load_pil_images(conversation)
prepare_inputs =<span> vl_chat_processor(
    conversations=conversation, images=pil_images, force_batchify=<span>True
).to(vl_gpt.device)

# # run image encoder to get the image embeddings
inputs_embeds = vl_gpt.prepare_inputs_embeds(**<span>prepare_inputs)

# # run the model to get the response
outputs =<span> vl_gpt.language_model.generate(
    inputs_embeds=<span>inputs_embeds,
    attention_mask=<span>prepare_inputs.attention_mask,
    pad_token_id=<span>tokenizer.eos_token_id,
    bos_token_id=<span>tokenizer.bos_token_id,
    eos_token_id=<span>tokenizer.eos_token_id,
    max_new_tokens=512<span>,
    do_sample=<span>False,
    use_cache=<span>True,
)

answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=<span>True)
print(f"{prepare_inputs['sft_format'][0]}", answer)</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></pre>
</div>
<p>&nbsp;</p>
<p>下载一张图片，地址：https://pics6.baidu.com/feed/09fa513d269759ee74c8d049640fcc1b6f22df9e.jpeg</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202505/1341090-20250508192923746-2079946853.jpg" alt=""></p>
<p>将此图片，重命名为aa.jpeg，存放在Janus目录</p>
<p>&nbsp;</p>
<p>最终Janus目录，文件如下：</p>
<div class="cnblogs_code">
<pre><span># ll
total 2976<span>
drwxr-xr-x 8 root root    4096 May  8 18:59 ./<span>
drwxr-xr-x 4 root root    4096 May  8 18:59 ../<span>
drwxr-xr-x 8 root root    4096 May  7 18:11 .git/
-rw-r--r-- 1 root root     115 May  7 18:11<span> .gitattributes
-rw-r--r-- 1 root root    7301 May  7 18:11<span> .gitignore
-rw-r--r-- 1 root root    1065 May  7 18:11 LICENSE-<span>CODE
-rw-r--r-- 1 root root   13718 May  7 18:11 LICENSE-<span>MODEL
-rw-r--r-- 1 root root    3069 May  7 18:11<span> Makefile
-rwxr-xr-x 1 root root   26781 May  7 18:11 README.md*
-rw-r--r-- 1 root root   62816 May  8 14:59<span> aa.jpeg
drwxr-xr-x 2 root root    4096 May  7 18:11 demo/<span>
drwxr-xr-x 2 root root    4096 May  8 17:19 generated_samples/
-rw-r--r-- 1 root root    4515 May  7 18:11<span> generation_inference.py
-rw-r--r-- 1 xiao xiao    4066 May  8 18:50<span> image_generation.py
-rw-r--r-- 1 root root    1594 May  8 18:58<span> image_understanding.py
drwxr-xr-x 2 root root    4096 May  7 18:11 images/
-rw-r--r-- 1 root root    2642 May  7 18:11<span> inference.py
-rw-r--r-- 1 root root    5188 May  7 18:11<span> interactivechat.py
drwxr-xr-x 6 root root    4096 May  7 19:01 janus/<span>
drwxr-xr-x 2 root root    4096 May  7 18:11 janus.egg-info/
-rw-r--r-- 1 root root 2846268 May  7 18:11<span> janus_pro_tech_report.pdf
-rw-r--r-- 1 root root    1111 May  7 18:11<span> pyproject.toml
-rw-r--r-- 1 root root     278 May  7 18:11 requirements.txt</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></pre>
</div>
<p>&nbsp;</p>
<p>运行代码，效果如下：</p>
<div class="cnblogs_code">
<pre><span style="color: rgba(0, 0, 0, 1)"># <span style="color: rgba(255, 0, 0, 1)">python image_understanding.py</span>
Python version is above </span><span style="color: rgba(128, 0, 128, 1)">3.10</span><span style="color: rgba(0, 0, 0, 1)">, patching the collections module.
</span>/root/anaconda3/envs/vll-Janus-Pro-7B/lib/python3.<span style="color: rgba(128, 0, 128, 1)">12</span>/site-packages/transformers/models/auto/image_processing_auto.py:<span style="color: rgba(128, 0, 128, 1)">604</span>: FutureWarning: The image_processor_class argument is deprecated and will be removed <span style="color: rgba(0, 0, 255, 1)">in</span> v4.<span style="color: rgba(128, 0, 128, 1)">42</span><span style="color: rgba(0, 0, 0, 1)">. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast</span>=True` will be the default behavior <span style="color: rgba(0, 0, 255, 1)">in</span> v4.<span style="color: rgba(128, 0, 128, 1)">52</span>, even <span style="color: rgba(0, 0, 255, 1)">if</span> the model was saved with a slow processor. This will result <span style="color: rgba(0, 0, 255, 1)">in</span> minor differences <span style="color: rgba(0, 0, 255, 1)">in</span> outputs. You<span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">ll still be able to use a slow processor with `use_fast=False`.</span>
You are using the default legacy behaviour of the &lt;class <span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast</span><span style="color: rgba(128, 0, 0, 1)">'</span>&gt;. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes <span style="color: rgba(0, 0, 255, 1)">for</span> you. If you want to use the new behaviour, set `legacy=False`. This should only be set <span style="color: rgba(0, 0, 255, 1)">if</span> you understand what it means, and thoroughly read the reason why this was added as explained <span style="color: rgba(0, 0, 255, 1)">in</span> https:<span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)">github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.</span>
Loading checkpoint shards: <span style="color: rgba(128, 0, 128, 1)">100</span>%|██████████████████████████████████████████████████████████| <span style="color: rgba(128, 0, 128, 1)">2</span>/<span style="color: rgba(128, 0, 128, 1)">2</span> [<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">10</span>&lt;<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>,  <span style="color: rgba(128, 0, 128, 1)">5</span>.18s/<span style="color: rgba(0, 0, 0, 1)">it]
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.

</span>&lt;|User|&gt;: &lt;image_placeholder&gt;<span style="color: rgba(0, 0, 0, 1)">
请说明一下这张图片

</span>&lt;|Assistant|&gt;: 这张图片展示了一位身穿传统服饰的女性，她正坐在户外，双手合十，闭着眼睛，似乎在进行冥想或祈祷。背景是绿色的树木和植物，阳光透过树叶洒在她的身上，营造出一种宁静、祥和的氛围。她的服装以淡雅的白色和粉色为主，带有精致的花纹，整体风格非常优雅。</pre>
</div>
<p>描述还是比较准确的</p>
<p>&nbsp;</p>
<h1>五、测试图片生成</h1>
<p>在Janus目录，新建image_generation.py脚本，代码如下：</p>
<div class="cnblogs_code">
<pre><span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> os
</span><span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> torch
</span><span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> numpy as np
</span><span style="color: rgba(0, 0, 255, 1)">from</span> PIL <span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> Image
</span><span style="color: rgba(0, 0, 255, 1)">from</span> transformers <span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> AutoModelForCausalLM
</span><span style="color: rgba(0, 0, 255, 1)">from</span> janus.models <span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> MultiModalityCausalLM, VLChatProcessor

model_path </span>= <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">../deepseek-ai/Janus-Pro-7B</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">
vl_chat_processor: VLChatProcessor </span>=<span style="color: rgba(0, 0, 0, 1)"> VLChatProcessor.from_pretrained(model_path)
tokenizer </span>=<span style="color: rgba(0, 0, 0, 1)"> vl_chat_processor.tokenizer

vl_gpt: MultiModalityCausalLM </span>=<span style="color: rgba(0, 0, 0, 1)"> AutoModelForCausalLM.from_pretrained(
    model_path, trust_remote_code</span>=<span style="color: rgba(0, 0, 0, 1)">True
)
vl_gpt </span>=<span style="color: rgba(0, 0, 0, 1)"> vl_gpt.to(torch.bfloat16).cuda().eval()

conversation </span>=<span style="color: rgba(0, 0, 0, 1)"> [
    {</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">role</span><span style="color: rgba(128, 0, 0, 1)">"</span>: <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">&lt;|User|&gt;</span><span style="color: rgba(128, 0, 0, 1)">"</span>, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">content</span><span style="color: rgba(128, 0, 0, 1)">"</span>: <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">超写实8K渲染，一位具有东方古典美的中国女性，瓜子脸，西昌的眉毛如弯弯的月牙，双眼明亮而深邃，犹如夜空中闪烁的星星。高挺的鼻梁，樱桃小嘴微微上扬，透露出一丝诱人的微笑。她的头发如黑色的瀑布般垂直落在减胖两侧，微风轻轻浮动发色。肌肤白皙如雪，在阳光下泛着微微的光泽。她身着乙烯白色的透薄如纱的连衣裙，裙摆在海风中轻轻飘动。</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">},
    {</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">role</span><span style="color: rgba(128, 0, 0, 1)">"</span>: <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">&lt;|Assistant|&gt;</span><span style="color: rgba(128, 0, 0, 1)">"</span>, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">content</span><span style="color: rgba(128, 0, 0, 1)">"</span>: <span style="color: rgba(128, 0, 0, 1)">""</span><span style="color: rgba(0, 0, 0, 1)">},
]

sft_format </span>=<span style="color: rgba(0, 0, 0, 1)"> vl_chat_processor.apply_sft_template_for_multi_turn_prompts(
    conversations</span>=<span style="color: rgba(0, 0, 0, 1)">conversation,
    sft_format</span>=<span style="color: rgba(0, 0, 0, 1)">vl_chat_processor.sft_format,
    system_prompt</span>=<span style="color: rgba(128, 0, 0, 1)">""</span><span style="color: rgba(0, 0, 0, 1)">
)
prompt </span>= sft_format +<span style="color: rgba(0, 0, 0, 1)"> vl_chat_processor.image_start_tag

@torch.inference_mode()
</span><span style="color: rgba(0, 0, 255, 1)">def</span><span style="color: rgba(0, 0, 0, 1)"> generate(
        mmgpt: MultiModalityCausalLM,
        vl_chat_processor: VLChatProcessor,
        prompt: str,
        temperature: float </span>= 1<span style="color: rgba(0, 0, 0, 1)">,
        parallel_size: int </span>= 1, <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 减小 parallel_size</span>
        cfg_weight: float = 5<span style="color: rgba(0, 0, 0, 1)">,
        image_token_num_per_image: int </span>= 576<span style="color: rgba(0, 0, 0, 1)">,
        img_size: int </span>= 384<span style="color: rgba(0, 0, 0, 1)">,
        patch_size: int </span>= 16<span style="color: rgba(0, 0, 0, 1)">,
):
    input_ids </span>=<span style="color: rgba(0, 0, 0, 1)"> vl_chat_processor.tokenizer.encode(prompt)
    input_ids </span>=<span style="color: rgba(0, 0, 0, 1)"> torch.LongTensor(input_ids)

    tokens </span>= torch.zeros((parallel_size * 2, len(input_ids)), dtype=<span style="color: rgba(0, 0, 0, 1)">torch.int).cuda()
    </span><span style="color: rgba(0, 0, 255, 1)">for</span> i <span style="color: rgba(0, 0, 255, 1)">in</span> range(parallel_size * 2<span style="color: rgba(0, 0, 0, 1)">):
        tokens[i, :] </span>=<span style="color: rgba(0, 0, 0, 1)"> input_ids
        </span><span style="color: rgba(0, 0, 255, 1)">if</span> i % 2 !=<span style="color: rgba(0, 0, 0, 1)"> 0:
            tokens[i, </span>1:-1] =<span style="color: rgba(0, 0, 0, 1)"> vl_chat_processor.pad_id

    inputs_embeds </span>=<span style="color: rgba(0, 0, 0, 1)"> mmgpt.language_model.get_input_embeddings()(tokens)

    generated_tokens </span>= torch.zeros((parallel_size, image_token_num_per_image), dtype=<span style="color: rgba(0, 0, 0, 1)">torch.int).cuda()

    </span><span style="color: rgba(0, 0, 255, 1)">for</span> i <span style="color: rgba(0, 0, 255, 1)">in</span><span style="color: rgba(0, 0, 0, 1)"> range(image_token_num_per_image):
        outputs </span>= mmgpt.language_model.model(inputs_embeds=inputs_embeds, use_cache=<span style="color: rgba(0, 0, 0, 1)">True,
                                             past_key_values</span>=outputs.past_key_values <span style="color: rgba(0, 0, 255, 1)">if</span> i != 0 <span style="color: rgba(0, 0, 255, 1)">else</span><span style="color: rgba(0, 0, 0, 1)"> None)
        hidden_states </span>=<span style="color: rgba(0, 0, 0, 1)"> outputs.last_hidden_state

        logits </span>= mmgpt.gen_head(hidden_states[:, -1<span style="color: rgba(0, 0, 0, 1)">, :])
        logit_cond </span>= logits[0::2<span style="color: rgba(0, 0, 0, 1)">, :]
        logit_uncond </span>= logits[1::2<span style="color: rgba(0, 0, 0, 1)">, :]

        logits </span>= logit_uncond + cfg_weight * (logit_cond -<span style="color: rgba(0, 0, 0, 1)"> logit_uncond)
        probs </span>= torch.softmax(logits / temperature, dim=-1<span style="color: rgba(0, 0, 0, 1)">)

        next_token </span>= torch.multinomial(probs, num_samples=1<span style="color: rgba(0, 0, 0, 1)">)
        generated_tokens[:, i] </span>= next_token.squeeze(dim=-1<span style="color: rgba(0, 0, 0, 1)">)
        next_token </span>= torch.cat([next_token.unsqueeze(dim=1<span style="color: rgba(0, 0, 0, 1)">),
                                next_token.unsqueeze(dim</span>=1)], dim=1).view(-1<span style="color: rgba(0, 0, 0, 1)">)
        img_embeds </span>=<span style="color: rgba(0, 0, 0, 1)"> mmgpt.prepare_gen_img_embeds(next_token)
        inputs_embeds </span>= img_embeds.unsqueeze(dim=1<span style="color: rgba(0, 0, 0, 1)">)
        </span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 添加显存清理</span>
        <span style="color: rgba(0, 0, 255, 1)">del</span><span style="color: rgba(0, 0, 0, 1)"> logits, logit_cond, logit_uncond, probs
        torch.cuda.empty_cache()

    dec </span>= mmgpt.gen_vision_model.decode_code(generated_tokens.to(dtype=<span style="color: rgba(0, 0, 0, 1)">torch.int),
                                             shape</span>=[parallel_size, 8, img_size // patch_size, img_size //<span style="color: rgba(0, 0, 0, 1)"> patch_size])
    dec </span>= dec.to(torch.float32).cpu().numpy().transpose(0, 2, 3, 1<span style="color: rgba(0, 0, 0, 1)">)

    dec </span>= np.clip((dec + 1) / 2 * 255, 0, 255<span style="color: rgba(0, 0, 0, 1)">)

    visual_img </span>= np.zeros((parallel_size, img_size, img_size, 3), dtype=<span style="color: rgba(0, 0, 0, 1)">np.uint8)
    visual_img[:, :, :] </span>=<span style="color: rgba(0, 0, 0, 1)"> dec

    os.makedirs(</span><span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">generated_samples</span><span style="color: rgba(128, 0, 0, 1)">'</span>, exist_ok=<span style="color: rgba(0, 0, 0, 1)">True)
    </span><span style="color: rgba(0, 0, 255, 1)">for</span> i <span style="color: rgba(0, 0, 255, 1)">in</span><span style="color: rgba(0, 0, 0, 1)"> range(parallel_size):
        save_path </span>= os.path.join(<span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">generated_samples</span><span style="color: rgba(128, 0, 0, 1)">'</span>, f<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">img_{i}.jpg</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">)
        img </span>=<span style="color: rgba(0, 0, 0, 1)"> Image.fromarray(visual_img[i])
        img.save(save_path)

generate(
    vl_gpt,
    vl_chat_processor,
    prompt,
)</span></pre>
</div>
<p><strong><span style="color: rgba(255, 0, 0, 1)">注意：提示词是可以写中文的，不一定非要是英文。</span></strong></p>
<p><span style="color: rgba(255, 0, 0, 1)"><strong>代码在默认的基础上做了优化，否则运行会导致英伟达5080显卡直接卡死。</strong></span></p>
<p>&nbsp;</p>
<p>运行代码，效果如下：</p>
<div class="cnblogs_code">
<pre><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> python image_generation.py</span>
Python version <span style="color: rgba(0, 0, 255, 1)">is</span> above 3.10<span style="color: rgba(0, 0, 0, 1)">, patching the collections module.
</span>/root/anaconda3/envs/vll-Janus-Pro-7B/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:604: FutureWarning: The image_processor_class argument <span style="color: rgba(0, 0, 255, 1)">is</span> deprecated <span style="color: rgba(0, 0, 255, 1)">and</span> will be removed <span style="color: rgba(0, 0, 255, 1)">in</span> v4.42. Please use `slow_image_processor_class`, <span style="color: rgba(0, 0, 255, 1)">or</span><span style="color: rgba(0, 0, 0, 1)"> `fast_image_processor_class` instead
  warnings.warn(
Using a slow image processor as `use_fast` </span><span style="color: rgba(0, 0, 255, 1)">is</span> unset <span style="color: rgba(0, 0, 255, 1)">and</span> a slow processor was saved with this model. `use_fast=True` will be the default behavior <span style="color: rgba(0, 0, 255, 1)">in</span> v4.52, even <span style="color: rgba(0, 0, 255, 1)">if</span> the model was saved with a slow processor. This will result <span style="color: rgba(0, 0, 255, 1)">in</span> minor differences <span style="color: rgba(0, 0, 255, 1)">in</span> outputs. You<span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">ll still be able to use a slow processor with `use_fast=False`.</span>
You are using the default legacy behaviour of the &lt;<span style="color: rgba(0, 0, 255, 1)">class</span> <span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast</span><span style="color: rgba(128, 0, 0, 1)">'</span>&gt;. This <span style="color: rgba(0, 0, 255, 1)">is</span> expected, <span style="color: rgba(0, 0, 255, 1)">and</span> simply means that the `legacy` (previous) behavior will be used so nothing changes <span style="color: rgba(0, 0, 255, 1)">for</span> you. If you want to use the new behaviour, set `legacy=False`. This should only be set <span style="color: rgba(0, 0, 255, 1)">if</span> you understand what it means, <span style="color: rgba(0, 0, 255, 1)">and</span> thoroughly read the reason why this was added as explained <span style="color: rgba(0, 0, 255, 1)">in</span> https://github.com/huggingface/transformers/pull/24565 - <span style="color: rgba(0, 0, 255, 1)">if</span> you loaded a llama tokenizer <span style="color: rgba(0, 0, 255, 1)">from</span><span style="color: rgba(0, 0, 0, 1)"> a GGUF file you can ignore this message.
Loading checkpoint shards: </span>100%|██████████████████████████████████████████████████████████| 1/1 [00:09&lt;00:00,  4.58s/it]</pre>
</div>
<p>&nbsp;</p>
<p>注意观察一下GPU使用情况，这里会很高。</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202505/1341090-20250508191414571-1353256890.png" alt="" loading="lazy"></p>
<p>&nbsp;RTX 5080显卡，16GB显存，几乎已经占满了。</p>
<p>&nbsp;</p>
<p>等待30秒左右，就会生成一张图片。</p>
<p>打开小企鹅</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202505/1341090-20250508192208441-1598612409.png" alt="" loading="lazy"></p>
<p>进入目录\home\xiao\vllm\Janus\generated_samples</p>
<p>这里会出现一张图片</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202505/1341090-20250508191630711-1160295926.png" alt="" loading="lazy"></p>
<p>&nbsp;打开图片，效果如下：</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202505/1341090-20250508192331575-235520069.jpg" alt=""></p>
<p>&nbsp;</p>
<p>效果还算可以，距离真正的8k画质，还是有点差距的。</p>
<p><span style="color: rgba(255, 0, 0, 1)"><strong>注意提示词，尽量丰富一点，生成的图片，才符合要求。</strong></span></p>
<p>如果不会写提示词，可以让deepseek帮你写一段提示词。</p>
<p>&nbsp;</p>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.20099686527314814" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-05-08 19:31">2025-05-08 19:17</span>&nbsp;
<a href="https://www.cnblogs.com/xiao987334176">肖祥</a>&nbsp;
阅读(<span id="post_view_count">26</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18864903);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18864903', targetLink: 'https://www.cnblogs.com/xiao987334176/p/18864903', title: 'DeepSeek 多模态模型 Janus-Pro 本地部署' })">举报</a>
</div>
        