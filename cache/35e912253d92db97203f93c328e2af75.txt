
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/flyup/p/18964695" title="发布于 2025-07-03 23:22">
    <span role="heading" aria-level="2">支持向量机（SVM）分类</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p>  支持向量机（Support Vector Machine，SVM）是一种经典的监督学习算法，主要用于分类任务，也可扩展到回归问题（称为支持向量回归，SVR）。其核心思想是通过寻找一个最优超平面，最大化不同类别数据之间的间隔（Margin），从而实现高效分类。</p>
<h2 id="一核心思想">一、核心思想</h2>
<p>  SVM的目标是找到一个决策边界（超平面），将不同类别的数据分开，并确保该边界到最近数据点（支持向量）的距离最大。这种“最大化间隔”的策略使得模型具有更好的泛化能力。</p>
<h3 id="超平面hyperplane">超平面（Hyperplane）：</h3>
<p>  在n维空间中，一个超平面是n-1维的子空间。对于二维数据，超平面是一条直线；三维数据中是一个平面。</p>
<h3 id="支持向量support-vectors">支持向量（Support Vectors）：</h3>
<p>  距离最优超平面最近的样本点称为支持向量，它们是决定超平面位置的关键样本。其他样本的位置对超平面无影响，这也是“SVM”名称的由来。</p>
<h3 id="间隔margin">间隔（Margin）：</h3>
<p>  超平面到两类最近支持向量的距离之和。SVM的目标是最大化间隔。<br>
  设超平面方程为<span class="math inline">\(w\cdot x+b=0\)</span>（其中<span class="math inline">\(w\)</span>是权重向量，<span class="math inline">\(b\)</span>是偏置），则单个样本点<span class="math inline">\(x_i\)</span>到超平面的距离为：</p>
<p></p><div class="math display">\[距离=\frac{\left| w\cdot x_i+b \right|}{\left| \left| w \right| \right|}
\]</div><p></p><p>  最优超平面需满足：对于正类样本，有<span class="math inline">\(w\cdot x_i+b\geq1\)</span>；对于负类样本，有<span class="math inline">\(w\cdot x_i+b\leq-1\)</span> 。此时，间隔为 <span class="math inline">\(\frac{2}{\left| \left| w \right| \right|}\)</span>，最大化间隔等价于最小化<span class="math inline">\(\left| \left| w \right| \right|^{2}\)</span>。</p>
<h2 id="二线性可分情况硬间隔svm">二、线性可分情况（硬间隔SVM）</h2>
<p>  假设数据线性可分，SVM的优化问题可表示为</p>
<p>    <span class="math inline">\(\min_{w,b}{\frac{1}{2}\left| \left| w \right| \right|^{2}}\)</span>     s.t. <span class="math inline">\(y_i(w\cdot x_i+b)\geq1 \quad (\forall i)\)</span></p>
<p>  目标：最小化<span class="math inline">\(\left| \left| w \right| \right|\)</span>（等价于最大化间隔<span class="math inline">\(\frac{2}{\left| \left| w \right| \right|}\)</span>）。<br>
  约束：确保所有样本被正确分类且位于间隔边界之外。</p>
<h2 id="三非线性可分情况软间隔svm">三、非线性可分情况（软间隔SVM）</h2>
<p>  当样本无法被线性超平面分隔时，SVM 通过以下方法处理：</p>
<h3 id="1-引入松弛变量slack-variables">1. 引入松弛变量（Slack Variables）</h3>
<p>  允许部分样本跨越超平面，但需在优化目标中加入惩罚项（即正则化参数<span class="math inline">\(C\)</span>），平衡间隔最大化和分类错误最小化</p>
<p>    <span class="math inline">\(\min_{w,b}{\frac{1}{2}\left| \left| x \right| \right|^{2}}+C\sum_{i}{\xi_i}\)</span>   s.t.  <span class="math inline">\(y_i(w\cdot x_i+b)\geq 1-\xi_i,\quad \xi_i\geq0\)</span></p>
<p>  <span class="math inline">\(C\)</span>的作用：控制分类错误的惩罚力度。<span class="math inline">\(C\)</span>越大，模型越严格（可能过拟合）；<span class="math inline">\(C\)</span>越小，允许更多错误（可能欠拟合）。</p>
<h3 id="2-核技巧kernel-trick">2. 核技巧（Kernel Trick）</h3>
<p>  对于非线性可分数据，SVM通过核函数将原始空间映射到高维特征空间，使数据在新空间中线性可分。常见核函数有<br>
  线性核：<span class="math inline">\(K(x_i,x_j)=x_i\cdot x_j\)</span><br>
  多项式核：<span class="math inline">\(K(x_i,x_j)=(x_i\cdot x_j+c)^{d}\)</span><br>
  高斯径向基核（RBF）：<span class="math inline">\(K(x_i,x_j)=exp(-\gamma \left| \left| x_i-x_j \right| \right|^{2})\)</span><br>
  Sigmoid核： <span class="math inline">\(K(x_i,x_j)=tanh(\alpha x_i\cdot x_j+c)\)</span></p>
<h2 id="四优化与求解">四、优化与求解</h2>
<p>  SVM通常转化为对偶问题，利用拉格朗日乘子法求解：</p>
<p>    <span class="math inline">\(max_{\alpha}{\sum_{i}{\alpha_i}}-\frac{1}{2}\sum_{i,j}{\alpha_i\alpha_jy_iy_jK(x_i,x_j)}\)</span>  s.t. <span class="math inline">\(0\leq\alpha_i\leq C,\sum_{i}{\alpha_iy_i=0}\)</span></p>
<p>  通过拉格朗日对偶性转化为对偶问题，优势在于：</p>
<p>    a) 将高维空间中的内积运算转化为核函数计算（避免直接处理高维数据）；<br>
    b) 解的形式仅依赖于支持向量，计算效率更高。</p>
<h2 id="五python实现示例">五、Python实现示例</h2>
<pre><code class="language-python">from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# 加载鸢尾花数据集
iris = datasets.load_iris()
X = iris.data  # 特征
y = iris.target  # 标签

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# 创建SVM分类器
clf = SVC(kernel='linear')  # 使用线性核函数

# 训练模型
clf.fit(X_train, y_train)

# 预测
y_pred = clf.predict(X_test)

# 评估模型
accuracy = accuracy_score(y_test, y_pred)
print(f"模型准确率: {accuracy:.2f}")

# 预测新样本
new_samples = [[5.1, 3.5, 1.4, 0.2], [6.3, 3.3, 4.7, 1.6]]
predictions = clf.predict(new_samples)
print(f"新样本预测结果: {[iris.target_names[p] for p in predictions]}")


</code></pre>
<p><img alt="结果图" loading="lazy" data-src="https://img2024.cnblogs.com/blog/2197714/202507/2197714-20250703232052453-1066095006.png" class="lazyload"></p>
<br>
<br>
<p><em><strong>End.</strong></em></p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.0020833333333333333" data-date-updated="2025-07-03 23:25">2025-07-03 23:22</span>&nbsp;
<a href="https://www.cnblogs.com/flyup">归去_来兮</a>&nbsp;
阅读(<span id="post_view_count">0</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18964695);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18964695', targetLink: 'https://www.cnblogs.com/flyup/p/18964695', title: '支持向量机（SVM）分类' })">举报</a>
</div>
        