
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/KubeExplorer/p/18655127" title="发布于 2025-01-06 13:33">
    <span role="heading" aria-level="2">使用 NodeLocalDNS 提升集群 DNS 性能和可靠性</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p><img src="https://img.lixueduan.com/kubernetes/cover/node-local-dns.png" alt="node-local-dns.png" loading="lazy"></p>
<p>本文主要分享如何使用 NodeLocal DNSCache 来提升集群中的 DNS 性能以及可靠性，包括部署、使用配置以及原理分析，最终通过压测表明使用后带来了高达 50% 的性能提升。</p>

<h2 id="1背景">1.背景</h2>
<h3 id="什么是-nodelocaldns">什么是 NodeLocalDNS</h3>
<p><code>NodeLocal DNSCache</code> 是一套 DNS 本地缓存解决方案。<strong>NodeLocal DNSCache 通过在集群节点上运行一个 DaemonSet 来提高集群 DNS 性能和可靠性</strong>。</p>
<h3 id="为什么需要-nodelocaldns">为什么需要 NodeLocalDNS</h3>
<p>处于 ClusterFirst 的 DNS 模式下的 Pod 可以连接到 kube-dns 的 serviceIP 进行 DNS 查询，通过 kube-proxy 组件添加的 iptables 规则将其转换为 CoreDNS 端点，最终请求到 CoreDNS Pod。</p>
<p>通过在每个集群节点上运行 DNS 缓存，<code>NodeLocal DNSCache</code> 可以缩短 DNS 查找的延迟时间、使 DNS 查找时间更加一致，以及减少发送到 kube-dns 的 DNS 查询次数。</p>
<p>在集群中运行 <code>NodeLocal DNSCache</code> 有如下几个好处：</p>
<ul>
<li>如果本地没有 CoreDNS 实例，则具有最高 DNS QPS 的 Pod 可能必须到另一个节点进行解析，使用 <code>NodeLocal DNSCache</code> 后，拥有本地缓存将有助于改善延迟</li>
<li>跳过 iptables DNAT 和连接跟踪将有助于减少 conntrack 竞争并避免 UDP DNS 条目填满 conntrack 表（上面提到的 5s 超时问题就是这个原因造成的）</li>
<li>从本地缓存代理到 kube-dns 服务的连接可以升级到 TCP，TCP conntrack 条目将在连接关闭时被删除，而 UDP 条目必须超时(默认 <code>nfconntrackudp_timeout</code> 是 30 秒)</li>
<li>将 DNS 查询从 UDP 升级到 TCP 将减少归因于丢弃的 UDP 数据包和 DNS 超时的尾部等待时间，通常长达 30 秒（3 次重试+ 10 秒超时）</li>
</ul>
<p><img src="https://img.lixueduan.com/kubernetes/nodelocaldns/node-local-dns-flow.png" alt="node-local-dns-flow.png" loading="lazy"></p>
<h2 id="2-如何使用-nodelocaldns">2. 如何使用 NodeLocalDNS</h2>
<h3 id="nodelocaldns-部署">NodeLocalDNS 部署</h3>
<p>要安装 <code>NodeLocal DNSCache</code> 也非常简单，直接获取官方的资源清单即可：</p>
<pre><code class="language-Ruby">wget -c https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml
</code></pre>
<p>默认使用的镜像为<code>registry.k8s.io/dns/k8s-dns-node-cache</code>如果无法拉取镜像，可以替换成国内的 <code>docker.io/dyrnq/k8s-dns-node-cache</code></p>
<pre><code class="language-Bash">cp nodelocaldns.yaml nodelocaldns.yaml.bak

sed -i 's#registry\.k8s\.io/dns/k8s-dns-node-cache#docker\.io/dyrnq/k8s-dns-node-cache#g' nodelocaldns.yaml
</code></pre>
<p>该资源清单文件中包含几个变量，各自含义如下：</p>
<ul>
<li><code>__PILLAR__DNS__DOMAIN__</code>：表示集群域，默认为 <code>cluster.local</code>，它是用于解析 Kubernetes 集群内部服务的域名后缀。</li>
<li><code>__PILLAR__LOCAL__DNS__</code>：表示 DNSCache 本地的 IP，也就是 NodeLocalDNS 要使用的 IP，默认为 169.254.20.10</li>
<li>_<code>_PILLAR__DNS__SERVER__</code> ：表示 kube-dns 这个 Service 的 ClusterIP，一般默认为 10.96.0.10。通过<code>kubectl get svc -n kube-system -l k8s-app=kube-dns -o jsonpath='{$.items[*].spec.clusterIP}'</code> 命令获取</li>
</ul>
<p>下面两个变量则不需要关系，NodeLocalNDS Pod 会自动配置，对应的值来源于 kube-dns 的 ConfigMap 和定制的 <code>Upstream Server</code> 配置。直接执行如下所示的命令即可安装：</p>
<ul>
<li><code>__PILLAR__CLUSTER__DNS__</code>： 表示集群内查询的上游 DNS 服务器，一般也指向 kube-dns 的 service IP，默认为 10.96.0.10。</li>
<li><code>__PILLAR__UPSTREAM__SERVERS__</code>：表示为外部查询的上游服务器，如果没有专门的自建 DNS 服务的话，也可以填 kube-dns 的 service ip。</li>
</ul>
<p>接下来将对应变量替换为真实值，具体如下：</p>
<pre><code class="language-Bash">kubedns=`kubectl get svc kube-dns -n kube-system -o jsonpath={.spec.clusterIP}`
domain=cluster.local
localdns=169.254.20.10

echo kubedns=$kubedns, domain=$domain, localdns=$localdns
</code></pre>
<p>需要注意的是：<strong>根据 kube-proxy 运行模式不同，要替换的参数也不同</strong>，使用以下命令查看 kube-proxy 所在模式</p>
<pre><code class="language-Bash">kubectl -n kube-system get cm kube-proxy -oyaml|grep mode
</code></pre>
<p>如果kube-proxy在 <strong>iptables</strong> 模式下运行, 则运行以下命令创建</p>
<pre><code class="language-Bash">cp nodelocaldns.yaml nodelocaldns-iptables.yaml
sed -i "s/__PILLAR__LOCAL__DNS__/$localdns/g; 
        s/__PILLAR__DNS__DOMAIN__/$domain/g; 
        s/__PILLAR__DNS__SERVER__/$kubedns/g" nodelocaldns-iptables.yaml
</code></pre>
<blockquote>
<p>node-local-dns Pod 会设置 <code>PILLAR__CLUSTER__DNS</code> 和 <code>PILLAR__UPSTREAM__SERVERS</code>。</p>
</blockquote>
<p>如果 kube-proxy 在 <strong>ipvs</strong> 模式下运行, 则运行以下命令创建</p>
<pre><code class="language-Bash">cp nodelocaldns.yaml nodelocaldns-ipvs.yaml

sed -i "s/__PILLAR__LOCAL__DNS__/$localdns/g; 
        s/__PILLAR__DNS__DOMAIN__/$domain/g; 
        s/,__PILLAR__DNS__SERVER__//g; 
        s/__PILLAR__CLUSTER__DNS__/$kubedns/g" nodelocaldns-ipvs.yaml
</code></pre>
<blockquote>
<p>node-local-dns Pod 会设置 <code>PILLAR__UPSTREAM__SERVERS</code></p>
</blockquote>
<p>然后就是将替换后的 yaml apply 到集群里：</p>
<pre><code class="language-Bash">#kubectl apply -f nodelocaldns-iptables.yaml
kubectl apply -f nodelocaldns-ipvs.yaml
</code></pre>
<p>会创建以下对象</p>
<pre><code class="language-Bash">serviceaccount/node-local-dns created
service/kube-dns-upstream created
configmap/node-local-dns created
daemonset.apps/node-local-dns created
service/node-local-dns created
</code></pre>
<p>创建完成后，就能看到每个节点上都运行了一个pod，这里只有一个节点，所以就运行了一个</p>
<pre><code class="language-Bash">[root@caas ~]# kubectl -n kube-system get po
NAME                         READY   STATUS    RESTARTS   AGE
node-local-dns-m8ktq         1/1     Running     0         8s
</code></pre>
<blockquote>
<p>需要注意的是这里使用 DaemonSet 部署 node-local-dns 使用了 hostNetwork=true，会占用宿主机的 <code>8080</code> 端口，所以需要保证该端口未被占用。</p>
</blockquote>
<h3 id="nodelocaldns-配置">NodeLocalDNS 配置</h3>
<p>上一步部署好 <strong>NodeLocal DNSCache，</strong>但是还差了很重要的一步，<strong>配置</strong> <strong>pod</strong> <strong>使用 NodeLocal DNSCache 作为优先的</strong> <strong>DNS</strong> <strong>服务器。</strong></p>
<p>有以下几种方式：</p>
<ul>
<li>方式一：修改 kubelet 中的 dns nameserver 参数，并重启节点 kubelet。<strong>存在业务中断风险，不推荐使用此方式</strong>。
<ul>
<li>测试时可以用这个方式，比较简单</li>
</ul>
</li>
<li>方式二：创建 Pod 时手动指定 DNSConfig，<strong>比较麻烦，不推荐。</strong></li>
<li>方式三：借助 DNSConfig 动态注入控制器在 Pod 创建时配置 DNSConfig 自动注入，<strong>推荐使用此方式。</strong>
<ul>
<li>需要自己实现一个 webhook，相当于把方式二自动化了，</li>
</ul>
</li>
</ul>
<h4 id="方式一修改-kubelet-参数">方式一：修改 kubelet 参数</h4>
<p>kubelet通过<code>--cluster-dns</code>和<code>--cluster-domain</code> 两个参数来全局控制Pod DNSConfig。</p>
<ul>
<li><strong>cluster-dns</strong>：部署Pod时，默认采用的DNS服务器地址，默认只引用了<code>kube-dns</code>的 ServiceIP，需要增加一个 NodeLocalDNS 的 169.254.20.10 。</li>
<li><strong>cluster-domain</strong>：部署 Pod 时，默认采用的 DNS 搜索域，保持原有搜索域即可，一般为<code>cluster.local</code>。</li>
</ul>
<p>在 <code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code> 配置文件中需要<strong>增加</strong>一个 <strong>--cluster-dns</strong> 参数，设置值为NodeLocalDNS 的 169.254.20.10。</p>
<blockquote>
<p><strong>注意是在原有的前面增加一个 --cluster-dns，不是把原本的改了。</strong></p>
<p>这样 Pod 中就会有两个 dns nameserver，如果新增的这个失效了，也可以使用旧的。</p>
</blockquote>
<pre><code class="language-Bash">vi /etc/systemd/system/kubelet.service.d/10-kubeadm.conf 
# 增加 --cluster-dns
--cluster-dns=169.254.20.10 --cluster-dns=&lt;kube-dns ip&gt; --cluster-domain=&lt;search domain&gt;
</code></pre>
<p>然后重启 kubelet 使其生效</p>
<pre><code class="language-Bash">sudo systemctl daemon-reload
sudo systemctl restart kubelet
</code></pre>
<h4 id="方式二自定义-pod-dnsconfig">方式二：自定义 Pod dnsConfig</h4>
<p><strong>通过 dnsConfig 字段自定义</strong> <strong>Pod</strong> <strong>的</strong> <strong>dns</strong> <strong>配置</strong>，nameservers 中除了指定 NodeLocalDNS 之外还指定了 KubeDNS，这样即使  NodeLocalDNS 异常也不影响 Pod 中的 DNS 解析。</p>
<pre><code class="language-YAML">apiVersion: v1
kind: Pod
metadata:
  name: alpine
  namespace: default
spec:
  containers:
  - image: alpine
    command:
      - sleep
      - "10000"
    imagePullPolicy: Always
    name: alpine
  dnsPolicy: None
  dnsConfig:
    nameservers: ["169.254.20.10","10.96.0.10"]
    searches:
    - default.svc.cluster.local
    - svc.cluster.local
    - cluster.local
    options:
    - name: ndots
      value: "3"
    - name: attempts
      value: "2"
    - name: timeout 
      value: "1"
</code></pre>
<ul>
<li>dnsPolicy：必须为<code>None</code>。</li>
<li>nameservers：配置成 169.254.20.10 和 kube-dns 的 ServiceIP 地址。</li>
<li>searches：设置搜索域，保证集群内部域名能够被正常解析。</li>
<li>ndots：默认为 5，可以适当降低 ndots 以提升解析效率。</li>
</ul>
<h4 id="方式三webhook-自动注入-dnsconfig">方式三：Webhook 自动注入 dnsConfig</h4>
<p>DNSConfig 动态注入控制器可用于自动注入DNSConfig至新建的Pod中，避免您手工配置Pod YAML进行注入。本应用默认会监听包含<code>node-local-dns-injection=enabled</code>标签的命名空间中新建Pod的请求，您可以通过以下命令给命名空间打上Label标签。</p>
<p>部署后，只需要给 Namespace  打上 <code>node-local-dns-injection=enabled</code> label 即可，Webhook 检测就会自动给该 Namespace 下所有 Pod 配置 DNSConfig。</p>
<blockquote>
<p>先挖个坑,下一篇做一个简单实现。</p>
</blockquote>
<h2 id="3-压测">3. 压测</h2>
<p>接下来进行压测，看一下性能提升。</p>
<blockquote>
<p>这里使用修改 kubelet 参数方式暂时让 Pod 都使用 NodeLocalDNS，便于测试</p>
</blockquote>
<p>测试环境：</p>
<p>1 master 1 worker 的 k8s 集群，节点规则统一 4C8G，空闲状态，未运行其他负载。</p>
<blockquote>
<p>可以参考 <a href="https://www.lixueduan.com/posts/kubernetes/11-install-by-kubeclipper/" target="_blank" rel="noopener nofollow">Kubernetes教程(十一)---使用 KubeClipper 通过一条命令快速创建 k8s 集群</a> 快速创建一个集群。</p>
</blockquote>
<h3 id="压测脚本">压测脚本</h3>
<p>使用下面这个文件进行性能测试</p>
<pre><code class="language-Go">// main.go
package main

import (
    "context"
    "flag"
    "fmt"
    "net"
    "sync/atomic"
    "time"
)

var host string
var connections int
var duration int64
var limit int64
var timeoutCount int64

func main() {
    flag.StringVar(&amp;host, "host", "", "Resolve host")
    flag.IntVar(&amp;connections, "c", 100, "Connections")
    flag.Int64Var(&amp;duration, "d", 0, "Duration(s)")
    flag.Int64Var(&amp;limit, "l", 0, "Limit(ms)")
    flag.Parse()

    var count int64 = 0
    var errCount int64 = 0
    pool := make(chan interface{}, connections)
    exit := make(chan bool)
    var (
       min int64 = 0
       max int64 = 0
       sum int64 = 0
    )

    go func() {
       time.Sleep(time.Second * time.Duration(duration))
       exit &lt;- true
    }()

endD:
    for {
       select {
       case pool &lt;- nil:
          go func() {
             defer func() {
                &lt;-pool
             }()
             resolver := &amp;net.Resolver{}
             now := time.Now()
             _, err := resolver.LookupIPAddr(context.Background(), host)
             use := time.Since(now).Nanoseconds() / int64(time.Millisecond)
             if min == 0 || use &lt; min {
                min = use
             }
             if use &gt; max {
                max = use
             }
             sum += use
             if limit &gt; 0 &amp;&amp; use &gt;= limit {
                timeoutCount++
             }
             atomic.AddInt64(&amp;count, 1)
             if err != nil {
                fmt.Println(err.Error())
                atomic.AddInt64(&amp;errCount, 1)
             }
          }()
       case &lt;-exit:
          break endD
       }
    }
    fmt.Printf("request count：%d\nerror count：%d\n", count, errCount)
    fmt.Printf("request time：min(%dms) max(%dms) avg(%dms) timeout(%dn)\n", min, max, sum/count, timeoutCount)
}
</code></pre>
<p>首先配置好 golang 环境，然后直接构建上面的测试应用：</p>
<pre><code class="language-Go">go build -o testdns main.go
</code></pre>
<p>构建完成后生成一个 testdns 的二进制文件</p>
<h3 id="跨节点-dns-性能测试">跨节点 DNS 性能测试</h3>
<p>首先测试跨节点 DNS 性能测试，因为随着集群规模扩大，CoreDNS 副本数和节点数很明显不能做到 1:1,因此大部分 DNS 请求都是跨节点的，这个性能也更能反映正常情况下的 DNS 性能。</p>
<blockquote>
<p>一般推荐是 1：8，即 8 个节点对应 1 个 CoreDNS Pod</p>
</blockquote>
<p>首先将 CoreDNS 副本数调整为 1，便于测试。</p>
<pre><code class="language-Bash">kubectl -n kube-system scale deploy coredns --replicas=1
</code></pre>
<p>这样就是两个节点对应一个 CoreDNS Pod，就可以测试跨节点 DNS 解析性能了。</p>
<pre><code class="language-Bash">[root@dns-1 go]# kubectl get node
NAME    STATUS   ROLES           AGE   VERSION
dns-1   Ready    control-plane   48m   v1.27.4
dns-2   Ready    &lt;none&gt;          48m   v1.27.4
[root@dns-1 go]# kubectl -n kube-system get po -owide -l k8s-app=kube-dns
NAME                       READY   STATUS    RESTARTS   AGE   IP             NODE    NOMINATED NODE   READINESS GATES
coredns-5d78c9869d-l7vgv   1/1     Running   0          12m   172.25.173.4   dns-1   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>当前 CoreDNS 在 dns-1 节点，那我们把测试 Pod 指定调度到 dns-2 节点。</p>
<p>通过 overrides  直接指定 nodeName，让 Pod 和 CoreDNS 分散到不同节点。</p>
<pre><code class="language-YAML">kubectl run busybox3 --image=busybox:latest --restart=Never --overrides='{ "spec": { "nodeName": "dns-2" } }' -- sleep 10000
</code></pre>
<p>然后我们将这个二进制文件拷贝到 Pod 中去进行测试：</p>
<pre><code class="language-Nginx">kubectl cp testdns busybox:/
</code></pre>
<p>拷贝完成后进入这个测试的 Pod 中去：</p>
<pre><code class="language-Bash">kubectl exec -it busybox -- /bin/sh
</code></pre>
<p>然后我们执行 testdns 程序来进行压力测试，比如执行 200 个并发，持续 30 秒：</p>
<pre><code class="language-Bash"># 对地址 kube-dns.kube-system 进行解析

/ # ./testdns -host kube-dns.kube-system -c 200 -d 30 -l 5000
lookup kube-dns.kube-system on 10.96.0.10:53: no such host
lookup kube-dns.kube-system on 10.96.0.10:53: no such host
lookup kube-dns.kube-system on 10.96.0.10:53: no such host
lookup kube-dns.kube-system on 10.96.0.10:53: no such host
lookup kube-dns.kube-system on 10.96.0.10:53: no such host
lookup kube-dns.kube-system on 10.96.0.10:53: no such host
lookup kube-dns.kube-system on 10.96.0.10:53: no such host
lookup kube-dns.kube-system on 10.96.0.10:53: no such host
lookup kube-dns.kube-system on 10.96.0.10:53: no such host
lookup kube-dns.kube-system on 10.96.0.10:53: no such host
lookup kube-dns.kube-system on 10.96.0.10:53: no such host
lookup kube-dns.kube-system on 10.96.0.10:53: no such host
lookup kube-dns.kube-system on 10.96.0.10:53: no such host
lookup kube-dns.kube-system on 10.96.0.10:53: no such host
lookup kube-dns.kube-system on 10.96.0.10:53: no such host
lookup kube-dns.kube-system on 10.96.0.10:53: no such host
lookup kube-dns.kube-system on 10.96.0.10:53: no such host
lookup kube-dns.kube-system on 10.96.0.10:53: no such host
lookup kube-dns.kube-system on 10.96.0.10:53: no such host
lookup kube-dns.kube-system on 10.96.0.10:53: no such host
lookup kube-dns.kube-system on 10.96.0.10:53: no such host
lookup kube-dns.kube-system on 10.96.0.10:53: no such host
lookup kube-dns.kube-system on 10.96.0.10:53: no such host
request count：131063
error count：23
request time：min(1ms) max(15050ms) avg(39ms) timeout(624n)
</code></pre>
<p>我们可以看到平均耗时为 39ms 左右，这个性能是比较差的，而且还有部分解析失败的条目。</p>
<h3 id="同节点-dns-性能测试">同节点 DNS 性能测试</h3>
<p>重新创建 busybox pod，指定调度到和 CoreDNS 同一个节点，测试同节点 DNS 解析性能。</p>
<blockquote>
<p>理论上同节点性能会比跨节点提升不少</p>
</blockquote>
<p>然后创建一个 Busybox Pod 用于测试，通过 overrides  直接指定 nodeName，让 Pod 和 CoreDNS 分散到不同节点。</p>
<pre><code class="language-YAML">kubectl delete pod busybox

kubectl run busybox --image=busybox:latest --restart=Never --overrides='{ "spec": { "nodeName": "dns-1" } }' -- sleep 10000
</code></pre>
<p>然后我们将这个二进制文件拷贝到 Pod 中去进行测试：</p>
<pre><code class="language-Nginx">kubectl cp testdns busybox:/
</code></pre>
<p>拷贝完成后进入这个测试的 Pod 中去：</p>
<pre><code class="language-Bash">kubectl exec -it busybox -- /bin/sh
</code></pre>
<p>然后我们执行 testdns 程序来进行压力测试，比如执行 200 个并发，持续 30 秒：</p>
<pre><code class="language-Bash"># 对地址 kube-dns.kube-system 进行解析

/ # ./testdns -host kube-dns.kube-system -c 200 -d 30 -l 5000
request count：217030
error count：0
request time：min(1ms) max(5062ms) avg(26ms) timeout(311n)
</code></pre>
<p>我们可以看到大部分平均耗时都是在 26ms 左右，相比之前的 40ms，提升了接近 50%，而且也没有出现超时、失败的情况。</p>
<h3 id="nodelocaldns-测试">NodeLocalDNS 测试</h3>
<p>直接启动 Pod</p>
<pre><code class="language-YAML">kubectl delete pod busybox
kubectl run busybox --image=busybox:latest --restart=Never -- sleep 10000
</code></pre>
<p>然后我们将这个二进制文件拷贝到 Pod 中去进行测试：</p>
<pre><code class="language-Nginx">kubectl cp testdns busybox:/
</code></pre>
<p>拷贝完成后进入这个测试的 Pod 中去：</p>
<pre><code class="language-Bash">kubectl exec -it busybox -- /bin/sh
</code></pre>
<p>然后我们执行 testdns 程序来进行压力测试，比如执行 200 个并发，持续 30 秒：</p>
<p>把 Pod 中的 DNS Nameserver 指向 169.254.20.10（即 NodeLocalDNS 地址），然后再次测试</p>
<pre><code class="language-Bash">vi /etc/resolv.conf
</code></pre>
<p>增加以下内容</p>
<pre><code class="language-Bash">nameserver 169.254.20.10
</code></pre>
<p>然后再次测试</p>
<pre><code class="language-Bash">/ # ./testdns -host kube-dns.kube-system -c 200 -d 30 -l 5000
request count：224103
error count：0
request time：min(1ms) max(5057ms) avg(24ms) timeout(333n)
</code></pre>
<p>可以看到，平均耗时都是 24ms，比跨节点的 39ms 提升 50%，和同节点的 26ms 接近，这样说明跨节点 DNS 解析有大量性能损失。</p>
<p>而 <strong>NodeLocalDNS 和同节点对比依旧存在一些提升</strong>，因为：</p>
<ul>
<li>访问 CoreDNS 使用的是 service 的 clusterIP 10.96.0.10 最终会进过 iptables / ipvs 等规则转发到后端 CoreDNS Pod 中</li>
<li>而访问 NodeLocalDNS 则是使用的 link-local ip 169.254.20.10，不会经过 iptables / ipvs 规则跳转，直接就会进入 NodeLocalDNS Pod。</li>
</ul>
<p>因此，有略微的性能提升。</p>
<h2 id="4nodelocal-dnscache-工作原理">4.NodeLocal DNSCache 工作原理</h2>
<p>这部分主要分析 NodeLocal DNSCache 工作原理。</p>
<h3 id="工作原理分析">工作原理分析</h3>
<p>NodeLocalDNS 实际就是在每个节点上加了一个缓存，<strong>类似于</strong> <strong>CDN</strong>，把 中心 CoreDNS 看做源站的话，node-local-dns 就是运行在不同区域的缓存。</p>
<p>Pod 优先从本地 NodeLocalDNS 做 DNS 解析，有数据则直接返回，否则 NodeLocalDNS 再找 KubeDNS 解析，然后本地把数据缓存下来。</p>
<p>具体流程正如 <a href="https://help.aliyun.com/zh/ack/ack-managed-and-ack-dedicated/user-guide/configure-nodelocal-dnscache" target="_blank" rel="noopener nofollow">阿里云文档</a>) 中的这个图所示：</p>
<p><img src="https://img.lixueduan.com/kubernetes/nodelocaldns/node-local-dns-flow2.svg" alt="node-local-dns-flow2.png" loading="lazy"></p>
<p>首先控制面，创建 Pod 时 Admission Webhook 会自动注入 DNSConfig，已经注入 DNSConfig 和 未注入 DNSConfig 的 Pod 会拥有不同的情况。</p>
<p>具体如下：</p>
<p><strong>1）已注入 DNS 本地缓存的Pod，默认会通过 NodeLocal DNSCache 监听于节点上的IP（169.254.20.10）解析域名。</strong></p>
<p>Pod 内的 DNS 配置如下:</p>
<pre><code class="language-Bash">/ # cat /etc/resolv.conf
search default.svc.cluster.local svc.cluster.local cluster.local
nameserver 169.254.20.10
nameserver 10.96.0.10
options ndots:5
</code></pre>
<p>169.254.20.10 为第一个 nameserver，因此会优先使用。</p>
<p><strong>2）NodeLocal DNSCache 本地若无缓存应答解析请求，则会通过 kube-dns 服务请求 CoreDNS 进行解析。</strong></p>
<p>NodeLocalDNS 的 Corefile 中相关配置如下：</p>
<pre><code class="language-YAML">    .:53 {
        errors
        cache 30
        reload
        loop
        bind 169.254.20.10 __PILLAR__DNS__SERVER__
        forward . __PILLAR__UPSTREAM__SERVERS__
        prometheus :9253
        }
</code></pre>
<p>当无法解析时，会转发到上游服务，也就是 kube-dns。</p>
<p><strong>3）已注入 DNS 本地缓存的 Pod，当无法连通 NodeLocal DNSCache 时，会继而直接通过 kube-dns 服务连接到CoreDNS 进行解析，此链路为备用链路。</strong></p>
<p>Pod 中的 DNS 配置：</p>
<pre><code class="language-YAML">/ # cat /etc/resolv.conf
search default.svc.cluster.local svc.cluster.local cluster.local
nameserver 169.254.20.10
nameserver 10.96.0.10
options ndots:5
</code></pre>
<p>Kube-dns 对应的 IP 10.96.0.10 也做为第二 nameserver ，因此NodeLocal DNS 异常时 Pod 也能正常进行 DNS 解析。</p>
<p><strong>4）未注入 DNS本地缓存的 Pod，会通过标准的 kube-dns 服务链路连接到 CoreDNS 进行解析。</strong></p>
<p>未注入 DNSConfig 的 Pod 默认 DNS 配置如下：</p>
<pre><code class="language-YAML">/ # cat /etc/resolv.conf
search default.svc.cluster.local svc.cluster.local cluster.local
nameserver 10.96.0.10
options ndots:5
</code></pre>
<p>自然会直接请求 kube-dns</p>
<p><strong>5）CoreDNS 对于非集群内域名，则会根据当前节点上的 /etc/resolv.conf 转发到外部</strong> <strong>DNS</strong> <strong>服务器。</strong></p>
<p>Kube-dns 的 Corefile 中相关配置如下：</p>
<pre><code class="language-YAML">    .:53 {
        errors
        health {
           lameduck 5s
        }
        // 省略...
        forward . /etc/resolv.conf {
           max_concurrent 1000
        }
    }
</code></pre>
<p>省略了其他无关配置，<code>forward . /etc/resolv.conf</code> 表示，遇到无法解析的请求时会根据 /etc/resolv.conf  文件中的配置进行转发。</p>
<blockquote>
<p>而 CoreDNS Pod 中的 /etc/resolv.conf  文件又是 Pod 启动时从当前节点 copy 进去的，因此具体转发到哪儿就和 Pod 启动时节点上的 /etc/resolv.conf  配置有关。</p>
</blockquote>
<h3 id="为什么是-1692542010-">为什么是 169.254.20.10 ？</h3>
<p><strong>为什么访问</strong> <strong><code>169.254.20.10</code></strong> <strong>这个</strong> <strong>IP</strong> <strong>就可以访问到 NodeLocalDNS ？</strong></p>
<p>NodeLocalDNS 以 DaemonSet 方式运行，因此会在集群中每个节点上都启动一个 Pod。该 Pod 会为当前节点增加一张网卡，并将 IP 指定为 <code>169.254.20.10</code>。</p>
<p>就像下面这样：</p>
<pre><code class="language-Bash">47: nodelocaldns: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN group default
    link/ether 56:9b:08:18:a6:75 brd ff:ff:ff:ff:ff:ff
    inet 169.254.20.10/32 scope global nodelocaldns
       valid_lft forever preferred_lft forever
</code></pre>
<p>NodelocalDNS 会以 hostNetwork 网络模式启动，并在前面新增网卡对应 IP (169.254.20.20)上启动服务。</p>
<p>由于我们前面的配置(修改 kubelet 或者 Pod 的 dnsConfig)，Pod 里优先级最高的 DNS 服务器就是 169.254.20.20，因此 Pod 需要 DNS 解析时会优先访问  169.254.20.10，最终请求被同节点的 NodelocalDNS Pod 处理。</p>
<p>添加这个网卡的具体作用如下：</p>
<ol>
<li>本地 DNS 服务: <code>nodelocaldns</code> 在每个节点上运行，通过监听 <code>169.254.20.10</code> 地址提供本地 DNS 服务。这个地址是一个 link-local 地址，仅在本地节点可达。Pod 内的 DNS 查询会被重定向到这个地址，从而实现在节点内解析服务的域名。</li>
<li>避免 DNS 查询离开节点: 由于 <code>nodelocaldns</code> 提供了节点内的 DNS 解析服务，这张网卡确保 DNS 查询不会离开节点。这对于集群内部的 DNS 查询来说是非常高效的，不需要离开节点就能解析服务的域名。</li>
<li>降低 DNS 查询延迟: 由于 <code>nodelocaldns</code> 在每个节点上运行，节点内的 DNS 查询可以更快速地完成，而不必经过集群网络。</li>
</ol>
<p>简单的做一个实验</p>
<pre><code class="language-Bash"># 创建一个新的网络接口 mynic
sudo ip link add mynic type dummy

# 分配IP地址给 eth1
sudo ip addr add 1.1.1.1/24 dev mynic

# 启动你的程序，让它监听在指定的IP地址上
# 例如，如果你有一个基于Python的简单HTTP服务器：
python3 -m http.server 9090 --bind 1.1.1.1

# 同一节点打开新终端测试能否访问到
curl 1.1.1.1:9090
</code></pre>
<p>是可以直接访问到的，NodeLocalDNS 添加网卡就是这个作用。</p>
<p><strong>至于为什么是 169.254.20.10 这个 IP?</strong></p>
<p>则是因为 169.254.0.0/16 地址范围是专门用于 link-local 通信的。这意味着这些地址仅在同一子网内可用，并且不需要经过路由器来进行通信。</p>
<p>在这个网络内使用 169.254.20.10 而不是.1 .2 这些则是留出几个位置，以避免冲突。</p>
<h2 id="5-小结">5. 小结</h2>
<p>CoreDNS 本身性能差是因为<strong>跨节点访问导致的大量性能损耗</strong>，同时由于内核 DNAT bug 导致超时等情况。</p>
<p>NodeLocal DNSCache 具有以下优势：</p>
<ul>
<li>减少了平均 DNS 查找时间</li>
<li>从 Pod 到其本地缓存的连接不会创建 conntrack 表条目。这样可以防止由于 conntrack 表耗尽和竞态条件而导致连接中断和连接被拒绝。</li>
</ul>
<p>使用 NodeLocalDNS 后性能提升接近 40%， DNS 解析延迟从 39ms 降低到 24ms，且报错次数大幅下降。</p>
<p>NodeLocalDNS 则使用 DaemonSet 方式启动在每个节点都启动一个 Pod，同时使用 hostnetwork + link-local 地址来保证 Pod 中的 DNS 请求只会请求到本地的 NodeLocalDNS Pod，从而避免了跨节点问题，大幅提升性能。</p>
<p>最后 NodeLocalDNS 使用 Link-local 地址也避免了默认情况下使用 service 的 clusterIP 需要 iptables/ipvs 等规则跳转的的问题，在同节点基础上也实现了略微的性能提升。</p>
<p>因此，对于大规模集群，存在高并发的 DNS 请求，推荐使用 NodeLocal DNSCache。</p>
<h2 id="6-参考">6. 参考</h2>
<p><a href="https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/nodelocaldns/" target="_blank" rel="noopener nofollow">在 Kubernetes 集群中使用 NodeLocal DNSCache</a></p>
<p><a href="https://help.aliyun.com/zh/ack/ack-managed-and-ack-dedicated/user-guide/configure-nodelocal-dnscache" target="_blank" rel="noopener nofollow">使用NodeLocal DNSCache</a></p>
<p><a href="https://www.qikqiak.com/k3s/network/localdns/" target="_blank" rel="noopener nofollow">DNS 超时问题分析</a></p>
<p><a href="https://www.flftuu.com/2021/03/01/NodeLocal-DNSCache-%E5%90%AF%E7%94%A8/#%E5%8E%8B%E6%B5%8B-dns-cache" target="_blank" rel="noopener nofollow">DNS 压测</a></p>
<p><a href="https://github.com/lixd/nodelocaldns-admission-webhook" target="_blank" rel="noopener nofollow">lixd/nodelocaldns-admission-webhook</a></p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.6946982817256945" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-01-06 13:33">2025-01-06 13:33</span>&nbsp;
<a href="https://www.cnblogs.com/KubeExplorer">探索云原生</a>&nbsp;
阅读(<span id="post_view_count">110</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18655127" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18655127);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18655127', targetLink: 'https://www.cnblogs.com/KubeExplorer/p/18655127', title: '使用 NodeLocalDNS 提升集群 DNS 性能和可靠性' })">举报</a>
</div>
        