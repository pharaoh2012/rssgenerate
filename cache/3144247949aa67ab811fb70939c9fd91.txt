
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/vivotech/p/18976396" title="发布于 2025-07-10 10:33">
    <span role="heading" aria-level="2">vivo Pulsar 万亿级消息处理实践（3）-KoP指标异常修复</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                    <div id="cnblogs_post_description" style="display: none">
        
        Apache Pulsar通过KoP兼容Kafka协议，使Kafka应用能无缝迁移至Pulsar，保留其生态优势，并提升性能、兼容性和可扩展性。vivo在使用Pulsar KoP的过程中遇到过一些问题，本篇主要分享一个分区消费指标缺失的问题。
    </div>
<div id="cnblogs_post_body" class="blogpost-body blogpost-body-html">
<blockquote data-pm-slice="0 0 []">
<p>作者：vivo 互联网大数据团队- Chen Jianbo</p>
<p>本文是《vivo Pulsar万亿级消息处理实践》系列文章第3篇。</p>
<p>Pulsar是Apache基金会的开源分布式流处理平台和消息中间件，它实现了Kafka的协议，可以让使用Kafka API的应用直接迁移至Pulsar，这使得Pulsar在Kafka生态系统中更加容易被接受和使用。KoP提供了从Kafka到Pulsar的无缝转换，用户可以使用Kafka API操作Pulsar集群，保留了Kafka的广泛用户基础和丰富生态系统。它使得Pulsar可以更好地与Kafka进行整合，提供更好的消息传输性能、更强的兼容性及可扩展性。vivo在使用Pulsar KoP的过程中遇到过一些问题，本篇主要分享一个分区消费指标缺失的问题。</p>
</blockquote>
<p>&nbsp;</p>
<p>系列文章：</p>
<ol>
<li>
<p data-number="1"><a href="https://mp.weixin.qq.com/s?__biz=MzI4NjY4MTU5Nw==&amp;mid=2247501335&amp;idx=1&amp;sn=3701be0b8b7b789e29c1ca53ba142e9d&amp;scene=21#wechat_redirect" data-type="link" data-id="link374729" rel="noopener nofollow">vivo Pulsar万亿级消息处理实践（1）-数据发送原理解析和性能调优</a></p>
</li>
<li>
<p data-number="2"><a href="https://mp.weixin.qq.com/s?__biz=MzI4NjY4MTU5Nw==&amp;mid=2247501426&amp;idx=1&amp;sn=76c04879cfa2c6b38a731b5c49f19d3a&amp;scene=21#wechat_redirect" data-type="link" data-id="link374729" rel="noopener nofollow">&nbsp;vivo Pulsar万亿级消息处理实践（2）-从0到1建设Pulsar指标监控链路</a></p>
</li>
</ol>
<p>&nbsp;</p>
<p>文章太长？1分钟看图抓住核心观点👇</p>
<p><img alt="图片" data-pm-attrs="{&quot;src&quot;:&quot;https://static001.geekbang.org/infoq/f8/f8e4ea72e6b629f6442897eb3978d5da.gif&quot;,&quot;alt&quot;:&quot;图片&quot;,&quot;title&quot;:null,&quot;style&quot;:[{&quot;key&quot;:&quot;width&quot;,&quot;value&quot;:&quot;75%&quot;},{&quot;key&quot;:&quot;bordertype&quot;,&quot;value&quot;:&quot;none&quot;}],&quot;href&quot;:null,&quot;fromPaste&quot;:true,&quot;pastePass&quot;:true}" data-src="https://static001.geekbang.org/infoq/f8/f8e4ea72e6b629f6442897eb3978d5da.gif" class="lazyload"></p>
<h1>一、问题背景</h1>
<p>在一次版本灰度升级中，我们发现某个使用KoP的业务topic的消费速率出现了显著下降，具体情况如下图所示：</p>
<p><img alt="图片" data-pm-attrs="{&quot;src&quot;:&quot;https://static001.geekbang.org/infoq/30/304579a316d7af6084d5764ccc382428.png&quot;,&quot;alt&quot;:&quot;图片&quot;,&quot;title&quot;:null,&quot;style&quot;:[{&quot;key&quot;:&quot;width&quot;,&quot;value&quot;:&quot;75%&quot;},{&quot;key&quot;:&quot;bordertype&quot;,&quot;value&quot;:&quot;none&quot;}],&quot;href&quot;:null,&quot;fromPaste&quot;:true,&quot;pastePass&quot;:true}" data-src="https://static001.geekbang.org/infoq/30/304579a316d7af6084d5764ccc382428.png" class="lazyload"></p>
<p>什么原因导致正常的升级重启服务器会出现这个问题呢？直接查看上报采集的数据报文：</p>
<pre class="highlighter-hljs"><code>kop_server_MESSAGE_OUT{group="",partition="0",tenant="kop",topic="persistent://kop-tenant/kop-ns/service-raw-stats"}&nbsp;3
kop_server_BYTES_OUT{group="",partition="0",tenant="kop",topic="persistent://kop-tenant/kop-ns/service-raw-stats"}&nbsp;188</code></pre>
<p>我们看到，KoP消费指标kop_server_MESSAGE</p>
<p>_OUT、kop_server_BYTES_OUT是有上报的，但指标数据里的group标签变成了空串（缺少消费组名称），分区的消费指标就无法展示了。是什么原因导致了消费组名称缺失？</p>
<p>&nbsp;</p>
<h1>二、问题分析</h1>
<p><strong>1、找到问题代码</strong></p>
<p>我们去找下这个消费组名称是在哪里获取的，是否逻辑存在什么问题。根据druid中的kop_subscription对应的消费指标kop_server_</p>
<p>MESSAGE_OUT、kop_server_BYTES_OUT，找到相关代码如下：</p>
<pre class="highlighter-hljs"><code>private&nbsp;void&nbsp;handleEntries(final&nbsp;List&lt;Entry&gt; entries,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;final&nbsp;TopicPartition topicPartition,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;final&nbsp;FetchRequest.PartitionData partitionData,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;final&nbsp;KafkaTopicConsumerManager tcm,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;final&nbsp;ManagedCursor cursor,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;final&nbsp;AtomicLong cursorOffset,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;final&nbsp;boolean&nbsp;readCommitted) {
....
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 处理消费数据时，获取消费组名称
&nbsp; &nbsp; &nbsp; &nbsp; CompletableFuture&lt;String&gt; groupNameFuture = requestHandler
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; .getCurrentConnectedGroup()
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; .computeIfAbsent(clientHost, clientHost -&gt; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; CompletableFuture&lt;String&gt; future =&nbsp;new&nbsp;CompletableFuture&lt;&gt;();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;String&nbsp;groupIdPath&nbsp;=&nbsp;GroupIdUtils.groupIdPathFormat(clientHost, header.clientId());
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; requestHandler.getMetadataStore()
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; .get(requestHandler.getGroupIdStoredPath() + groupIdPath)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; .thenAccept(getResultOpt -&gt; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(getResultOpt.isPresent()) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;GetResult&nbsp;getResult&nbsp;=&nbsp;getResultOpt.get();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; future.complete(new&nbsp;String(getResult.getValue() ==&nbsp;null
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ?&nbsp;new&nbsp;byte[0] : getResult.getValue(), StandardCharsets.UTF_8));
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;else&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 从zk节点 /client_group_id/xxx 获取不到消费组，消费组就是空的
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; future.complete("");
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }).exceptionally(ex -&gt; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; future.completeExceptionally(ex);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return&nbsp;null;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; });
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; returnfuture;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; });

&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// this part is heavyweight, and we should not execute in the ManagedLedger Ordered executor thread
&nbsp; &nbsp; &nbsp; &nbsp; groupNameFuture.whenCompleteAsync((groupName, ex) -&gt; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(ex !=&nbsp;null) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; log.error("Get groupId failed.", ex);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; groupName =&nbsp;"";
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
.....
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 获得消费组名称后，记录消费组对应的消费指标
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; decodeResult.updateConsumerStats(topicPartition,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; entries.size(),
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; groupName,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; statsLogger);</code></pre>
<p>代码的逻辑是，从requestHandler的currentConnectedGroup(map)中通过host获取groupName，不存在则通过MetadataStore（带缓存的zk存储对象）获取，如果zk缓存也没有，再发起zk读请求（路径为/client_group_id/host-clientId）。读取到消费组名称后，用它来更新消费组指标。从复现的集群确定走的是这个分支，即是从metadataStore(带缓存的zk客户端)获取不到对应zk节点/client_group_id/xxx。</p>
<p>&nbsp;</p>
<p><strong>2、查找可能导致zk节点/client_group_id/xxx节点获取不到的原因</strong></p>
<p>有两种可能性：一是没写进去，二是写进去但是被删除了。</p>
<pre class="highlighter-hljs"><code>&nbsp; &nbsp;&nbsp;@Override
&nbsp; &nbsp;&nbsp;protected&nbsp;void&nbsp;handleFindCoordinatorRequest(KafkaHeaderAndRequest findCoordinator,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; CompletableFuture&lt;AbstractResponse&gt; resultFuture) {
...
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// Store group name to metadata store for current client, use to collect consumer metrics.
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;storeGroupId(groupId, groupIdPath)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; .whenComplete((stat, ex) -&gt; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(ex !=&nbsp;null) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// /client_group_id/xxx节点写入失败
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; log.warn("Store groupId failed, the groupId might already stored.", ex);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;findBroker(TopicName.get(pulsarTopicName))
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; .whenComplete((node, throwable) -&gt; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ....
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; });
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; });
...</code></pre>
<p>从代码看到，clientId与groupId的关联关系是通过handleFindCoordinatorRequest（FindCoordinator）写进去的，而且只有这个方法入口。由于没有找到warn日志，排除了第一种没写进去的可能性。看看删除的逻辑：</p>
<pre class="highlighter-hljs"><code>protected&nbsp;void&nbsp;close(){
&nbsp; &nbsp;&nbsp;if&nbsp;(isActive.getAndSet(false)) {
&nbsp; &nbsp; &nbsp; &nbsp; ...
&nbsp; &nbsp; &nbsp; &nbsp; currentConnectedClientId.forEach(clientId -&gt; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;String&nbsp;path = groupIdStoredPath +&nbsp;GroupIdUtils.groupIdPathFormat(clientHost, clientId);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 删除zk上的 /client_group_id/xxx 节点
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; metadataStore.delete(path,&nbsp;Optional.empty())
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; .whenComplete((__, ex) -&gt; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(ex !=&nbsp;null) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(ex.getCause()&nbsp;instanceof&nbsp;MetadataStoreException.NotFoundException) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(log.isDebugEnabled()) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; log.debug("The groupId store path doesn't exist. Path: [{}]", path);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; log.error("Delete groupId failed. Path: [{}]", path, ex);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(log.isDebugEnabled()) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; log.debug("Delete groupId success. Path: [{}]", path);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; });
&nbsp; &nbsp; &nbsp; &nbsp; });
&nbsp; &nbsp; }
}</code></pre>
<p>删除是在requsetHandler.close方法中执行，也就是说连接断开就会触发zk节点删除。</p>
<p>但有几个<strong>疑问：</strong></p>
<ul>
<li>
<p>/client_group_id/xxx 到底是干嘛用的？消费指标为什么要依赖它</p>
</li>
<li>
<p>为什么要在handleFindCoordinatorRequest写入？</p>
</li>
<li>
<p>节点/client_group_id/xxx为什么要删除，而且是在连接断开时删除，删除时机是否有问题？</p>
</li>
</ul>
<p>首先回答第1个问题，通过阅读代码可以知道，/client_group_id/xxx 这个zk节点是用于在不同broker实例间交换数据用的(相当redis cache)，用于临时存放IP+clientId与groupId的映射关系。由于fetch接口（拉取数据）的request没有groupId的，只能依赖加入Group过程中的元数据，在fetch消费时才能知道当前拉数据的consumer是哪个消费组的。</p>
<p><img alt="图片" data-pm-attrs="{&quot;src&quot;:&quot;https://static001.geekbang.org/infoq/98/98e8fd0e9b565bda9ad638fde500453b.png&quot;,&quot;alt&quot;:&quot;图片&quot;,&quot;title&quot;:null,&quot;style&quot;:[{&quot;key&quot;:&quot;width&quot;,&quot;value&quot;:&quot;75%&quot;},{&quot;key&quot;:&quot;bordertype&quot;,&quot;value&quot;:&quot;none&quot;}],&quot;href&quot;:null,&quot;fromPaste&quot;:true,&quot;pastePass&quot;:true}" data-src="https://static001.geekbang.org/infoq/98/98e8fd0e9b565bda9ad638fde500453b.png" class="lazyload"></p>
<p>&nbsp;</p>
<p><strong>3、复现</strong></p>
<p>若要解决问题，最好能够稳定地复现出问题，这样才能确定问题的根本原因，并且确认修复是否完成。</p>
<p>因为节点是在requsetHandle.close方法中执行删除，broker节点关闭会触发连接关闭，进而触发删除。假设：客户端通过brokerA发起FindCoordinator请求，写入zk节点/client_group</p>
<p>_id/xxx，同时请求返回brokerB作为Coordinator，后续与brokerB进行joinGroup、syncGroup等交互确定消费关系，客户端在brokerA、brokerB、brokerC都有分区消费。这时重启brokerA，分区均衡到BrokerC上，但此时/client_group_id/xxx因关闭broker而断开连接被删除，consumer消费刚转移到topic1-partition-1的分区就无法获取到groupId。</p>
<p><img alt="图片" data-pm-attrs="{&quot;src&quot;:&quot;https://static001.geekbang.org/infoq/cd/cdffe20b8cea8bc8cdbef12b1e84a6bb.png&quot;,&quot;alt&quot;:&quot;图片&quot;,&quot;title&quot;:null,&quot;style&quot;:[{&quot;key&quot;:&quot;width&quot;,&quot;value&quot;:&quot;75%&quot;},{&quot;key&quot;:&quot;bordertype&quot;,&quot;value&quot;:&quot;none&quot;}],&quot;href&quot;:null,&quot;fromPaste&quot;:true,&quot;pastePass&quot;:true}" data-src="https://static001.geekbang.org/infoq/cd/cdffe20b8cea8bc8cdbef12b1e84a6bb.png" class="lazyload"></p>
<p>按照假设，有3个broker，开启生产和消费，通过在FindCoordinator返回前获取node.leader()的返回节点BrokerB，关闭brokerA后，brokerC出现断点复现，再关闭brokerC，brokerA也会复现（假设分区在brokerA与brokerC之间转移）。</p>
<p><img alt="图片" data-pm-attrs="{&quot;src&quot;:&quot;https://static001.geekbang.org/infoq/4c/4c0b2db443a5f7658384eb6b271e46b7.png&quot;,&quot;alt&quot;:&quot;图片&quot;,&quot;title&quot;:null,&quot;style&quot;:[{&quot;key&quot;:&quot;width&quot;,&quot;value&quot;:&quot;75%&quot;},{&quot;key&quot;:&quot;bordertype&quot;,&quot;value&quot;:&quot;none&quot;}],&quot;href&quot;:null,&quot;fromPaste&quot;:true,&quot;pastePass&quot;:true}" data-src="https://static001.geekbang.org/infoq/4c/4c0b2db443a5f7658384eb6b271e46b7.png" class="lazyload"></p>
<p>复现要几个条件：</p>
<ol>
<li>
<p data-number="1">broker数量要足够多(不小于3个）</p>
</li>
<li>
<p data-number="2">&nbsp;broker内部有zk缓存metadataCache默认为5分钟，可以把时间调小为1毫秒，相当于没有cache</p>
</li>
<li>
<p data-number="3">&nbsp;findCoordinator返回的必须是其他broker的IP</p>
</li>
<li>
<p data-number="4">&nbsp;重启的必须是接收到findCoordinator请求那台broker，而不是真正的coordinator，这时会从zk删除节点</p>
</li>
<li>
<p data-number="5">分区转移到其他broker，这时新的broker会重新读取zk节点数据</p>
</li>
</ol>
<p>到此，我们基本上清楚了问题原因：连接关闭导致zk节点被删除了，别的broker节点需要时就读取不到了。那怎么解决？</p>
<p>&nbsp;</p>
<h1>三、问题解决</h1>
<p><strong>方案一</strong></p>
<p>既然知道把消费者与FindCoordinator的连接进行绑定不合适的，那么是否应该把FindCoordinator写入zk节点换成由JoinGroup写入，断连即删除。</p>
<p><img alt="图片" data-pm-attrs="{&quot;src&quot;:&quot;https://static001.geekbang.org/infoq/d0/d08470f5d1d0d3d2c14931425f7acc2d.png&quot;,&quot;alt&quot;:&quot;图片&quot;,&quot;title&quot;:null,&quot;style&quot;:[{&quot;key&quot;:&quot;width&quot;,&quot;value&quot;:&quot;75%&quot;},{&quot;key&quot;:&quot;bordertype&quot;,&quot;value&quot;:&quot;none&quot;}],&quot;href&quot;:null,&quot;fromPaste&quot;:true,&quot;pastePass&quot;:true}" data-src="https://static001.geekbang.org/infoq/d0/d08470f5d1d0d3d2c14931425f7acc2d.png" class="lazyload"></p>
<p>consumer统一由Coordinator管理，由于FindCoordinator接口不一定是Coordinator处理的，如果换成由Coordinator处理的JoinGroup接口是否就可以了，这样consumer断开与Coordinator的连接就应该删除数据。但实现验证时却发现，客户端在断连后也不会再重连，所以没法重新写入zk，不符合预期。</p>
<p>&nbsp;</p>
<p><strong>方案二</strong></p>
<p>还是由FindCoordinator写入zk节点，但删除改为GroupCoordinator监听consumer断开触发。</p>
<p>因为consumer统一由Coordinator管理，它能监听到consumer加入或者离开。GroupCoordinator的removeMemberAndUpdateGroup方法是coordinator对consumer成员管理。</p>
<pre class="highlighter-hljs"><code>private&nbsp;void&nbsp;removeMemberAndUpdateGroup(GroupMetadata&nbsp;group,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MemberMetadata member) {
&nbsp; &nbsp;&nbsp;group.remove(member.memberId());
&nbsp; &nbsp;&nbsp;switch&nbsp;(group.currentState()) {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;case&nbsp;Dead:
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;case&nbsp;Empty:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;case&nbsp;Stable:
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;case&nbsp;CompletingRebalance:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; maybePrepareRebalance(group);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;break;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;case&nbsp;PreparingRebalance:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; joinPurgatory.checkAndComplete(new&nbsp;GroupKey(group.groupId()));
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;break;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;default:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;break;
&nbsp; &nbsp; }
&nbsp; &nbsp;&nbsp;// 删除 /client_group_id/xxx 节点
&nbsp; &nbsp; deleteClientIdGroupMapping(group, member.clientHost(), member.clientId());
}</code></pre>
<p>&nbsp;</p>
<p>调用入口有两个，其中handleLeaveGroup是主动离开，onExpireHeartbeat是超时被动离开，客户端正常退出或者宕机都可以调用removeMemberAndUpdateGroup方法触发删除。</p>
<pre class="highlighter-hljs"><code>public&nbsp;CompletableFuture&lt;Errors&gt;&nbsp;handleLeaveGroup(
&nbsp; &nbsp; String groupId,
&nbsp; &nbsp; String memberId
) {
&nbsp; &nbsp;&nbsp;return&nbsp;validateGroupStatus(groupId, ApiKeys.LEAVE_GROUP).map(error -&gt;
&nbsp; &nbsp; &nbsp; &nbsp; CompletableFuture.completedFuture(error)
&nbsp; &nbsp; ).orElseGet(() -&gt; {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return&nbsp;groupManager.getGroup(groupId).map(group&nbsp;-&gt; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return&nbsp;group.inLock(() -&gt; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(group.is(Dead) || !group.has(memberId)) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return&nbsp;CompletableFuture.completedFuture(Errors.UNKNOWN_MEMBER_ID);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }&nbsp;else&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ...
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 触发删除消费者consumer
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; removeMemberAndUpdateGroup(group, member);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return&nbsp;CompletableFuture.completedFuture(Errors.NONE);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; });
&nbsp; &nbsp; &nbsp; &nbsp; })
&nbsp; &nbsp; &nbsp; &nbsp; ....
&nbsp; &nbsp; });
}</code></pre>
<p>&nbsp;</p>
<pre class="highlighter-hljs"><code>void&nbsp;onExpireHeartbeat(GroupMetadata&nbsp;group,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MemberMetadata member,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;long&nbsp;heartbeatDeadline) {
&nbsp; &nbsp;&nbsp;group.inLock(() -&gt; {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(!shouldKeepMemberAlive(member, heartbeatDeadline)) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; log.info("Member {} in group {} has failed, removing it from the group",
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; member.memberId(),&nbsp;group.groupId());
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 触发删除消费者consumer
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; removeMemberAndUpdateGroup(group, member);
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return&nbsp;null;
&nbsp; &nbsp; });
}</code></pre>
<p>但这个方案有个问题是，日志运维关闭broker也会触发一个onExpireHeartbeat事件删除zk节点，与此同时客户端发现Coordinator断开了会马上触发FindCoordinator写入新的zk节点，但如果删除晚于写入的话，会导致误删除新写入的节点。我们干脆在关闭broker时，使用ShutdownHook加上shuttingdown状态防止关闭broker时删除zk节点，只有客户端断开时才删除。</p>
<p>这个方案修改上线半个月后，还是出现了一个客户端的消费指标无法上报的情况。后来定位发现，如果客户端因FullGC出现卡顿情况，客户端可能会先于broker触发超时，也就是先超时的客户端新写入的数据被后监听到超时的broker误删除了。因为写入与删除并不是由同一个节点处理，所以无法在进程级别做并发控制，而且也无法判断哪次删除对应哪次的写入，所以用zk也是很难实现并发控制。</p>
<p>&nbsp;</p>
<p><strong>方案三</strong></p>
<p>其实这并不是新的方案，只是在方案二基础上优化：数据一致性检查。</p>
<p>既然我们很难控制好写入与删除的先后顺序，我们可以做数据一致性检查，类似于交易系统里的对账。因为GroupCoordinator是负责管理consumer成员的，维护着consumer的实时状态，就算zk节点被误删除，我们也可以从consumer成员信息中恢复，重新写入zk节点。</p>
<pre class="highlighter-hljs"><code>private&nbsp;void&nbsp;checkZkGroupMapping(){ &nbsp;
&nbsp; &nbsp;&nbsp;for&nbsp;(GroupMetadata&nbsp;group&nbsp;: groupManager.currentGroups()) { &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;for&nbsp;(MemberMetadata memberMetadata :&nbsp;group.allMemberMetadata()) { &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; String clientPath = GroupIdUtils.groupIdPathFormat(memberMetadata.clientHost(), memberMetadata.clientId()); &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; String zkGroupClientPath = kafkaConfig.getGroupIdZooKeeperPath() + clientPath; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 查找zk中是否存在节点
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; metadataStore.get(zkGroupClientPath).thenAccept(resultOpt -&gt; { &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;if&nbsp;(!resultOpt.isPresent()) { &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;// 不存在则进行补偿修复
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; metadataStore.put(zkGroupClientPath, memberMetadata.groupId().getBytes(UTF\_8), Optional.empty()) &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; .thenAccept(stat -&gt; { &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; log.info("repaired clientId and group mapping: {}({})", &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; zkGroupClientPath, memberMetadata.groupId()); &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }) &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; .exceptionally(ex -&gt; { &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; log.warn("repaired clientId and group mapping failed: {}({})", &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; zkGroupClientPath, memberMetadata.groupId()); &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return&nbsp;null; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }); &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; } &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }).exceptionally(ex -&gt; { &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; log.warn("repaired clientId and group mapping failed: {} ", zkGroupClientPath, ex); &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;return&nbsp;null; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }); &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; } &nbsp;
&nbsp; &nbsp; } &nbsp;
}</code></pre>
<p>经过方案三的优化上线，即使是历史存在问题的消费组，个别分区消费流量指标缺少group字段的问题也得到了修复。具体效果如下图所示：</p>
<p><img alt="图片" data-pm-attrs="{&quot;src&quot;:&quot;https://static001.geekbang.org/infoq/e7/e729d5dbc1791dee7e507efb3c603ede.png&quot;,&quot;alt&quot;:&quot;图片&quot;,&quot;title&quot;:null,&quot;style&quot;:[{&quot;key&quot;:&quot;width&quot;,&quot;value&quot;:&quot;75%&quot;},{&quot;key&quot;:&quot;bordertype&quot;,&quot;value&quot;:&quot;none&quot;}],&quot;href&quot;:null,&quot;fromPaste&quot;:true,&quot;pastePass&quot;:true}" data-src="https://static001.geekbang.org/infoq/e7/e729d5dbc1791dee7e507efb3c603ede.png" class="lazyload"></p>
<p>&nbsp;</p>
<h1>四、总结</h1>
<p>经过多个版本的优化和线上验证，最终通过方案三比较完美的解决了这个消费指标问题。在分布式系统中，并发问题往往难以模拟和复现，我们也在尝试多个版本后才找到有效的解决方案。如果您在这方面有更好的经验或想法，欢迎提出，我们共同探讨和交流。</p>
</div>
<div id="MySignature" role="contentinfo">
    分享 vivo 互联网技术干货与沙龙活动，推荐最新行业动态与热门会议。
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0" data-date-updated="2025-07-10 10:34">2025-07-10 10:33</span>&nbsp;
<a href="https://www.cnblogs.com/vivotech">vivo互联网技术</a>&nbsp;
阅读(<span id="post_view_count">102</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18976396);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18976396', targetLink: 'https://www.cnblogs.com/vivotech/p/18976396', title: 'vivo Pulsar 万亿级消息处理实践（3）-KoP指标异常修复' })">举报</a>
</div>
        