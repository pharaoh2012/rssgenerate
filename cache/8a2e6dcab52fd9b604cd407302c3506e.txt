
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/hvker/p/18786501" title="发布于 2025-03-22 13:51">
    <span role="heading" aria-level="2">一步一步教你部署ktransformers，大内存单显卡用上Deepseek-R1</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h1 id="环境准备">环境准备</h1>
<h2 id="硬件环境">硬件环境</h2>
<p>CPU：intel四代至强及以上，AMD参考同时期产品<br>
内存：800GB以上，内存性能越强越好，建议DDR5起步<br>
显卡：Nvidia显卡，单卡显存至少24GB（用T4-16GB显卡实测会在加载模型过程中爆显存），nvidia compute capability至少8.0（<a href="https://developer.nvidia.com/cuda-gpus" target="_blank" rel="noopener nofollow">CUDA GPUs - Compute Capability | NVIDIA Developer</a>）</p>
<h2 id="系统环境">系统环境</h2>
<p>Ubuntu Server 24.04 LTS</p>
<h2 id="cuda-124">CUDA 12.4</h2>
<h3 id="屏蔽开源nvidia显卡驱动">屏蔽开源nvidia显卡驱动</h3>
<pre><code class="language-shell"># 编辑黑名单配置，屏蔽开源nvidia显卡驱动，以安装官方驱动
sudo vi /etc/modprobe.d/blacklist.conf
</code></pre>
<p>在文件的最后添加下面两行</p>
<pre><code class="language-ini">blacklist nouveau
options nouveau modeset=0
</code></pre>
<p>输入下面的命令更新并重启</p>
<pre><code class="language-shell">sudo update-initramfs -u
sudo reboot
</code></pre>
<p>继续执行命令</p>
<pre><code class="language-shell">lsmod | grep nouveau # 验证是否禁用成功，成功的话这行命令不会有输出
sudo apt-get purge nvidia* # 卸载已有的驱动
</code></pre>
<h3 id="安装cuda工具包">安装cuda工具包</h3>
<p>将<a href="https://developer.nvidia.com/cuda-12-4-0-download-archive?target_os=Linux&amp;target_arch=x86_64&amp;Distribution=Ubuntu&amp;target_version=22.04&amp;target_type=runfile_local" target="_blank" rel="noopener nofollow">cuda工具包</a>下载好后（不要盲目追求新版，平台会不适配），在驱动所在目录执行（选择安装选项中，默认选中安装显卡驱动）</p>
<pre><code class="language-shell">sudo chmod +x cuda_12.4.0_550.54.14_linux.run
sudo sh cuda_12.4.0_550.54.14_linux.run
</code></pre>
<p>设置全局环境变量</p>
<pre><code class="language-shell">sudo vi /etc/profile.d/myenv.sh
</code></pre>
<pre><code class="language-ini">export PATH="$PATH:/usr/local/cuda-12.4/bin"
export LD_LIBRARY_PATH="/usr/local/cuda-12.4/lib64:$LD_LIBRARY_PATH"
export LIBRARY_PATH="/usr/local/cuda-12.4/lib64:$LIBRARY_PATH"
export CUDA_PATH="/usr/local/cuda-12.4"
</code></pre>
<p>重启使生效，指令<code>nvcc -V</code>测试</p>
<h3 id="安装cudnn">安装cuDNN</h3>
<p>将<a href="https://developer.nvidia.com/cudnn-downloads?target_os=Linux&amp;target_arch=x86_64&amp;Distribution=Ubuntu&amp;target_version=24.04&amp;target_type=deb_local" target="_blank" rel="noopener nofollow">cuDNN</a>下载好后，在所在目录执行</p>
<pre><code class="language-shell">sudo dpkg -i cudnn-local-repo-ubuntu2404-9.7.1_1.0-1_amd64.deb
sudo cp /var/cudnn-local-repo-ubuntu2404-9.7.1/cudnn-*-keyring.gpg /usr/share/keyrings/
sudo apt-get update
sudo apt-get -y install cudnn-cuda-12
</code></pre>
<p>更新cuDNN时，在apt-get update前删除旧版，比如</p>
<pre><code class="language-shell">sudo rm -rf /var/cudnn-local-repo-ubuntu2404-9.6.0
sudo rm cudnn-local-ubuntu2404-9.6.0.list
</code></pre>
<h2 id="系统软件包">系统软件包</h2>
<pre><code class="language-shell">sudo apt-get update
sudo apt-get install build-essential cmake ninja-build
</code></pre>
<h2 id="miniconda3">Miniconda3</h2>
<p><a href="https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh" target="_blank" rel="noopener nofollow">Miniconda3安装脚本</a></p>
<h3 id="conda环境创建">conda环境创建</h3>
<p>访问pypi困难的小伙伴可以参考换源方法：<br>
<a href="https://mirrors.huaweicloud.com/mirrorDetail/5ea14ecab05943f36fb75ee6?mirrorName=python&amp;catalog=tool" target="_blank" rel="noopener nofollow">https://mirrors.huaweicloud.com/mirrorDetail/5ea14ecab05943f36fb75ee6?mirrorName=python&amp;catalog=tool</a></p>
<pre><code class="language-shell">conda create --name ktransformers python=3.11
conda activate ktransformers # 首次使用你可能需要执行 ‘conda init’ 并重开终端

conda install -c conda-forge libstdcxx-ng # Anaconda provides a package called `libstdcxx-ng` that includes a newer version of `libstdc++`, which can be installed via `conda-forge`.

strings ~/miniconda3/envs/ktransformers/lib/libstdc++.so.6 | grep GLIBCXX

pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124 pip3 install packaging ninja cpufeature numpy
</code></pre>
<p>注：download.pytorch.org不挂代理的话速度很慢。截至2025年3月22日，pytorch不加--index-url参数默认即 whl/cu124，所以可以去掉--index-url参数以事先设置好的全局国内源下载。实时具体情况可在<a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener nofollow">Start Locally | PyTorch</a>查看。</p>
<h2 id="flash-attention">flash-attention</h2>
<p><a href="https://github.com/Dao-AILab/flash-attention/releases" target="_blank" rel="noopener nofollow">flash-attention下载页</a><br>
注：打开链接后，点击show more可以查看更多版本，根据以上安装的版本信息下载相应版本，一般cxx11abi为FALSE</p>
<h1 id="安装ktransformers">安装ktransformers</h1>
<h3 id="拉取源码并编译">拉取源码并编译</h3>
<p>注：访问不了github的小伙伴可以在网上搜索github换源方法</p>
<pre><code class="language-shell">git clone https://github.com/kvcache-ai/ktransformers.git
cd ktransformers
git submodule init
git submodule update
</code></pre>
<p>安装<br>
<strong>如果服务器是2CPU+2倍模型大小及以上内存的需要先执行：</strong></p>
<pre><code class="language-shell"> # Make sure your system has dual sockets and double size RAM than the model's size (e.g. 1T RAM for 512G model)
apt install libnuma-dev
export USE_NUMA=1
</code></pre>
<p>开始安装</p>
<pre><code class="language-shell">bash install.sh
</code></pre>
<h1 id="启用服务">启用服务</h1>
<h2 id="下载模型">下载模型</h2>
<h3 id="模型下载源">模型下载源</h3>
<ol>
<li><a href="https://hf-mirror.com/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-Q4_K_M" target="_blank" rel="noopener nofollow">unsloth/DeepSeek-R1-Q4_K_M</a></li>
<li>ollama拉取后，在blob内找到最大的文件，添加后缀.gguf</li>
</ol>
<h3 id="模型存放">模型存放</h3>
<p>以下按照模型存放于<code>/mnt/data/models/DeepSeek-R1-Q4_K_M_GGUF/</code>为例<br>
将下载好的gguf文件全部放入该文件夹<br>
然后下载<br>
<a href="https://hf-mirror.com/unsloth/DeepSeek-R1-GGUF/blob/main/config.json" target="_blank" rel="noopener nofollow">config.json</a><br>
<a href="https://hf-mirror.com/deepseek-ai/DeepSeek-R1/blob/main/configuration_deepseek.py" target="_blank" rel="noopener nofollow">configuration_deepseek.py</a><br>
<a href="https://hf-mirror.com/deepseek-ai/DeepSeek-R1/blob/main/generation_config.json" target="_blank" rel="noopener nofollow">generation_config.json</a><br>
<a href="https://hf-mirror.com/deepseek-ai/DeepSeek-R1/blob/main/model.safetensors.index.json" target="_blank" rel="noopener nofollow">model.safetensors.index.json</a><br>
<a href="https://hf-mirror.com/deepseek-ai/DeepSeek-R1/blob/main/modeling_deepseek.py" target="_blank" rel="noopener nofollow">modeling_deepseek.py</a><br>
<a href="https://hf-mirror.com/deepseek-ai/DeepSeek-R1/blob/main/tokenizer.json" target="_blank" rel="noopener nofollow">tokenizer.json</a><br>
<a href="https://hf-mirror.com/deepseek-ai/DeepSeek-R1/blob/main/tokenizer_config.json" target="_blank" rel="noopener nofollow">tokenizer_config.json</a><br>
存入该文件夹</p>
<h2 id="启用restful-api服务">启用RESTful API服务</h2>
<pre><code class="language-shell">ktransformers --model_name DeepSeek-R1-q4_k_m --model_path /mnt/data/models/DeepSeek-R1-Q4_K_M_GGUF --gguf_path /mnt/data/models/DeepSeek-R1-Q4_K_M_GGUF --optimize_config_path ~/ktransformers/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat.yaml --port 10002 --cpu_infer 65 --max_new_tokens 2048 --force_think
</code></pre>
<p>注：</p>
<ol>
<li>--optimize_config_path后面的文件夹位置相应换成你git clone ktransformers时的位置</li>
<li>The command args&nbsp;<code>--cpu_infer 65</code>&nbsp;specifies how many cores to use (it's ok that it exceeds the physical number, but it's not the more the better. Adjust it slightly lower to your actual number of cores)</li>
</ol>
<p>API兼容OpenAI和Ollama</p>
<h1 id="测试效果">测试效果</h1>
<h2 id="试验环境">试验环境</h2>
<p>CPU：至强Gold-6454S-2.20GHz@32核心 x2<br>
内存：DDR5-64G x16<br>
显卡：NV L20-48G x4<br>
硬盘：1.92TB SATA SSD x2<br>
RAID卡：1G缓存 带电容<br>
网卡：2个千兆电口+2个万兆光口(含光模块)</p>
<h2 id="效果截图">效果截图</h2>
<p><img src="https://img2024.cnblogs.com/blog/3538846/202503/3538846-20250322135005281-1135003073.png" alt="" loading="lazy"></p>
<p><img src="https://img2024.cnblogs.com/blog/3538846/202503/3538846-20250322135013102-1193998556.png" alt="" loading="lazy"><br>
测试每秒tokens在6-13tokens/s左右</p>
<p><img src="https://img2024.cnblogs.com/blog/3538846/202503/3538846-20250322135019544-2128946462.png" alt="" loading="lazy"></p>
<p><img src="https://img2024.cnblogs.com/blog/3538846/202503/3538846-20250322135024093-1639141160.png" alt="" loading="lazy"></p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.18340732920833333" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-03-22 13:52">2025-03-22 13:51</span>&nbsp;
<a href="https://www.cnblogs.com/hvker">hvker</a>&nbsp;
阅读(<span id="post_view_count">35</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18786501" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18786501);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18786501', targetLink: 'https://www.cnblogs.com/hvker/p/18786501', title: '一步一步教你部署ktransformers，大内存单显卡用上Deepseek-R1' })">举报</a>
</div>
        