
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/moonout/p/18983738" title="发布于 2025-07-14 12:38">
    <span role="heading" aria-level="2">Skill Discovery | DoDont：使用 do + don't 示例视频，引导 agent 学习人类期望的 skill</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                    <div id="cnblogs_post_description" style="display: none">
        
        dodont 将好坏行为的分类器 p hat 融入了 metra 框架里，因此看起来很有直觉。
    </div>
<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<br>
<ul>
<li>论文标题：Do's and Don'ts: Learning Desirable Skills with Instruction Videos</li>
<li>NeurIPS 2024 poster。</li>
<li>arxiv：<a href="https://arxiv.org/abs/2406.00324" target="_blank" rel="noopener nofollow">https://arxiv.org/abs/2406.00324</a></li>
<li>pdf：<a href="https://arxiv.org/pdf/2406.00324" target="_blank" rel="noopener nofollow">https://arxiv.org/pdf/2406.00324</a></li>
<li>html：<a href="https://arxiv.org/html/2406.00324" target="_blank" rel="noopener nofollow">https://arxiv.org/html/2406.00324</a></li>
<li>website：<a href="https://mynsng.github.io/dodont/" target="_blank" rel="noopener nofollow">https://mynsng.github.io/dodont/</a></li>
<li>open review：<a href="https://openreview.net/forum?id=7X5zu6GIuW" target="_blank" rel="noopener nofollow">https://openreview.net/forum?id=7X5zu6GIuW</a></li>
</ul>
<p>主要内容：</p>
<ul>
<li>这篇文章关注 skill discovery，希望通过好视频和坏视频，引导 agent 学到我们希望它探索的 skill。提出了 DoDont 方法。</li>
<li>动机：纯粹无监督学习在现实场景不安全且低效。用视频表达人类意图：低成本（无需动作标签）、易扩展（8个视频即可）。</li>
<li>主要方法是 metra（<a href="https://www.cnblogs.com/moonout/p/18980763" target="_blank">本站博客</a>）魔改。</li>
<li>具体的，4.1 节训了一个分类器 <span class="math inline">\(\hat p_\psi(s_t, s_{t+1})\)</span>，用来判别 transition <span class="math inline">\((s_t, s_{t+1})\)</span> 是否是人类喜欢的，直接用 cross-entropy loss function。</li>
<li>然后，在 4.2 节，dodont 把 metra 的 temporal distance 直接魔改成 <span class="math inline">\(\|\phi(s)-\phi(s')\| \le \hat p_\psi(s, s')\)</span> ，虽然这样看起来没什么道理，但可以借用 metra 的形式，得到这个形式：</li>
</ul>
<p></p><div class="math display">\[\sup_{\pi, \phi} \quad \mathbb{E}_{p(\tau, z)} \left[ \sum_{t=0}^{T-1} \hat p_\psi(s_t, s_{t+1})[\phi(s_{t+1}) - \phi(s_t)]^\top z \right] \quad \text{s.t.} \quad \|\phi(s) - \phi(s')\|_2 \leq 1 \tag{1}
\]</div><p></p><ul>
<li>这个形式只需要魔改 metra，在原先的 reward 上乘一个 <span class="math inline">\(\hat p_\psi(s_t, s_{t+1})\)</span> ，实现起来非常方便。</li>
</ul>
<hr>
<p></p><div class="toc"><div class="toc-container-header">目录</div><ul><li><a href="#1-dodont-故事纯故事method-很简单" rel="noopener nofollow">1 dodont 故事（纯故事，method 很简单）</a><ul><li><a href="#11-motivation故事的核心---现有方法的-gap-与解决方案" rel="noopener nofollow">1.1 motivation：故事的核心 - 现有方法的 Gap 与解决方案</a></li><li><a href="#12-method-的合理性与-intuition-方法包装" rel="noopener nofollow">1.2 method 的合理性与 Intuition (方法包装)</a></li><li><a href="#13-总结故事线" rel="noopener nofollow">1.3 总结故事线</a></li></ul></li><li><a href="#2-dodont-的方法" rel="noopener nofollow">2 dodont 的方法</a></li><li><a href="#3-dodont-的实验" rel="noopener nofollow">3 dodont 的实验</a><ul><li><a href="#31-实验-setting" rel="noopener nofollow">3.1 实验 setting</a></li><li><a href="#32-baseline" rel="noopener nofollow">3.2 baseline</a></li><li><a href="#33-评价指标" rel="noopener nofollow">3.3 评价指标</a></li><li><a href="#34-实验结果" rel="noopener nofollow">3.4 实验结果</a></li><li><a href="#35-更多细节" rel="noopener nofollow">3.5 更多细节</a></li></ul></li></ul></div><p></p>
<hr>
<h2 id="1-dodont-故事纯故事method-很简单">1 dodont 故事（纯故事，method 很简单）</h2>
<p>这篇论文讲了一个关于让 AI 智能体 <strong>更安全、更高效地自学技能</strong> 的故事。核心是解决无监督技能发现（unsupervised skill discovery）中的两个大问题：学不会复杂技能、学会危险 / 无用技能。</p>
<h3 id="11-motivation故事的核心---现有方法的-gap-与解决方案">1.1 motivation：故事的核心 - 现有方法的 Gap 与解决方案</h3>
<ul>
<li>Gap 1: 学不会复杂技能。现有的 skill discovery 方法，如最大化互信息或状态距离，在简单环境还能 work，但在复杂环境 如多关节四足机器人跑步中效率低下，可能根本学不会目标技能，如跑步。</li>
<li>Gap 2: 学会危险 / 无用技能。更严重的是，纯无监督探索没有约束，智能体很容易学会危险或无效的行为，比如：
<ul>
<li>物理危险：摔倒、翻滚（可能损坏机器人）。</li>
<li>导航危险：掉进坑里、进入禁区。</li>
<li>无效行为：停留在原地做无意义动作。</li>
</ul>
</li>
<li>如何解决，Intuition：人类学习不仅靠自我探索，即内部动机，也需要一些观察示范和避免错误，即外部动机。dodont 想将这种外部动机，具体的，该做什么（Do’s）和 不该做什么（Don’ts）的指导融入 skill discovery 。</li>
<li>关键创新点（Motivation 的落脚点）：与其费时费力地手工设计复杂的奖励函数，难以平衡多个目标且不通用，dodont 提出用 <strong>低成本</strong>、<strong>无动作标签</strong> 的 <strong>教学视频</strong> (Instruction Videos) 来传达人类的意图。只需要少量（&lt;8个）展示好行为 do 和坏行为 don't 的视频。</li>
</ul>
<h3 id="12-method-的合理性与-intuition-方法包装">1.2 method 的合理性与 Intuition (方法包装)</h3>
<ul>
<li>核心思想： 用视频训练一个“教学网络”（Instruction Network）来区分行为的好坏，然后将这个网络的输出 <strong>融入</strong> 到原有的 skill discovery 优化目标中，引导智能体学 do，避开 don't。</li>
<li>方法流程与合理性：</li>
<li>1 教学阶段 (Offline)：
<ul>
<li>收集少量 Do’s 视频 期望行为，如跑步，和 Don’ts 视频 禁止行为，如翻滚、掉坑。</li>
<li>训练一个二元分类网络 <span class="math inline">\(\hat p_\psi(s_t, s_{t+1})\)</span>，这个网络学会给“好”的状态转移 <span class="math inline">\((s_t, s_{t+1})\)</span> 打高分（接近1），给“坏”的转移打低分（接近0）。根据 dodont 的故事，它本质上是一个 <strong>基于人类意图的距离 / 价值函数</strong>。</li>
</ul>
</li>
<li>2 技能学习阶段 (Online)：
<ul>
<li>首先，选择一个强大的 skill discovery 基础算法，比如距离最大化的 skill discovery（如 METRA <a href="https://www.cnblogs.com/moonout/p/18980763" target="_blank">本站博客</a>）。这类方法的核心，是基于一个 embedding，最大化一个距离函数 <span class="math inline">\(d(s, s')\)</span>，鼓励智能体产生不同的、能到达远处状态的行为轨迹。</li>
<li>关键融合步骤：将训练好的教学网络 <span class="math inline">\(\hat p_\psi(s_t, s_{t+1})\)</span> 直接作为 skill discovery 算法中的距离函数 <span class="math inline">\(d(s, s')\)</span>。</li>
<li>Intuition 1 (合理性)：在距离最大化框架下，<span class="math inline">\(d(s, s')\)</span> 越大，智能体越被鼓励执行从 s 到 s' 的转移。
<ul>
<li>直观上，<span class="math inline">\(\hat p_\psi(s_t, s_{t+1})\)</span> 对“好”转移打高分，意味着智能体会被 强烈鼓励 去执行这些转移（相当于放大了好行为的“吸引力”）。</li>
<li>反之，对“坏”转移打低分（甚至接近 0），意味着这些转移几乎没有“距离收益”，智能体自然没有动力去学习它们（相当于抑制了坏行为的“吸引力”）。</li>
</ul>
</li>
<li>Intuition 2 (有效性)：dodont 发现，对比加法奖励和直接优化 metra 距离乘 <span class="math inline">\(\hat p_\psi(s_t, s_{t+1})\)</span> 的形式，直接将 <span class="math inline">\(\hat p_\psi(s_t, s_{t+1})\)</span> 乘到 skill discovery 算法的内在奖励上效果最好。</li>
</ul>
</li>
</ul>
<h3 id="13-总结故事线">1.3 总结故事线</h3>
<ol>
<li>问题：纯无监督技能发现（skill discovery）在复杂环境中效率低，且会学危险 / 无用技能。</li>
<li>intuition：模仿人类学习，引入“该做(Do’s)”和“不该做(Don’ts)”的外部指导。</li>
<li>应用 intuition 的方案：用 <strong>极少量无标签视频</strong> 训练一个 <strong>教学网络</strong> 来量化行为好坏。</li>
<li>与 metra 的融合：将教学网络 <strong>无缝嵌入</strong> 强大的 skill discovery 算法（METRA）中，<strong>直接替换其核心的距离函数</strong>，让算法在最大化距离/多样性的同时，<strong>天然地</strong> 被引导向好行为、远离坏行为。</li>
<li>结果：实验证明，该方法能高效学习复杂技能（如跑步），同时有效避免危险行为，且行为多样性保持良好。个人感觉，核心在于分类器网络跟 metra 形式的融合良好，因此，视频信息直接、精准地重塑了 skill discovery 的探索方向。</li>
</ol>
<p>个人思考，感觉这篇文章的故事 好在两个地方：1. 使用了 metra 的理论框架，理论支持看起来很强；2. 范式比较新，听起来有意义，感觉一个 novel 且有意义的范式是很重要的。</p>
<h2 id="2-dodont-的方法">2 dodont 的方法</h2>
<p>首先，metra（<a href="https://www.cnblogs.com/moonout/p/18980763" target="_blank">本站博客</a>）的目标函数能写成以下形式：</p>
<p></p><div class="math display">\[\sup_{\pi, \phi} \quad \mathbb{E}_{p(\tau, z)} \left[ \sum_{t=0}^{T-1} (\phi(s_{t+1}) - \phi(s_t))^\top z \right] \quad \text{s.t.} \quad \|\phi(s) - \phi(s')\|_2 \leq 1 \tag{2}
\]</div><p></p><p>从而，可以得到以下三个 loss function / policy 的 intrinsic reward：</p>
<ul>
<li>训练 state embedding <span class="math inline">\(\phi(s)\)</span> ：<span class="math inline">\(\max \mathbb{E}_{(s,z,s') \sim \mathcal{D}} [(\phi(s') - \phi(s))^\top z + \lambda \cdot \min(\varepsilon, 1 - \|\phi(s) - \phi(s')\|_2^2)]\)</span> ；</li>
<li>更新 Lagrange 乘子 <span class="math inline">\(\lambda\)</span> ：<span class="math inline">\(\min \mathbb{E}_{(s,z,s') \sim \mathcal{D}} [\lambda \cdot \min(\varepsilon, 1 - \|\phi(s) - \phi(s')\|_2^2)]\)</span> ；</li>
<li>更新 policy <span class="math inline">\(\pi(a|s,z)\)</span> ：reward <span class="math inline">\(r(s, z, s') = (\phi(s') - \phi(s))^\top z\)</span>，使用 SAC 算法。</li>
</ul>
<p>dodont 直接把 (2) 式变成这样：</p>
<p></p><div class="math display">\[\sup_{\pi, \phi} \quad \mathbb{E}_{p(\tau, z)} \left[ \sum_{t=0}^{T-1} (\phi(s_{t+1}) - \phi(s_t))^\top z \right] \quad \text{s.t.} \quad \|\phi(s) - \phi(s')\|_2 \leq \hat p_\psi(s,s') \tag{3}
\]</div><p></p><p>然后，通过假设新 <span class="math inline">\(\tilde\phi(s_t) = \phi(s_t) / \hat p_\psi(s_t, s_{t+1})\)</span> ，发现 (3) 式可以变成这样：</p>
<p></p><div class="math display">\[\sup_{\pi, \phi} \quad \mathbb{E}_{p(\tau, z)} \left[ \sum_{t=0}^{T-1} \hat p_\psi(s_t, s_{t+1})[\phi(s_{t+1}) - \phi(s_t)]^\top z \right] \quad \text{s.t.} \quad \|\phi(s) - \phi(s')\|_2 \leq 1 \tag{1}
\]</div><p></p><p>（或许，应该写成 <span class="math inline">\(\tilde\phi(s_t) = \phi(s_t) / \mathbb E_{s_{t+1}\sim [\pi,~p_\text{env}]}~\hat p_\psi(s_t, s_{t+1})\)</span> 的形式）</p>
<p>最后，metra 的三个 oss function / policy 的 intrinsic reward 目标，可以被改成这样：</p>
<ul>
<li>（不变）训练 state embedding <span class="math inline">\(\phi(s)\)</span> ：<span class="math inline">\(\max \mathbb{E}_{(s,z,s') \sim \mathcal{D}} [(\phi(s') - \phi(s))^\top z + \lambda \cdot \min(\varepsilon, 1 - \|\phi(s) - \phi(s')\|_2^2)]\)</span> ；</li>
<li>（不变）更新 Lagrange 乘子 <span class="math inline">\(\lambda\)</span> ：<span class="math inline">\(\min \mathbb{E}_{(s,z,s') \sim \mathcal{D}} [\lambda \cdot \min(\varepsilon, 1 - \|\phi(s) - \phi(s')\|_2^2)]\)</span> ；</li>
<li>更新 policy <span class="math inline">\(\pi(a|s,z)\)</span> ：reward <span class="math inline">\(r(s, z, s') = \hat p_\psi(s_t, s_{t+1})\cdot(\phi(s') - \phi(s))^\top z\)</span>，使用 SAC 算法。</li>
</ul>
<p>只需得到 <span class="math inline">\(\hat p_\psi(s_t, s_{t+1})\)</span> 之后，简单改改 metra 代码即可。</p>
<h2 id="3-dodont-的实验">3 dodont 的实验</h2>
<p>感觉整个实验设置，也是完全为故事服务的…</p>
<h3 id="31-实验-setting">3.1 实验 setting</h3>
<ul>
<li>环境：
<ul>
<li>Cheetah / Quadruped（DeepMind Control Suite）：学习复杂运动，如奔跑、避障 。</li>
<li>Kitchen：学习精细操作，如开微波炉、开关柜门 。</li>
</ul>
</li>
<li>输入：状态向量或像素图像；输出：连续动作控制机器人。</li>
<li>目标：无预设奖励下学习<strong>多样技能</strong>，并<strong>避免危险行为</strong>（如跌倒、进入危险区）。</li>
<li>dodont 视频：8 段视频（4×Do's + 4×Don'ts）
<ul>
<li>对于 5.2.1 节，do 视频是往四个方向跑的视频，而 dont 视频是随机行为，这样，相当于鼓励 agent 要跑，而不要（比如说）原地转圈 或者抬左脚抬右脚之类，所以性能超过 metra 是可以理解的。</li>
<li>对于 5.2.2 节，do 视频是往右跑的视频，而 dont 视频是往左跑的。从而，dodont 可以训到只往右跑的 agent。</li>
<li>对于 kitchen，do 视频直接采用 d4rl 里的全部视频，而 dont 视频采用 10 个随机行为。</li>
</ul>
</li>
</ul>
<h3 id="32-baseline">3.2 baseline</h3>
<ul>
<li>METRA：skill discovery baseline，仅最大化技能多样性。</li>
<li>METRA+：METRA 变体，<strong>人工设计奖励函数</strong> 作为距离度量，例如 安全区 +1，危险区 0。</li>
<li>SMERL / DGPO：混合任务奖励和多样性奖励的方法，用指令网络代替任务奖励。</li>
<li>仅鼓励正面行为（Only Do's）：消融实验，不惩罚负面行为。</li>
</ul>
<h3 id="33-评价指标">3.3 评价指标</h3>
<ul>
<li>状态覆盖率：衡量技能探索范围，如 Quadruped 的 x-y 平面覆盖。</li>
<li>零样本任务奖励：预训练技能直接执行任务的性能，如奔跑速度。</li>
<li>安全状态覆盖率：安全区域探索比例，危险区得分 -1，安全区 +1。</li>
<li>任务完成数（Kitchen）：成功完成 6 类操作的数量  。</li>
</ul>
<h3 id="34-实验结果">3.4 实验结果</h3>
<ul>
<li>效率提升：DoDont 学习奔跑技能比 METRA 快 30%，覆盖率提高 25%。<strong>仅需 8 段视频</strong>（4 正 4 负），即可有效引导.</li>
<li>安全性： 危险区域规避率 &gt;90%，METRA 仅 50%。翻滚等危险行为减少 70%。</li>
<li>多任务能力：Kitchen 任务完成数达 4.2 / 6，超过 METRA 的 3.1。</li>
<li>baseline 的局限性：
<ul>
<li>SMERL / DGPO 因互信息目标失效，技能多样性差。</li>
<li>METRA+ 在复杂环境奖励设计困难，如奔跑与翻滚奖励冲突。</li>
</ul>
</li>
</ul>
<h3 id="35-更多细节">3.5 更多细节</h3>
<ul>
<li>附录提到，We use the interquartile mean (IQM) for overall aggregation to assess performance，使用四分位距均值（IQM）[ 2] 进行整体聚合以评估性能，不知道是什么 trick。</li>
<li>看实验，总体感觉 dodont 相比 metra 和 metra+ 的性能，cheetah 不如 quadruped 的好。</li>
<li>dodont 和 metra 都有神秘的 offline 版本。metra offline 版本：<a href="https://github.com/seohongpark/HILP" target="_blank" rel="noopener nofollow">https://github.com/seohongpark/HILP</a></li>
<li>训 classifier 的超参数：使用四个卷积层作为骨干网络，并使用三个全连接层作为预测头。为了提高稳定性，使用 Tanh 函数对输出进行约束，此外，因为输入是 pixel ，所以还采用了简单的随机平移增强。使用 Adam 优化器，学习率为 1e-4，batch 大小为 1024。</li>
<li>dodont 声称，在 5.2.1（agent 要跑）、5.2.3（agent 不能摔倒翻滚）和 5.3（ablation）节中，由于包含了行为意图，他们使用基于状态的环境，并使用非彩色的默认像素作为人类意图。看起来，在 5.2.2 节（agent 要往右跑），dodont 使用的是跟 metra 一样的彩色地板 + pixel-based observation。至少，训 classifier 都是用视频训的。</li>
<li>训练效率：单卡 RTX 3090 训练 &lt; 28 小时。</li>
<li>dodont 声称，如果只有 Do's 视频，就学不了这么好了，听起来像对比学习。</li>
</ul>
<br>
<br>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.019444444444444445" data-date-updated="2025-07-14 13:06">2025-07-14 12:38</span>&nbsp;
<a href="https://www.cnblogs.com/moonout">MoonOut</a>&nbsp;
阅读(<span id="post_view_count">14</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18983738);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18983738', targetLink: 'https://www.cnblogs.com/moonout/p/18983738', title: 'Skill Discovery | DoDont：使用 do + don&amp;#39;t 示例视频，引导 agent 学习人类期望的 skill' })">举报</a>
</div>
        