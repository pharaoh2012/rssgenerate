
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/huggingface/p/18659784" title="发布于 2025-01-08 15:27">
    <span role="heading" aria-level="2">自动评估基准 | 一些评估测试集</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h1 id="一些评估测试集">一些评估测试集</h1>
<blockquote>
<p>这是 <strong>自动评估基准</strong> 系列文章的第三篇，敬请关注系列文章:</p>
<ul>
<li>基础概念</li>
<li>设计你的自动评估任务</li>
<li>一些评估测试集</li>
<li>技巧与提示</li>
</ul>
</blockquote>
<p>如果你感兴趣的任务已经得到充分研究，很可能评估数据集已经存在了。</p>
<p>下面列出了一些近年来开发构建的评估数据集。需要注意的是：</p>
<ul>
<li>大部分数据集有些 “过时”，因为它们是在 LLM 出现之前构建的，当时是为了评估语言文本的某个特定属性 (如翻译、摘要)，但是可能已经不适合现在的 LLM 评估方法了 (现在的评估方法倾向于通用、整体性)。<br>
(<em>如果你有空余时间可以对下列数据集添加出版日期，会对本文非常有帮助!</em>)<br>
(<em>这部分后续也会更新包含大语言模型的评估</em>)</li>
<li>有些数据集可能受到污染，因为它们已经在网络上公开了很多年了。不过这并不意味着在你的任务中它们就毫无用处！</li>
</ul>
<h2 id="pre-llm-数据集">Pre-LLM 数据集</h2>
<table>
<thead>
<tr>
<th>评估名称</th>
<th>任务类型</th>
<th>任务数据</th>
<th>任务内容</th>
<th>源</th>
<th>数据集</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>DeepFix</td>
<td>编码任务、代码转换、代码修正</td>
<td>7K 由学生编写的带有错误的 C 程序</td>
<td>修正 C 程序</td>
<td><a href="https://ojs.aaai.org/index.php/AAAI/article/view/10742" target="_blank" rel="noopener nofollow">论文</a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>MLSum</td>
<td>生成、多语言、摘要总结</td>
<td>1.5 百万篇新闻摘要与文章配对，取材自 Daily Mail、Le Monde、Süddeutsche Zeitung、El Pais、Moskovskij Komsomolets 和 Internet Haber (涵盖英语、法语、德语、西班牙语、俄语和土耳其语)。</td>
<td>总结文章</td>
<td><a href="https://arxiv.org/abs/2004.14900" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://huggingface.co/datasets/mlsum" target="_blank" rel="noopener nofollow">Hugging Face</a></td>
<td>Palm：以提示符为前缀，将文章截断至 2048 个标记</td>
</tr>
<tr>
<td>TransCoder</td>
<td>编码任务、代码转换</td>
<td>用 Python/Java/C++ 语言编写的 852 个并行函数</td>
<td>将一种语言翻译成另一种语言</td>
<td><a href="https://arxiv.org/abs/2006.03511" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://github.com/facebookresearch/CodeGen/blob/main/docs/transcoder.md" target="_blank" rel="noopener nofollow">来自论文</a></td>
<td></td>
</tr>
<tr>
<td>WMT</td>
<td>多语言、翻译</td>
<td>来自 WMT 机器翻译会议的数据集 - 可用的数据集取决于年份</td>
<td>将一种语言翻译成另一种语言</td>
<td><a href="https://www.statmt.org/wmt20/" target="_blank" rel="noopener nofollow">会议</a> <br>Replace the 2 digits by the conference year</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Adversarial NLI</td>
<td>语言推理</td>
<td>10K 蕴涵数据集是通过人机协作的对抗性攻击生成的，旨在寻找迫使模型预测错误蕴涵标签的谓词 (该数据集使用了来自 StoryCloze、CommonCrawl、Wikipedia、开放注解国家语料库、WikiHow 和 GLUE 的上下文)。</td>
<td>预测蕴含关系</td>
<td><a href="https://arxiv.org/abs/1910.14599" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://dl.fbaipublicfiles.com/anli/anli_v1.0.zip" target="_blank" rel="noopener nofollow">数据</a><br><a href="https://github.com/facebookresearch/anli" target="_blank" rel="noopener nofollow">Github</a></td>
<td>R1 至 R3 = 数据生成的轮次</td>
</tr>
<tr>
<td>APPS</td>
<td>文生代码</td>
<td>10K 用自然语言描述的 Python 编程问题，这些问题是从 LeetCode 网站抓取而来，并附带有一套测试用例。</td>
<td>解决 Python 问题</td>
<td><a href="https://arxiv.org/abs/2105.09938" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://github.com/hendrycks/apps" target="_blank" rel="noopener nofollow">Github</a> <br><a href="https://people.eecs.berkeley.edu/~hendrycks/APPS.tar.gz" target="_blank" rel="noopener nofollow">数据</a></td>
<td></td>
</tr>
<tr>
<td>AQuA</td>
<td>算术、论证</td>
<td>100K 选择题 (来源包括 GMAT、GRE 及其他资源)，包含题目/选项/答案解析。</td>
<td>选择正确的多项选择题答案 (MCQA)</td>
<td><a href="https://arxiv.org/abs/1705.04146" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://github.com/deepmind/AQuA" target="_blank" rel="noopener nofollow">Github</a></td>
<td>最佳结果是在添加了外部计算器后获得的</td>
</tr>
<tr>
<td>ARC</td>
<td>常识、论证</td>
<td>8K 小学科学问题：e = 简单题集，c = 挑战题集</td>
<td>选择正确的多项选择题答案 (MCQA)</td>
<td><a href="https://arxiv.org/abs/1803.05457" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://allenai.org/data/arc" target="_blank" rel="noopener nofollow">数据</a></td>
<td>注意，这是 AI2 论证 Challenge ，不是抽象与论证语料库。</td>
</tr>
<tr>
<td>ASDiv</td>
<td>分析推理、算术、论证</td>
<td>2.3K 数学世界小学问题，收集自各个网站，由一位硕士研究生标注答案和难度级别 (可能是高质量的)</td>
<td>解决问题</td>
<td><a href="https://aclanthology.org/2020.acl-main.92/" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://github.com/chaochun/nlu-asdiv-dataset" target="_blank" rel="noopener nofollow">Github</a></td>
<td></td>
</tr>
<tr>
<td>bAbI</td>
<td>论证</td>
<td>20 项任务，每项包含 2K 自动生成的问题 + 短场景 (通过模拟的文字冒险游戏生成的连续动作)。</td>
<td>对句子进行推理以选择正确的结论</td>
<td><a href="https://arxiv.org/abs/1502.05698" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://github.com/facebookarchive/bAbI-tasks" target="_blank" rel="noopener nofollow">Github</a> <br><a href="https://research.facebook.com/downloads/babi/" target="_blank" rel="noopener nofollow">数据</a></td>
<td>参见第 4 部分以了解模拟环境及其约束，这是一个相当有趣的想法。对于其他类型的推理，可能不会太难重现。</td>
</tr>
<tr>
<td>BBQ</td>
<td>偏差检测</td>
<td>58K 示例，每个示例包含两种情境 (模糊情境和明确表现出偏见的情境) 、两个问题 (负面问题和非负面问题) 以及可能的答案。这些示例是根据手动模板构建的，并通过众包方式进行检查。</td>
<td>预测正确且无偏的答案。根据上下文或问题的不同，准确率之间的差异可以构建一个偏见分数。</td>
<td><a href="https://aclanthology.org/2022.findings-acl.165/" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://github.com/nyu-mll/BBQ/tree/main/data" target="_blank" rel="noopener nofollow">Github</a></td>
<td></td>
</tr>
<tr>
<td>BLiMP</td>
<td>语言理解</td>
<td>67 个数据集，每个数据集包含人工生成的 1K 最小对，用于测试句法、形态学和语义知识，这些数据集通过 MTurk 平台进行了验证。</td>
<td>通过检查模型分配给正确句子的对数概率是否更高来衡量准确性。</td>
<td><a href="https://aclanthology.org/2020.tacl-1.25/" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://github.com/alexwarstadt/blimp/tree/master/data" target="_blank" rel="noopener nofollow">Github</a></td>
<td>测试的内容包括：回指一致性、论元结构、绑定、控制/提升、冠词-名词一致性、省略、填充语-空位、不规则形式、孤岛效应、NPI 许可、量化词、主谓一致</td>
</tr>
<tr>
<td>BOLD</td>
<td>生成、有害内容检测</td>
<td>从包含种族/宗教/政治/性别/职业群体成员的维基百科句子开头提取的 23K 提示 (例如：女性艺术家，对应性别=女性)。</td>
<td>任务: 生成句子结尾，毒性通过一系列指标进行评估 (情感分析、使用分类器等)。在 HELM 中，毒性是通过 Perspective API 进行测量的。</td>
<td><a href="https://arxiv.org/abs/2101.11718" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://github.com/amazon-science/bold/tree/main/prompts" target="_blank" rel="noopener nofollow">Github</a></td>
<td></td>
</tr>
<tr>
<td>BooksCorpus</td>
<td>N/A</td>
<td>从网络上抓取的 11K 未出版书籍，每本书超过 20K 单词 (涵盖 16 种不同体裁)。</td>
<td>在原始论文中，它被用于训练一个句子嵌入模型 — 在模型论文中，它常常被用于污染或困惑度评估。</td>
<td><a href="https://arxiv.org/abs/1506.06724" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://huggingface.co/datasets/bookcorpus" target="_blank" rel="noopener nofollow">Hugging Face</a></td>
<td></td>
</tr>
<tr>
<td>BooksCorpus_HELM</td>
<td>生成、记忆</td>
<td>来自 BooksCorpus 的 1K 随机抽样书籍。</td>
<td>任务: 从一段文本的随机数量的标记开始，模型必须生成后续内容 — 测量精确和近乎精确的再现。</td>
<td><a href="https://arxiv.org/abs/2211.09110" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://drive.google.com/file/d/10uC4jM6tgI1pgtq--07FFHQ2Te7-SXGA/view" target="_blank" rel="noopener nofollow">数据</a></td>
<td></td>
</tr>
<tr>
<td>BoolQ</td>
<td>语言推理、语言理解</td>
<td>来自 Wikipedia 的 16K 自然发生的是/否问答句子，来源于问题 + 上下文。</td>
<td>回答多项选择题 (MCQA)</td>
<td><a href="https://arxiv.org/abs/1905.10044" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://super.gluebenchmark.com/tasks" target="_blank" rel="noopener nofollow">网站</a></td>
<td></td>
</tr>
<tr>
<td>CB</td>
<td>语言理解</td>
<td>1.2K 的语篇 (来自《华尔街日报》的新闻、来自 《英国国家语料库》的小说、来自《Switchboard》的对话)，包含上下文 + 目标句子。</td>
<td>预测承诺蕴含</td>
<td><a href="https://semanticsarchive.net/Archive/Tg3ZGI2M/Marneffe.pdf" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://super.gluebenchmark.com/tasks" target="_blank" rel="noopener nofollow">网站</a></td>
<td></td>
</tr>
<tr>
<td>Civil comments</td>
<td>有害内容检测</td>
<td>1.8M 在线评论，伴有根据 Perspective API 指南通过众包形式获得的人工标签以标注毒性和身份术语，在这些评论中，450 K 附有身份术语标签 (众包，从列表中选择)。</td>
<td>任务: 毒性预测，标签用于识别模型中的偏差区域。</td>
<td><a href="https://arxiv.org/abs/1903.04561" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification" target="_blank" rel="noopener nofollow">Kaggle</a><br><a href="https://huggingface.co/datasets/civil_comments" target="_blank" rel="noopener nofollow">Hugging Face</a></td>
<td>原始论文包含了一个合成测试集 (77K 示例是从使用50个身份术语的模板生成的，毒性内容的比例为50/50) 和一个人工标注的数据集 (描述在任务内容栏中) —— 我认为可用的数据集是后者。</td>
</tr>
<tr>
<td>Clean E2E NLG</td>
<td>描述、生成</td>
<td>50K 通过众包生成的餐厅描述，给定关键词和值 (食物类型 = X，预算 = Y……)。</td>
<td></td>
<td><a href="https://arxiv.org/abs/1706.09254" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://huggingface.co/datasets/e2e_nlg_cleaned" target="_blank" rel="noopener nofollow">Hugging Face</a></td>
<td>Palm: 在提示符前缀下，将文章截断至2048个标记。</td>
</tr>
<tr>
<td>CNN/DailyMail</td>
<td>完形填空、总结</td>
<td>原始数据集：200 K 新文档 (CNN/ Daily Mail 在 2007 年到 2015 年间) 被转换为完形填空格式，通过移除一些文本中的命名实体，并将它们用作关键词。</td>
<td>在 HELM 中: 使用上述文档 (以完整形式) 作为文本进行总结，并使用其重点作为黄金摘要。</td>
<td><a href="https://arxiv.org/abs/1506.03340" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://huggingface.co/datasets/cnn_dailymail" target="_blank" rel="noopener nofollow">Hugging Face</a> <br><a href="https://cs.nyu.edu/~kcho/DMQA/" target="_blank" rel="noopener nofollow">数据</a></td>
<td>(我怀疑这并不能产生非常好的摘要)</td>
</tr>
<tr>
<td>CommonsenseQA</td>
<td>常识、论证</td>
<td>12K 众包的问答对 (基于 ConceptNet 关联初始化)，然后通过质量进行过滤，并从 Google 搜索查询中添加了上下文 &gt; 部分文本可能与 CC 数据重叠</td>
<td>回答多项选择题 (MCQA)</td>
<td><a href="https://aclanthology.org/N19-1421/" target="_blank" rel="noopener nofollow">论文</a></td>
<td></td>
<td>最佳结果是在添加了外部计算器的情况下获得的</td>
</tr>
<tr>
<td>Contrast Sets</td>
<td>生成、健壮性</td>
<td>10 组对比集，每组最多 1 K 示例，用于他们的数据集 (详见评论)，这些是由研究人员 (通常是原始论文的作者) 制作的 (增加问题中的推理步骤，用反义词替换词语，改变数量等)。</td>
<td>遵循原始任务设置并使用新示例，观察性能是否下降 。<br>在 HELM 中: 使用 IMDb 和 DROP 对比集。</td>
<td><a href="https://aclanthology.org/2020.findings-emnlp.117/" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://allenai.org/data/contrast-sets" target="_blank" rel="noopener nofollow">数据</a></td>
<td>NLVR2、IMDb 情感分析、MATRES 时间关系识别、英文 UD 解析、PERSPECTRUM、DROP、Quoref、ROPES、BoolQ、MC-TACO 。</td>
</tr>
<tr>
<td>COPA</td>
<td>常识、语言理解</td>
<td>1K 前提 + 因果问题 (附带选项，常识性问题)</td>
<td></td>
<td><a href="https://people.ict.usc.edu/~gordon/publications/AAAI-SPRING11A.PDF" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://super.gluebenchmark.com/tasks" target="_blank" rel="noopener nofollow">网站</a></td>
<td></td>
</tr>
<tr>
<td>CoQA</td>
<td>上下文阅读理解</td>
<td>127K 对话式问答，基于给定的上下文 (必须同时提供理由)——由标注者撰写</td>
<td></td>
<td><a href="https://arxiv.org/abs/1808.07042" target="_blank" rel="noopener nofollow">论文</a></td>
<td>v1.0 from <a href="https://stanfordnlp.github.io/coqa/" target="_blank" rel="noopener nofollow">数据</a></td>
<td></td>
</tr>
<tr>
<td>DataImputation</td>
<td>现实任务、论证、结构化的数据</td>
<td>8 个来自多个来源的结构化数据集。</td>
<td>任务: 从一组带有空缺的属性中，模型必须填充这些空缺 (例如: 从电话号码推断城市，从规格推断电话品牌)。</td>
<td><a href="https://sxsong.github.io/doc/21icde-imputation.pdf" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://www.cs.utexas.edu/users/ml/riddle/data/restaurant.tar.gz" target="_blank" rel="noopener nofollow">Data restaurant</a><br><a href="https://dbs.uni-leipzig.de/file/Abt-Buy.zip" target="_blank" rel="noopener nofollow">Data Buy</a></td>
<td>参见表 2 获取所有数据源。<br><br>在 HELM 中：使用子集 Buy 和 Restaurant，将输入转换为自然语言，测试准确性。</td>
</tr>
<tr>
<td>Digits arithmetics (2D+, 2D-, 3D+, 3D-, …)</td>
<td>算术</td>
<td>基本的 n 位数算术任务，包括加法、减法、复合运算，每种运算各有 2K 个示例。</td>
<td>任务：解决数学问题</td>
<td><a href="https://arxiv.org/abs/2005.14165" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://raw.githubusercontent.com/openai/gpt-3/master/data/" target="_blank" rel="noopener nofollow">Github</a></td>
<td>所有链接均来自 lm-evaluation-harness/lm_eval/datasets/arithmetic</td>
</tr>
<tr>
<td>DROP</td>
<td>算术、上下文阅读理解</td>
<td>55K 对抗性问题，这些问题需要 1) 从文本中选择相关项目，并且 2) 对这些项目进行计算 (排序/计数/……) 以获得正确答案。</td>
<td>任务：选择并计数以提供正确的答案</td>
<td><a href="https://aclanthology.org/N19-1246/" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://allenai.org/data/drop" target="_blank" rel="noopener nofollow">数据</a></td>
<td></td>
</tr>
<tr>
<td>Dyck language_HELM</td>
<td>符号处理</td>
<td>500 个 D_n 单词，长度在 52 到 100 个字符之间 (“单词”由嵌套的括号组成)，其中最后 i 个字符已被移除。</td>
<td>任务：预测唯一的闭合括号序列。</td>
<td><a href="https://arxiv.org/abs/2211.09110" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://github.com/stanford-crfm/helm/blob/main/src/helm/benchmark/scenarios/dyck_language_scenario.py" target="_blank" rel="noopener nofollow">Github</a> <br>Also has a different version in BigBench</td>
<td></td>
</tr>
<tr>
<td>GSM8K</td>
<td>分析推理、论证</td>
<td>8.5K 各类小学数学问题</td>
<td></td>
<td><a href="https://arxiv.org/abs/2110.14168v2" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://github.com/openai/grade-school-math" target="_blank" rel="noopener nofollow">Github</a> <br><a href="https://huggingface.co/datasets/gsm8k" target="_blank" rel="noopener nofollow">Hugging Face</a></td>
<td>最佳结果是在添加了外部计算器的情况下获得的</td>
</tr>
<tr>
<td>HellaSwag</td>
<td>完形填空</td>
<td>60K 对抗性过滤的多项选择题问答</td>
<td>选择正确的下一个句子 (来自字幕或 WikiHow)。</td>
<td><a href="https://aclanthology.org/P19-1472/" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://github.com/rowanz/hellaswag/tree/master/data" target="_blank" rel="noopener nofollow">Github</a></td>
<td></td>
</tr>
<tr>
<td>HumanEval</td>
<td>编码任务、文生代码</td>
<td>164 手写编程问题，包含函数签名、文档字符串、函数体 + 单元测试</td>
<td>目标是完成函数以通过单元测试。</td>
<td><a href="https://arxiv.org/abs/2107.03374" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://huggingface.co/datasets/openai_humaneval" target="_blank" rel="noopener nofollow">Hugging Face</a></td>
<td></td>
</tr>
<tr>
<td>IMDB</td>
<td>情感分析</td>
<td>50K 条来自 IMDB 的评论，其中正面评价 (评分 ≥ 7) 与负面评价 (评分 ≤ 4) 均衡分布 (不含中立评价)。</td>
<td>分类正面/负面评论。</td>
<td><a href="https://aclanthology.org/P11-1015/" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://ai.stanford.edu/~amaas/data/sentiment/" target="_blank" rel="noopener nofollow">网站</a></td>
<td></td>
</tr>
<tr>
<td>LAMBADA</td>
<td>完形填空</td>
<td>10K 叙事上下文 (来自 BookCorpus)，随后是一个句子，其中最后一个词被遮掩且必须被预测。特别构建以强制使用上下文。</td>
<td>预测最后一个词。</td>
<td><a href="https://aclanthology.org/P16-1144/" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://zenodo.org/record/2630551#.YFJVaWT7S_w" target="_blank" rel="noopener nofollow">Zenodo</a></td>
<td></td>
</tr>
<tr>
<td>Language Modeling Evaluation_HELM</td>
<td>语言模型</td>
<td>在 HELM 中编译了多个数据集：WikiText-103、ThePile (特别是 arXiv、BooksCorpus2、Enron Emails、PubMed Central、Wikipedia)、TwitterAAE、ICE。</td>
<td>任务：获取完整序列的条件对数概率 (困惑度测量)。</td>
<td><a href="https://arxiv.org/abs/2211.09110" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://pile.eleuther.ai/" target="_blank" rel="noopener nofollow">The pile website</a><br><a href="https://github.com/alexwarstadt/blimp" target="_blank" rel="noopener nofollow">BLIMP Github</a><br><a href="https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip" target="_blank" rel="noopener nofollow">Wikitext data </a><br><a href="http://slanglab.cs.umass.edu/TwitterAAE/" target="_blank" rel="noopener nofollow">Twitter AAE data</a><br><a href="https://www.ice-corpora.uzh.ch/en/access.htm" target="_blank" rel="noopener nofollow">ICE data</a></td>
<td></td>
</tr>
<tr>
<td>LegalSupport</td>
<td>蕴含, 现实任务、论证</td>
<td>20K 个法律蕴含情景，构建于州/联邦法律意见之上 (断言用作上下文，并随机选取 2 个支持来源 ("参见 X，规则"))。</td>
<td>任务：确定哪条规则最支持该断言。</td>
<td><a href="https://arxiv.org/abs/2211.09110" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://docs.google.com/uc?export=download&amp;id=1PVoyddrCHChMxYrLhsI-zu7Xzs5S8N77" target="_blank" rel="noopener nofollow">数据</a></td>
<td></td>
</tr>
<tr>
<td>LinuxKernel_HELM</td>
<td>生成、记忆</td>
<td>2K 个从 Linux 内核中随机抽取的函数。</td>
<td>任务：从函数起始的随机行数出发，模型必须生成后续内容——衡量精确和近乎精确的再现。</td>
<td><a href="https://arxiv.org/abs/2211.09110" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://drive.google.com/file/d/1Y5piYwil7T6n8toT_-d7NWqVZHh9NVxJ/view" target="_blank" rel="noopener nofollow">数据</a></td>
<td></td>
</tr>
<tr>
<td>LSAT</td>
<td>分析推理、上下文阅读理解、逻辑推理</td>
<td>10K 个来自法学院入学考试 (分析、逻辑推理和阅读理解) 的问题，包含上下文。</td>
<td>回答多项选择题 (MCQA) correctly</td>
<td><a href="https://arxiv.org/abs/2108.00648" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://github.com/zhongwanjun/AR-LSAT/tree/main/data" target="_blank" rel="noopener nofollow">Github</a></td>
<td></td>
</tr>
<tr>
<td>Magellan Benchmark</td>
<td>现实任务、论证、结构化的数据</td>
<td>23 个来自多个来源的数据集，包含具有属性的实体。脏数据集包含故意制造的错误，例如属性位于错误的列中、拼写错误等。</td>
<td>任务：给定来自两个不同表的两个实体，确定它们是否相同。</td>
<td><a href="https://pages.cs.wisc.edu/~anhai/papers1/deepmatcher-sigmod18.pdf" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://github.com/anhaidgroup/deepmatcher/blob/master/Datasets.md" target="_blank" rel="noopener nofollow">Github</a></td>
<td>很可能 Abt-Buy 和 Buy 是相同的数据集。</td>
</tr>
<tr>
<td>MATH (Mathematics Aptitude Test of Heuristics)</td>
<td>分析推理、逻辑推理、论证、符号处理</td>
<td>12.5K 来自真实比赛的数学问题，以自然语言和 LaTeX 格式呈现。</td>
<td></td>
<td><a href="https://arxiv.org/abs/2103.03874" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://people.eecs.berkeley.edu/~hendrycks/MATH.tar" target="_blank" rel="noopener nofollow">数据</a></td>
<td>HELM: 使用一个 "use_chain_of_thought" 标志位。</td>
</tr>
<tr>
<td>MAWPS</td>
<td>算术、论证</td>
<td>3.3K 数学应用题 (问题 + 答案 + 模板，来源于现有数据集：AddSub、SingleOp、MultiArith、SingleEq、SimulEq-S 和 SimulEq-L)</td>
<td>解决数学问题</td>
<td><a href="https://aclanthology.org/N16-1136/" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://github.com/sroy9/mawps" target="_blank" rel="noopener nofollow">Github</a></td>
<td>最佳结果是在添加了外部计算器的情况下获得。</td>
</tr>
<tr>
<td>MBPP</td>
<td>编码任务、文生代码</td>
<td>1K 入门级 Python 群体众包编程问题 (描述，解决方案，3 个单元测试用例) - (58 % 数学相关，43% 列表处理，19% 字符串处理，9% 整数序列，以及 2% 其他)。</td>
<td>解决 Python 编程问题</td>
<td><a href="https://arxiv.org/abs/2108.07732" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://github.com/google-research/google-research/tree/master/mbpp" target="_blank" rel="noopener nofollow">Github</a><br><a href="https://huggingface.co/datasets/mbpp" target="_blank" rel="noopener nofollow">Hugging Face</a></td>
<td>还包含一个编辑版本 (400 项)，具有明确的提示和良好的签名 (之后可以查看以了解提示对代码生成的影响) + 一个 MathQA - Python 数据集 (MathQA 数据集的改编版本)</td>
</tr>
<tr>
<td>MMLU</td>
<td>语言理解</td>
<td>15K 多选题问答 是从各种在线资源手动收集的，涵盖多个主题 (法律、哲学、经济学、心理学、STEM、医学等， - 从高中到专业水平)。</td>
<td>回答多项选择题 (MCQA)</td>
<td><a href="https://arxiv.org/abs/2009.03300" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://huggingface.co/datasets/lukaemon/mmlu" target="_blank" rel="noopener nofollow">Hugging Face</a><br><a href="https://github.com/hendrycks/test" target="_blank" rel="noopener nofollow">Github</a></td>
<td>看起来像是一个强大/高质量的基准线</td>
</tr>
<tr>
<td>MRF (Misinfo Reaction Frames)</td>
<td>生成、虚假信息生成能力</td>
<td>200 K 条新闻标题中的声明配对 (气候变化、COVID-19、癌症疾病，详细来源见评论) + 标签 (真实/虚假信息)，前者由 MTurk 工人标注了真实性、传播可能性、作者意图。</td>
<td>任务：必须预测黄金标签或生成可能的作者意图、读者感知等……</td>
<td><a href="https://aclanthology.org/2022.acl-long.222/" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://github.com/skgabriel/mrf-modeling" target="_blank" rel="noopener nofollow">Github</a></td>
<td>(包含来自 NELA-GT-2018-2020, SciDCC, Climate-FEVER, CoAID, CoronaVirusFacts/DatosCoronaVirusAlliance 数据库, ESOC Covid-19 错误信息数据集, DETERRENT 的数据)</td>
</tr>
<tr>
<td>MS MARCO</td>
<td>问答、检索</td>
<td>100 万条匿名化的问题，配有自由格式的人工生成答案 (来自相关网页文档摘录)，其中一些问题还添加了改写版本。</td>
<td>原始论文包含 3 个任务：1) 尽可能生成正确的答案，2) 同上但答案即使在没有上下文的情况下也应该合理，3) 对 1000 个段落按照与问题的相关性进行排序。</td>
<td><a href="https://arxiv.org/abs/1611.09268" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://microsoft.github.io/msmarco/" target="_blank" rel="noopener nofollow">Github</a></td>
<td>包含对 QA 数据集在文献综述中的扩展描述 。<br>在 HELM 中，仅考察排序任务，并且通过查看在询问“段落是否回答了查询？”时预测的对数似然性来估计相关性。</td>
</tr>
<tr>
<td>MS MARCO TREC, aka TREC 2019</td>
<td>检索</td>
<td>源自 MS MARCO 的数据集，编辑用于段落或文档检索任务，执行完整检索或顶级-n 重排序 (文档为 100，段落为 1000)。(参见 MS MARCO)</td>
<td></td>
<td><a href="https://arxiv.org/abs/2003.07820" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://trec.nist.gov/data/deep2019.html" target="_blank" rel="noopener nofollow">数据</a> <br><a href="https://microsoft.github.io/msmarco/TREC-Deep-Learning-2019.html" target="_blank" rel="noopener nofollow">Github</a></td>
<td></td>
</tr>
<tr>
<td>MultiRC</td>
<td>语言理解、问答</td>
<td>6K 多选题涵盖多种主题</td>
<td></td>
<td><a href="https://aclanthology.org/N18-1023/" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://super.gluebenchmark.com/tasks" target="_blank" rel="noopener nofollow">数据</a></td>
<td></td>
</tr>
<tr>
<td>NarrativeQA</td>
<td>问答、检索</td>
<td>47K 自由形式的人工生成问题和答案，关联到 1.5K 书籍 (Gutenberg 项目) 和电影剧本 (抓取)，并与情节概要相匹配。</td>
<td>任务：根据摘要或故事，回答或选择正确答案。</td>
<td><a href="https://arxiv.org/abs/1712.07040" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://github.com/deepmind/narrativeqa" target="_blank" rel="noopener nofollow">Github</a></td>
<td>对于长距离上下文测试，我们可以使用这个数据集从完整的故事中进行问答。对于任何对话系统而言，这可能都很有趣。</td>
</tr>
<tr>
<td>Natural Questions</td>
<td>开放域/闭书任务、问答</td>
<td>207K 汇总的 Google 搜索查询 + 标注的 Wikipedia 示例答案</td>
<td></td>
<td><a href="https://aclanthology.org/Q19-1026/" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://ai.google.com/research/NaturalQuestions/download" target="_blank" rel="noopener nofollow">数据</a></td>
<td></td>
</tr>
<tr>
<td>NewsQA</td>
<td>问答</td>
<td>100K 人工生成的问答对来自 12K 新闻文章 (CNN)。问题是从标题 + 摘要生成的，答案是从问题 + 文章中得出的，然后通过验证机制保留 。 <br>可能与 CNN / Daily Mail 有交集，因为数据提取脚本是相同的。</td>
<td></td>
<td><a href="https://aclanthology.org/W17-2623/" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://github.com/Maluuba/newsqa" target="_blank" rel="noopener nofollow">Github</a></td>
<td></td>
</tr>
<tr>
<td>OpenBookQA</td>
<td>常识、论证</td>
<td>6K 句子，需要运用常识知识进行科学推理，以将结论外推到新情况。</td>
<td></td>
<td><a href="https://arxiv.org/abs/1809.02789" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://allenai.org/data/open-book-qa" target="_blank" rel="noopener nofollow">数据</a></td>
<td></td>
</tr>
<tr>
<td>PIQA</td>
<td>常识、论证</td>
<td>20K 物理常识推理情境</td>
<td>从上下文中选择正确的操作并作答</td>
<td><a href="https://arxiv.org/abs/1911.11641" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://yonatanbisk.com/piqa/data/" target="_blank" rel="noopener nofollow">数据</a></td>
<td></td>
</tr>
<tr>
<td>PopularBooksCorpus_HELM</td>
<td>生成、记忆</td>
<td>来自 BooksCorpus 的 20 本书，这些书出现在畅销书列表中。</td>
<td>任务：从构成书籍第一个段落的随机数量的标记开始，模型必须生成后续内容——衡量精确和近乎精确的再现。</td>
<td><a href="https://arxiv.org/abs/2211.09110" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://drive.google.com/file/d/1RT29rRKNNXKgZBhXNbqevLwR440g44it/view" target="_blank" rel="noopener nofollow">数据</a></td>
<td></td>
</tr>
<tr>
<td>QuAC</td>
<td>上下文阅读理解</td>
<td>100K 个信息寻求型 QA (问答) 情境中的问题 (使用 Wikipedia 来生成数据集)</td>
<td></td>
<td><a href="https://aclanthology.org/D18-1241/" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://quac.ai/" target="_blank" rel="noopener nofollow">数据</a></td>
<td></td>
</tr>
<tr>
<td>RACE</td>
<td>上下文阅读理解</td>
<td>100K 个来自中国中学生/高中生英语阅读理解考试的问题</td>
<td></td>
<td><a href="https://aclanthology.org/D17-1082/" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://www.cs.cmu.edu/~glai1/data/race/" target="_blank" rel="noopener nofollow">数据</a></td>
<td></td>
</tr>
<tr>
<td>RAFT</td>
<td>现实任务、文本分类</td>
<td>11 个自然发生的分类任务数据集的汇编，测试项目数量在 150 到 5 K 之间。</td>
<td>任务：在少量样本中，从 50 个标注示例出发，提供有意义的标签。(领域：医疗、金融、研究、英语语言、法律、物理、人工智能安全、社交网络)</td>
<td><a href="https://arxiv.org/abs/2109.14076" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://huggingface.co/datasets/ought/raft" target="_blank" rel="noopener nofollow">Hugging Face</a></td>
<td>语料库: (ADE 语料库 v2、Banking77、NeurIPS 2020 影响声明风险、OneStopEnglish、Overrruling、系统评价纳入、TAI 安全研究、服务条款、TweetEval 恶意内容、Twitter 投诉、半导体组织类型)</td>
</tr>
<tr>
<td>RealToxicityPrompts</td>
<td>生成、有害内容检测</td>
<td>100K 自然发生的句子 (从 OpenWebText 语料库中选取，基本上等同于 Reddit，并使用 PerspectiveAPI 对毒性进行评分) 被分成两部分，以创建提示和延续。</td>
<td>任务：从句子开头生成续写内容，然后使用 Perspective API 对生成内容的毒性进行评估。</td>
<td><a href="https://arxiv.org/abs/2009.11462" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://allenai.org/data/real-toxicity-prompts" target="_blank" rel="noopener nofollow">数据</a><br><a href="https://github.com/allenai/real-toxicity-prompts" target="_blank" rel="noopener nofollow">Github</a> (the repo lacks a lot of info)</td>
<td></td>
</tr>
<tr>
<td>ReCoRD</td>
<td>语言理解</td>
<td>120K 段落/完形填空的查询/答案 示例 来自新闻 (CNN、DailyMail)，并经过人工筛选</td>
<td></td>
<td><a href="https://arxiv.org/abs/1810.12885" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://super.gluebenchmark.com/tasks" target="_blank" rel="noopener nofollow">数据</a></td>
<td></td>
</tr>
<tr>
<td>RTE</td>
<td>语言理解</td>
<td>3K 竞赛数据汇编 关于蕴含关系</td>
<td></td>
<td><a href="https://w4ngatang.github.io/static/papers/superglue.pdf" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://super.gluebenchmark.com/tasks" target="_blank" rel="noopener nofollow">数据</a></td>
<td></td>
</tr>
<tr>
<td>SAT analogies</td>
<td>语言理解</td>
<td>374 个 2005 年之前的 SAT 类比问题 (a 与 b 的关系如同 c 与多项选择题的关系；词语不是最常见的)</td>
<td></td>
<td><a href="https://arxiv.org/abs/2005.14165" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://goo.gl/XWjas1" target="_blank" rel="noopener nofollow">Data dev</a> <br><a href="https://goo.gl/BcTtB4" target="_blank" rel="noopener nofollow">Data test</a></td>
<td></td>
</tr>
<tr>
<td>SIQA</td>
<td>问答</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>SQuADv2</td>
<td>上下文阅读理解</td>
<td>结合了 SQuAD 与 50K 不可回答的问题</td>
<td>根据上下文给出答案，但仅在可能的情况下。</td>
<td><a href="https://arxiv.org/abs/1806.03822" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://rajpurkar.github.io/SQuAD-explorer/" target="_blank" rel="noopener nofollow">Github</a></td>
<td></td>
</tr>
<tr>
<td>StoryCloze</td>
<td>完形填空、常识</td>
<td>50K 五句话的常识故事</td>
<td>选择正确的结尾</td>
<td><a href="https://aclanthology.org/N16-1098/" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://huggingface.co/datasets/story_cloze" target="_blank" rel="noopener nofollow">Hugging Face</a></td>
<td></td>
</tr>
<tr>
<td>StrategyQA</td>
<td>常识、论证</td>
<td>2.8K 需要从隐性知识进行推理的问题</td>
<td></td>
<td><a href="https://arxiv.org/abs/2101.02235" target="_blank" rel="noopener nofollow">论文</a></td>
<td></td>
<td>最佳结果需添加外部计算器</td>
</tr>
<tr>
<td>SVAMP</td>
<td>算术、论证</td>
<td>1K 数学应用题 (采样自 ASDivA，质量高于 MAWPS)，具有不同的表述变化</td>
<td></td>
<td><a href="https://aclanthology.org/2021.naacl-main.168/" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://github.com/arkilpatel/SVAMP/blob/main/SVAMP.json" target="_blank" rel="noopener nofollow">Github</a></td>
<td>最佳结果需添加外部计算器</td>
</tr>
<tr>
<td>Synthetic reasoning (natural)</td>
<td>逻辑推理、论证</td>
<td>即时生成的合成数据，包含一组合成规则 (条件语句)、事实 (属性) 以及逻辑上的标准输出。</td>
<td></td>
<td><a href="https://arxiv.org/abs/2211.09110" target="_blank" rel="noopener nofollow">论文</a></td>
<td>Can be generated with <a href="https://github.com/stanford-crfm/helm/blob/main/src/helm/benchmark/scenarios/synthetic_reasoning_natural_scenario.py" target="_blank" rel="noopener nofollow">Github</a></td>
<td>也被称为 HELM 中的 rule_induct</td>
</tr>
<tr>
<td>Synthetic reasoning (symbolic)_HELM</td>
<td>逻辑推理、符号处理</td>
<td>使用模式模板即时生成的合成数据。</td>
<td>要么测试模型是否能够识别模式（例如“beach + beach - pear”具有“A + A - B”的模式），要么测试模型是否能够在给定的模式中替换字符串。</td>
<td><a href="https://arxiv.org/abs/2211.09110" target="_blank" rel="noopener nofollow">论文</a></td>
<td>Can be generated with <a href="https://github.com/stanford-crfm/helm/blob/main/src/helm/benchmark/scenarios/synthetic_reasoning_scenario.py" target="_blank" rel="noopener nofollow">Github</a></td>
<td></td>
</tr>
<tr>
<td>TriviaQA</td>
<td>开放域/闭书任务、问答</td>
<td>95K 杂学问答 (组合型问题，语法变异性)</td>
<td></td>
<td><a href="https://aclanthology.org/P17-1147/" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://nlp.cs.washington.edu/triviaqa/" target="_blank" rel="noopener nofollow">数据</a></td>
<td></td>
</tr>
<tr>
<td>TruthfulQA</td>
<td>问答</td>
<td>817 个关于复杂事实主张的问题 (常见误解、错误等)，涵盖 38 个类别，附带真和假的参考答案 + 支持真实答案的来源 (以及额外增加的 380 个问题)。</td>
<td></td>
<td><a href="https://arxiv.org/abs/2109.07958" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://github.com/sylinrl/TruthfulQA" target="_blank" rel="noopener nofollow">Github</a></td>
<td></td>
</tr>
<tr>
<td>TyDiQA-GoldP</td>
<td>多语言、问答</td>
<td>204K 多语言 QA 对 (从提示中不限制地引出问题，然后检索维基百科文章，并在文章中选择特定答案 (如果可能的话)) (语言包括：英语、阿拉伯语、孟加拉语、芬兰语、印度尼西亚语、日语、韩语、俄语、泰卢固语、泰语和斯瓦希里语)。</td>
<td>MCQA</td>
<td><a href="https://aclanthology.org/2020.tacl-1.30/" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://github.com/google-research-datasets/tydiqa" target="_blank" rel="noopener nofollow">Github</a></td>
<td>生成的数据集可以展示有趣的问题、目标不清楚的问题和问题与答案语言水平之间的不匹配。可能比其他数据集更具挑战性。</td>
</tr>
<tr>
<td>Web Questions</td>
<td>开放域/闭书任务、问答</td>
<td>从 Google Search API 中提取了 100K 个 “Wh?” 类型的问题，然后由 MTurkers 进行标注 - 我怀疑部分答案可能已经过时。</td>
<td>MCQA</td>
<td><a href="https://aclanthology.org/D13-1160/" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://nlp.stanford.edu/software/sempre/" target="_blank" rel="noopener nofollow">网站</a></td>
<td></td>
</tr>
<tr>
<td>WebNLG</td>
<td>生成、言语化</td>
<td>13K 个三元组 (主题, 属性, 客体) 与句子表述之间的映射关系，这些三元组是从 DBPedia 构建的，而 DBPedia 是源自 Wikipedia 的知识库；句子表述是由众包工人完成的，涉及特定主题 (宇航员、大学、纪念碑、建筑物、漫画角色、食品、机场、运动队、文学作品)。</td>
<td>任务：以语法正确的方式表达。</td>
<td><a href="https://aclanthology.org/P17-1017/" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://huggingface.co/datasets/web_nlg" target="_blank" rel="noopener nofollow">Hugging Face</a></td>
<td>有一项关于流畅性的句子选择，生成的句子相对简单，但没有描述注释者/众包者的来源 &gt; 可能部分数据不是“标准英语”。</td>
</tr>
<tr>
<td>WiC</td>
<td>语言理解</td>
<td>7K 数据，判断一个词在两个不同语境中出现时是否具有相同含义的分类。</td>
<td></td>
<td><a href="https://aclanthology.org/N19-1128/" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://super.gluebenchmark.com/tasks" target="_blank" rel="noopener nofollow">Site</a></td>
<td></td>
</tr>
<tr>
<td>WikiFact_HELM</td>
<td>完形填空</td>
<td>12 个领域，每个领域包含 1K 三元组 (主题，关系，对象)，这些三元组采样自 Wikipedia 并经过清理。</td>
<td>任务：预测由关系构成的句子中缺失的项目。</td>
<td><a href="https://arxiv.org/abs/2211.09110" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://worksheets.codalab.org/rest/bundles/0x8c3b60eb7c6b462e822a150f194d3b35/" target="_blank" rel="noopener nofollow">Codalab</a><br><a href="https://github.com/stanford-crfm/helm/blob/main/src/helm/benchmark/scenarios/wikifact_scenario.py" target="_blank" rel="noopener nofollow">Github</a></td>
<td></td>
</tr>
<tr>
<td>WikiLingua</td>
<td>生成、多语言、总结</td>
<td>43K 文章 / 摘要对由 WikiHow 构建，涵盖 18 种语言 (在网站上，文章是以每个步骤一个总结句 + 详细段落的形式书写的：在数据集中，摘要为总结句的连接，而文章则是详细段落的组合)。</td>
<td>总结</td>
<td><a href="https://aclanthology.org/2020.findings-emnlp.360/" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://github.com/esdurmus/Wikilingua" target="_blank" rel="noopener nofollow">Github</a></td>
<td>Palm: 以提示符为前缀，将文章截断至2048个标记<br>我怀疑数据创建可能导致总结基线的语言非常“机械化”，这可能会突出更流畅的总结 (尽管 ROUGE 对此不应该过于敏感)。</td>
</tr>
<tr>
<td>Winogender</td>
<td>偏差检测</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Winograd</td>
<td>论证、温格拉德模式</td>
<td>273 至 285 个例子，其中必须消除歧义以确定代词指的是谁或什么，这些句子是特意构造的，在统计上是模糊的，但对人类来说不是。</td>
<td>代词的消歧</td>
<td><a href="https://dl.acm.org/doi/10.5555/3031843.3031909" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://cs.nyu.edu/~davise/papers/WinogradSchemas/WSCollection.xml" target="_blank" rel="noopener nofollow">网站</a></td>
<td>不确定 GPT3 是否在这个数据集或 SuperGLUE 数据集上进行了评估。</td>
</tr>
<tr>
<td>WinoGrande</td>
<td>论证、温格拉德模式</td>
<td>43K 句子 对抗性 Winograd</td>
<td></td>
<td><a href="https://arxiv.org/abs/1907.10641" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://winogrande.allenai.org/" target="_blank" rel="noopener nofollow">网站</a></td>
<td></td>
</tr>
<tr>
<td>WSC</td>
<td>语言理解、温格拉德模式</td>
<td>Winograd 模式挑战 (见 Winograd)</td>
<td></td>
<td><a href="https://w4ngatang.github.io/static/papers/superglue.pdf" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://super.gluebenchmark.com/tasks" target="_blank" rel="noopener nofollow">网站</a></td>
<td></td>
</tr>
<tr>
<td>XSUM</td>
<td>总结</td>
<td>226K 篇新闻文章 (BBC，2010 年至 2017 年) 与其单句摘要相匹配 (摘自文章)。任务：概括总结。领域：新闻、政治、体育、天气、商业、科技、科学、健康、家庭、教育、娱乐和艺术。</td>
<td></td>
<td><a href="https://aclanthology.org/D18-1206/" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://github.com/EdinburghNLP/XSum" target="_blank" rel="noopener nofollow">Github</a></td>
<td></td>
</tr>
<tr>
<td>XSum</td>
<td>生成、总结</td>
<td>226K 篇来自 BBC (2010 - 2017 年) 的新闻摘要/文章对，从 WayBack 机器中提取。</td>
<td></td>
<td><a href="https://aclanthology.org/D18-1206/" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://huggingface.co/datasets/xsum" target="_blank" rel="noopener nofollow">Hugging Face</a></td>
<td>可以手动检查最近的知识是否导致了旧新闻摘要中的差异，这可能会很有趣。</td>
</tr>
</tbody>
</table>
<h2 id="可手动重现的数据集想法">可手动重现的数据集想法</h2>
<table>
<thead>
<tr>
<th>评估名称</th>
<th>任务类型</th>
<th>任务内容</th>
<th>源</th>
<th>数据集</th>
<th>备注</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>✍️ GSM8K-Python</td>
<td>编码任务、文生代码</td>
<td>GSM8K 数据集的 Python 版本 (8.5K 小学数学问题)</td>
<td><a href="https://arxiv.org/abs/2204.02311" target="_blank" rel="noopener nofollow">论文</a></td>
<td>N/A</td>
<td></td>
<td></td>
</tr>
<tr>
<td>✍️ MRF</td>
<td>生成、人工评估、虚假信息生成能力</td>
<td>从MRF 数据集抽取了 250 个标题，并依据论点将其分组为 80 个簇。任务要求：基于每个论点及其对应的 5 个标题，模型需要生成能够支持该论点的合理标题。评估人员将对生成的标题进行评价，判断其是否 1) 支持论点，以及 2) 看起来真实可信。</td>
<td><a href="https://arxiv.org/abs/2211.09110" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://drive.google.com/uc?export=download&amp;id=1uVJbsgPCHFAvH43I6SVvU3Ayo8dh-y_N" target="_blank" rel="noopener nofollow">数据</a></td>
<td>参见 <a href="https://cset.georgetown.edu/publication/truth-lies-and-automation/" target="_blank" rel="noopener nofollow">报告</a> 第 6 页获取关于原始过程的更多说明，以及 HEMLM 论文的章节 8.5.2、E.5 和 5.5。</td>
<td></td>
</tr>
<tr>
<td>✍️ News article generation</td>
<td>生成</td>
<td>根据标题和副标题生成了 25 篇文章，80 名人类评估者需要判断这些文章是生成的还是原始的。</td>
<td><a href="https://arxiv.org/abs/2005.14165" target="_blank" rel="noopener nofollow">论文</a></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>✍️ Numeracy Prediction</td>
<td>符号处理</td>
<td>要求模型根据给定的几个例子执行符号回归，并将数字关系应用于新的输入。</td>
<td></td>
<td><a href="https://arxiv.org/abs/2211.09110" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://github.com/stanford-crfm/helm/blob/main/src/helm/benchmark/scenarios/numeracy_scenario.py" target="_blank" rel="noopener nofollow">Github</a></td>
<td></td>
</tr>
<tr>
<td>✍️ SVG datasets</td>
<td></td>
<td>可构造一个 SVG 数据集，用于检查模型是否确实能够生成或者分解 SVG 图形</td>
<td><a href="https://twitter.com/zswitten/status/1631178997508997120" target="_blank" rel="noopener nofollow">Twitter 会话</a></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>✍️ Theory of the mind datasets</td>
<td></td>
<td>可能很容易生成</td>
<td><a href="https://arxiv.org/abs/2302.08399" target="_blank" rel="noopener nofollow">论文</a></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>✍️ Wedging prompts</td>
<td>生成, 人工评估、虚假信息生成能力</td>
<td>具有特定意图的 11 个提示 (例如：影响投票行为，通过生成支持/反对 X 的言论来针对特定群体)，并附有 3 个示例。任务：生成后续示例。</td>
<td><a href="https://cset.georgetown.edu/wp-content/uploads/CSET-Truth-Lies-and-Automation.pdf" target="_blank" rel="noopener nofollow">论文</a></td>
<td><a href="https://drive.google.com/uc?export=download&amp;id=1kWB3_F4Tobc_oVGC_T-a5DHEh-AB4GTc" target="_blank" rel="noopener nofollow">数据</a></td>
<td>在 HELM 中：使用人工评估来确定生成的消息 1) 是否针对特定群体；2) 是否支持预期的信息；3) 是否具有分裂性。</td>
<td></td>
</tr>
<tr>
<td>✍️ Word scrambling</td>
<td>符号处理</td>
<td>10,000 个示例，涵盖 5 项字符操作任务 (字母循环移位的单词、字母重组、随机插入、反转)。模型需要恢复原始单词。</td>
<td><a href="https://arxiv.org/abs/2005.14165" target="_blank" rel="noopener nofollow">论文</a></td>
<td></td>
<td>易于生成/自动化，参见 GPT3 论文的第 3.9.2 节。</td>
<td></td>
</tr>
</tbody>
</table>
<hr>
<blockquote>
<p>英文原文: <a href="https://github.com/huggingface/evaluation-guidebook/blob/main/translations/zh/contents/automated-benchmarks/some-evaluation-datasets.md" target="_blank" rel="noopener nofollow">https://github.com/huggingface/evaluation-guidebook/blob/main/translations/zh/contents/automated-benchmarks/some-evaluation-datasets.md</a></p>
<p>原文作者: clefourrier</p>
<p>译者: SuSung-boy</p>
<p>审校: adeenayakup</p>
</blockquote>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.20863259837152778" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-01-08 15:27">2025-01-08 15:27</span>&nbsp;
<a href="https://www.cnblogs.com/huggingface">HuggingFace</a>&nbsp;
阅读(<span id="post_view_count">1</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18659784" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18659784);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18659784', targetLink: 'https://www.cnblogs.com/huggingface/p/18659784', title: '自动评估基准 | 一些评估测试集' })">举报</a>
</div>
        