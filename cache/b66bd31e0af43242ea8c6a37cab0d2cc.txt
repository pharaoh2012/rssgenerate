
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/uzuki/p/19056169" title="发布于 2025-08-25 01:13">
    <span role="heading" aria-level="2">LLM 指标 | PPL vs. BLEU vs. ROUGE-L vs. METEOR vs. CIDEr</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h1 id="llm-指标--ppl-vs-bleu-vs-rouge-l-vs-meteor-vs-cider">LLM 指标 | PPL vs. BLEU vs. ROUGE-L vs. METEOR vs. CIDEr</h1>
<h2 id="困惑度perplexity-ppl">困惑度（Perplexity, PPL）↓</h2>
<p>PPL的意义非常明了，用于测量模型对生成文本的不确定程度，不确定程度越低，模型的表现就越好。其计算方法是计算句子每个token的平均对数似然，再过一个指数函数。</p>
<h3 id="定义">定义</h3>
<p>给定一个长度为<span class="math inline">\(n\)</span>的token序列：</p>
<p></p><div class="math display">\[S=(w_1,w_2,\cdots,w_n)
\]</div><p></p><p>那么该序列的PPL为：</p>
<p></p><div class="math display">\[PPL(S)=\exp\big(-\frac{1}{N}\sum^N_{i=1}\log P(w_i|w_1,\cdots,w_{i-1}\big)
\]</div><p></p><h2 id="bleubilingual-evaluation-understudy">BLEU（Bilingual Evaluation Understudy）↑</h2>
<p>BLEU出自文章<em>BLEU: a Method for Automatic Evaluation of Machine Translation."<br>
Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</em>，主要用以评估翻译任务中生成文本与参考文本的匹配程度。其更具体的形式<strong>BLEU@N</strong>用以定义在<span class="math inline">\(1,2,\cdots,N\)</span>-gram情况下，生成文本与参考文本的匹配程度，再通过一个<strong>长度惩罚项（brevity penalty, BP）</strong>避免模型的生成文本过短。</p>
<h3 id="定义-1">定义</h3>
<p>给定生成文本为<span class="math inline">\(C\)</span>，参考文本为<span class="math inline">\(\{R_1,R_2,\cdots,R_m\}\)</span></p>
<p>首先我们定义modified n-gram precision：</p>
<p>给定N-gram <span class="math inline">\(g_n\)</span>，<span class="math inline">\(Count_S(g_n)\)</span>表示在序列<span class="math inline">\(S\)</span>中<span class="math inline">\(g\)</span>出现的次数，那么我们可以定义modified N-gram precision：</p>
<p></p><div class="math display">\[p_n = \frac{\sum_{g_n\in C}\min(Count_C(g_n), \max_j Count_{R_i}(g_n))}{\sum_{g_n\in C}Count_C(g)}
\]</div><p></p><p>通俗解释<span class="math inline">\(p_n\)</span>定义了生成文本与参考文本之间的重叠程度，并且设定了每个词的出现次数上界为参考文本中出现次数上界。</p>
<p>接下来我们计算<span class="math inline">\(1,2,\cdots,N\)</span>-gram的几何平均值（通常取<span class="math inline">\(n=1,2,3,4\)</span>）</p>
<p>有：</p>
<p></p><div class="math display">\[P=\exp(\frac{1}{N}\sum^N_{n=1}\log p_n)
\]</div><p></p><p>接下来计算<strong>长度惩罚项BP</strong>：</p>
<p></p><div class="math display">\[\text{BP} =
\begin{cases} 
1 &amp; \text{if } c &gt; r \\[2mm]
\exp\left(1 - \frac{r}{c}\right) &amp; \text{if } c \leq r
\end{cases}
\]</div><p></p><p>其中<span class="math inline">\(c\)</span>为生成文本的长度，<span class="math inline">\(r\)</span>为与生成文本<span class="math inline">\(c\)</span>长度最接近的参考文本的长度</p>
<p>最后相乘得到<strong>BLEU</strong>：</p>
<p></p><div class="math display">\[BLEU=P\cdot BP
\]</div><p></p><h2 id="rouge-l-">ROUGE-L ↑</h2>
<p>ROUGE-L（Recall-Oriented Understudy for Gisting Evaluation - LCS）通过计算<strong>最长公共子串LCS</strong>评估生成文本与参考文本之间的匹配程度，为此给定生成文本<span class="math inline">\(C\)</span>和参考文本<span class="math inline">\(R\)</span>，我们可以定义其precison，recall以及F1-score：</p>
<h3 id="rouge--l-precison">ROUGE- L Precison</h3>
<p></p><div class="math display">\[P_{LCS} = \frac{LCS(C,R)}{|C|}
\]</div><p></p><h3 id="recall">recall</h3>
<p></p><div class="math display">\[R_{LCS}=\frac{LCS(C, R)}{|R|}
\]</div><p></p><h3 id="f1-score">F1-score</h3>
<p></p><div class="math display">\[F_{LCS}=\frac{2\cdot R_{LCS}\cdot {P_{LCS}}}{R_{LCS}+P_{LCS}}
\]</div><p></p><h2 id="meteor">METEOR↑</h2>
<p>METEOR出自文章<em><a href="https://dl.acm.org/doi/pdf/10.5555/1626355.1626389" target="_blank" rel="noopener nofollow">Meteor: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments</a></em>，为了解决BLEU指标<strong>不能处理非精确匹配</strong>以及<strong>语序不敏感</strong>的两个缺陷，加入了语义对齐以及碎片化惩罚因子两个步骤。</p>
<h3 id="语义对齐">语义对齐</h3>
<p>首先通过贪心算法对所有的词语进行一一匹配，这种匹配考虑精确匹配、词干匹配或同义词匹配</p>
<p>PS：细节再补上去</p>
<h3 id="计算recall和precision">计算recall和precision</h3>
<p>给定生成文本<span class="math inline">\(C\)</span>以及参考文本<span class="math inline">\(R\)</span>，<span class="math inline">\(m\)</span>为成功匹配的词语数量，我们分别可以计算其precision和recall</p>
<p></p><div class="math display">\[P=\frac{m}{|C|},R=\frac{m}{|R|}
\]</div><p></p><h3 id="计算f-score">计算F-score</h3>
<p>通过上一步计算的precision和recall</p>
<p></p><div class="math display">\[F_{mean} = \frac{P\cdot R}{(1-\alpha)\cdot R + \alpha \cdot P}
\]</div><p></p><p>其中<span class="math inline">\(\alpha\)</span>为平衡recall和precision的权重，一般取<span class="math inline">\(\alpha = 0.9\)</span></p>
<h4 id="计算碎片化惩罚项fragmentation-penalty">计算碎片化惩罚项（fragmentation penalty）</h4>
<p>生成文本中连续的匹配词会被同一个块（chunk）中，块的数量越多说明语序越不匹配，我们可以给出其定义：</p>
<p></p><div class="math display">\[FP =\gamma \bigg(\frac{\#\text{chunks}}{m}\bigg)^\beta
\]</div><p></p><p>其中<span class="math inline">\(\beta\)</span>定义了惩罚项函数的形状，<span class="math inline">\(\gamma\)</span>定义了惩罚项的相对权重，一般取<span class="math inline">\(\beta = 3, \gamma=0.5\)</span></p>
<p>最后我们可以计算METEOR：</p>
<p></p><div class="math display">\[METEOR = F_{mean}*(1-FP)
\]</div><p></p><h2 id="cider">CIDEr↑</h2>
<p>CIDEr（Consensus-based Image Description Evaluation）出自文章<em><a href="https://arxiv.org/abs/1411.5726" target="_blank" rel="noopener nofollow">CIDEr: Consensus-based Image Description Evaluation</a></em>，主要用于Image Captioning任务中，评估生成文本与参考文本的匹配程度</p>
<p>通过计算TF-IDF N-gram向量的余弦相似度评估生成样本与评估样本的匹配程度</p>
<h3 id="tf-idf-n-gram向量">TF-IDF N-gram向量</h3>
<p>给定生成文本<span class="math inline">\(C\)</span>以及参考文本<span class="math inline">\(\{R_1,R_2,\cdots,R_m\}\)</span>，我们可以计算每个文本<span class="math inline">\(S\)</span>的TF-IDF向量：</p>
<p></p><div class="math display">\[g_n(S)=[w_S(g_1),w_S(g_2),\cdots,w_S(g_k)]
\]</div><p></p><p>其中<span class="math inline">\(w_S(g_k)\)</span>为N-gram <span class="math inline">\(g_k\)</span>在文本<span class="math inline">\(S\)</span>中的TF-IDF权重，定义为：</p>
<p></p><div class="math display">\[w_S(g_k)=TF(g_k,S)\cdot\log\frac{m}{\sum^m_{i=1}\mathbf{1}(g_k\in R_i)}
\]</div><p></p><p>其中<span class="math inline">\(\sum^m_{i=1}\mathbf{1}(g_k\in R_i)\)</span>表示在N-gram <span class="math inline">\(g_k\)</span>在多少个参考文本中出现了</p>
<h4 id="n-gram向量余弦相似度">N-gram向量余弦相似度</h4>
<p>接下来我们可以计算生成文本<span class="math inline">\(C\)</span>和所有参考文本之间的N-gram向量余弦相似度</p>
<p></p><div class="math display">\[sim_n(C,R_i)=\frac{g_n(S)\cdot g_n(R_i)}{||g_n(S)||\ ||g_n(R_i)||}
\]</div><p></p><h3 id="对所有参考文本取平均得到">对所有参考文本取平均得到<span class="math inline">\(CIDEr_n\)</span></h3>
<p></p><div class="math display">\[CIDEr_n(C,R) = \frac{1}{m}\sum^m_{i=1}sim_n(C,R_i)
\]</div><p></p><h3 id="计算cider">计算CIDEr</h3>
<p>接下来我们计算<span class="math inline">\(1,2,\cdots,N\)</span>-gram情况的平均值（通常取<span class="math inline">\(n=1,2,3,4\)</span>）</p>
<p></p><div class="math display">\[CIDEr = \sum^N_{n=1}w_n\cdot CIDEr_n(C,R)
\]</div><p></p><p>其中<span class="math inline">\(w_n\)</span>为不同的N-gram情况的权重，一般取<span class="math inline">\(w_n=\frac{1}{N}\)</span></p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.3972222222222222" data-date-updated="2025-08-25 10:45">2025-08-25 01:13</span>&nbsp;
<a href="https://www.cnblogs.com/uzuki">Uzuki</a>&nbsp;
阅读(<span id="post_view_count">38</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(19056169);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '19056169', targetLink: 'https://www.cnblogs.com/uzuki/p/19056169', title: 'LLM 指标 | PPL vs. BLEU vs. ROUGE-L vs. METEOR vs. CIDEr' })">举报</a>
</div>
        