
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/luohenyueji/p/18744026" title="发布于 2025-02-28 20:26">
    <span role="heading" aria-level="2">[深度学习] 大模型学习2-提示词工程指北</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p>在文章<a href="https://blog.csdn.net/LuohenYJ/article/details/144858528" target="_blank" rel="noopener nofollow">大语言模型基础知识</a>里，提示词工程（Prompt Engineering）作为大语言模型（Large Language Model，LLM）应用构建的一种方式被简要提及，本文将着重对该技术进行介绍。</p>
<p>提示词工程就是在和LLM聊天时，用来让模型回答得更好的一种方法。LLM的工作原理是猜下一个字或词是什么，而当你向它提供一段话（即提示词，prompt）时，这段话便成为模型的参考信息。随后根据这一提示，模型会像续写故事一样生成回答。在这个过程中，提示词的质量至关重要。就像给他人布置任务时，表达越清晰，对方完成得就越好。同样对prompt的描述越明确，模型就越有可能给出高质量的答案。提示词工程正是专注于如何设计和优化这些prompt，以引导模型生成更准确、更贴合需求的输出。它就像是在告诉模型：“你需要重点关注这些信息”或“你需要从这个角度理解问题”。需要注意的是，提示词工程并非万能。因为模型的回答只能基于其已有的知识储备，无法超出这个范围。</p>
<p>当前提示词工程的教程铺天盖地。然而，其中不少方法对于普通开发者来说，复杂度过高，掌握起来颇具难度。与此同时，许多教程中的技术已然落后，且并非适用于所有的LLM。实际上，普通人在日常与LLM互时，也会运用到提示词工程，只不过形式较为简单，常见的如 “请润色这句话……” 或 “请回答这个问题……” 这类表述。提示词的核心价值并非体现于编写复杂提示词的能力，而是在于精准界定自身期望达成的目标。相较之下，具备恰当的评估标准，以及明晰如何衡量人工智能输出成果的能力，才是至关重要的。鉴于此，本文将着重聚焦于一系列实用且常用的提示词工程方法，详尽阐释如何将其灵活应用于实际场景。</p>
<p>若想深入了解提示词工程的详细内容和使用方法，可参考以下资源：<a href="https://platform.openai.com/docs/guides/prompt-engineering" target="_blank" rel="noopener nofollow">Prompt engineering</a>，<a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/prompt-engineering" target="_blank" rel="noopener nofollow">Prompt engineering techniques</a>，<a href="https://github.com/modelscope/modelscope-classroom" target="_blank" rel="noopener nofollow">modelscope-classroom</a> 。</p>
<p></p><div class="toc"><div class="toc-container-header">目录</div><ul><li><a href="#1-提示词工程介绍" rel="noopener nofollow">1 提示词工程介绍</a><ul><li><a href="#11-提示词工程发展历程" rel="noopener nofollow">1.1 提示词工程发展历程</a></li><li><a href="#12-llm中的角色" rel="noopener nofollow">1.2 LLM中的角色</a><ul><li><a href="#121-system角色" rel="noopener nofollow">1.2.1 System角色</a></li><li><a href="#122-提示词工程" rel="noopener nofollow">1.2.2 提示词工程</a></li><li><a href="#123-撰写优质提示词的原则" rel="noopener nofollow">1.2.3 撰写优质提示词的原则</a></li><li><a href="#124-提示词框架" rel="noopener nofollow">1.2.4 提示词框架</a></li></ul></li></ul></li><li><a href="#2-参考" rel="noopener nofollow">2 参考</a></li></ul></div><p></p>
<h1 id="1-提示词工程介绍">1 提示词工程介绍</h1>
<h2 id="11-提示词工程发展历程">1.1 提示词工程发展历程</h2>
<p>2020年，OpenAI推出的GPT-3以1750亿参数的庞大体量，成为当时全球规模最大的语言模型。它广泛应用于文本生成、问答系统等自然语言处理任务，展现出强大的语言处理能力。伴随应用场景的持续拓展，精心设计的提示词在引导模型产出高质量、契合预期的输出成果上，关键作用日益凸显，进而直接促使提示词工程蓬勃兴起。<br>
2022年，ChatGPT的发布为LLM的发展带来了新的突破。它凭借简洁友好的界面和卓越的对话能力，极大地提升了LLM的公众可及性，让普通用户也能轻松上手。在实际使用过程中，用户切实感受到了提示词设计对交互效果的重大影响，这使得提示词工程不仅在专业领域受到重视，更在大众层面迅速受到广泛关注并蓬勃发展。<br>
2023年，GPT-4发布，成为自然语言处理领域的重要里程碑。作为能处理文本和图像输入的先进AI模型，GPT-4极大拓展了应用场景的广度与深度。与此同时，多模态提示词工程兴起，成为推动AI发展的关键力量。多模态提示词工程整合文本、图像、音频乃至视频等多种输入模态来构建指令或查询。借助GPT-4、DALL-E等先进LLM模型，这类提示可实现对不同格式内容的处理与生成。例如，GPT-4能同时处理文本和图像输入，输出高质量文本；DALL-E则专注于依据文本描述生成图像。多模态提示的应用显著提升了AI系统的多功能性与智能性，使其能更精准地解析和应对复杂现实场景中涉及多种数据形式的挑战。<br>
但是，无论各种任务的提示词形式如何多样，它们的本质都是为LLM提供明确的指令或上下文信息，引导其按照人类的意图进行思考并生成内容。</p>
<p><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A02-%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B%E6%8C%87%E5%8C%97/img/img1.jpg" alt="https://synoptek.com/insights/it-blogs/data-insights/prompt-engineering-strategies-for-optimizing-ai-language-models/" loading="lazy"></p>
<p>在这样的技术发展背景下，提示词工程师这一新兴职业应运而生。提示词工程师扮演着人与大语言模型之间的 “翻译官” 角色，他们不仅要对各类语言模型的性能、特点了如指掌，还需精通如何设计和优化模型的prompt，以帮助模型更好地理解人类意图并生成高质量的输出。然而，提示词工程师的未来是一个充满争议且具有不确定性的话题。</p>
<p>本人认为提示词工程师的未来并不乐观。随着人工智能的进步，模型将更智能，不再依赖复杂提示词。自动化工具的涌现，使简单需求也能生成可用提示词，且将愈发智能高效。更重要的是如今，LLM的深度思考能力使其在语言理解和生成方面达到了较高水平，而联网搜索功能的加持则进一步拓展了其知识来源和应用范围。这两者的结合，能够为用户提供更强大、更智能的语言交互服务，从而在很大程度上削弱了对专业提示词工程师的依赖。此外，提示词工程师的工作成果高度依赖于模型本身的性能，且需要针对不同模型进行单独优化，这使得工作本身复杂且不稳定。一旦模型升级，过往的经验可能瞬间变得无用，职业发展的不确定性也随之大大提高。</p>
<p><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A02-%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B%E6%8C%87%E5%8C%97/img/img7.jpg" alt="https://www.cut-the-saas.com/informative/what-are-the-career-and-future-prospects-of-prompt-engineering" loading="lazy"></p>
<h2 id="12-llm中的角色">1.2 LLM中的角色</h2>
<p>在LLM的交互场景或应用中，通常存在system（系统）、user（用户）和assistant（助手）这三种角色。虽然在特定应用场景下，LLM可能会根据需求引入其他角色，但在绝大多数情况下，这三种角色构成了对话交互的基础框架。具体说明如下：</p>
<ul>
<li>system：在对话启动前，明确设定模型在对话中的角色定位。例如设定为老师、科学家、医生等。不同的角色设定将使模型在特定领域表现出更强的专业性。</li>
<li>user：代表用户输入或生成的信息，是与LLM进行交互的主体。<strong>Prompt通常指的是user输入的内容</strong>。</li>
<li>assistant：由LLM自动生成并回复的内容。</li>
</ul>
<p>注意，通常在基于LLM搭建的对话应用中，system角色的内容通常不会直接呈现给用户。System角色的设定一般在后端发起API调用时完成，通过设置如“你是一个有帮助的助手”等指令，引导模型生成高质量回答。这些底层设置对普通用户是隐藏的。用户使用网页应用时，只能看到自己与模型的交互结果：自己以user角色输入的问题，以及模型以assistant角色给出的回答。</p>
<p>然而，在调用LLM应用的API过程中，system角色的相关设置将会展现出来。以使用OpenAI各类LLM模型的API接口为例，在构建API参数时，通常会明确区分system、assistant和user三种角色的内容，如下所示：</p>
<pre><code class="language-python">messages=[
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Knock knock."},
    {"role": "assistant", "content": "Who's there?"},
    {"role": "user", "content": "Orange."},
],
</code></pre>
<p><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A02-%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B%E6%8C%87%E5%8C%97/img/img5.jpg" alt="https://chrispogeek.medium.com/the-llm-wants-to-talk-e1514043ae9c" loading="lazy"></p>
<h3 id="121-system角色">1.2.1 System角色</h3>
<p>在本节内容中，本文推荐的LLM使用环境为：<a href="https://modelscope.cn/studios/Qwen/Qwen2-72B-Instruct-demo" target="_blank" rel="noopener nofollow">Qwen2-72B</a>。</p>
<p>System角色设置的指令决定了LLM的角色风格。用户可以通过System指令塑造其回答的独特个性。这些指令可以明确界定LLM应回答和不应回答的内容，甚至还能规定回答的格式。在大多数LLM模型中，system指令的权重高于用户手动输入的prompt，并且其在多轮对话中始终保持稳定。默认的system角色通常为“你是一个有帮助的助手”或“You are a helpful assistant”。</p>
<p>针对同一个问题，当LLM被赋予不同的系统角色时，能生成截然不同的回答。比如，在面对“请描述一下你最喜欢的季节”这一问题时，不同系统角色的回答如下：</p>
<table>
<thead>
<tr>
<th>系统角色</th>
<th>回答</th>
</tr>
</thead>
<tbody>
<tr>
<td>默认设置</td>
<td>我作为一个语言模型，没有个人喜好。但我可以提供关于不同季节的客观信息。例如，春天万物复苏，夏天阳光明媚，秋天色彩斑斓，冬天银装素裹。每个季节都有其独特的魅力。</td>
</tr>
<tr>
<td>诗人</td>
<td>细雨湿流光，落花人独立。最是人间四月天，风光不与四时同。春，那是一幅水墨画，晕染着诗情画意。花开满枝，燕子呢喃，一切都是那么美好。</td>
</tr>
<tr>
<td>时尚博主</td>
<td>我最爱夏天啦！阳光、沙滩、海浪，还有各种各样的时尚单品。想象一下，穿着一条飘逸的吊带裙，踩着精致的凉鞋，在海边漫步，简直太惬意了！</td>
</tr>
<tr>
<td>文言文大师</td>
<td>吾独爱秋。天高云淡，桂子飘香，正是登高望远之佳季。秋风拂面，令人神清气爽，心旷神怡。</td>
</tr>
<tr>
<td>唐僧</td>
<td>贫僧四大皆空，无喜无悲。然若论及四季，则冬日最为清静，可静心修行。</td>
</tr>
<tr>
<td>吃货</td>
<td>我最喜欢秋天了！这个季节的美食实在太多了！螃蟹肥美，柿子甜糯，还有各种各样的水果，真是让人垂涎三尺。</td>
</tr>
</tbody>
</table>
<p>System指令还能够限定LLM的回应范畴，同时规范其行为表现。例如，可设置这样一条系统指令：</p>
<blockquote>
<p>假定你是一位在计算机视觉领域拥有深厚经验的资深专家。你的核心职责在于精准解答各类与计算机视觉相关的问题，能够全面、深入地阐述计算机视觉的概念、前沿技术以及最佳实践方案。在涉及实现计算机视觉算法、挑选适配的工具与框架，以及搭建端到端的计算机视觉项目时，你需要提供条理清晰的分步指导。请明确以下两点：其一，你的知识范畴被严格限定在计算机视觉领域；其二，若遇到无法确定答案的问题，请回复‘我不知道’，并同时建议用户浏览博客获取更多信息，博客链接为：<a href="https://blog.csdn.net/LuohenYJ%E3%80%82" target="_blank" rel="noopener nofollow">https://blog.csdn.net/LuohenYJ。</a></p>
</blockquote>
<p>基于此设定，不同问题会得到如下不同形式的回答：</p>
<table>
<thead>
<tr>
<th>能否回答</th>
<th>问题</th>
<th>回答</th>
</tr>
</thead>
<tbody>
<tr>
<td>能回答</td>
<td>在计算机视觉中，目标检测和语义分割有什么区别？</td>
<td>目标检测和语义分割都是计算机视觉领域的重要任务，但它们存在明显区别...</td>
</tr>
<tr>
<td>不能回答</td>
<td>量子计算如何结合应用于金融领域？</td>
<td>我不知道，建议您浏览博客获取更多信息：<a href="https://blog.csdn.net/LuohenYJ%E3%80%82" target="_blank" rel="noopener nofollow">https://blog.csdn.net/LuohenYJ。</a></td>
</tr>
</tbody>
</table>
<p>通过系统角色设定来定制LLM回答的方式，极大地拓展了LLM的应用场景。需要明确的是，LLM回答的质量在很大程度上取决于训练数据的质量和数量。若训练数据匮乏或质量欠佳，可能会对角色扮演的效果产生负面影响。此外，如果角色定义不够清晰，模型可能会误解角色意图。</p>
<h3 id="122-提示词工程">1.2.2 提示词工程</h3>
<p>User角色是使用LLM的主体，prompt是用户向LLM传达提问意图、需求的工具。在LLM兴起的初期，提示词工程曾是一项备受关注的技术，其中包含了各种单个或一系列prompt技巧。这些技巧虽然形式各异，但方法和目标是一致的，本质上也都比较相似。然而，随着LLM相关技术日新月异，其智能程度实现了质的飞跃。到了当下，曾经备受关注的prompt技术，已不再是运用LLM的核心要素，使用者仅需稍有了解即可满足基本需求。</p>
<p><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A02-%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B%E6%8C%87%E5%8C%97/img/img2.jpg" alt="https://www.humanfirst.ai/blog/12-prompt-engineering-techniques" loading="lazy"></p>
<p>以下是一些常用的提示词技巧及其简要介绍。请注意，这里的介绍仅为入门概述，更详细的说明请自行搜索相关文档：</p>
<ol>
<li>
<p>Least-To-Most（从最少到最多）</p>
<ul>
<li>问题示例：计算在1-200中，能被5整除且各位数字之和大于8的数的个数。</li>
<li>使用原因：问题较为复杂，直接求解有难度，通过分解为简单子问题可降低难度。</li>
<li>技术特点：将复杂任务分解为一系列简单的、有序的子任务，依次解决子任务来完成最终复杂任务，类似于人类解决复杂问题时的分而治之策略。</li>
<li>第一个prompt：1-200中能被5整除的数有哪些？</li>
<li>回答可能：5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100,105,110,115,120,125,130,135,140,145,150,155,160,165,170,175,180,185,190,195,200。</li>
<li>第二个prompt：在上述能被5整除的数中，计算各位数字之和。</li>
<li>回答可能：对于5，数字和为5；对于10，数字和为1+0=1；对于15，数字和为1+5=6……（依次计算并列出）</li>
<li>第三个prompt：在前面计算了各位数字之和的数中，找出各位数字之和大于8的数，并统计个数。</li>
<li>回答可能：经过筛选，符合条件的数有……，共[X]个。</li>
</ul>
</li>
<li>
<p>Self-Ask（自我询问）</p>
<ul>
<li>问题示例：如何提高在线课程的学生参与度？</li>
<li>使用原因：问题开放且宽泛，通过模型自身不断生成相关问题来逐步聚焦关键因素和解决方案。</li>
<li>技术特点：模型基于初始问题自我生成一系列相关问题，模拟人类在思考复杂问题时不断提问和探索的过程。</li>
<li>第一个prompt：如何提高在线课程的学生参与度？</li>
<li>回答可能：从课程内容、互动方式等方面考虑。</li>
<li>第二个prompt：从课程内容角度，有哪些具体方法可以提高学生参与度？</li>
<li>回答可能：可以使内容更有趣、实用，结合实际案例等。</li>
<li>第三个prompt：在互动方式上，有哪些有效的做法？</li>
<li>回答可能：增加实时问答、小组讨论、在线测验等。</li>
</ul>
</li>
<li>
<p>Meta-Prompt（元提示）</p>
<ul>
<li>问题示例：写一篇关于环保生活方式的短文。</li>
<li>使用原因：希望模型生成特定要求的提示，以便更好地引导写作。</li>
<li>技术特点：利用提示来生成提示，即先给出一个关于如何生成提示的元指令，让模型基于此生成具体的、用于解决实际问题（如创作）的提示，实现对模型输出的更精准引导。</li>
<li>prompt：生成一个关于写环保生活方式短文的提示，要包含字数要求、内容侧重点（如日常行为、环保意义等）。</li>
<li>回答可能：写一篇300-500字的短文，重点阐述日常生活中常见的环保行为，如垃圾分类、节约水电等，并简要说明这些行为对环境保护的意义。</li>
</ul>
</li>
<li>
<p>Chain-Of-Thought（思维链）</p>
<ul>
<li>问题示例：小红去商店买文具，一支铅笔2元，一个笔记本5元，她买了3支铅笔和2个笔记本，一共花了多少钱？</li>
<li>使用原因：帮助模型展示清晰的推理过程，提高答案准确性和可解释性。</li>
<li>技术特点：在问题中嵌入推理步骤，引导模型按照特定的逻辑顺序进行思考和计算，使模型不仅给出答案，还展示得出答案的过程。</li>
<li>prompt：小红去商店买文具，一支铅笔2元，一个笔记本5元，她买了3支铅笔和2个笔记本。我们先计算买铅笔花的钱，即2×3=6元；再计算买笔记本花的钱，即5×2=10元；最后求总共花的钱，就是买铅笔的钱加上买笔记本的钱。请问一共花了多少钱？</li>
<li>回答可能：买铅笔花了6元，买笔记本花了10元，总共花了6+10=16元。</li>
</ul>
</li>
<li>
<p>ReAct（推理与行动）</p>
<ul>
<li>问题示例：查找一下2024年奥运会举办城市巴黎的著名旅游景点，并推荐适合游客的游览路线。</li>
<li>使用原因：需要先推理获取信息，再基于信息行动规划路线。</li>
<li>技术特点：将推理和行动相结合，模型先进行推理分析确定所需采取的行动（如调用工具、查询信息等），然后执行行动获取信息，最后基于信息进行回答，增强了模型解决实际问题的能力。</li>
<li>prompt：查找一下2024年奥运会举办城市巴黎的著名旅游景点，并推荐适合游客的游览路线。</li>
<li>推理过程：模型意识到需要先查找巴黎著名景点信息，可能调用网络搜索工具。</li>
<li>行动：通过网络搜索获取诸如埃菲尔铁塔、卢浮宫、巴黎圣母院等景点信息。</li>
<li>回答可能：巴黎著名旅游景点有埃菲尔铁塔、卢浮宫、巴黎圣母院等。推荐游览路线为：第一天上午前往卢浮宫欣赏艺术珍品，下午去埃菲尔铁塔俯瞰城市；第二天参观巴黎圣母院……</li>
</ul>
</li>
<li>
<p>Symbolic Reasoning（符号推理）</p>
<ul>
<li>问题示例：已知A=3，B=4，C=A*B+2，求C的值。</li>
<li>使用原因：问题基于符号和数学逻辑关系，适合用符号推理解决。</li>
<li>技术特点：按照相应的规则进行计算和推导，以得出答案，常用于数学、逻辑等领域的问题求解。</li>
<li>prompt：已知A=3，B=4，C=A*B+2，求C的值。</li>
<li>回答可能：先计算A<em>B=3</em>4=12，再计算C=12+2=14。</li>
</ul>
</li>
<li>
<p>PAL（程序辅助语言模型）</p>
<ul>
<li>问题示例：计算1-100中所有奇数的平方和。</li>
<li>使用原因：通过程序代码能高效准确计算，借助模型将问题转化为程序求解。</li>
<li>技术特点：将自然语言问题转化为程序代码执行。</li>
<li>第一个prompt：计算1-100中所有奇数的平方和，使用Python代码实现并给出结果。</li>
<li>回答可能：Python代码为：...，结果为166650。</li>
</ul>
</li>
<li>
<p>Iterative Prompting（迭代提示）</p>
<ul>
<li>问题示例：描述一下人工智能在医疗领域的应用。</li>
<li>使用原因：初始回答可能不够全面准确，通过迭代逐步完善。</li>
<li>技术特点：基于模型的初始回答进行分析和反馈，生成新的提示来引导模型进一步思考和完善答案，通过多次迭代，不断优化模型的输出，直到达到满意的效果。</li>
<li>第一个prompt：描述一下人工智能在医疗领域的应用。</li>
<li>回答可能：人工智能在医疗领域可用于疾病诊断。</li>
<li>第二个prompt：在前面回答基础上，详细说明人工智能在疾病诊断方面的具体应用，以及其他医疗领域的应用。</li>
<li>回答可能：在疾病诊断方面，人工智能可通过分析医学影像辅助诊断疾病；在药物研发上，可预测药物活性……</li>
<li>第三个prompt（若还不满意）：进一步阐述人工智能在药物研发中预测药物活性的具体原理和方法。</li>
<li>回答可能：人工智能通过分析大量化合物结构和生物活性数据……</li>
</ul>
</li>
<li>
<p>Sequential Prompting（顺序提示）</p>
<ul>
<li>问题示例：写一首关于春天的现代诗。</li>
<li>使用原因：将写诗过程分步引导，使模型更好完成任务。</li>
<li>技术特点：把一个复杂任务按照逻辑顺序分解为多个子任务，依次提供每个子任务的提示，类似于项目管理中的分步执行策略。</li>
<li>第一个prompt：确定关于春天现代诗的主题方向，比如春天的景色、春天的生机等。</li>
<li>回答可能：主题方向为春天的景色。</li>
<li>第二个prompt：根据‘春天的景色’这一主题，列出诗中要描写的具体景物。</li>
<li>回答可能：花朵、绿草、溪流、燕子。</li>
<li>第三个prompt：结合上述景物，写一首关于春天景色的现代诗。</li>
<li>回答可能：在春天的田野，花朵绽放笑颜，绿草铺满大地……（具体现代诗内容）</li>
</ul>
</li>
<li>
<p>Self-Consistency（自一致性）</p>
<ul>
<li>问题示例：分析小说《呐喊》中孔乙己这一人物形象。</li>
<li>使用原因：从不同角度分析可使对人物形象的理解更全面准确，通过自一致性判断得到更可靠结论。</li>
<li>技术特点：模拟人类在面对复杂问题时综合多方观点的思维方式。</li>
<li>第一个prompt：分析小说《呐喊》中孔乙己这一人物形象。</li>
<li>回答可能1：孔乙己是一个深受封建科举制度毒害的落魄书生，他自命清高，却又穷困潦倒。</li>
<li>回答可能2：孔乙己是旧社会的悲剧人物，他善良但又有迂腐的一面，在人们的嘲笑中生存。</li>
<li>经过自一致性判断后最终回答：综合多个回答，孔乙己是一个深受封建科举制度毒害，自命清高、穷困潦倒，善良又迂腐的旧社会悲剧人物。</li>
</ul>
</li>
<li>
<p>Automatic Reasoning and Tool-use(ART)（自动推理与工具使用）</p>
<ul>
<li>问题示例：计算圆的面积，已知半径为5厘米。</li>
<li>使用原因：需要调用数学计算工具来准确计算面积。</li>
<li>技术特点：模型具备自动推理能力，能自动调用工具获取信息，结合自身推理得出答案，实现了知识获取和推理的自动化。</li>
<li>prompt：计算圆的面积，已知半径为5厘米。</li>
<li>推理过程：模型判断需要使用圆面积公式和计算工具。</li>
<li>行动：调用数学计算功能，根据公式S=πr²（π取3.14）计算。</li>
<li>回答可能：圆的面积为3.14×5²=78.5平方厘米。</li>
</ul>
</li>
<li>
<p>Generated Knowledge（生成知识）</p>
<ul>
<li>问题示例：探讨虚拟现实技术在教育领域的潜在应用。</li>
<li>使用原因：先生成相关知识有助于全面深入探讨应用。</li>
</ul>
<ul>
<li>技术特点：模型在处理问题时，先自主生成与问题相关的知识，如概念、特点、需求等，基于这些生成的知识再进行深入分析和回答。</li>
</ul>
<ul>
<li>prompt：探讨虚拟现实技术在教育领域的潜在应用。</li>
<li>生成知识过程：模型生成如虚拟现实技术特点（沉浸式体验等）、教育领域需求（提高学习兴趣等）相关知识。</li>
<li>回答可能：虚拟现实技术具有沉浸式体验特点，教育领域中可利用这一特点。历史课程中让学生身临其境地感受历史场景，提高学习兴趣和理解深度；在科学实验课程中模拟危险或难以操作的实验……</li>
</ul>
</li>
</ol>
<p><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A02-%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B%E6%8C%87%E5%8C%97/img/img6.jpg" alt="https://medium.com/the-modern-scientist/prompt-engineering-classification-of-techniques-and-prompt-tuning-6d4247b9b64c" loading="lazy"></p>
<h3 id="123-撰写优质提示词的原则">1.2.3 撰写优质提示词的原则</h3>
<p>在LLM应用中，优质prompt的编写是获取高质量输出的核心要素。优质prompt形式丰富多样、灵活多变，编写技术也各具特色。不过，遵循以下实用原则和框架，可显著提高获得理想结果的概率。本质上，编写prompt的过程类似于人与人之间的沟通，其核心在于清晰表达意图，使模型能够准确地理解并执行任务：</p>
<ol>
<li>
<p>指令明确：具体、清晰、结构化</p>
<ul>
<li>原则解释：向模型传达指令时，避免模糊不清和歧义，尽可能详细地说明任务内容、目标和要求，并且按照一定的逻辑结构组织语言，让模型能准确理解任务意图。</li>
<li>正面示例：请列举出5种中国传统节日，按照节日时间先后顺序排列，并简要说明每个节日的主要习俗。</li>
<li>反面示例：讲讲中国节日。（过于简略，没有明确的任务方向和要求，模型不知道需要提供哪些具体信息以及以何种形式呈现。）</li>
</ul>
</li>
<li>
<p>示例丰富：少样本学习，提供参考</p>
<ul>
<li>原则解释：通过给出少量与任务相关的具体示例，让模型更好地理解任务的模式和要求，从示例中学习如何生成合适的输出。</li>
<li>正面示例：请根据给定的水果名称，写出它们的颜色和口感。例如：苹果-红色、脆甜；香蕉-黄色、软糯。现在请描述草莓、葡萄和橙子。</li>
<li>反面示例：请描述水果的颜色和口感。（没有示例，模型可能对任务的具体形式和期望输出不太明确，可能导致回答不规范或不符合预期。）</li>
</ul>
</li>
<li>
<p>分隔清晰：逻辑分明，易于理解</p>
<ul>
<li>原则解释：在输入内容中，将不同的信息部分、任务要求等进行明确的分隔，使模型能清晰区分各个部分的作用，从而更准确地处理和回应。</li>
<li>正面示例：以下是一个故事创作任务。
<ul>
<li>背景设定：一个神秘的森林。</li>
<li>角色设定：勇敢的探险家、智慧的老精灵。</li>
<li>任务要求：编写一个500字左右的故事，讲述探险家和老精灵在森林中的冒险经历。</li>
</ul>
</li>
<li>反面示例：在一个神秘森林里有勇敢的探险家和智慧的老精灵编写500字左右讲述他们冒险经历的故事 （所有信息混在一起，没有明确分隔，模型较难快速梳理出关键信息和任务结构。）</li>
</ul>
</li>
<li>
<p>思维链提示：步步分解，引导模型</p>
<ul>
<li>原则解释：将复杂的任务分解成一系列逐步推进的步骤，向模型展示思考过程和逻辑链条，引导模型按照这个思路进行推理和生成答案。</li>
<li>正面示例：计算25乘以36再加上18除以6的结果。请按照以下步骤进行计算：
<ol>
<li>第一步，先计算25乘以36的积。</li>
<li>第二步，计算18除以6的商。</li>
<li>第三步，将第一步得到的积和第二步得到的商相加，得出最终结果。</li>
</ol>
</li>
<li>反面示例：计算25×36+18÷6。（没有给出思维步骤，模型可能直接计算，但对于一些需要详细推理过程的任务，这种简单指令不利于模型展示完整的思考过程。）</li>
</ul>
</li>
<li>
<p>格式规范：输出要求明确，注明来源</p>
<ul>
<li>原则解释：明确告知模型期望的输出格式，如列表、段落、表格等，并且如果需要引用外部信息，要注明信息来源的要求，以便模型生成符合格式要求且来源可追溯的内容。</li>
<li>正面示例：请列出2025年春节期间票房排名前五的电影，按照票房从高到低的顺序排列，输出格式为表格，包含电影名称、上映日期、票房数据（单位：亿元）。数据来源请注明为权威电影票房统计平台。</li>
<li>反面示例：说下2025年春节期间票房高的电影。（没有规定输出格式，也未提及数据来源要求，模型输出的内容可能格式混乱且无法验证数据可靠性。）</li>
</ul>
</li>
<li>
<p>激励探索：鼓励反思，提供容错</p>
<ul>
<li>原则解释：在指令中适当鼓励模型进行探索性思考，对生成的内容进行自我反思和优化。同时，对于模型可能出现的一些小错误或不完美的回答，给予一定的容错空间，引导其改进。</li>
<li>正面示例：请为即将到来的蛇年春节创作一条祝福短信，要富有创意和情感。如果觉得自己创作的内容不够好，可以尝试从不同角度重新构思，多思考一些独特的表达和意象。例如：人类的优点和缺点、人类对你的影响、你对人类的期望等。</li>
<li>反面示例：马上写一条蛇年春节祝福短信，不准写得太差，写不好重新写。（这种指令过于强硬，没有给模型积极的引导和探索空间，可能导致模型生成内容时受到限制，缺乏创新性。 ）</li>
</ul>
</li>
<li>
<p>借助LLM：利用其语言能力，生成适配的提示词</p>
<ul>
<li>原则解释：LLM本身具备强大的自然语言处理能力，可以先向其输入宽泛的主题或任务描述，让模型基于已有知识和理解，帮助生成或优化更加精准、有效的提示词。</li>
<li>正面示例：你希望了解中国传统文化中关于蛇年的独特寓意。先向LLM询问 “如何设计一个询问中国传统文化中蛇年寓意的优质提问？”，模型可能回复 “请从民俗、神话、传统艺术等方面，详细阐述中国传统文化中蛇年所蕴含的吉祥寓意和象征意义，列举具体事例说明”。然后你使用这个生成的提示词向模型提问，就能获取更全面深入的回答。</li>
<li>反面示例：直接向模型询问 “蛇年有什么寓意”。（这样简单的提问缺乏针对性和引导性，模型给出的回答可能比较笼统，无法满足对信息深度和广度的需求，而没有借助模型自身能力来优化提示词是导致这种情况的原因之一。）</li>
</ul>
</li>
<li>
<p>使用分隔符：通过分隔符明确提示词的结构，帮助模型精准处理。</p>
<ul>
<li>原则解释：分隔符可以帮助模型更好地理解和处理提示词的结构，确保各部分内容的清晰和独立，提高模型的响应质量。常见的分隔符包括“#”,“```”,“-”,“：”和“|”等，可以根据需求选择合适的分隔符。</li>
<li>正面示例：你希望模型帮助你撰写一篇关于人工智能在医疗领域应用的文章，可以使用分隔符明确各部分的内容，如：“# 引言 # 人工智能在医疗领域的应用背景和意义。## 主要应用 ## 诊断、治疗、药物研发等方面的应用实例。## 未来展望 ## 人工智能在医疗领域的未来发展趋势和挑战。” 这样模型可以更好地理解每个部分的要求，生成结构清晰的文章。</li>
<li>反面示例：直接向模型询问 “人工智能在医疗领域的应用”。（这样没有明确结构的提问，模型可能会给出一个较为笼统的回答，缺乏深度和条理性。）</li>
</ul>
</li>
<li>
<p>风格设置：通过设定文本风格、赋予模型特定角色，让生成内容契合特定需求与场景。</p>
<ul>
<li>原则解释：风格设置可改变生成文本的语言风格、情感基调，或让模型从特定角色角度，依据其特点、认知和口吻创作，提升内容针对性、独特性与吸引力。<strong>风格设置类似补充system角色功能，此时system角色通常较中性，但若已设system个性角色，风格设置可能失效、冲突或融合</strong>。</li>
<li>正面示例：当你想创作一篇鼓励职场新人勇敢迎接挑战的鸡汤文案时，你可以这样要求：“请以励志激昂的风格，扮演一位经验丰富的职场前辈，给刚踏入职场的新人写一段话，讲讲如何克服初入职场的困难与压力，激励他们积极进取 。”这样模型就会用充满力量的语言，以过来人的视角为职场新人提供鼓励与建议。</li>
<li>反面示例：简单地说“写一段鼓励职场新人的话”。（模型生成的内容可能缺乏感染力，无法精准针对职场新人的痛点和心态，难以达到有效激励的效果。）</li>
</ul>
</li>
</ol>
<p><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A02-%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B%E6%8C%87%E5%8C%97/img/img4.jpg" alt="https://blog.hubspot.com/marketing/write-ai-prompts" loading="lazy"></p>
<h3 id="124-提示词框架">1.2.4 提示词框架</h3>
<p>提示词技巧往往呈现零散状态，缺乏系统性。而提示词框架的出现，为其提供了系统的结构化方法。不同的框架适用于各异的场景与需求，能够帮助用户在与LLM交互时，高效地构造出有效的提示词。常见的提示词框架包括：RACE、CARE、APE、ROSES、COAST、CREATE、TAG、PAIN、RISE、CREO。</p>
<p><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A02-%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B%E6%8C%87%E5%8C%97/img/img3.gif" alt="https://medium.com/@slakhyani20/10-chatgpt-prompt-engineering-frameworks-you-need-to-know-41d4b76ed384" loading="lazy"></p>
<p>以TAG框架的运用为例，TAG是“任务（Task）、行动（Action）、目标（Goal）”的缩写。TAG通过结构化和逻辑化的方式拆解提问，确保问题精准清晰，进而引导AI给出更准确的回复。具体来说：</p>
<ul>
<li>任务（Task）：作为提问的开端，需明确自己想要解决的问题。含混不清的任务描述，往往是导致AI输出无效回答的主因。
<ul>
<li>错误示例：
<ul>
<li>“给我提供点建议”，错误在于任务模糊，AI无法判断具体需求。</li>
<li>“说下你们的看法”，没有指明看法针对的对象。</li>
</ul>
</li>
<li>正确示例：
<ul>
<li>“针对公司下个季度的销售策略，给我提供一些创新建议” 。</li>
<li>“就如何提升员工工作效率，谈谈你们的看法”。</li>
</ul>
</li>
</ul>
</li>
<li>行动（Action）：是提问的执行部分，需要清晰阐述期望AI完成的具体工作。
<ul>
<li>错误示例：
<ul>
<li>“介绍下健身知识”，问题太过宽泛，AI难以确定重点。</li>
<li>“做个海报”，没有说明海报的主题、风格和使用场景。</li>
</ul>
</li>
<li>正确示例：
<ul>
<li>“列举5种适合上班族的高效健身方法” 。</li>
<li>“制作一张宣传公司年会的海报，风格喜庆活泼，用于社交媒体宣传”。</li>
</ul>
</li>
</ul>
</li>
<li>目标（Goal）：是提问的落脚点，需明确最终期望获得的成果以及对输出内容的预期。
<ul>
<li>错误示例：
<ul>
<li>“给我做个有价值的方案”，没有界定“有价值”的标准，AI难以把握。</li>
<li>“分析下当前情况”，未说明分析的维度和程度。</li>
</ul>
</li>
<li>正确示例：
<ul>
<li>“制定一份详细的项目推广方案，包含预算和时间节点，用于公司内部会议讨论” 。</li>
<li>“分析当前市场竞争对手的优劣势，形成数据可视化报告，为公司战略调整提供依据”。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>基于TAG框架，以个人健康管理计划制定为例，最终提问的完整示例prompt如下：</p>
<pre><code>## 任务
改善睡眠质量。
## 行动
提供一份详细的睡眠改善计划。
## 目标
帮助我制定一个可行的睡眠计划，包括入睡前的准备、睡眠环境的调整以及改善睡眠习惯的建议，最终目标是每天能够深度睡眠7小时以上。
</code></pre>
<p>类似于TAG框架，其他提示词框架同样是一种结构化方式，若需了解各种提示词框架的具体使用方法，可自行搜索获取。提示词框架并非必须掌握的硬性技能，部分LLM应用或许已内置基础的提示词框架。不过，提示词框架能够显著提升与LLM交互的效率与效果。</p>
<h1 id="2-参考">2 参考</h1>
<ul>
<li><a href="https://blog.csdn.net/LuohenYJ/article/details/144858528" target="_blank" rel="noopener nofollow">大模型学习1-大语言模型基础知识</a></li>
<li><a href="https://platform.openai.com/docs/guides/prompt-engineering" target="_blank" rel="noopener nofollow">Prompt engineering</a></li>
<li><a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/prompt-engineering" target="_blank" rel="noopener nofollow">Prompt engineering techniques</a></li>
<li><a href="https://github.com/modelscope/modelscope-classroom" target="_blank" rel="noopener nofollow">modelscope-classroom</a></li>
</ul>

</div>
<div id="MySignature" role="contentinfo">
    <p>本文来自博客园，作者：<a href="https://www.cnblogs.com/luohenyueji/" target="_blank">落痕的寒假</a>，转载请注明原文链接：<a href="https://www.cnblogs.com/luohenyueji/p/18744026" target="_blank">https://www.cnblogs.com/luohenyueji/p/18744026</a></p>

<div style="text-align:center">
    <img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/wechat/content/%E5%8A%A0%E6%B2%B9%E9%B8%AD.gif">
</div>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.6610345081111111" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-02-28 20:26">2025-02-28 20:26</span>&nbsp;
<a href="https://www.cnblogs.com/luohenyueji">落痕的寒假</a>&nbsp;
阅读(<span id="post_view_count">57</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18744026" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18744026);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18744026', targetLink: 'https://www.cnblogs.com/luohenyueji/p/18744026', title: '[深度学习] 大模型学习2-提示词工程指北' })">举报</a>
</div>
        