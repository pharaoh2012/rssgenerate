
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/xiaoxi666/p/18695381" title="发布于 2025-01-30 21:09">
    <span role="heading" aria-level="2">近期最值得关注的AI技术报告与Agent综述！</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                    <div id="cnblogs_post_description" style="display: none">
        
        Deepseek这么火，来学学底层技术吧！
    </div>
<div id="cnblogs_post_body" class="blogpost-body blogpost-body-html">
<h1 id="activity-name" class="rich_media_title "><span data-mpa-action-id="m6jc3j3u175"><span data-mpa-emphasize-underline="t" data-mpa-action-id="m6jc3lkj14bt">写在前面</span></span></h1>
<div id="js_content" class="rich_media_content js_underline_content                       autoTypeSetting24psection            ">
<p><span data-mpa-action-id="m6jblxm41unr">如题，近期优秀的大模型层出不穷。作为技术人，需要阅读高质量的AI技术报告或论文，并且掌握未来应用趋势。本文将推荐一些高质量的AI技术报告，以及Agent智能体综述。</span></p>
<p><span>&nbsp;</span></p>
<h1><span data-mpa-action-id="m6jc3ox11bei"><span data-mpa-emphasize-underline="t" data-mpa-action-id="m6jc3r5lsoo"><span><span>大模型技术报告</span></span></span></span></h1>
<p><span data-mpa-action-id="m6jbylc7zs3"><span>DeepSeek-V3 Technical Report</span></span></p>
<p><span><span>作者：<span>DeepSeek</span></span></span></p>
<p><span><span>时间：<span>2024.12.27</span></span></span></p>
<p><span><span>内容提要：<span>主要介绍了DeepSeek-V3模型，这是一个拥有6710亿参数的专家混合（MoE）语言模型，其中每个token激活370亿参数。通过算法、框架和硬件的协同设计，该模型克服了跨节点MoE训练中的通信瓶颈，实现了近完全的计算-通信重叠，显著提高了训练效率并降低了训练成本。在仅花费266.4万H800 GPU小时的情况下，DeepSeek-V3完成了14.8万亿token的预训练，成为目前最强的开源基础模型。此外，该模型还引入了从DeepSeek-R1系列模型中提取推理能力的创新方法，并在知识、代码、数学和推理等多个基准测试中表现出色，性能与领先的闭源模型相当。</span></span></span></p>
<p><span><span>链接：<a href="https://arxiv.org/pdf/2412.19437" target="_blank" rel="noopener nofollow"><span>arxiv.org/pdf/2412.19437</span></a></span></span></p>
<p><span>&nbsp;</span></p>
<p><span data-mpa-action-id="m6jbyie07qf"><span>DeepSeek_R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</span></span></p>
<p><span><span>作者：<span>DeepSeek</span></span></span></p>
<p><span><span>时间：<span>2025.01.23</span></span></span></p>
<p><span><span>内容提要：<span>R1是近期火爆全网的深度求索模型。文中介绍了DeepSeek-AI团队通过强化学习（RL）开发的第一代推理模型DeepSeek-R1-Zero和DeepSeek-R1，其中DeepSeek-R1-Zero通过纯RL训练展示了强大的推理能力但存在可读性问题，而DeepSeek-R1通过引入冷启动数据和多阶段训练进一步提升了推理性能，达到了与OpenAI-o1-1217相当的水平；文章还展示了通过蒸馏技术将推理能力迁移到小模型上的成功实践，显著提升了小模型的推理表现，并开源了多个模型供研究社区使用，同时探讨了蒸馏与RL的优劣，指出未来研究方向包括提升通用能力、解决语言混合问题及优化软件工程任务性能。</span></span></span></p>
<p><span><span>链接：<a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf" target="_blank" rel="noopener nofollow"><span>github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf</span></a></span></span></p>
<p><span>&nbsp;</span></p>
<p><span data-mpa-action-id="m6jbyf1c23vo"><span>DeepSeek MoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models</span></span></p>
<p><span><span>作者：<span>DeepSeek</span></span></span></p>
<p><span><span>时间：<span>2024.01.11</span></span></span></p>
<p><span><span>内容提要：<span>详细介绍了DeepSeek-MoE模型的设计，提出细粒度专家分割（Fine-grained Expert Segmentation）和共享专家隔离（Shared Expert Isolation）技术，解决传统MoE模型专家冗余和泛化性不足的问题。 仅用13B激活参数量达到与Llama2 70B相当的性能，训练成本降低80%。</span></span></span></p>
<p><span><span>链接：<a href="https://arxiv.org/pdf/2401.06066" target="_blank" rel="noopener nofollow"><span>arxiv.org/pdf/2401.06066</span></a></span></span></p>
<p><span>&nbsp;</span></p>
<p><span data-mpa-action-id="m6jbya9b1eh7"><span>Kimi k1.5</span></span></p>
<p><span><span>作者：<span>Moonshot</span></span></span></p>
<p><span><span>时间：<span>2025.01.22</span></span></span></p>
<p><span><span>内容提要：<span>Kimi一如既往认为长文本是核心。其中，Kimi k1.5 是一个通过强化学习（RL）训练的多模态大型语言模型（LLM）。Kimi k1.5通过扩展上下文窗口和改进的策略优化方法，在多个基准测试中达到了最先进的推理性能，与OpenAI的o1模型相当。此外，文章还提出了long2short方法，通过长链推理（CoT）技术提升短链推理模型的性能，取得了显著的性能提升。这些方法不仅提高了模型的推理能力，还增强了其在多模态任务中的表现。</span></span></span></p>
<p><span><span>链接：<a href="https://github.com/MoonshotAI/Kimi-k1.5/blob/main/Kimi_k1.5.pdf" target="_blank" rel="noopener nofollow"><span>github.com/MoonshotAI/Kimi-k1.5/blob/main/Kimi_k1.5.pdf</span></a></span></span></p>
<p><span>&nbsp;</span></p>
<p><span data-mpa-action-id="m6jby6ka1tcw"><span>Extending Context Window of Large Language Models via Semantic Compression</span></span></p>
<p><span><span>作者：<span>Department of Mathematical Sciences, Tsinghua University, Theory Lab, 2012 Labs, Huawei Technologies</span></span></span></p>
<p><span><span>时间：<span>2023.12.15</span></span></span></p>
<p><span><span>内容提要：<span>这篇文章提出了一种新颖的语义压缩方法，用于扩展大型语言模型（LLMs）的上下文窗口，使其能够处理比原始模型长6-8倍的文本，而无需对预训练模型进行微调或增加计算成本。该方法通过利用信息论中的源编码概念，使用预训练模型在将输入传递给LLMs之前减少长输入的语义冗余。实验结果表明，这种方法在包括问答、总结、少样本学习和信息检索等多种任务中有效地扩展了LLMs的上下文窗口，并在保持文本生成流畅性的同时减少了计算开销。</span></span></span></p>
<p><span><span>链接：<a href="https://arxiv.org/pdf/2312.09571" target="_blank" rel="noopener nofollow"><span>arxiv.org/pdf/2312.09571</span></a></span></span></p>
<p>&nbsp;</p>
<p>Reinforcement Learning: An Overview</p>
<p>作者：Kevin P. Murphy</p>
<p>时间：2024.12.09</p>
<p>内容提要：强化学习（Reinforcement Learning, RL）的综述性文章，全面介绍了RL的基本概念、方法、理论基础以及多种扩展应用。文章从序贯决策制定问题的定义出发，详细讨论了部分可观测马尔可夫决策过程（POMDPs）、马尔可夫决策过程（MDPs）、上下文MDPs、上下文bandits等不同类型的模型，并介绍了值函数、策略、模型等不同类型的RL方法。文章还探讨了探索与利用的权衡问题、RL作为后验推断问题的视角、分布强化学习、奖励函数设计、层次强化学习、模仿学习、离线强化学习等重要主题，并讨论了这些方法在实际应用中的挑战和解决方案。此外，文章还涉及了如何利用大型语言模型（LLMs）与RL结合的最新进展，以及RL在实现通用人工智能（AGI）中的潜在作用。</p>
<p>链接：<a href="https://arxiv.org/pdf/2412.05265" target="_blank" rel="noopener nofollow">https://arxiv.org/pdf/2412.05265</a></p>
<p><span>&nbsp;</span></p>
<h1><span data-mpa-action-id="m6jc3xtwfv1"><span data-mpa-emphasize-underline="t" data-mpa-action-id="m6jc40ip4fk"><span><span>Agent综述</span></span></span></span></h1>
<p><span data-mpa-action-id="m6jbxyum8s"><span>Agent AI: Surveying the Horizons of Multimodal Interaction</span></span></p>
<p><span><span>作者：<span>斯坦福大学李飞飞团队</span></span></span></p>
<p><span><span>时间：<span>2024.01.25</span></span></span></p>
<p><span>内容提要：这篇80页的综述系统性地总结了多模态AI智能体的发展，探讨了其在具身交互、跨现实任务中的应用，以及如何结合大语言模型（LLM）和视觉语言模型（VLM）构建更复杂的智能体系统。论文还提出了“无限代理”概念，支持跨物理和虚拟环境的多模态生成与编辑。</span></p>
<p><span><span>链接：<a href="https://arxiv.org/pdf/2401.03568" target="_blank" rel="noopener nofollow"><span>arxiv.org/pdf/2401.03568</span></a></span></span></p>
<p><span>&nbsp;</span></p>
<p><span data-mpa-action-id="m6jby2q61vq7"><span>Google Whiterpaper Agents2</span></span></p>
<p><span><span>作者：<span>Google</span></span></span></p>
<p><span><span>时间：<span>2024.09</span></span></span></p>
<p><span><span>内容提要：<span>Google 出品的 Agents白皮书。详细介绍了AI代理的核心架构，包括模型层（Model Layer）、工具层（Tool Layer） 和 编排层（Orchestration Layer），并探讨了其与传统语言模型的区别、学习能力、实际应用以及未来发展，旨在推动AI代理在各领域的广泛应用。</span></span></span></p>
<p><span><span>链接：<a href="https://drive.google.com/file/d/1oEjiRCTbd54aSdB_eEe3UShxLBWK9xkt/view" target="_blank" rel="noopener nofollow"><span>drive.google.com/file/d/1oEjiRCTbd54aSdB_eEe3UShxLBWK9xkt/view</span></a></span></span></p>
<p><span><span>参考实现：<a href="https://github.com/alibaba/spring-ai-alibaba/" target="_blank" rel="noopener nofollow"><span>github.com/alibaba/spring-ai-alibaba/</span></a></span></span></p>
</div>
</div>
<div id="MySignature" role="contentinfo">
    『注:本文来自博客园“小溪的博客”，若非声明均为原创内容，请勿用于商业用途，转载请注明出处http://www.cnblogs.com/xiaoxi666/』
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.4719413528310185" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-02-01 11:06">2025-01-30 21:09</span>&nbsp;
<a href="https://www.cnblogs.com/xiaoxi666">xiaoxi666</a>&nbsp;
阅读(<span id="post_view_count">158</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18695381" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18695381);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18695381', targetLink: 'https://www.cnblogs.com/xiaoxi666/p/18695381', title: '近期最值得关注的AI技术报告与Agent综述！' })">举报</a>
</div>
        