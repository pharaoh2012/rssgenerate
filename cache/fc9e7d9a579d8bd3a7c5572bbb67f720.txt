
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/siwuliu-916/p/19000499" title="发布于 2025-07-23 12:27">
    <span role="heading" aria-level="2">如何本地部署Deepseek大模型</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h1 id="本地部署deepseek大模型">本地部署Deepseek大模型</h1>
<h2 id="1下载ollama">1、下载Ollama</h2>
<p>去ollama官网https://ollama.com/ 下载可执行程序，可选macos、linux和Windows版本下载</p>
<p><img src="https://img2024.cnblogs.com/blog/2990335/202507/2990335-20250723122110078-841485440.png" alt="image-20250716152024626" loading="lazy"></p>
<p>下载之后如果点击直接安装(<code>install</code>)默认会安装在<code>C</code>盘.</p>
<p><img src="https://img2024.cnblogs.com/blog/2990335/202507/2990335-20250723122223589-439460768.png" alt="Snipaste_2025-07-16_14-47-36" loading="lazy"></p>
<p>在可执行程序目录级下打开终端窗口执行（这个是更改安装路径）</p>
<pre><code>OllamaSetup.exe /DIR=D:\Ollama

##OllamaSetup.exe: 这是一个安装程序的执行文件，通常用于安装 Ollama 软件。
##/DIR=D:\Ollama: 这是命令行参数，告诉安装程序将 Ollama 安装到 D 盘的 Ollama 文件夹中。如果指定的目录不存在，安装程序通常会创建该文件夹
</code></pre>
<p>安装成功显示</p>
<p><img src="https://img2024.cnblogs.com/blog/2990335/202507/2990335-20250723122301096-1317352290.png" alt="Snipaste_2025-07-16_14-51-44" loading="lazy"></p>
<h2 id="2大模型下载和卸载">2、大模型下载和卸载</h2>
<p>同样是在<code>Ollama</code>官网上选择<code>Models</code></p>
<p><img src="https://img2024.cnblogs.com/blog/2990335/202507/2990335-20250723122327773-697269521.png" alt="image-20250716152635593" loading="lazy"></p>
<p>点击对应的Deepseek-r1可以看到对应版本的模型</p>
<p><img src="https://img2024.cnblogs.com/blog/2990335/202507/2990335-20250723122343742-1600554750.png" alt="image-20250716152733695" loading="lazy"></p>
<p>点击对应 模型，即可展示对应的部署命令</p>
<p><img src="https://img2024.cnblogs.com/blog/2990335/202507/2990335-20250723122402151-1409012856.png" alt="image-20250716154147963" loading="lazy"></p>
<table>
<thead>
<tr>
<th><strong>模型参数规模</strong></th>
<th><strong>典型用途</strong></th>
<th><strong>CPU 建议</strong></th>
<th><strong>GPU 建议</strong></th>
<th><strong>内存建议 (RAM)</strong></th>
<th><strong>磁盘空间建议</strong></th>
<th><strong>适用场景</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>1.5b (15亿)</strong></td>
<td>小型推理、轻量级任务</td>
<td>4核以上 (Intel i5 / AMD Ryzen 5)</td>
<td>可选，入门级 GPU (如 NVIDIA GTX 1650, 4GB 显存)</td>
<td>8GB</td>
<td>10GB 以上 SSD</td>
<td>小型 NLP 任务、文本生成、简单分类</td>
</tr>
<tr>
<td><strong>7b (70亿)</strong></td>
<td>中等推理、通用任务</td>
<td>6核以上 (Intel i7 / AMD Ryzen 7)</td>
<td>中端 GPU (如 NVIDIA RTX 3060, 12GB 显存)</td>
<td>16GB</td>
<td>20GB 以上 SSD</td>
<td>中等规模 NLP、对话系统、文本分析</td>
</tr>
<tr>
<td><strong>14b (140亿)</strong></td>
<td>中大型推理、复杂任务</td>
<td>8核以上 (Intel i9 / AMD Ryzen 9)</td>
<td>高端 GPU (如 NVIDIA RTX 3090, 24GB 显存)</td>
<td>32GB</td>
<td>50GB 以上 SSD</td>
<td>复杂 NLP、多轮对话、知识问答</td>
</tr>
<tr>
<td><strong>32b (320亿)</strong></td>
<td>大型推理、高性能任务</td>
<td>12核以上 (Intel Xeon / AMD Threadripper)</td>
<td>高性能 GPU (如 NVIDIA A100, 40GB 显存)</td>
<td>64GB</td>
<td>100GB 以上 SSD</td>
<td>大规模 NLP、多模态任务、研究用途</td>
</tr>
<tr>
<td><strong>70b (700亿)</strong></td>
<td>超大规模推理、研究任务</td>
<td>16核以上 (服务器级 CPU)</td>
<td>多 GPU 并行 (如 2x NVIDIA A100, 80GB 显存)</td>
<td>128GB</td>
<td>200GB 以上 SSD</td>
<td>超大规模模型、研究、企业级应用</td>
</tr>
<tr>
<td><strong>671b (6710亿)</strong></td>
<td>超大规模训练、企业级任务</td>
<td>服务器级 CPU (如 AMD EPYC / Intel Xeon)</td>
<td>多 GPU 集群 (如 8x NVIDIA A100, 320GB 显存)</td>
<td>256GB 或更高</td>
<td>1TB 以上 NVMe SSD</td>
<td>超大规模训练、企业级 AI 平台</td>
</tr>
</tbody>
</table>
<p><strong>总结：配置越高，可部署的模型模型参数规模越大（通俗点讲就是硬件性能越好，问的问题可以更加复杂，回答的越精准）</strong></p>
<p>在cmd命令行下复制前面的命令即可进行模型下载，如果下载速度慢，<code>Ctrl + C</code>，终止进程再次进行安装。</p>
<p><img src="https://img2024.cnblogs.com/blog/2990335/202507/2990335-20250723122559077-1813535106.png" alt="Snipaste_2025-07-16_15-04-29" loading="lazy"></p>
<p>下载完之后就可以，进行模型使用了。</p>
<p>关于Ollama的使用，需要知道以下命令即可</p>
<pre><code class="language-ollama">## 安装模型/启动模型（后面就是模型名称）
ollama run deepseek-r1:7b

## 卸载模型
ollama rm deepseek-r1:7b

## 查看模型
ollama list
</code></pre>
<h2 id="拓展">拓展</h2>
<p>如果不喜欢上述命令行的提问方式，可以下载Chatbox AI 可视化工具https://chatboxai.app/zh</p>
<p>登录进来之后，软件会提示使用什么AI模型，这里选择<code>使用自己的API Key 或本地模型</code> ，然后选择<code>Ollama</code>，点击<code>获取</code>即可得到本地部署模型</p>
<p><img src="https://img2024.cnblogs.com/blog/2990335/202507/2990335-20250723122420722-1131463561.png" alt="Snipaste_2025-07-16_15-08-14" loading="lazy"></p>
<p>最终效果：</p>
<p><img src="https://img2024.cnblogs.com/blog/2990335/202507/2990335-20250723122504975-1037487635.png" alt="image-20250716154805120" loading="lazy"></p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0" data-date-updated="2025-07-23 12:28">2025-07-23 12:27</span>&nbsp;
<a href="https://www.cnblogs.com/siwuliu-916">肆伍陸</a>&nbsp;
阅读(<span id="post_view_count">0</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(19000499);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '19000499', targetLink: 'https://www.cnblogs.com/siwuliu-916/p/19000499', title: '如何本地部署Deepseek大模型' })">举报</a>
</div>
        