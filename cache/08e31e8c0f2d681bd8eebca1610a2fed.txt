
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/xueweihan/p/18629166" title="发布于 2024-12-25 08:24">
    <span role="heading" aria-level="2">跟着 8.6k Star 的开源数据库，搞 RAG！</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p><img src="https://img2024.cnblogs.com/blog/759200/202412/759200-20241224182032108-1405558941.png" alt="" loading="lazy"></p>
<p>过去 9 年里，HelloGitHub 月刊累计收录了 3000 多个开源项目。然而，随着项目数量的增加，不少用户反馈：“搜索功能不好用，找不到想要的项目！” 这让我意识到，仅仅收录项目是不够的，还需要通过更智能的方式，帮助用户找到心仪的开源项目。于是，<strong>我开始探索如何通过 RAG 技术解决这个问题</strong>。</p>
<blockquote>
<p>检索增强生成(RAG)，是赋予生成式人工智能模型信息检索能力的技术。</p>
</blockquote>
<p>RAG 技术我早有耳闻，但却一直不知道该从哪里入手。虽然现在有不少容易上手的 RAG 低代码平台，但我不想只停留在“会用”的层面，更希望了解它的实现细节，否则不敢在生产环境中用。不过，要让我直接用 LangChain 和 Ollama 从零搭建一个 RAG 系统，还真有点心里没底。</p>
<p><img src="https://img2024.cnblogs.com/blog/759200/202412/759200-20241224182056625-1187198548.jpg" alt="" loading="lazy"></p>
<p>还好最近 OceanBase 搞事情，在 4.3.3 版本里支持了向量检索功能，更贴心的是，还专门为像我这样对 RAG 感兴趣的新手，准备了一个用 Python 搭建 RAG 聊天机器人的<a href="https://mp.weixin.qq.com/s/PY-pKi2p6BnV6xhxYMT8Lg" target="_blank" rel="noopener nofollow">实战教程</a>。</p>
<p><img src="https://img2024.cnblogs.com/blog/759200/202412/759200-20241224182112795-25003395.jpg" alt="" loading="lazy"></p>
<blockquote>
<p>GitHub 地址：<a href="https://github.com/oceanbase/oceanbase" target="_blank" rel="noopener nofollow">github.com/oceanbase/oceanbase</a></p>
</blockquote>
<p>光看永远只是纸上谈兵，所以我干脆上手把玩了一番。</p>
<p><img src="https://img2024.cnblogs.com/blog/759200/202412/759200-20241224182220313-1757087551.gif" alt="" loading="lazy"></p>
<p>接下来，我将分享如何基于该项目，打造一款 HelloGitHub 开源社区的聊天机器人，内容包括实现过程、细节优化，以及对 RAG 技术的理解与未来展望。</p>
<h2 id="一介绍">一、介绍</h2>
<p>OceanBase 开源的 RAG 聊天机器人，能够通过自然对话更精准地回答与 OceanBase 文档相关的问题。</p>
<p><img src="https://img2024.cnblogs.com/blog/759200/202412/759200-20241224182120329-1637623729.png" alt="" loading="lazy"></p>
<p>该项目是基于 langchain、langchain-oceanbase 和 streamlit 构建，处理流程是先将 OceanBase 数据库的文档，通过 Embedding 模型转化为向量数据，并存储在 OceanBase 数据库中。当用户提问时，系统会用相同的模型将问题转化为向量，然后通过向量检索找到相关的文档内容，再将这些文档作为上下文提交给大语言模型，从而生成更精准的回答。</p>
<blockquote>
<p>在线体验：<a href="https://www.oceanbase.com/obi" target="_blank" rel="noopener nofollow">oceanbase.com/obi</a></p>
</blockquote>
<p>体验后，我感觉效果还不错，于是就萌生了一个想法：<strong>能不能把 OceanBase 的文档，换成 HelloGitHub 月刊的 Markdown 文件，灌进系统里，这样不就摇身一变，成为 HelloGitHub 专属的聊天机器人了吗？</strong> 说干就干！</p>
<h2 id="二安装运行">二、安装运行</h2>
<p>在开始改造之前，首先需要把项目跑起来。安装运行的步骤在 OceanBase 提供的<a href="https://mp.weixin.qq.com/s/PY-pKi2p6BnV6xhxYMT8Lg" target="_blank" rel="noopener nofollow">实战教程</a>中已经很详细了，这里不再过多介绍。运行步骤如下：</p>
<ol>
<li>执行 <code>embed_docs.py</code> 脚本，将文档内容向量化后存储到 OB</li>
<li>启动项目 <code>streamlit run --server.runOnSave false chat_ui.py</code></li>
</ol>
<p>启动成功将自动跳转至此界面：</p>
<p><img src="https://img2024.cnblogs.com/blog/759200/202412/759200-20241224182123954-1720464344.png" alt="" loading="lazy"></p>
<p><strong>建议</strong>：</p>
<ol>
<li>Python 版本管理：运行需要 Python 3.9+，建议使用 <code>pyenv</code> 管理项目的 Python 版本。</li>
<li>查看数据库：不论是通过 Docker 部署 OceanBase 还是使用 OB Cloud，都建议在本地通过 GUI 工具查看数据库，有助于开发和调试。</li>
</ol>
<p><img src="https://img2024.cnblogs.com/blog/759200/202412/759200-20241224182127558-1399553472.png" alt="" loading="lazy"></p>
<p>运行 <code>embed_docs.py</code> 脚本后，查看数据库中的表，你会发现这些字段：</p>
<ul>
<li><code>document</code>：存储原始的文档内容</li>
<li><code>embedding</code>：存储文档向量化后的数据</li>
<li><code>metadata</code>：记录文档的名称、路径以及切分后的标题等信息</li>
</ul>
<p>其中 <code>embedding</code> 列是一个类似数组形式的数据，这个就是通过 Embedding 模型将文档片段转化为向量数据的结果。这些向量数据能够捕捉文本的语义信息，使计算机能够更好地理解文本的含义，从而实现类似语义搜索的功能（计算距离），为后续问题与文档内容的匹配提供基础。</p>
<p><img src="https://img2024.cnblogs.com/blog/759200/202412/759200-20241224190524987-1578844440.png" alt="" loading="lazy"></p>
<h2 id="三动手改造">三、动手改造</h2>
<p>这个项目除了支持 LLMs API，还可以切换为本地的 Ollama API 使用，只需修改 <code>.env</code> 配置文件即可完成调整：</p>
<pre><code># 使用支持 embed API 的模型
OLLAMA_URL=localhost:11434/api/embed
OLLAMA_TOKEN=
OLLAMA_MODEL=all-minilm
</code></pre>
<p><strong>注意</strong>：在调用第三方付费 API 时，一定要注意使用量，建议仅导入部分文档用于测试，或用本地 LLM 调试逻辑，避免不必要的花费。</p>
<h3 id="31-导入-hellogithub-月刊">3.1 导入 HelloGitHub 月刊</h3>
<p>通过 <code>embed_docs.py</code> 脚本，将 HelloGitHub 月刊内容向量化并导入到 OceanBase 数据库，命令如下：</p>
<pre><code>python embed_docs.py --doc_base /HelloGitHub/content --table_name hg
</code></pre>
<p><strong>参数说明：</strong></p>
<ul>
<li><code>doc_base</code>：HelloGitHub 内容目录</li>
<li><code>table_name</code>：脚本会自动创建该表，并将数据存储到表中。</li>
</ul>
<p>但是运行后，我查看数据库时发现 <code>document</code> 字段中包含了许多无意义的内容，例如格式符号或无关信息：</p>
<p><img src="https://img2024.cnblogs.com/blog/759200/202412/759200-20241224182138249-1034851785.png" alt="" loading="lazy"></p>
<p>面对这些噪声数据，我编写了一个脚本，清洗 HelloGitHub 月刊文件中无关的格式符号和冗余内容，并重新导入数据库。</p>
<p><img src="https://img2024.cnblogs.com/blog/759200/202412/759200-20241224182141789-690533084.png" alt="" loading="lazy"></p>
<h3 id="32-启动服务">3.2 启动服务</h3>
<p>在启动服务时，需要通过环境变量 <code>TABLE_NAME</code> 指定要使用的表。命令如下：</p>
<pre><code>TABLE_NAME=hg2 streamlit run --server.runOnSave false chat_ui.py
</code></pre>
<p>我试了一下，回答的效果并不理想：</p>
<p><img src="https://img2024.cnblogs.com/blog/759200/202412/759200-20241224182145514-15064855.png" alt="" loading="lazy"></p>
<p>经过测试，我分析问答效果欠佳的原因，可能包括以下几点：</p>
<ol>
<li>向量化效果：所选用的模型 all-minilm 仅有 384 维度，可以尝试更大的 Embedding 模型；</li>
<li>数据清理：虽然清理了一部分无用内容，但可能还有一些噪声数据未处理完全；</li>
<li>文档完整性：HelloGitHub 的内容结构是否适合问答模型需要进一步分析；</li>
<li>提示词：需要完善提示词设计，补充更多上下文；</li>
</ol>
<h2 id="四优化问答效果">四、优化问答效果</h2>
<p>我开始对 RAG 有些感觉了，所以准备切换到付费但效果更好的通义千问 <code>text-embedding-v3</code> 模型（1024 维度），进行调试。</p>
<h3 id="41-数据优化">4.1 数据优化</h3>
<p>为提升问答效果，我决定进一步优化 document 的构造方式。具体思路是：<strong>将 HelloGitHub 网站中的表导入至 OceanBase 数据库，并基于这些表的数据，构建更干净和精准的内容</strong>。这样可以最大程度地确保项目数据的全面性，同时减少无关内容的干扰，提升向量检索相关性。</p>
<p><strong>导入表到 OceanBase</strong></p>
<p>OceanBase 和 MySQL 高度兼容，因此，我直接用 Navicat 将 HelloGitHub 的数据表结构和内容，从 MySQL 无缝迁移到了 OceanBase。然后我写了一个 <code>embed_sql.py</code> 脚本，通过直接查询相关表的数据，进而生成更精简的内容（document），同时补充元数据（metadata），并存储到数据库。核心代码如下：</p>
<pre><code class="language-python"># 构建内容（document）
content = f"""{row.get('name', '未知')}：{row.get('title', '未知标题')}。{row.get('summary', '暂无概要')}"""

# 构建元数据（metadata）
metadata = {
    "repository_name": row.get("name", "N/A"),  # 仓库名称
    "repository_url": row.get("url", "N/A"),  # 仓库链接
    "description": row.get("summary", "N/A"),  # 项目描述
    "category_name": row.get("category_name", "N/A"),  # 类别名称
    "language": row.get("primary_lang", "N/A"),  # 主要编程语言
    "chunk_title": row.get("name", "N/A"), 
    "enhanced_title": f'内容 -&gt; {row.get("category_name", "N/A")} -&gt; {row.get("name", "N/A")}'
    ...
}

# 将内容和元数据添加到文档对象
docs.append(Document(page_content=content.strip(), metadata=metadata))
# 存储到数据库
vs.add_documents(
    docs,
    ids=[str(uuid.uuid4()) for _ in range(len(docs))],
)
</code></pre>
<p>经过多轮调试和对比，我发现 <strong>document 数据越精简，向量检索效果越好</strong>，随后将完整的数据集存入 OceanBase 数据库的 <code>hg5</code> 表。</p>
<pre><code>python embed_sql.py --table_name hg5 --limit=4000              
args Namespace(table_name='hg5', batch_size=4, limit=4000, echo=False)
Using RemoteOpenAI
Processing: 100%|███████████████████████████████████████████████████████████████████████▉| 3356/3357 [09:33&lt;00:00,  5.85row/s]
</code></pre>
<p><img src="https://img2024.cnblogs.com/blog/759200/202412/759200-20241224182152536-88448186.png" alt="" loading="lazy"></p>
<p>至此，基于数据库表构造的 document 数据，已经非常干净了。</p>
<h3 id="42-提示词优化">4.2 提示词优化</h3>
<p>在优化完数据后，我开始思考如何优化提示词，并对 LLM 的回答进行引导和强化。以下是针对 LLM 提示词优化的方向：</p>
<ol>
<li>明确背景和任务：在提示词中设定问答的背景并限制问题的范围，例如，确保问题只涉及开源项目或 HelloGitHub 的内容。</li>
<li>丰富上下文：将 metadata（元数据） 和 document（项目描述）同时提供给大模型，让 LLM 有更多上下文来生成精确回答。</li>
<li>高质量示例：提供高质量的回答示例，统一输出格式。</li>
<li>约束逻辑：明确要求 LLM 不得虚构答案。如无法回答问题，需清楚指出知识盲点，并合理提供方向性建议。</li>
</ol>
<h3 id="43-处理流程优化">4.3 处理流程优化</h3>
<p>在优化向量检索和回答的流程方面，我做了以下改进：</p>
<ol>
<li>扩大检索范围：向量检索默认只返回前 10 条最高相似度的内容。我将其扩展至 20 条，为 LLM 提供更多上下文选择。</li>
<li>判断相关性：使用提示词指导 LLM 在输出答案前，先判断问题是否与 HelloGitHub 或开源项目相关，避免生成无关回答。</li>
<li>提炼回答：基于用户输入分析意图后，选出最相关的 5 个项目，并结合元数据生成更贴合用户需求的回答。</li>
</ol>
<h3 id="44-效果展示">4.4 效果展示</h3>
<p>除了上面的优化，我还进一步简化了页面、删除用不到的代码，最终呈现效果如下：</p>
<p><img src="https://img2024.cnblogs.com/blog/759200/202412/759200-20241224182156160-462927108.png" alt="" loading="lazy"></p>
<p>回答效果对比：</p>
<p><img src="https://img2024.cnblogs.com/blog/759200/202412/759200-20241224182200005-1730055512.png" alt="" loading="lazy"></p>
<p>通过切换至通义千问 text-embedding-v3 模型，同时优化数据、提示词策略和问答流程，让这套 RAG 系统的回答质量有了明显提升，我打算自己盘一盘再上线。所以先放出源码，感兴趣的小伙伴可以作为参考：</p>
<blockquote>
<p>GitHub 地址：<a href="https://github.com/521xueweihan/ai-workshop-2024" target="_blank" rel="noopener nofollow">github.com/521xueweihan/ai-workshop-2024</a></p>
</blockquote>
<h2 id="五最后">五、最后</h2>
<p>在构建 HelloGitHub 的 RAG 聊天机器人过程中，回答效果一直不好，让我一度产生了放弃的念头。但当我通过查询表里的数据构造 document，并使用维度更大的 Embeding 模型后，回答效果直线提升，才让我重新看到了希望。</p>
<p><img src="https://img2024.cnblogs.com/blog/759200/202412/759200-20241224182203484-1823778261.jpg" alt="" loading="lazy"></p>
<p>这段经历也让我开始认真思考：优化 RAG 的关键是什么？我的答案是 <strong>数据+检索</strong>。如今，许多企业希望借助 AI 技术赋能已有服务，RAG 则是一种门槛较低的通用解决方案。在这一过程中，<strong>数据质量决定了基础，高质量数据往往是从海量数据提纯而来。检索则是确保内容能够被快速且准确提取的关键</strong>。否则不管提示词再怎么优化，也无法检索到有价值的内容，就无法实现增强的效果。</p>
<p>另外，我认为在未来的 RAG 应用中，除了向量数据，数据库还需要具备一些关键能力来确保检索和生成的高效性。例如，支持关系型数据和向量数据的混合搜索，不仅能处理结构化和非结构化数据，还能有效减少 RAG 模型中的“幻觉”问题，从而让生成的答案更准确、更有根据。图搜索（知识图谱）同样很重要，它为 RAG 提供复杂推理所需的背景信息，提升生成质量。此外，RAG 应用在许多场景中需要频繁更新和同步数据，因此数据库还需支持实时查询、低延迟响应、事务处理和高可用性，这些是确保 RAG 高效运行的基础。</p>
<p><strong>OceanBase 的分布式架构优势，让它在面对海量数据时依然游刃有余</strong>。而新引入的向量存储和检索能力，使得我们能够通过 SQL 轻松获取最“干净”的数据，并在同一个数据库内完成向量化操作。OceanBase 未来可期！</p>
<blockquote>
<p>GitHub 地址：<a href="https://github.com/oceanbase/oceanbase" target="_blank" rel="noopener nofollow">github.com/oceanbase/oceanbase</a></p>
</blockquote>
<p>虽然 RAG 技术目前还不像 Web 开发那么成熟，但作为一个潜力巨大的技术方向，值得我们持续关注和学习。作为一名刚刚踏上 RAG 探索之旅的小白，我还有很多需要学习的地方。如果你也对 RAG 充满兴趣，欢迎结伴而行、共同成长！</p>

</div>
<div id="MySignature" role="contentinfo">
    <div>    
    <p style="border-top: #e0e0e0 1px dashed; border-right: #e0e0e0 1px dashed; border-bottom: #e0e0e0 1px dashed; border-left: #e0e0e0 1px dashed; padding-top: 5px; padding-right: 10px; padding-bottom: 10px; padding-left: 150px; background: url(https://images.cnblogs.com/cnblogs_com/xueweihan/859919/o_200924043112qrcode_for_gh_4fb030b35bb4_258.jpg) #e5f1f4 no-repeat 1% 50%; background-size:130px 130px;font-family: 微软雅黑; font-size: 13px" id="PSignature">
    <br>
    作者：<a href="https://github.com/521xueweihan" target="_blank">削微寒</a>

    <br>
    <strong>扫描左侧的二维码可以联系到我</strong>
    <br>

    <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh"><img alt="知识共享许可协议" style="border-width: 0" src="https://licensebuttons.net/l/by-nc-nd/4.0/88x31.png"></a><br>本作品采用<a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh">署名-非商业性使用-禁止演绎 4.0 国际 </a>进行许可。
    </p>
</div>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.3289903479224537" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2024-12-25 08:25">2024-12-25 08:24</span>&nbsp;
<a href="https://www.cnblogs.com/xueweihan">削微寒</a>&nbsp;
阅读(<span id="post_view_count">926</span>)&nbsp;
评论(<span id="post_comment_count">4</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18629166" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18629166);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18629166', targetLink: 'https://www.cnblogs.com/xueweihan/p/18629166', title: '跟着 8.6k Star 的开源数据库，搞 RAG！' })">举报</a>
</div>
        