
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/vipstone/p/18705268" title="发布于 2025-02-08 19:44">
    <span role="heading" aria-level="2">1分钟学会DeepSeek本地部署，小白也能搞定！</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p>DeepSeek 是国内顶尖 AI 团队「深度求索」开发的多模态大模型，具备数学推理、代码生成等深度能力，堪称"AI界的六边形战士"。</p>
<p>DeepSeek 身上的标签有很多，其中最具代表性的标签有以下两个：</p>
<ol>
<li><strong>低成本</strong>（不挑硬件、开源）</li>
<li><strong>高性能</strong>（推理能力极强、回答准确）</li>
</ol>
<h2 id="一为什么要部署本地deepseek">一、为什么要部署本地DeepSeek？</h2>
<p>相信大家在使用 DeepSeek 时都会遇到这样的问题：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2025/png/92791/1739008193900-60d42b80-8d95-4d2e-8026-dc3d84380036.png" alt="" loading="lazy"></p>
<p>这是由于 DeepSeek 大火之后访问量比较大，再加上漂亮国大规模、持续的恶意攻击，导致 DeepSeek 的服务器很不稳定。所以，这个此时在本地部署一个 DeepSeek 大模型就非常有必要了。</p>
<p>再者说，有些数据比较敏感，咱也不想随便传到网上去，毕竟安全第一嘛。这时候，本地大模型的优势就凸显出来了。它就在你自己的电脑上运行，完全不用担心网络问题，而且数据都在本地，隐私更有保障。而且，本地大模型可以根据你的需求进行定制，想怎么用就怎么用，灵活性超强！</p>
<h2 id="二怎么部署本地大模型">二、怎么部署本地大模型？</h2>
<p>在本地部署 DeepSeek 只需要以下三步：</p>
<ol>
<li><strong>安装 Ollama。</strong></li>
<li><strong>部署 DeepSeek。</strong></li>
<li><strong>使用 DeepSeek</strong>：这里我们使用 ChatBox 客户端操作 DeepSeek（此步骤非必须）。</li>
</ol>
<p>Ollama、DeepSeek 和 ChatBox 之间的关系如下：</p>
<ul>
<li>Ollama 是“大管家”，负责把 DeepSeek 安装到你的电脑上。</li>
<li>DeepSeek 是“超级大脑”，住在 Ollama 搭建好的环境里，帮你做各种事情。</li>
<li>ChatBox 是“聊天工具”，让你更方便地和 DeepSeek 交流。</li>
</ul>
<h3 id="安装ollama">安装Ollama</h3>
<p>Ollama 是一个开源的大型语言模型服务工具。它的主要作用是<strong>帮助用户快速在本地运行大模型</strong>，简化了在 Docker 容器内部署和管理大语言模型（LLM）的过程。</p>
<p>PS：Ollama 就是大模型届的“Docker”。</p>
<p>Ollama 优点如下：</p>
<ul>
<li><strong>易于使用</strong>：即使是没有经验的用户也能轻松上手，无需开发即可直接与模型进行交互。</li>
<li><strong>轻量级</strong>：代码简洁，运行时占用资源少，能够在本地高效运行，不需要大量的计算资源。</li>
<li><strong>可扩展</strong>：支持多种模型架构，并易于添加新模型或更新现有模型，还支持热加载模型文件，无需重新启动即可切换不同的模型，具有较高的灵活性。</li>
<li><strong>预构建模型库</strong>：包含一系列预先训练好的大型语言模型，可用于各种任务，如文本生成、翻译、问答等，方便在本地运行大型语言模型。</li>
</ul>
<p>Ollama 官网：<a href="https://ollama.com/" target="_blank" rel="noopener nofollow">https://ollama.com/</a></p>
<h4 id="下载并安装ollama">下载并安装Ollama</h4>
<p>下载地址：<a href="https://ollama.com/" target="_blank" rel="noopener nofollow">https://ollama.com/</a></p>
<p><img src="https://cdn.nlark.com/yuque/0/2025/png/92791/1739008193945-8253d5b7-50df-4200-93cf-6e394f6c0201.png" alt="" loading="lazy"></p>
<p>用户根据自己的操作系统选择对应的安装包，然后安装 Ollama 软件即可。</p>
<p>安装完成之后，你的电脑上就会有这样一个 Ollama 应用：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2025/png/92791/1739008193890-a04880b3-48a7-47f0-b5f1-01a45884f648.png" alt="" loading="lazy"></p>
<p>点击应用就会运行 Ollama，此时在你电脑状态栏就可以看到 Ollama 的小图标，测试 Ollama 有没有安装成功，使用命令窗口输入“ollama -v”指令，能够正常响应并显示 Ollama 版本号就说明安装成功了，如下图所示：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2025/png/92791/1739008193891-f4fcc86c-8e07-4fda-98a3-45cddd630fc1.png" alt="" loading="lazy"></p>
<h3 id="部署deepseek">部署DeepSeek</h3>
<p>Ollama 支持大模型列表：<a href="https://ollama.com/library" target="_blank" rel="noopener nofollow">https://ollama.com/library</a></p>
<p>选择 DeepSeek 大模型版本，如下图所示：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2025/png/92791/1739008194029-ab203ced-bc9d-4610-a3ad-c1c68231e48f.png" alt="" loading="lazy"></p>
<h4 id="deepseek版本介绍">DeepSeek版本介绍</h4>
<table>
<thead>
<tr>
<th style="text-align: left"><strong><font style="color: rgba(74, 74, 74, 1)">模型参数规模</font></strong></th>
<th style="text-align: left"><strong><font style="color: rgba(74, 74, 74, 1)">典型用途</font></strong></th>
<th style="text-align: left"><strong><font style="color: rgba(74, 74, 74, 1)">CPU 建议</font></strong></th>
<th style="text-align: left"><strong><font style="color: rgba(74, 74, 74, 1)">GPU 建议</font></strong></th>
<th style="text-align: left"><strong><font style="color: rgba(74, 74, 74, 1)">内存建议 (RAM) </font></strong></th>
<th style="text-align: left"><strong><font style="color: rgba(74, 74, 74, 1)">磁盘空间建议</font></strong></th>
<th style="text-align: left"><strong><font style="color: rgba(74, 74, 74, 1)">适用场景</font></strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left"><strong><font style="color: rgba(74, 74, 74, 1)">1.5b (15亿)</font></strong></td>
<td style="text-align: left"><font style="color: rgba(63, 63, 63, 1)">小型推理、轻量级任务</font></td>
<td style="text-align: left"><font style="color: rgba(63, 63, 63, 1)">4核以上 (Intel i5 / AMD Ryzen 5)</font></td>
<td style="text-align: left"><font style="color: rgba(63, 63, 63, 1)">可选，入门级 GPU (如 NVIDIA GTX 1650, 4GB 显存)</font></td>
<td style="text-align: left"><font style="color: rgba(63, 63, 63, 1)">8GB</font></td>
<td style="text-align: left"><font style="color: rgba(63, 63, 63, 1)">10GB 以上 SSD</font></td>
<td style="text-align: left"><font style="color: rgba(63, 63, 63, 1)">小型 NLP 任务、文本生成、简单分类</font></td>
</tr>
<tr>
<td style="text-align: left"><strong><font style="color: rgba(74, 74, 74, 1)">7b (70亿)</font></strong></td>
<td style="text-align: left"><font style="color: rgba(0, 0, 0, 1)">中等推理、通用任务</font></td>
<td style="text-align: left"><font style="color: rgba(0, 0, 0, 1)">6核以上 (Intel i7 / AMD Ryzen 7)</font></td>
<td style="text-align: left"><font style="color: rgba(0, 0, 0, 1)">中端 GPU (如 NVIDIA RTX 3060, 12GB 显存)</font></td>
<td style="text-align: left"><font style="color: rgba(0, 0, 0, 1)">16GB</font></td>
<td style="text-align: left"><font style="color: rgba(0, 0, 0, 1)">20GB 以上 SSD</font></td>
<td style="text-align: left"><font style="color: rgba(0, 0, 0, 1)">中等规模 NLP、对话系统、文本分析</font></td>
</tr>
<tr>
<td style="text-align: left"><strong><font style="color: rgba(74, 74, 74, 1)">14b (140亿)</font></strong></td>
<td style="text-align: left"><font style="color: rgba(63, 63, 63, 1)">中大型推理、复杂任务</font></td>
<td style="text-align: left"><font style="color: rgba(63, 63, 63, 1)">8核以上 (Intel i9 / AMD Ryzen 9)</font></td>
<td style="text-align: left"><font style="color: rgba(63, 63, 63, 1)">高端 GPU (如 NVIDIA RTX 3090, 24GB 显存)</font></td>
<td style="text-align: left"><font style="color: rgba(63, 63, 63, 1)">32GB</font></td>
<td style="text-align: left"><font style="color: rgba(63, 63, 63, 1)">50GB 以上 SSD</font></td>
<td style="text-align: left"><font style="color: rgba(63, 63, 63, 1)">复杂 NLP、多轮对话、知识问答</font></td>
</tr>
<tr>
<td style="text-align: left"><strong><font style="color: rgba(74, 74, 74, 1)">32b (320亿)</font></strong></td>
<td style="text-align: left"><font style="color: rgba(0, 0, 0, 1)">大型推理、高性能任务</font></td>
<td style="text-align: left"><font style="color: rgba(0, 0, 0, 1)">12核以上 (Intel Xeon / AMD Threadripper)</font></td>
<td style="text-align: left"><font style="color: rgba(0, 0, 0, 1)">高性能 GPU (如 NVIDIA A100, 40GB 显存)</font></td>
<td style="text-align: left"><font style="color: rgba(0, 0, 0, 1)">64GB</font></td>
<td style="text-align: left"><font style="color: rgba(0, 0, 0, 1)">100GB 以上 SSD</font></td>
<td style="text-align: left"><font style="color: rgba(0, 0, 0, 1)">大规模 NLP、多模态任务、研究用途</font></td>
</tr>
<tr>
<td style="text-align: left"><strong><font style="color: rgba(74, 74, 74, 1)">70b (700亿)</font></strong></td>
<td style="text-align: left"><font style="color: rgba(63, 63, 63, 1)">超大规模推理、研究任务</font></td>
<td style="text-align: left"><font style="color: rgba(63, 63, 63, 1)">16核以上 (服务器级 CPU)</font></td>
<td style="text-align: left"><font style="color: rgba(63, 63, 63, 1)">多 GPU 并行 (如 2x NVIDIA A100, 80GB 显存)</font></td>
<td style="text-align: left"><font style="color: rgba(63, 63, 63, 1)">128GB</font></td>
<td style="text-align: left"><font style="color: rgba(63, 63, 63, 1)">200GB 以上 SSD</font></td>
<td style="text-align: left"><font style="color: rgba(63, 63, 63, 1)">超大规模模型、研究、企业级应用</font></td>
</tr>
<tr>
<td style="text-align: left"><strong><font style="color: rgba(74, 74, 74, 1)">671b (6710亿)</font></strong></td>
<td style="text-align: left"><font style="color: rgba(0, 0, 0, 1)">超大规模训练、企业级任务</font></td>
<td style="text-align: left"><font style="color: rgba(0, 0, 0, 1)">服务器级 CPU (如 AMD EPYC / Intel Xeon)</font></td>
<td style="text-align: left"><font style="color: rgba(0, 0, 0, 1)">多 GPU 集群 (如 8x NVIDIA A100, 320GB 显存)</font></td>
<td style="text-align: left"><font style="color: rgba(0, 0, 0, 1)">256GB 或更高</font></td>
<td style="text-align: left"><font style="color: rgba(0, 0, 0, 1)">1TB 以上 NVMe SSD</font></td>
<td style="text-align: left"><font style="color: rgba(0, 0, 0, 1)">超大规模训练、企业级 AI 平台</font></td>
</tr>
</tbody>
</table>
<p>例如，安装并运行 DeepSeek：ollama run deepseek-r1:1.5b</p>
<p><img src="https://cdn.nlark.com/yuque/0/2025/png/92791/1739008194367-3ac431f6-8c8c-4267-a025-f3b79bc88a3e.png" alt="" loading="lazy"></p>
<h3 id="使用deepseek">使用DeepSeek</h3>
<p>这里我们使用 ChatBox 调用 DeepSeek 进行交互，ChatBox 就是一个前端工具，用于方便的对接各种大模型（其中包括 DeepSeek），并且它支持跨平台，更直观易用。</p>
<p>ChatBox 官网地址：<a href="https://chatboxai.app/zh" target="_blank" rel="noopener nofollow">https://chatboxai.app/zh</a></p>
<p>点击下载按钮获取 ChatBox 安装包：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2025/png/92791/1739008194381-e7cfe410-4bfd-4227-a77f-b1b011fc0078.png" alt="" loading="lazy"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2025/png/92791/1739008194400-38a12f96-723e-4ede-b3b6-706b9a29f88e.png" alt="" loading="lazy"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2025/png/92791/1739008194418-a7a96186-900c-4012-9ff5-4b326adfcf1c.png" alt="" loading="lazy"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2025/png/92791/1739008194568-6df219f8-0dc6-42f9-b7f5-06e22a157339.png" alt="" loading="lazy"></p>
<p>安装完 Chatbox 之后就是配置 DeepSeek 到 Chatbox 了，如下界面所示：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2025/png/92791/1739008194772-4bd7881e-f2c6-43ad-90ca-20db34f8a6f4.png" alt="" loading="lazy"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2025/png/92791/1739008194838-9bad230c-5521-40c6-a2f1-a504ae33fd0a.png" alt="" loading="lazy"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2025/png/92791/1739008194871-c6b80b92-986e-4a37-a3b1-eb2f9062ae46.png" alt="" loading="lazy"></p>
<p>使用 DeepSeek，如下图所示：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2025/png/92791/1739008194926-2e5b4ce0-70ae-4383-8139-82395673bf06.png" alt="" loading="lazy"></p>
<h2 id="三扩展知识本地deepseek集成idea">三、扩展知识：本地DeepSeek集成Idea</h2>
<h3 id="安装codegpt插件">安装CodeGPT插件</h3>
<p><img src="https://cdn.nlark.com/yuque/0/2025/png/92791/1739008195108-d79f2f49-0af4-4de2-ab70-ab56a5e5b714.png" alt="" loading="lazy"></p>
<h3 id="配置ollama">配置Ollama</h3>
<p>Ollama API 默认调用端口号：11434</p>
<p>检查相应的配置，如下所示：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2025/png/92791/1739008195460-b0637549-7d62-407d-a091-8f7ee3e30e05.png" alt="" loading="lazy"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2025/png/92791/1739008195413-7785c308-c001-4b1d-b2c7-97bfb9658774.png" alt="" loading="lazy"></p>
<h3 id="使用ollama">使用Ollama</h3>
<p><img src="https://cdn.nlark.com/yuque/0/2025/png/92791/1739008195561-4d76f729-d351-4079-bd5d-40fecda288f2.png" alt="" loading="lazy"></p>
<h2 id="四优缺点分析">四、优缺点分析</h2>
<p>本地大模型的优缺点分析说完部署，我们来分析一下本地大模型的优缺点，好让大家心里有个数。</p>
<h3 id="优点">优点</h3>
<ul>
<li><strong>隐私性高</strong>：数据都在本地，不用担心泄露问题，对于一些敏感数据处理来说，这是最大的优势。</li>
<li><strong>稳定性强</strong>：不受网络影响，只要电脑不坏，模型就能稳定运行，不用担心中途卡顿或者断线。</li>
<li><strong>可定制性强</strong>：可以根据自己的需求进行调整和优化，想让它做什么功能就做什么功能，灵活性很高。</li>
</ul>
<h3 id="缺点">缺点</h3>
<ul>
<li><strong>硬件要求高</strong>：大模型对电脑的性能要求不低，如果电脑配置不够，可能会运行很卡，甚至跑不起来。</li>
<li><strong>部署复杂</strong>：对于小白来说，一开始可能会觉得有点复杂，需要安装各种东西，还得配置参数，不过只要按照教程来，其实也没那么难。</li>
<li><strong>维护成本高</strong>：如果模型出了问题，可能需要自己去排查和解决，不像在线工具，有问题直接找客服就行。</li>
</ul>
<h2 id="五最后">五、最后</h2>
<p>小伙伴们，看完这些，是不是觉得本地大模型其实也没那么可怕呢？其实只要按照步骤来，小白也能轻松搞定。动手做起来吧，说不定你就能发现更多好玩的功能，让这个大模型成为你工作和学习的得力助手呢！要是你在部署过程中遇到什么问题，别忘了留言问我哦，我们一起解决！快去试试吧，开启你的本地大模型之旅！</p>
<p>我这里提供了一份清华大学《DeepSeek：从入门到精通》PDF 文档（总共 104 页），加我免费获取：vipStone【备注：DK】</p>

</div>
<div id="MySignature" role="contentinfo">
    <div style="text-align: center; color: red">
关注下面二维码，订阅更多精彩内容。
<br>
<img style="margin-left: 0px" src="https://images.cnblogs.com/cnblogs_com/vipstone/848916/o_211225130402_gognzhonghao.jpg">
</div>

<div style="display: none">
    <img src="http://icdn.apigo.cn/gitchat/rabbitmq.png?imageView2/0/w/500/h/400">
</div>
<div style="margin-bottom: 50px; display: none">

<img title="微信打赏" src="http://icdn.apigo.cn/myinfo/wchat-pay.png" alt="微信打赏">
<br>

<div style="display: none">
<span style="display: block; position: absolute; height: 40px; top: 50%; margin-top: -20px">关注公众号（加好友）：</span>

<img style="margin-left: 144px" src="http://icdn.apigo.cn/gongzhonghao2.png?imageView2/0/w/120/h/120">
</div>
<p></p>

<div id="AllanboltSignature">
    <p style="border-top: #e0e0e0 1px dashed; border-right: #e0e0e0 1px dashed; border-bottom: #e0e0e0 1px dashed; border-left: #e0e0e0 1px dashed; padding-top: 10px; padding-right: 10px; padding-bottom: 10px; padding-left: 60px; background: url(&quot;https://images.cnblogs.com/cnblogs_com/lloydsheng/239039/o_copyright.gif&quot;) #e5f1f4 no-repeat 1% 50%; font-family: 微软雅黑; font-size: 11px" id="PSignature">
        <br> 作者：
        <a href="http://vipstone.cnblogs.com/" target="_blank">王磊的博客</a>
        <br> 出处：
        <a href="http://vipstone.cnblogs.com/" target="_blank">http://vipstone.cnblogs.com/</a>
        <br>
    </p>
</div></div>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.026959990033564814" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-02-08 19:44">2025-02-08 19:44</span>&nbsp;
<a href="https://www.cnblogs.com/vipstone">磊哥|www.javacn.site</a>&nbsp;
阅读(<span id="post_view_count">51</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18705268" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18705268);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18705268', targetLink: 'https://www.cnblogs.com/vipstone/p/18705268', title: '1分钟学会DeepSeek本地部署，小白也能搞定！' })">举报</a>
</div>
        