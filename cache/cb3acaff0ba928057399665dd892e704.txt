
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/SuahiStudy/p/18851636" title="发布于 2025-04-28 14:38">
    <span role="heading" aria-level="2">【深度学习】MLE视角下的VAE与DDPM损失函数推导</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h2 id="正文">正文</h2>
<h3 id="最大似然估计的由来">最大似然估计的由来</h3>
<blockquote>
<p>VAE和DDPM都是likelihood-based生成模型，都是通过学习分布-&gt;采样实现图像生成的；</p>
</blockquote>
<p>这类模型<strong>最大的特点</strong>就是希望实现</p>
<p></p><div class="math display">\[\theta = \arg\max \limits_{\theta} \mathbb{E}_{x \sim p_{data}(x)}[log(p_{\theta}(x))]
\]</div><p></p><p>上述式子是啥意思呢？<span class="math inline">\(\theta\)</span>是神经网络的参数集合，<span class="math inline">\(p_{\theta}(x)\)</span>是神经网络模型学习（拟合）得到的分布。所以上式意思是我希望我的神经网络<strong>生成的图片足够逼真</strong>，生成出符合原始数据分布的<strong>概率足够高</strong>。</p>
<p>换一个思路去考虑这个问题，我现在有一个神经网络参数化的<span class="math inline">\(p_{\theta}\)</span>，和真实数据分布<span class="math inline">\(p_{data}\)</span></p>
<blockquote>
<p>[!NOTE]</p>
<p>这里教个小技巧，看到<span class="math inline">\(p_{\theta}\)</span>就当作<span class="math inline">\(N(\mu_{\theta},\sigma_{\theta}^2)\)</span>去理解（并不是说所有神经网络都在拟合高斯分布，只是这种情况比较多，且这么理解更加直观。）</p>
</blockquote>
<p>我们本质的目的还是说<span class="math inline">\(p_{\theta} \rightarrow p_{data}\)</span>，<strong>尽可能的逼近</strong></p>
<p></p><div class="math display">\[\begin{aligned}
D_{KL}(p_{data}||p_{\theta}) &amp;= \int p_{data}(x)log\frac{p_{data}(x)}{p_{\theta}(x)}dx \\
&amp;= \int p_{data}(x)logp_{data}(x)dx - \int p_{data}(x)logp_{\theta}(x)dx \\
&amp;=\mathbb{E}_{x \sim p_{data}(x)}[logp_{data}(x)]-\mathbb{E}_{x \sim p_{data}(x)}[logp_{\theta}(x)]
\end{aligned}
\]</div><p></p><p>又因为<span class="math inline">\(D_{KL}(p_{data}||p_{\theta}) \geq 0\)</span>，这就要求<span class="math inline">\(\mathbb{E}_{x \sim p_{data}(x)}[logp_{\theta}(x)]\)</span><strong>尽可能的大</strong>，以离散的形式理解：</p>
<p></p><div class="math display">\[\begin{aligned}
\mathbb{E}_{x \sim p_{data}(x)}[logp_{data}(x)]-\mathbb{E}_{x \sim p_{data}(x)}[logp_{\theta}(x)] \approx \frac{1}{N}\sum_{i=1}^N logp_{data}(x_i)-\frac{1}{N}\sum_{i=1}^N logp_{\theta}(x_i),x_i \sim p_{data} 
\end{aligned}
\]</div><p></p><p>当<span class="math inline">\(p_\theta \rightarrow p_{data}\)</span>时，那么每次采样得到的<span class="math inline">\(p_{\theta}(x_i)\)</span>就是等于<span class="math inline">\(p_{data}(x_i)\)</span>，这样就保证<span class="math inline">\(D_{KL}(p_{data}||p_{\theta})\)</span>最小。</p>
<blockquote>
<p>[!NOTE]</p>
<p>有没有可能<span class="math inline">\(p_{\theta}\)</span>和<span class="math inline">\(p_{data}\)</span><strong>不相等</strong>，但是也有样本概率的整体差趋近于0呢？以高斯分布举例<span class="math inline">\(p_{data}(x)=N(175,10^2I),p_{\theta}(x)=N(165,5^2I)\)</span>，那么有可能<strong>个别甚至一部分</strong>从<span class="math inline">\(p_{data}\)</span>采样得到的<span class="math inline">\(x_i\)</span>在<span class="math inline">\(p_{\theta}(x_i)\)</span>中的<strong>概率值会高于</strong><span class="math inline">\(p_{data}(x_i)\)</span>，但是就整体而言，<strong>其余部分的均值会拉低这个水平</strong>，导致最终的<span class="math inline">\(D_{KL}(p_{\theta}||p_{data})\)</span>还是会很高。</p>
<p>另一个方面，<span class="math inline">\(D_{KL}(p_{\theta}||p_{data}) \geq 0\)</span>，除了<strong>两个分布相等</strong>之外没有别的可能实现。</p>
</blockquote>
<p>那为什么有些资料中，最大似然估计中没有涵盖<span class="math inline">\(\mathbb{E}_{x \sim p_{data}(x)}\)</span>这项呢？</p>
<p>假设有<span class="math inline">\(N\)</span>个独立同分布（i.i.d）的样本<span class="math inline">\(\{x_1,x_2,\cdots,x_N\}\)</span>，MLE的目标是<strong>最大化样本的联合概率</strong></p>
<p></p><div class="math display">\[\theta=\arg\max \limits_{\theta}=\prod_{i=1}^Np_{\theta}​(x_i)
\]</div><p></p><p>直观来说，我希望<span class="math inline">\(p_{\theta}(x)\)</span>足够接近于<span class="math inline">\(p_{data}(x)\)</span>，这样从<span class="math inline">\(p_{data}(x)\)</span>采样得到的<span class="math inline">\(x_i\)</span>在<span class="math inline">\(p_{\theta}(x)\)</span>分布下的概率值才会尽可能的高。</p>
<p>取对数后转化为求和的形式</p>
<p></p><div class="math display">\[\theta = \arg \max \limits_{\theta} \sum_{i=1}^Nlogp_{\theta}(x_i)
\]</div><p></p><p>当<span class="math inline">\(N \rightarrow \infty\)</span>时，根据大数定律有</p>
<p></p><div class="math display">\[\frac{1}{N}\sum_{i=1}^Nlogp_{\theta}(x_i) \rightarrow \mathbb{E}_{x \sim p_{data}(x)}[log(p_{\theta}(x))]
\]</div><p></p><p>因此<span class="math inline">\(\theta = \arg \max \limits_{\theta} \sum_{i=1}^Nlogp_{\theta}(x_i)\)</span>和<span class="math inline">\(\theta = \arg\max \limits_{\theta} \mathbb{E}_{x \sim p_{data}(x)}[log(p_{\theta}(x))]\)</span>在形式上取得一致。</p>
<h3 id="vae的loss推导">VAE的Loss推导</h3>
<p></p><div class="math display">\[\begin{aligned}
logp_{\theta}(x)&amp;=logp_{\theta}(x)\int_zq_{\phi}(z|x)dz \\
&amp;=\int_z q_{\phi}(z|x)logp_{\theta}(x)dz \\
&amp;=\mathbb{E}_{z \sim q_{\phi}(z|x)}[logp_{\theta}(x)] \\
&amp;=\mathbb{E}_{z \sim q_{\phi}(z|x)}[log\frac{p_{\theta}(x,z)}{p(z|x)}] \\
&amp;=\mathbb{E}_{z \sim q_{\phi}(z|x)}[log\frac{p_{\theta}(x,z)}{p_{\theta}(z|x)}\frac{q_{\phi}(z|x)}{q_{\phi}(z|x)}] \\
&amp;=\mathbb{E}_{z \sim q_{\phi}(z|x)}[log\frac{p_{\theta}(x,z)}{q_{\phi}(z|x)}]+\mathbb{E}_{z \sim q_{\phi}(z|x)}[\frac{q_{\phi}(z|x)}{p_{\theta}(z|x)}] \\
&amp;=\mathbb{E}_{z \sim q_{\phi}(z|x)}[log\frac{p_{\theta}(x,z)}{q_{\phi}(z|x)}]+D_{KL}(q_{\phi}(z|x)||p_{\theta}(z|x))\\
&amp;\geq \mathbb{E}_{z \sim q_{\phi}(z|x)}[log\frac{p_{\theta}(x,z)}{q_{\phi}(z|x)}] = ELBO
\end{aligned}
\]</div><p></p><blockquote>
<p>[!NOTE] MLE、ELBO与Loss之间的联系</p>
<ol>
<li>对于一些显式的概率模型，直接使用<span class="math inline">\(\theta = \arg \max \limits_{\theta} \sum_{i=1}^Nlogp_{\theta}(x_i)\)</span>公式；</li>
<li>但是对于包含隐变量的概率模型，由于<span class="math inline">\(p_{\theta}(x)=\int p_{\theta}(x,z)dz\)</span>中对于<span class="math inline">\(z\)</span>变量的积分过于复杂而<strong>不直接使用MLE的方法进行计算</strong>；取而代之，是通过构建变分下界（<span class="math inline">\(ELBO\)</span>）<strong>不等式</strong>的方式，通过<strong>最大化<span class="math inline">\(ELBO\)</span>的方式去逼近MLE的目标</strong>。通过分解单个数据点的<span class="math inline">\(logp_{\theta}(x)\)</span>，再扩展到全体数据<span class="math inline">\(\sum_{i=1}^N ELBO_i\)</span>，最终与MLE目标等价，并且通过该不等式<strong>引出最终的损失函数</strong>‘；</li>
<li>当目标是<strong>显式函数</strong>时，Loss是MLE本身；目标是<strong>隐式函数</strong>时，Loss是ELBO（MLE的下界）</li>
</ol>
</blockquote>
<p>其中，由于<span class="math inline">\(D_{KL}(q_{\phi}(z|x)||p_{\theta}(z|x)) \geq 0\)</span>，所以把<span class="math inline">\(\mathbb{E}_{z \sim q_{\phi}(z|x)}[log\frac{p_{\theta}(x,z)}{q_{\phi}(z|x)}]\)</span>作为变分下界（ELBO）</p>
<p></p><div class="math display">\[\begin{aligned}
ELBO &amp;= \mathbb{E}_{z \sim q_{\phi}(z|x)}[log\frac{p_{\theta}(x,z)}{q_{\phi}(z|x)}] \\
&amp;= \mathbb{E}_{z \sim q_{\phi}(z|x)}[log\frac{p_{\theta}(x|z)p(z)}{q_{\phi}(z|x)}] \\
&amp;=\mathbb{E}_{z \sim q_{\phi}(z|x)}[logp_{\theta}(x|z)]-\mathbb{E}_{z \sim q_{\phi}(z|x)}[log\frac{q_{\phi}(z|x)}{p(z)}] \\
&amp;=\mathbb{E}_{z \sim q_{\phi}(z|x)}[logp_{\theta}(x|z)]-D_{KL}(q_{\phi}(z|x)||p(z))
\end{aligned}
\]</div><p></p><p>在这里我需要更新一下对VAE的认识，之前的文章也是从<strong>流程</strong>的角度去解释为什么需要一个<span class="math inline">\(q_{\phi}(z|x)\)</span>去对后验分布进行拟合，这里我想以MLE推导得出ELBO的角度去进行更深入的解释。</p>
<h4 id="vae的动态平衡调节">VAE的动态平衡调节</h4>
<blockquote>
<p>生成图像流程：<span class="math inline">\(x \rightarrow q_{\phi}(z|x) \rightarrow z \rightarrow p_{\theta}(x|z) \rightarrow \hat{x}\)</span></p>
</blockquote>
<p>因此根据梯度调优时，VAE的调优策略类似于EM算法</p>
<ol>
<li>固定<span class="math inline">\(\phi\)</span>参数，优化<span class="math inline">\(\theta\)</span>参数。在ELBO中<span class="math inline">\(\mathbb{E}_{z \sim q_{\phi}(z|x)}[logp_{\theta}(x|z)]\)</span>表现于提高由<strong>解码器生成图像的精确度</strong>。但是此时<span class="math inline">\(\theta\)</span>的变动导致与似然分布<span class="math inline">\(p_{\theta}(x|z)\)</span>强相关的<strong>代理后验分布<span class="math inline">\(p_{\theta}(z|x)\)</span>也发生了变化</strong>，导致<span class="math inline">\(D_{KL}(q_{\phi}(z|x)||p_{\theta}(z|x))\)</span>值发生了变化，进而<strong>影响了ELBO损失函数的值</strong>；</li>
<li>固定<span class="math inline">\(\theta\)</span>参数，优化<span class="math inline">\(\phi\)</span>参数。因此<span class="math inline">\(q_{\phi}(z|x)\)</span>会天然的通过调整<span class="math inline">\(\mu_{\phi}\)</span>和<span class="math inline">\(\sigma_{\phi}\)</span>去<strong>最大化ELBO的数值</strong>，体现在最小化<span class="math inline">\(D_{KL}(q_{\phi}(z|x)||N(0,I))=\frac{1}{2}(1+log\sigma_{\phi}^2-\mu_{\phi}^2-\sigma_{\phi}^2)\)</span>，提高<strong>编码器分布与先验分布的近似度</strong>。同时，<span class="math inline">\(\mu_{\phi}\)</span>和<span class="math inline">\(\sigma_{\phi}\)</span>值的变动生成不同的<span class="math inline">\(z\)</span>值采样，又需要让<span class="math inline">\(\theta\)</span>参数<strong>不断更新</strong>进而学习如何精准生图；</li>
</ol>
<p>二者是一个<strong>动态平衡</strong>的效果。</p>
<blockquote>
<p>[!NOTE] 如果提高解码器精度所调整的<span class="math inline">\(\theta\)</span>导致代理后验分布<span class="math inline">\(p_{\theta}(z|x)\)</span>偏离标准正态分布很远（趋向于一个尖锐的分布，<span class="math inline">\(z\)</span>集中于一个位置）。那么<span class="math inline">\(q_{\phi}(z|x) \rightarrow p(z)\)</span>以提高ELBO与<span class="math inline">\(q_{\phi}(z|x) \rightarrow p_{\theta}(z|x)\)</span>以降低<span class="math inline">\(D_{KL}(q_{\phi}(z|x)||p_{\theta}(z|x))\)</span>最终提升ELBO不是<strong>背道而驰</strong>的嘛？</p>
<p>事实上，由于无法显式计算<span class="math inline">\(D_{KL}(q_{\phi}(z|x)||p_{\theta}(z|x))\)</span>，只能通过最大化ELBO，即<span class="math inline">\(q_{\phi}(z|x) \rightarrow p(z)\)</span>的形式去提升ELBO，这也导致了：</p>
<ol>
<li>最终的<span class="math inline">\(q_{\phi}(z|x)\)</span>可能与真实的<span class="math inline">\(p_{\theta}(z|x)\)</span><strong>相隔甚远</strong>，<span class="math inline">\(q_{\phi}(z|x) \rightarrow N(0,I)\)</span>是<strong>对抗中妥协</strong>的结果，试想少了<span class="math inline">\(D_{KL}(q_{\phi}(z|x)||p(z))\)</span>这层约束项，<span class="math inline">\(p_{\theta}(z|x)\)</span>跟着解码器<span class="math inline">\(\theta\)</span>参数一变，<span class="math inline">\(q_{\phi}(z|x)\)</span>为了防止<span class="math inline">\(D_{KL}(q_{\phi}(z|x)||p_{\theta}(z|x))\)</span>过大影响ELBO的值，真的会跟着走，<strong>导致最终趋向于狄拉克分布，这是我们不想看到的</strong>；</li>
<li>工程实际而言，保证了<span class="math inline">\(q_{\phi}(z|x)\)</span>空间结构趋向于<span class="math inline">\(N(0,I)\)</span>，这是<strong>精度和多样性的一个妥协</strong>；</li>
<li>个人理解，<span class="math inline">\(q_{\phi}(z|x) \rightarrow N(0,I)\)</span>也<strong>降低了<span class="math inline">\(p_{\theta}(x|z)\)</span>的学习成本</strong>，让二者更好的形成一个平衡，最终也会<strong>矫正</strong>代理后验分布<span class="math inline">\(p_{\theta}(z|x)\)</span>回归标准正态分布；</li>
</ol>
</blockquote>
<h3 id="mhvae的loss推导">MHVAE的Loss推导</h3>
<blockquote>
<p>Markovian Hierarchical Varitational AutoEncoder，马尔可夫级联VAE</p>
</blockquote>
<p><img src="https://suahi-1311668441.cos.ap-shanghai.myqcloud.com/2024/20250427185327650.png" alt="image.png" loading="lazy"></p>
<p>参考之前由MLE推导得到ELBO的公式可知</p>
<p></p><div class="math display">\[\begin{aligned}
logp_{\theta}(x)&amp;=log\int p_{\theta}(x,z_{1:T})dz_{1:T}\\
&amp;=log\int\frac{p_{\theta}(x,z_{1:T})q_{\phi}(z_{1:T}|x)}{q_{\phi}(z_{1:T}|x)}dz_{1:T} \\
&amp;=log\mathbb{E}_{q_{\phi}(z_{1:T}|x)}[\frac{p_{\theta}(x,z_{1:T})}{q_{\phi}(z_{1:T}|x)}] \\
&amp;\geq\mathbb{E}_{q_{\phi}(z_{1:T}|x)}[log\frac{p_{\theta}(x,z_{1:T})}{q_{\phi}(z_{1:T}|x)}] = ELBO \\
\end{aligned}
\]</div><p></p><blockquote>
<p>[!NOTE]</p>
<p>最后一步用到了<code>琴生不等式</code>，对于一个凸函数而言：<span class="math inline">\(\frac{log(x_1) + log(x_2)}{2} \leq log(\frac{x_1 + x_2}{2})\)</span></p>
</blockquote>
<h3 id="ddpm的loss推导">DDPM的Loss推导</h3>
<p><img src="https://suahi-1311668441.cos.ap-shanghai.myqcloud.com/2024/20250427190311096.png" alt="image.png" loading="lazy"></p>
<p>图中是基于MHVAE的标注，替换为<span class="math inline">\(x \rightarrow x_0\)</span>、<span class="math inline">\(z_i \rightarrow x_i\)</span></p>
<p>其中加噪过程<span class="math inline">\(q(x_{t}|x_{t-1})\)</span>是人为的，具体公式参考[[001 DDPM-v2]]，因此不添加<span class="math inline">\(\phi\)</span>参数；<br>
其中去噪过程<span class="math inline">\(p_{\theta}(x_{t-1}|x_t)\)</span>是需要学习的，因此添加<span class="math inline">\(\theta\)</span>参数进行<strong>神经网络参数化</strong>操作；</p>
<h4 id="ddpm的elbo">DDPM的ELBO</h4>
<p>参考上述MLE推导得到ELBO的公式</p>
<p></p><div class="math display">\[\begin{aligned}
logp_{\theta}(x)&amp;=log\int p_{\theta}(x_{0:T})dx_{1:T}\\
&amp;=log\int\frac{p_{\theta}(x_{0:T})q(x_{1:T}|x_0)}{q(x_{1:T}|x_0)}dx_{1:T} \\
&amp;=log\mathbb{E}_{q(x_{1:T}|x_0)}[\frac{p_{\theta}(x_{0:T})}{q(x_{1:T}|x_0)}] \\
&amp;\geq\mathbb{E}_{q(x_{1:T}|x_0)}[log\frac{p_{\theta}(x_{0:T})}{q(x_{1:T}|x_0)}] = ELBO \\
\end{aligned}
\]</div><p></p><p>其中</p>
<p></p><div class="math display">\[\begin{aligned}
p_{\theta}(x_{0:T}) &amp;= p(x_T)p(x_{0:T-1}|x_T)\\
&amp;=p(x_T)p(x_{T-1}|x_{T})p(x_{0:T-2}|x_{T-1},x_T)\\
&amp;=p(x_T)p(x_{T-1}|x_{T})p(x_{0:T-2}|x_{T-1}) \\
&amp;=\cdots \\
&amp;=p(x_T)p(x_{T-1}|x_{T})\cdots p(x_0|x_1) \\
&amp;=p(x_T)\prod_{t=1}^Tp(x_{t-1}|x_t)
\end{aligned} 
\]</div><p></p><p></p><div class="math display">\[\begin{aligned}
q(x_{1:T}|x_0) &amp;= q(x_{2:T}|x_1)q(x_1|x_0) \\
&amp;=q(x_{3:T}|x_2,x_1)q(x_2|x_1)q(x_1|x_0) \\
&amp;=q(x_{3:T}|x_2)q(x_2|x_1)q(x_1|x_0) \\
&amp;=\cdots \\
&amp;=q(x_{T}|x_{T-1})\cdots q(x_2|x_1)q(x_1|x_0)\\
&amp;=\prod_{t=1}^Tq(x_t|x_{t-1})
\end{aligned}
\]</div><p></p><p>代入得</p>
<p></p><div class="math display">\[\begin{aligned}
logp_{\theta}(x)&amp;\geq\mathbb{E}_{q(x_{1:T}|x_0)}[log\frac{p_{\theta}(x_{0:T})}{q(x_{1:T}|x_0)}]  \\
&amp;=\mathbb{E}_{q(x_{1:T}|x_0)}[log\frac{p_{\theta}(x_T)\prod_{t=1}^Tp_{\theta}(x_{t-1}|x_t)}{\prod_{t=1}^Tq(x_t|x_{t-1})}] \\
&amp;=\mathbb{E}_{q(x_{1:T}|x_0)}[log\frac{p_{\theta}(x_T)p_{\theta}(x_0|x_1)\prod_{t=2}^Tp_{\theta}(x_{t-1}|x_t)}{\prod_{t=1}^Tq(x_t|x_{t-1})}] \\
&amp;=\mathbb{E}_{q(x_{1:T}|x_0)}[log\frac{p_{\theta}(x_T)p_{\theta}(x_0|x_1)\prod_{t=1}^{T-1}p_{\theta}(x_{t}|x_{t+1})}{q(x_T|x_{T-1})\prod_{t=1}^{T-1}q(x_t|x_{t-1})}] \\
&amp;=\mathbb{E}_{q(x_{1:T}|x_0)}[log\frac{p_{\theta}(x_T)p_{\theta}(x_0|x_1)}{q(x_T|x_{T-1})}]+\mathbb{E}_{q(x_{1:T}|x_0)}[log\prod_{t=1}^{T-1}\frac{p_{\theta}(x_{t}|x_{t+1})}{q(x_t|x_{t-1})}] \\
&amp;=\mathbb{E}_{q(x_{1:T}|x_0)}[logp_{\theta}(x_0|x_1)]+\mathbb{E}_{q(x_{1:T}|x_0)}[log\frac{p_{\theta}(x_T)}{q(x_T|x_{T-1})}]+\mathbb{E}_{q(x_{1:T}|x_0)}[\sum_{t=1}^{T-1}log\frac{p_{\theta}(x_{t}|x_{t+1})}{q(x_t|x_{t-1})}] \\
&amp;=\mathbb{E}_{q(x_{1:T}|x_0)}[logp_{\theta}(x_0|x_1)]+\mathbb{E}_{q(x_{1:T}|x_0)}[log\frac{p_{\theta}(x_T)}{q(x_T|x_{T-1})}]+\sum_{t=1}^{T-1}\mathbb{E}_{q(x_{1:T}|x_0)}[log\frac{p_{\theta}(x_{t}|x_{t+1})}{q(x_t|x_{t-1})}] \\
&amp;=\mathbb{E}_{q(x_{1}|x_0)}[logp_{\theta}(x_0|x_1)]+\mathbb{E}_{q(x_{T},x_{T-1}|x_0)}[log\frac{p_{\theta}(x_T)}{q(x_T|x_{T-1})}]+\sum_{t=1}^{T-1}\mathbb{E}_{q(x_{t-1},x_t,x_{t+1}|x_0)}[log\frac{p_{\theta}(x_{t}|x_{t+1})}{q(x_t|x_{t-1})}] \\
\end{aligned}
\]</div><p></p><blockquote>
<p>[!NOTE]</p>
<ol>
<li><span class="math inline">\(\prod_{t=2}^Tp_{\theta}(x_{t-1}|x_t)\)</span>可以通过<strong>换元法</strong>，改写成<span class="math inline">\(\prod_{t=1}^{T-1}p_{\theta}(x_{t}|x_{t+1})\)</span>；</li>
<li><strong>期望的和 等于 和的期望</strong>；</li>
<li>最后一行由于其它变量都没有用上，因此<strong>只保留相关的变量</strong>进行采样；</li>
</ol>
</blockquote>
<h4 id="消除变量">消除变量</h4>
<p></p><div class="math display">\[\begin{aligned}
\mathbb{E}_{q(x_{T},x_{T-1}|x_0)}[log\frac{p_{\theta}(x_T)}{q(x_T|x_{T-1})}] &amp;= \iint q(x_T,x_{T-1}|x_0)log\frac{p_{\theta}(x_T)}{q(x_T|x_{T-1})}dx_{T-1}dx_T \\
&amp;=\iint log\frac{p_{\theta}(x_T)}{q(x_T|x_{T-1})} q(x_T|x_{T-1},x_0)q(x_{T-1}|x_0)dx_{T-1}dx_T \\
&amp;=\iint log\frac{p_{\theta}(x_T)}{q(x_T|x_{T-1})} q(x_T|x_{T-1})q(x_{T-1}|x_0)dx_{T-1}dx_T \\
&amp;=\int [\int log\frac{p_{\theta}(x_T)}{q(x_T|x_{T-1})}q(x_T|x_{T-1})dx_T]q(x_{T-1}|x_0)dx_{T-1} \\
&amp;=\int q(x_{T-1}|x_0)[-D_{KL}(q(x_T|x_{T-1})||p_{\theta}(x_T))]dx_{T-1} \\
&amp;=\mathbb{E}_{q(x_{T-1}|x_0)}[-D_{KL}(q(x_T|x_{T-1})||p_{\theta}(x_T))]
\end{aligned}
\]</div><p></p><p>这里<strong>尤其尤其要注意的是</strong>，<strong>积分顺序非常关键</strong>！</p>
<p>我踩得坑是：</p>
<p></p><div class="math display">\[\begin{aligned}
\mathbb{E}_{q(x_{T},x_{T-1}|x_0)}[log\frac{p_{\theta}(x_T)}{q(x_T|x_{T-1})}] &amp;= \iint q(x_T,x_{T-1}|x_0)log\frac{p_{\theta}(x_T)}{q(x_T|x_{T-1})}dx_{T-1}dx_T \\
&amp;=\iint log\frac{p_{\theta}(x_T)}{q(x_T|x_{T-1})} q(x_T|x_{T-1},x_0)q(x_{T-1}|x_0)dx_{T-1}dx_T \\
&amp;=\iint log\frac{p_{\theta}(x_T)}{q(x_T|x_{T-1})} q(x_T|x_{T-1})q(x_{T-1}|x_0)dx_{T-1}dx_T \\
&amp;=\int [\int q(x_{T-1}|x_0)dx_{T-1}]log\frac{p_{\theta}(x_T)}{q(x_T|x_{T-1})}q(x_T|x_{T-1})dx_T \\
&amp;=\int 1 \times log\frac{p_{\theta}(x_T)}{q(x_T|x_{T-1})}q(x_T|x_{T-1})dx_T \\
&amp;=-D_{KL}(q(x_T|x_{T-1})||p_{\theta}(x_T))
\end{aligned}
\]</div><p></p><blockquote>
<p>[!important]</p>
<ol>
<li><span class="math inline">\(\int p(x_1|x_2) dx_1=1\)</span>，要看清楚这里是积分，<strong><span class="math inline">\(x_1\)</span>是变量，积分完之后<span class="math inline">\(x_1\)</span>变量就消失了</strong></li>
<li>代入上式，若先把<span class="math inline">\(x_{T-1}\)</span>当作<strong>变量积分掉的话</strong>，剩下的带有<span class="math inline">\(x_{T-1}\)</span><strong>条件概率的积分就无法完成</strong></li>
<li>因此只能<strong>先把<span class="math inline">\(x_T\)</span>当作变量积分掉</strong>，因为剩下的<span class="math inline">\(x_{T-1}\)</span>变量<strong>没有<span class="math inline">\(x_T\)</span>的条件概率</strong>。</li>
</ol>
</blockquote>
<p>同理</p>
<p></p><div class="math display">\[\begin{aligned}
\mathbb{E}_{q(x_{t-1},x_t,x_{t+1}|x_0)}[log\frac{p_{\theta}(x_{t}|x_{t+1})}{q(x_t|x_{t-1})}] &amp;= \iiint q(x_{t-1},x_t,x_{t+1}|x_0)log\frac{p_{\theta}(x_{t}|x_{t+1})}{q(x_t|x_{t-1})}dx_{t-1}dx_t dx_{t+1} \\
&amp;=\iiint q(x_{t+1},x_{t-1}|x_0)q(x_t|x_{t-1})log\frac{p_{\theta}(x_{t}|x_{t+1})}{q(x_t|x_{t-1})}dx_{t-1}dx_t dx_{t+1} \\
&amp;=\iint [\int log\frac{p_{\theta}(x_{t}|x_{t+1})}{q(x_t|x_{t-1})}q(x_t|x_{t-1})dx_t]q(x_{t+1},x_{t-1}|x_0)dx_{t-1}dx_{t+1} \\
&amp;=\iint q(x_{t+1},x_{t-1}|x_0)[-D_{KL}(q(x_t|x_{t-1})||p_{\theta}(x_{t}|x_{t+1}))]dx_{t-1}dx_{t+1}  \\
&amp;=\mathbb{E}_{q(x_{t-1},x_{t+1}|x_0)}[-D_{KL}(q(x_t|x_{t-1})||p_{\theta}(x_{t}|x_{t+1}))]
\end{aligned}
\]</div><p></p><p>此时有</p>
<p></p><div class="math display">\[\begin{aligned}
logp_{\theta}(x)&amp;\geq\mathbb{E}_{q(x_{1}|x_0)}[logp_{\theta}(x_0|x_1)]+\mathbb{E}_{q(x_{T-1}|x_0)}[-D_{KL}(q(x_T|x_{T-1})||p_{\theta}(x_T))]+\mathbb{E}_{q(x_{t-1},x_{t+1}|x_0)}[-D_{KL}(q(x_t|x_{t-1})||p_{\theta}(x_{t}|x_{t+1}))] \\
\end{aligned}
\]</div><p></p><p><img src="https://suahi-1311668441.cos.ap-shanghai.myqcloud.com/2024/20250428133426892.png" alt="image.png" loading="lazy"></p>
<p>这里出现了一个问题，<strong>多元变量求期望方差会很大</strong>，那么能不能通过一些方法<strong>消去部分的变量</strong>呢？</p>
<h4 id="马尔可夫性质贝叶斯">马尔可夫性质贝叶斯</h4>
<p></p><div class="math display">\[\begin{aligned}
logp_{\theta}(x)&amp;\geq\mathbb{E}_{q(x_{1:T}|x_0)}[log\frac{p_{\theta}(x_{0:T})}{q(x_{1:T}|x_0)}]  \\
&amp;=\mathbb{E}_{q(x_{1:T}|x_0)}[log\frac{p_{\theta}(x_T)\prod_{t=1}^Tp_{\theta}(x_{t-1}|x_t)}{\prod_{t=1}^Tq(x_t|x_{t-1})}] \\
&amp;=\mathbb{E}_{q(x_{1:T}|x_0)}[log\frac{p_{\theta}(x_T)p_{\theta}(x_0|x_1)\prod_{t=2}^Tp_{\theta}(x_{t-1}|x_t)}{\prod_{t=1}^Tq(x_t|x_{t-1})}] \\
&amp;=\mathbb{E}_{q(x_{1:T}|x_0)}[log\frac{p_{\theta}(x_T)p_{\theta}(x_0|x_1)\prod_{t=2}^{T}p_{\theta}(x_{t-1}|x_{t})}{q(x_1|x_0)\prod_{t=2}^{T}q(x_t|x_{t-1})}] \\
&amp;=\mathbb{E}_{q(x_{1:T}|x_0)}[log\frac{p_{\theta}(x_T)p_{\theta}(x_0|x_1)}{q(x_1|x_{0})}]+\mathbb{E}_{q(x_{1:T}|x_0)}[log\prod_{t=2}^{T}\frac{p_{\theta}(x_{t-1}|x_{t})}{q(x_t|x_{t-1})}] \\
&amp;=\mathbb{E}_{q(x_{1:T}|x_0)}[log\frac{p_{\theta}(x_T)p_{\theta}(x_0|x_1)}{q(x_1|x_{0})}]+\mathbb{E}_{q(x_{1:T}|x_0)}[log\prod_{t=2}^{T}\frac{p_{\theta}(x_{t-1}|x_{t})}{q(x_t|x_{t-1},x_0)}] \\
&amp;=\mathbb{E}_{q(x_{1:T}|x_0)}[log\frac{p_{\theta}(x_T)p_{\theta}(x_0|x_1)}{q(x_1|x_{0})}]+\mathbb{E}_{q(x_{1:T}|x_0)}[log\prod_{t=2}^{T}\frac{p_{\theta}(x_{t-1}|x_{t})}{\frac{q(x_{t-1}|x_{t},x_0)q(x_t|x_0)}{q(x_{t-1}|x_0)}}] \\
\end{aligned}
\]</div><p></p><blockquote>
<p>[!NOTE]</p>
<ol>
<li>由于马尔可夫性质规定：<span class="math inline">\(x_{t}\)</span>时刻只与<span class="math inline">\(x_{t-1}\)</span>时刻相关。因此<span class="math inline">\(q(x_t|x_{t-1},x_0)=q(x_t|x_{t-1})\)</span>；</li>
<li>但是<strong>逆向过程并不满足马尔可夫性质</strong>，即<span class="math inline">\(q(x_{t-1}|x_{t},x_0) \neq q(x_{t-1}|x_{t})\)</span>，因此后文中<span class="math inline">\(q(x_{t-1}|x_{t},x_0)\)</span>中的<span class="math inline">\(x_0\)</span>一直没有删除；</li>
<li>值得注意的是，我们从原理正向推导出发时，直接在逆向非马尔可夫性质条件下在<span class="math inline">\(p(x_{t}|x_{t-1})\)</span>中添加<span class="math inline">\(x_0\)</span>条件，并通过预估<span class="math inline">\(\hat{x_0}=f(x_t,t)\)</span>的形式来消除新增的<span class="math inline">\(x_0\)</span>条件。上面的思路显得<strong>比较跳跃且难以想象</strong>，通过MLE估计ELBO的推导中，在满足马尔科夫性质下利用<span class="math inline">\(q(x_t|x_{t-1})=q(x_t|x_{t-1},x_0)\)</span>公式进行推导显得<strong>更为合理</strong>，<strong>极度怀疑正向推导加<span class="math inline">\(x_0\)</span>的措施是根据ELBO推导过程的trick来的。</strong></li>
</ol>
</blockquote>
<p>其中</p>
<p></p><div class="math display">\[\begin{aligned}
\mathbb{E}_{q(x_{1:T}|x_0)}[log\prod_{t=2}^{T}{\frac{q(x_{t-1}|x_{t},x_0)q(x_t|x_0)}{q(x_{t-1}|x_0)}}] &amp;=\mathbb{E}_{q(x_{1:T}|x_0)}[log\prod_{t=2}^{T}q(x_{t-1}|x_{t},x_0)]+\mathbb{E}_{q(x_{1:T}|x_0)}[log\frac{\cancel{q(x_2|x_0)}}{q(x_1|x_0)}+log\frac{\cancel{q(x_3|x_1)}}{\cancel{q(x_2|x_0)}}+\cdots+log\frac{q(x_T|x_0)}{\cancel{q(x_{T-1}|x_0)}}] \\
&amp;=\mathbb{E}_{q(x_{1:T}|x_0)}[log\prod_{t=2}^{T}q(x_{t-1}|x_{t},x_0)]+\mathbb{E}_{q(x_{1:T}|x_0)}[log\frac{q(x_T|x_0)}{q(x_{1}|x_0)}]

\end{aligned}
\]</div><p></p><p>代入原式得</p>
<p></p><div class="math display">\[\begin{aligned}
logp_{\theta}(x)&amp;\geq\mathbb{E}_{q(x_{1:T}|x_0)}[log\frac{p_{\theta}(x_T)p_{\theta}(x_0|x_1)}{q(x_1|x_{0})}]+\mathbb{E}_{q(x_{1:T}|x_0)}[log\prod_{t=2}^{T}\frac{p_{\theta}(x_{t-1}|x_{t})}{\frac{q(x_{t-1}|x_{t},x_0)q(x_t|x_0)}{q(x_{t-1}|x_0)}}] \\
&amp;=\mathbb{E}_{q(x_{1:T}|x_0)}[log\frac{p_{\theta}(x_T)p_{\theta}(x_0|x_1)}{\cancel{q(x_1|x_{0})}}]+\mathbb{E}_{q(x_{1:T}|x_0)}[log\prod_{t=2}^{T}\frac{p_{\theta}(x_{t-1}|x_{t})}{q(x_{t-1}|x_{t},x_0)}]+\mathbb{E}_{q(x_{1:T}|x_0)}[log\frac{\cancel{q(x_1|x_0)}}{q(x_{T}|x_0)}]\\
&amp;=\mathbb{E}_{q(x_{1:T}|x_0)}[log\frac{p_{\theta}(x_T)p_{\theta}(x_0|x_1)}{q(x_{T}|x_0)}]+\mathbb{E}_{q(x_{1:T}|x_0)}[\sum_{t=2}^{T}log\frac{p_{\theta}(x_{t-1}|x_{t})}{q(x_{t-1}|x_{t},x_0)}]\\
&amp;=\mathbb{E}_{q(x_{1}|x_0)}[logp_{\theta}(x_0|x_1)]+\mathbb{E}_{q(x_{T}|x_0)}[log\frac{p_{\theta}(x_T)}{q(x_{T}|x_0)}]+\mathbb{E}_{q(x_{t-1},x_t|x_0)}[\sum_{t=2}^{T}log\frac{p_{\theta}(x_{t-1}|x_{t})}{q(x_{t-1}|x_{t},x_0)}] \\
&amp;=\mathbb{E}_{q(x_{1}|x_0)}[logp_{\theta}(x_0|x_1)]+\mathbb{E}_{q(x_{T}|x_0)}[log\frac{p_{\theta}(x_T)}{q(x_{T}|x_0)}]+\sum_{t=2}^{T}\mathbb{E}_{q(x_{t-1},x_t|x_0)}[log\frac{p_{\theta}(x_{t-1}|x_{t})}{q(x_{t-1}|x_{t},x_0)}] \\
&amp;=\mathbb{E}_{q(x_{1}|x_0)}[logp_{\theta}(x_0|x_1)]-D_{KL}(q(x_{T}|x_0)||p_{\theta}(x_T))+\sum_{t=2}^{T}\mathbb{E}_{q(x_{t-1},x_t|x_0)}[log\frac{p_{\theta}(x_{t-1}|x_{t})}{q(x_{t-1}|x_{t},x_0)}]
\end{aligned}
\]</div><p></p><p>其中</p>
<p></p><div class="math display">\[\begin{aligned}
\mathbb{E}_{q(x_{t-1},x_t|x_0)}[log\frac{p_{\theta}(x_{t-1}|x_{t})}{q(x_{t-1}|x_{t},x_0)}] &amp;= \iint q(x_{t-1},x_t|x_0)log\frac{p_{\theta}(x_{t-1}|x_{t})}{q(x_{t-1}|x_{t},x_0)}dx_{t-1}dx_t \\
&amp;=\iint q(x_{t-1}|x_{t},x_0)q(x_{t}|x_{0})log\frac{p_{\theta}(x_{t-1}|x_{t})}{q(x_{t-1}|x_{t},x_0)}dx_{t-1}dx_t \\
&amp;=\int [\int log\frac{p_{\theta}(x_{t-1}|x_{t})}{q(x_{t-1}|x_{t},x_0)}q(x_{t-1}|x_{t},x_0)dx_{t-1}]q(x_{t}|x_0)dx_{t} \\
&amp;=\mathbb{E}_{q(x_{t}|x_0)}[-D_{KL}(q(x_{t-1}|x_{t},x_0)||p_{\theta}(x_{t-1}|x_{t}))]
\end{aligned}
\]</div><p></p><blockquote>
<p>[!NOTE] Title</p>
<p>注意上式不能将<span class="math inline">\(q(x_{t-1},x_t|x_0)\)</span>分解为<span class="math inline">\(q(x_t|x_{t-1})q(x_{t-1}|x_0)\)</span>，因为不管先对<span class="math inline">\(x_{t-1}\)</span>还是<span class="math inline">\(x_t\)</span>积分，都会<strong>在后续被积函数中作为条件存在</strong></p>
</blockquote>
<p>至此<strong>利用马尔可夫性质对完成了多元变量的消除</strong>工作：</p>
<p></p><div class="math display">\[\begin{aligned}
logp_{\theta}(x)&amp;\geq\underbrace{\mathbb{E}_{q(x_{1}|x_0)}[logp_{\theta}(x_0|x_1)]}_{重构项}-\underbrace{D_{KL}(q(x_{T}|x_0)||p_{\theta}(x_T))}_{正则项}+\underbrace{\sum_{t=2}^{T}\mathbb{E}_{q(x_{t}|x_0)}[-D_{KL}(q(x_{t-1}|x_{t},x_0)||p_{\theta}(x_{t-1}|x_{t}))]}_{去噪匹配项}
\end{aligned}
\]</div><p></p><p>也可以看到这里前两项<strong>与VAE具有相同的形式</strong>。</p>
<p>当<span class="math inline">\(T=1\)</span>时，即意味着只有一个潜变量<span class="math inline">\(x_1=z\)</span>，这时退化到与VAE的ELBO具有<strong>完全相同的表达式</strong>。</p>
<h4 id="elbo解析">ELBO解析</h4>
<p><span class="math inline">\(\sum_{t=2}^{T}\mathbb{E}_{q(x_{t}|x_0)}[-D_{KL}(q(x_{t-1}|x_{t},x_0)||p_{\theta}(x_{t-1}|x_{t}))]\)</span>是ELBO中占比最大的，优先看这个。</p>
<p>其中<span class="math inline">\(p_{\theta}(x_{t-1}|x_{t})\)</span>是模型参数化的结果，<span class="math inline">\(q(x_{t-1}|x_{t},x_0)\)</span>是模型需要靠近的对象（ground-truth）。</p>
<p>对[[001 DDPM-v2#后向生成过程|ground-truth的推导]]不再赘述，最终结果为</p>
<p></p><div class="math display">\[q(x_{t-1}|x_{t},x_0) = N(\frac{1}{\sqrt{\alpha_{t}}}[x_{t}-\frac{\beta_{t}}{\sqrt{\bar{\beta}_{t}}}\bar{\epsilon}_{t}], \frac{\beta_{t}\bar{\beta}_{t-1}}{\bar{\beta}_{t}}I)
\]</div><p></p><p>由于最终模型参数化<span class="math inline">\(p_{\theta}(x_{t-1}|x_{t})\)</span>是为了接近<span class="math inline">\(q(x_{t-1}|x_{t},x_0)\)</span>，那不妨：</p>
<ol>
<li>直接使用<span class="math inline">\(q(x_{t-1}|x_{t},x_0)\)</span>的方差：<span class="math inline">\(\frac{\beta_{t}\bar{\beta}_{t-1}}{\bar{\beta}_{t}}\)</span>；</li>
<li>参考<span class="math inline">\(q(x_{t-1}|x_{t},x_0)\)</span>均值的形式去设置预测的变量：<span class="math inline">\(\frac{1}{\sqrt{\alpha_{t}}}[x_{t}-\frac{\beta_{t}}{\sqrt{\bar{\beta}_{t}}}\bar{\epsilon}_{\theta}]\)</span></li>
</ol>
<p>代入上述假设，展开<span class="math inline">\(D_{KL}(q(x_{t-1}|x_{t},x_0)||p_{\theta}(x_{t-1}|x_{t}))\)</span></p>
<p></p><div class="math display">\[\begin{aligned}
D_{KL}(q(x_{t-1}|x_{t},x_0)||p_{\theta}(x_{t-1}|x_{t})) &amp;= D_{KL}(N(\frac{1}{\sqrt{\alpha_{t}}}[x_{t}-\frac{\beta_{t}}{\sqrt{\bar{\beta}_{t}}}\bar{\epsilon}_{t}], \frac{\beta_{t}\bar{\beta}_{t-1}}{\bar{\beta}_{t}}I)||N(\frac{1}{\sqrt{\alpha_{t}}}[x_{t}-\frac{\beta_{t}}{\sqrt{\bar{\beta}_{t}}}\bar{\epsilon}_{\theta}], \frac{\beta_{t}\bar{\beta}_{t-1}}{\bar{\beta}_{t}}I)) \\
\end{aligned}
\]</div><p></p><p>参考</p>
<p></p><div class="math display">\[D_{KL}(N(\mu_1,\sigma_1^2I)||N(\mu_2,\sigma_2^2))=\log\frac{\sigma_2}{\sigma_1}+\frac{\sigma_1^2+(\mu_1-\mu_2)^2}{2\sigma_2^2}-\frac{1}{2}
\]</div><p></p><p>得到最终的值为</p>
<p></p><div class="math display">\[\begin{aligned}
D_{KL}(q(x_{t-1}|x_{t},x_0)||p_{\theta}(x_{t-1}|x_{t})) &amp;= \log\frac{\sqrt{\frac{\beta_{t}\bar{\beta}_{t-1}}{\bar{\beta}_{t}}}}{\sqrt{\frac{\beta_{t}\bar{\beta}_{t-1}}{\bar{\beta}_{t}}}}+\frac{\frac{\beta_{t}\bar{\beta}_{t-1}}{\bar{\beta}_{t}}+(\frac{1}{\sqrt{\alpha_{t}}}[x_{t}-\frac{\beta_{t}}{\sqrt{\bar{\beta}_{t}}}\bar{\epsilon}_{t}]-\frac{1}{\sqrt{\alpha_{t}}}[x_{t}-\frac{\beta_{t}}{\sqrt{\bar{\beta}_{t}}}\bar{\epsilon}_{\theta}])^2}{2\frac{\beta_{t}\bar{\beta}_{t-1}}{\bar{\beta}_{t}}}-\frac{1}{2} \\
&amp;=\frac{\beta_t}{2\alpha_t\bar{\beta}_{t-1}}\Vert \bar{\epsilon}_{\theta}-\bar{\epsilon}_{t} \Vert^2
\end{aligned}
\]</div><p></p>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.40398073274189816" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-04-28 14:38">2025-04-28 14:38</span>&nbsp;
<a href="https://www.cnblogs.com/SuahiStudy">9镑15便士</a>&nbsp;
阅读(<span id="post_view_count">0</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18851636);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18851636', targetLink: 'https://www.cnblogs.com/SuahiStudy/p/18851636', title: '【深度学习】MLE视角下的VAE与DDPM损失函数推导' })">举报</a>
</div>
        