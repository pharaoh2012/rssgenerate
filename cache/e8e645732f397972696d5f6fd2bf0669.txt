
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/sunstrikes/p/18712147" title="发布于 2025-02-12 21:15">
    <span role="heading" aria-level="2">deepseek-v3 论文阅读</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                    <div id="cnblogs_post_description" style="display: none">
        
        deepseek-v3论文中 AIInfra相关的技术学习
    </div>
<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h2 id="模型结构">模型结构</h2>
<h3 id="mlamulti-head-latent-attention">MLA(Multi-Head Latent Attention)</h3>
<p>主要作用是在保证效果的基础上, 利用低秩压缩的原理优化kvCache, 加速推理, 同时节省训练显存.</p>
<p>先回忆下MHA, 在每个head上, 分别经过K, V生成 $ attnweights=(W_Qh_i)^T∗(W_Kh_j) $, 然后再乘上V得到: $attnsv=attnweights * (W_vh_j) $, 当输入的token一致时, 经过 <span class="math inline">\(W_k\)</span>计算的结果一致的, 所以就可以把经过K和V计算的中间结果缓存下来用于节省算力. 但序列变长也会导致KVCache的数量爆炸, 导致显存瓶颈.</p>
<p>在MLA上, 将所有head的K共有信息提取出来, 每个head都用相同的一个compress_K矩阵, 这样就能把多个头的K cache压缩成1个, 而不同head的k中的特有信息通过矩阵乘法交换律的特点, <span class="math inline">\(attnweights=(h_i^TW_Q^TW_k)∗(h_j)\)</span> , 把这部分信息转移到了Qhead里. 这样就能避免出现特有信息的丢失.</p>
<img src="https://img2023.cnblogs.com/blog/1439743/202502/1439743-20250212210810656-1628873306.png" alt="image-20250211165015019" style="zoom: 50%">
<p>最后，使用压缩后了共有信息的compress_k，和吸收了相异信息的q_head做计算，得到attn_weights.</p>
<h3 id="moe-auxiliary-loss-free-load-balancing">MOE Auxiliary-Loss-Free Load Balancing</h3>
<p>对MOE负载均衡的优化, 像之前的<a href="https://www.cnblogs.com/sunstrikes/p/18310517" target="_blank">Gshard算法</a>, 加一个辅助loss, 用来惩罚某个expert溢出的情况. 但是这种方法会影响模型精度, 所以deepseek设计了无辅助loss的负载均衡算法.</p>
<img src="https://img2023.cnblogs.com/blog/1439743/202502/1439743-20250212210806673-1029621396.png" alt="image-20250211172355437" style="zoom: 50%">
<p>解释上面公式列出的MOE步骤:</p>
<ol>
<li>经过gating网络对expert进行打分. 公式15</li>
<li>选出topk的expert, mask掉不是topk的expert, 公式14</li>
<li>对gating输出进行归一化, 使得向量加和为1</li>
<li>经过moe网络的结果为<span class="math inline">\(U_t\)</span>(即输入本身, 对应残差概念) + 经过共享expert的输出 + 经过门控筛选的expert的输出</li>
</ol>
<img src="https://img2023.cnblogs.com/blog/1439743/202502/1439743-20250212210803500-125269060.png" alt="image-20250211173142158" style="zoom: 50%">
<p>加了一个bias向量, 在每个train step里, 如果被选中的top expert溢出了, bias在更新的时候减去<span class="math inline">\(\gamma\)</span> (指定的bias更新超参数), 在没溢出的时候加上<span class="math inline">\(\gamma\)</span>.</p>
<img src="https://img2023.cnblogs.com/blog/1439743/202502/1439743-20250212210759420-54398955.png" alt="image-20250211173226486" style="zoom: 50%">
<h3 id="序列级辅助loss">序列级辅助loss</h3>
<img src="https://img2023.cnblogs.com/blog/1439743/202502/1439743-20250212210756466-1868016779.png" alt="image-20250211192125867" style="zoom: 50%">
<p>主要目的是防止单序列中的极度不平衡的情况出现, 加了个辅助loss用于避免这个情况.</p>
<p>T: 序列里的token数  <span class="math inline">\(\alpha\)</span> 是一个较小的超参数</p>
<p><span class="math inline">\(P_i\)</span> : 表示第i个专家在所有token里出现的平均概率.</p>
<p><span class="math inline">\(f_i\)</span>: 表示第i个专家在序列中被选择到的频率.</p>
<p>loss在minimize的时候, 如果<span class="math inline">\(P_i\)</span> 很大, 同时他被选择到的次数也很多这种极限情况, 就会导致loss很大, 而小的<span class="math inline">\(P_i\)</span>也有适当的选择频率后loss就会比较小</p>
<h3 id="multi-token-prediction">Multi-Token Prediction</h3>
<p>一次训练多个token, 提高数据利用效率. 另外和llama实现不一样的地方在于这里是顺序预测而不是并行预测.</p>
<p>注意: 上个module的output和下一个输入emb concat, 因为维度double, 需要进行linear projection进行降维</p>
<img src="https://img2023.cnblogs.com/blog/1439743/202502/1439743-20250212210752371-285981198.png" alt="image-20250211200554488" style="zoom: 50%">
<h2 id="infra优化">Infra优化</h2>
<p>并行方案: PP: 16    EP: 64   DP: zero1, 没有使用TP(通信量太大)</p>
<h3 id="pp优化dualpipe">PP优化(DualPipe)</h3>
<p>DualPipe看着是整个论文最重要的部分之一, 因为模型结构MOE的特性, 在MOE前后的2个AllToAll通信数据量非常大, 成为整个训练的瓶颈. DualPipe的实现主要借鉴了2个思想:</p>
<h4 id="backward拆分">BackWard拆分:</h4>
<p>ZeroBubble的主要目的是基于1F1B调度场景减少Bubble率, 在模型整体计算里, BP计算量是要比FP计算量大的, BP包含两个计算: 计算输入X的梯度,和计算参数W的梯度分别用<span class="math inline">\(B\)</span>和<span class="math inline">\(W\)</span>来表示, 流水线执行依赖X的梯度用于下个stage的计算, 但是对W梯度并没有串行限制, 这就可以通过拆分bp计算, 先算B再算W的方式减小Bubble.</p>
<img src="https://img2023.cnblogs.com/blog/1439743/202502/1439743-20250212210748489-760971249.png" alt="image-20250212201625280" style="zoom: 50%">
<h4 id="双流pp">双流PP:</h4>
<p>双向流水线的主要设计思路在于: 如果存储2份模型副本, 同时在两个方向上开始训练. 那么副本0的FP和副本1的BP就能没有依赖关系的完全并行起来. 通过这种方式来减少流水线空等耗时.</p>
<p>蓝色和黄色块表示向下的前向和反向传播，绿色和橙色表示向上的传播</p>
<img src="https://img2023.cnblogs.com/blog/1439743/202502/1439743-20250212210744565-185218412.png" alt="image-20250212201954136" style="zoom: 50%">
<p>基于上面两个算法的思想出现了DualPipe</p>
<img src="https://img2023.cnblogs.com/blog/1439743/202502/1439743-20250212210741052-946013848.png" alt="image-20250212202733971" style="zoom: 50%">
<p>以 8PP 为例：</p>
<ul>
<li>Stage 0 上有 Layer 0, 1 以及 Layer 14, 15 的权重</li>
<li>Stage 1 上有 Layer 2, 3 以及 Layer 12, 13 的权重</li>
<li>Stage 7 上有 Layer 14, 15 以及 Layer 0, 1 的权重</li>
<li>相当于有 2 份相同的模型副本，Forward 的顺序可以从 Stage 0 到 7，也可以从 Stage 7 到 0。(因为deepseek设置的EP很大, 增加一份参数副本不会有太大的显存负担)</li>
</ul>
<h4 id="通信overlap">通信overlap:</h4>
<img src="https://img2023.cnblogs.com/blog/1439743/202502/1439743-20250212210737174-918476045.png" alt="image-20250211202826318" style="zoom: 50%">
<ol>
<li>限制每个token最多被发到4个node上, 从而减少IB流量</li>
<li>NVLink 提供 160 GB/s 带宽，大约是 IB（50 GB/s）的 3.2倍. 当每个节点最多选3.2个专家的时候, IB和nvlink的通信可以完全overlap住. 所以deepseek选择8个expert是有通信上专门考虑的(&lt; 4*3.2=12.8)</li>
<li>对于每个 Token，在做出路由决策时，首先通过 IB 传输到其目标节点上具有相同节点内索引的 GPU。一旦到达目标节点，将努力确保它通过 NVLink 立即转发到承载目标专家的特定 GPU，而不会被随后到达的 Token 阻塞。（PS：比如说，节点 A 上 GPU 0 的 Token 要发送到节点 B 上的 GPU 3，则对应的路径为：节点 A GPU 0 -&gt; 节点 B GPU 0 -&gt; 节点 B GPU 3。这样做是因为高性能 GPU 训练集群往往会采用轨道优化，同号 GPU 在一个 Leaf Switch 下，如下图所示，因此可以利用高速的 NVLink 来代替从 Leaf Switch 到 Spine Switch 的流量，从而降低 IB 通信时延，并且减少 Leaf Switch 和 Spine Switch 之间的流量）</li>
<li>Dispatching工作: （1）InfiniBand（IB）发送、（2）IB 至 NVLink 转发，以及（3）NVLink接收分别由相应的 warp 处理</li>
<li>combing工作: （1）NVLink 发送、（2）NVLink 至 IB 转发与累加，以及（3）IB 接收与累加</li>
<li>这里之所以使用定制化的PTX优化, 是为了自动调整dispatching/Combining这两个通信kernel占用的warp数, 从而减少L2 cache使用率避免影响计算中的SM.</li>
</ol>
<h3 id="显存优化手段">显存优化手段</h3>
<ol>
<li>
<p><a href="https://www.cnblogs.com/sunstrikes/p/18313798#%E9%80%89%E6%8B%A9%E6%80%A7%E6%BF%80%E6%B4%BB%E9%87%8D%E8%AE%A1%E7%AE%97" target="_blank">选择性激活重计算</a>: 经典优化手段, 把RMSNorm和MLA里上投影 在bp需要用到的fp计算出的激活放弃缓存, 在bp时重新计算. 通过重计算的方式节省显存.</p>
</li>
<li>
<p>指数移动平均（Exponential Moving Average, EMA）通过赋予近期数据更高权重，平滑时间序列数据，常用于评估大语言模型（LLM）的训练稳定性和收敛性。</p>
<ul>
<li><strong>平滑损失曲线</strong>：训练损失通常波动较大，EMA 可生成平滑曲线，更容易识别长期趋势（如持续下降或平台期）。</li>
<li>如果EMA 损失持续下降，说明模型有效学习；若剧烈波动，可能需调整超参数（如学习率）。</li>
</ul>
<p>deepseek把EMA参数在训练完成后异步传入到内存里, 用来评估模型训练是否正常.通过这种方式来节省显存.</p>
</li>
<li>
<p>DualPipe的PP设计, 使得embedding layer和output head在同一个PP stage上, 这种设计可以让MTP实现在物理显存里共享embed和output的参数和梯度, 节省存储.</p>
</li>
</ol>
<h3 id="fp8训练">FP8训练</h3>
<h4 id="混合精度优化">混合精度优化</h4>
<p>对比主流BF16的实现方案, 主要区别在于以下几点:</p>
<ol>
<li>激活/optimizer states 在训练过程中的存储其实主要还是BF16格式, 只是在计算前转成FP8</li>
<li>GEMM优化后, 输入是FP8, 输出是FP32, 再转回BF16进行数据流转.</li>
<li>FP和BP用到的weight, 从BF16变成了FP8.</li>
<li>不是所有的计算都变成了FP8, 部分对精度要求高的网络结构运算的时候还是使用BF16/FP32: <code>the embedding module, the output head, MoE gating modules, normalization operators, and attention operators</code></li>
<li>在MOE dispatch之前的激活进行FP8量化再进行allToAll通信, 但是在combine的时候需要保持BF16保证训练精度</li>
</ol>
<img src="https://img2023.cnblogs.com/blog/1439743/202502/1439743-20250212210732859-403335056.png" alt="image-20250211205216070" style="zoom: 50%">
<h4 id="量化优化">量化优化</h4>
<p>重点是解决FP8上下界溢出的问题. 通过分组scale的方式进行</p>
<img src="https://img2023.cnblogs.com/blog/1439743/202502/1439743-20250212210728829-1841132485.png" alt="image-20250212192113567" style="zoom: 50%">
<p>对激活量化: 每128分一个block进行scale系数计算</p>
<p>对W量化: 128*128的block进行量化(TODO: 没看懂input channel和output channel的意思)</p>
<h4 id="mma矩阵相加精度优化">MMA(矩阵相加)精度优化</h4>
<img src="https://img2023.cnblogs.com/blog/1439743/202502/1439743-20250212210725194-499457573.png" alt="image-20250212193945336" style="zoom: 50%">
<p>FP8 GEMM的精度损失主要受限于在GEMM计算每个子矩阵移动累加的时候, 容易超出FP8的精度范围. deepseek的解决方法是每执行<span class="math inline">\(N_c =128\)</span>次MMA之后, 就把累加结果复制到FP32的cudacore里面, 进行全精度的FP32累加. 而且结合在之前细粒度量化里的scaling Factor, 可以很方便的在cudacore里进行反量化.</p>
<p>这个修改虽然会降低单个WGMMA的指令发射效率, 但在H800架构上通常由2个WGMMA在并发执行, 一个warpgroup在进行promotion操作的时候, 另一个在进行MMA计算, 上面的搬运操作可以被这两个操作overlap掉.</p>
<p><strong>promotion</strong>解释:</p>
<p>指的是多个线程束（warp，通常由32个线程组成）通过协作将数据从低层次内存（如全局内存）高效地提升到高层次内存（如共享内存或寄存器）的优化操作。这一过程旨在减少全局内存访问延迟并提高内存带宽利用率，通常在需要高性能计算的场景（如矩阵乘法、张量核心操作）中尤为重要。</p>
<h2 id="参考">参考:</h2>
<p>MLA解析: <a href="https://mp.weixin.qq.com/s/E7NwwMYw14FRT6OKzuVXFA" target="_blank" rel="noopener nofollow">https://mp.weixin.qq.com/s/E7NwwMYw14FRT6OKzuVXFA</a></p>
<p>nvidia FP8 bench: <a href="https://developer.nvidia.com/zh-cn/blog/nvidia-gpu-fp8-training-inference/" target="_blank" rel="noopener nofollow">https://developer.nvidia.com/zh-cn/blog/nvidia-gpu-fp8-training-inference/</a></p>
<p>EMA解释: <a href="https://zhuanlan.zhihu.com/p/554955968" target="_blank" rel="noopener nofollow">https://zhuanlan.zhihu.com/p/554955968</a></p>
<p>Zero Bubble论文: <a href="https://arxiv.org/abs/2401.10241" target="_blank" rel="noopener nofollow">https://arxiv.org/abs/2401.10241</a> zero Bubble分析: <a href="https://zhuanlan.zhihu.com/p/681363624" target="_blank" rel="noopener nofollow">https://zhuanlan.zhihu.com/p/681363624</a></p>
<p>Chimera论文: <a href="https://arxiv.org/abs/2107.06925" target="_blank" rel="noopener nofollow">https://arxiv.org/abs/2107.06925</a></p>
<p>Deepseek-V3解读: <a href="https://mp.weixin.qq.com/s/DKdXcguKcCS5gcwIRLH-Cg" target="_blank" rel="noopener nofollow">https://mp.weixin.qq.com/s/DKdXcguKcCS5gcwIRLH-Cg</a></p>
<p>Warp serialization解释(DeepSeek R1):</p>
<ul>
<li><strong>性能瓶颈</strong>：当所有Warp执行相同任务时，可能出现资源争用（如计算单元或内存带宽）或依赖延迟（如全局内存访问），导致SM（流多处理器）利用率低下。</li>
<li><strong>解决方案</strong>：通过为不同Warp分配不同角色，允许它们异步协同工作，隐藏延迟并提高吞吐量。</li>
</ul>
<p>Warp Specialization主要通过以下方式实现:</p>
<ul>
<li>
<p><strong>任务划分</strong>：</p>
<ul>
<li><strong>计算型Warp</strong>：专注于执行密集计算任务（如矩阵乘法、物理模拟）。</li>
<li><strong>内存型Warp</strong>：负责全局内存与共享内存之间的数据搬运（如预取数据或存储结果）。</li>
<li><strong>同步型Warp</strong>：管理线程间同步或与其他Warp的协调。</li>
</ul>
<p>这种划分类似于CPU中的<strong>生产者-消费者模型</strong>，通过流水线化减少等待时间。</p>
</li>
<li>
<p><strong>编程实现</strong>：</p>
<ul>
<li><strong>条件分支</strong>：在同一Kernel中使用 <code>if (warp_id % N == 0)</code> 划分不同任务路径（需结合 <code>__shfl_sync</code> 等同步操作）。</li>
<li><strong>协作组（Cooperative Groups）</strong>：利用CUDA 9+引入的协作组API（如 <code>coalesced_group</code> 或 <code>thread_block_tile</code>），显式控制Warp的行为。</li>
<li><strong>动态并行</strong>：在支持CUDA Dynamic Parallelism的架构中，子Kernel可特定化执行某些任务。</li>
</ul>
</li>
</ul>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.03987168800925926" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-02-12 21:17">2025-02-12 21:15</span>&nbsp;
<a href="https://www.cnblogs.com/sunstrikes">SunStriKE</a>&nbsp;
阅读(<span id="post_view_count">11</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18712147" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18712147);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18712147', targetLink: 'https://www.cnblogs.com/sunstrikes/p/18712147', title: 'deepseek-v3 论文阅读' })">举报</a>
</div>
        