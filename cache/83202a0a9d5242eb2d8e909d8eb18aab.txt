
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/Big-Yellow/p/18940170" title="发布于 2025-06-21 15:23">
    <span role="heading" aria-level="2">深入浅出了解生成模型-3：Diffusion模型原理以及代码</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p>更加好的排版：<a href="https://www.big-yellow-j.top/posts/2025/05/19/DiffusionModel.html" target="_blank" rel="noopener nofollow">https://www.big-yellow-j.top/posts/2025/05/19/DiffusionModel.html</a><br>
前文已经介绍了VAE以及GAN这里介绍另外一个模型：Diffusion Model，除此之外介绍Conditional diffusion model、Latent diffusion model</p>
<h2 id="diffusion-model">Diffusion Model</h2>
<p>diffusion model（后续简称df）模型原理很简单：<em>前向过程</em>在一张图像基础上不断添加噪声得到一张新的图片之后，<em>反向过程</em>从这张被添加了很多噪声的图像中将其还原出来。原理很简单，下面直接介绍其数学原理：<br>
<img src="https://s2.loli.net/2025/06/21/7jOQ3YgHxMwouzG.webp" alt="https://arxiv.org/pdf/2208.11970" loading="lazy"></p>
<blockquote>
<p>上图中实线代表：反向过程（去噪）；虚线代表：前向过程（加噪）</p>
</blockquote>
<p>那么我们假设最开始的图像为 <span class="math inline">\(x_0\)</span>通过不断添加噪声（添加噪声过程假设为<span class="math inline">\(t\)</span>）那么我们的 <strong>前向过程</strong>：<span class="math inline">\(q(x_1,...,x_T\vert x_0)=q(x_0)\prod_{t=1}^T q(x_t\vert x_{t-1})\)</span>，同理 <strong>反向过程</strong>：<span class="math inline">\(p_\theta(x_0,...\vert x_{T})=p(x_T)\prod_{t=1}^Tp_\theta(x_{t-1}\vert x_t)\)</span></p>
<h3 id="前向过程">前向过程</h3>
<p>在df的前向过程中：</p>
<p></p><div class="math display">\[q(x_1,...,x_T\vert x_0)=q(x_0)\prod_{t=1}^T q(x_t\vert x_{t-1})
\]</div><p></p><p>通常定义如下的高斯分布：<span class="math inline">\(q(x_t\vert x_{t-1})=N(x_t;\sqrt{1-\beta_t}x_{t-1},\beta_tI)\)</span>，其中参数<span class="math inline">\(\beta\)</span>就是我们的 <strong>噪声调度</strong>参数来控制我们每一步所添加的噪声的“权重”（这个权重可以固定也可以时间依赖，对于时间依赖很好理解最开始图像是“清晰”的在不断加噪声过程中图像变得越来越模糊），于此同时随着不断的添加噪声那么数据<span class="math inline">\(x_0\)</span>就会逐渐的接近标准正态分布 <span class="math inline">\(N(0,I)\)</span>的 <span class="math inline">\(x_t\)</span>，整个加噪过程就为：</p>
<p></p><div class="math display">\[\begin{align*}
t=1 \quad &amp; x_1 = \sqrt{1 - \beta_1} x_0 + \sqrt{\beta_1} \epsilon_1 \\
t=2 \quad &amp; x_2 = \sqrt{1 - \beta_2} x_1 + \sqrt{\beta_2} \epsilon_2 \\
&amp;\vdots \\
t=T \quad &amp; x_T = \sqrt{1 - \beta_T} x_{T-1} + \sqrt{\beta_T} \epsilon_T
\end{align*}
\]</div><p></p><p>在上述过程中我们可以将<span class="math inline">\(t=1\)</span>得到的 <span class="math inline">\(x_1\)</span>代到下面 <span class="math inline">\(t=2\)</span>的公式中，类似的我们就可以得到下面的结果：<span class="math inline">\(x_2=\sqrt{(1-\beta_2)(1-\beta_1)}x_0+ \sqrt{1-(1-\beta_2)(1-\beta_1)}\epsilon\)</span> （之所以用一个<span class="math inline">\(\epsilon\)</span>是因为上面两个都是服从相同高斯分布就可以直接等同过来）那么依次类推就可以得到下面结果：</p>
<p></p><div class="math display">\[\begin{align*}
    x_T=\sqrt{(1-\beta_1)\dots(1-\beta_T)}x_0+ \sqrt{1-(1-\beta_1)\dots(1-\beta_T)}\epsilon \\
\Rightarrow x_T=\sqrt{\bar{\alpha_T}}x_0+ \sqrt{1-\bar{\alpha_T}}\epsilon
\end{align*}
\]</div><p></p><p>其中：<span class="math inline">\(\bar{\alpha_T}=\sqrt{(1-\beta_1)\dots(1-\beta_T)}\)</span>，那么也就是说对于前向过程（加噪过程）可以从<span class="math inline">\(x_0\)</span>到 <span class="math inline">\(x_T\)</span>一步到位，不需要说再去逐步计算中间状态了。</p>
<h3 id="反向过程">反向过程</h3>
<p><strong>反向过程</strong>：<span class="math inline">\(p_\theta(x_0,...\vert x_{T})=p(x_T)\prod_{t=1}^Tp_\theta(x_{t-1}\vert x_t)\)</span>，也就是从最开始的标准正态分布的 <span class="math inline">\(x_t\)</span>逐步去除噪声最后还原得到 <span class="math inline">\(x_0\)</span>。仔细阅读上面提到的前向和反向过程中都是条件概率但是在反向传播过程中会使用一个参数<span class="math inline">\(\theta\)</span>，这是因为前向过程最开始的图像和噪声我们是都知道的，而反向过程比如<span class="math inline">\(p(x_{t-1}\vert x_t)\)</span>是难以直接计算的，需要知道整个数据分布，因此我们可以通过神经网路去近似这个分布，而这个神经网络就是我们的参数：<span class="math inline">\(\theta\)</span>。于此同时反向过程也会建模为正态分布：<span class="math inline">\(p_\theta(x_{t-1}\vert x_t)=N(x_{t-1};\mu_\theta(x_t,t),\sum_\theta(x_t,t))\)</span>，其中 <span class="math inline">\(\sum_\theta(x_t,t)\)</span>为我们的方差对于在值可以固定也可以采用网络预测<sup class="footnote-ref"><a href="#fn1" id="fnref1" rel="noopener nofollow">[1]</a></sup></p>
<blockquote>
<p>在OpenAI的Improved DDPM中使用的就是使用预测的方法：<span class="math inline">\(\sum_\theta(x_t,t)=\exp(v\log\beta_t+(1-v)\hat{\beta_t})\)</span>，直接去预测系数：<span class="math inline">\(v\)</span></p>
</blockquote>
<p>回顾一下生成模型都在做什么。在<a href="./2025-05-08-GAN.md" target="_blank" rel="noopener nofollow">GAN</a>中是通过 <em>生成器网络</em> 来拟合正式的数据分布也就是是 <span class="math inline">\(G_\theta(x)≈P(x)\)</span>，在 <a href="./2025-05-11-VAE.md" target="_blank" rel="noopener nofollow">VAE</a>中则是通过将原始的数据分布通过一个 低纬的<strong>潜在空间</strong>来表示其优化的目标也就是让 <span class="math inline">\(p_\theta(x)≈p(x)\)</span>，而在Diffusion Model中则是直接通过让我们 去噪过程得到结果 和 加噪过程结果接近，什么意思呢？df就像是一个无监督学习我所有的GT都是知道的（每一步结果我都知道）也就是是让：<span class="math inline">\(p_\theta(x_{t-1}\vert x_t)≈p(x_{t-1}\vert x_t)\)</span> 换句话说就是让我们最后解码得到的数据分布和正式的数据分布相似：<span class="math inline">\(p_\theta(x_0)≈p(x_0)\)</span> 既然如此知道我们需要优化的目标之后下一步就是直接构建损失函数然后去优化即可。</p>
<h3 id="优化过程">优化过程</h3>
<p>通过上面分析，发现df模型的优化目标和VAE的优化目标很相似，其损失函数也是相似的，首先我们的优化目标是最大化下面的边际对数似然<sup class="footnote-ref"><a href="#fn2" id="fnref2" rel="noopener nofollow">[2]</a></sup>：<span class="math inline">\(\log p_\theta(x_0)=\log \int_{x_{1:T}}p_\theta(x_0,x_{1:T})dx_{1:T}\)</span>，对于这个积分计算是比较困难的，因此引入：<span class="math inline">\(q(x_{1:T}\vert x_0)\)</span> 那么对于这个公式有：</p>
<p></p><div class="math display">\[\begin{align*}
    \log p_\theta(x_0)&amp;=\log \int_{x_{1:T}}p_\theta(x_{0:T})dx_{1:T} \\
    &amp;=\log \int_{x_{1:T}} q(x_{1:T}\vert x_0)\frac{p_\theta(x_{0:T})}{q(x_{1:T}\vert x_0)}dx_{1:T}\\
    &amp;=\log\mathbb{E}_{q(x_{1:T|x_0})}[\frac{p_\theta(x_{0:T})}{q(x_{1:T}\vert x_0)}]\\
    &amp;≥\mathbb{E}_{q(x_{1:T|x_0})}[\log \frac{p_\theta(x_{0:T})}{q(x_{1:T}\vert x_0)}]\\
    &amp;=\underbrace{\mathbb{E}_{q(\boldsymbol{x}_1|\boldsymbol{x}_0)}[\log p_\theta(\boldsymbol{x}_0|\boldsymbol{x}_1)]}_{\text{reconstruction term}} - \underbrace{\mathbb{E}_{q(\boldsymbol{x}_{T-1}|\boldsymbol{x}_0)}[D_{\text{KL}}(q(\boldsymbol{x}_T|\boldsymbol{x}_{T-1})\parallel p(\boldsymbol{x}_T))]}_{\text{prior matching term}} - \sum_{t=1}^{T-1} \underbrace{\mathbb{E}_{q(\boldsymbol{x}_{t-1},\boldsymbol{x}_{t+1}|\boldsymbol{x}_0)}[D_{\text{KL}}(q(\boldsymbol{x}_t|\boldsymbol{x}_{t-1})\parallel p_\theta(\boldsymbol{x}_t|\boldsymbol{x}_{t+1})]}_{\text{consistency term}}\\
    &amp;=\underbrace{\mathbb{E}_{q(\boldsymbol{x}_1|\boldsymbol{x}_0)}[\log p_\theta(\boldsymbol{x}_0|\boldsymbol{x}_1)]}_{\text{reconstruction term}} - 
    \underbrace{D_{KL}(q(x_T|x_0)||p(x_T))}_{\text{prior matching term}} - 
    \sum_{t=2}^{T} \underbrace{\mathbb{E}_{q(\boldsymbol{x}_{t}|\boldsymbol{x}_0)}[D_{\text{KL}}(q(\boldsymbol{x}_{t-1}|\boldsymbol{x}_{t}, x_0)\parallel p_\theta(\boldsymbol{x}_{t-1}|\boldsymbol{x}_{t})]}_{\text{denoisiong matching term}}
\end{align*}
\]</div><p></p><p>中间化简步骤可以见论文<sup class="footnote-ref"><a href="#fn2" id="fnref2:1" rel="noopener nofollow">[2:1]</a></sup>中的描述（论文里面有两个推导，推导步骤直接省略，第二个等式： <span class="math inline">\(q(x_t\vert x_{t-1})=q(x_t\vert x_{t-1},x_0)\)</span>），那么上面结果分析，在计算我们的参数<span class="math inline">\(\theta\)</span>时候（反向传播求导计算）第2项直接为0，第1项可以直接通过蒙特卡洛模拟就行计算，那么整个结果就只有第三项，因此对于第二个灯饰为例可以将优化目标变为：<span class="math inline">\(\text{arg}\min_\theta D_{KL}(q(x_{t-1}\vert x_t, x_0)\Vert p_\theta(x_{t-1}\vert x_t))\)</span><br>
对于这个优化目标根据论文<sup class="footnote-ref"><a href="#fn3" id="fnref3" rel="noopener nofollow">[3]</a></sup>可以得到：</p>
<p></p><div class="math display">\[L_{\mathrm{simple}}=\mathbb{E}_{t,\mathbf{x}_0,\epsilon}\left[\left\|\epsilon-\epsilon_\theta(\sqrt{\bar{\alpha}_t}\mathbf{x}_0+\sqrt{1-\bar{\alpha}_t}\epsilon,t)\right\|^2\right]
\]</div><p></p><p>最终，训练目标是让神经网络 <span class="math inline">\(\epsilon_\theta\)</span> 准确预测前向过程中添加的噪声，从而实现高效的去噪生成,因此整个DF模型训练和<strong>采样过程</strong>就变为<sup class="footnote-ref"><a href="#fn3" id="fnref3:1" rel="noopener nofollow">[3:1]</a></sup>：</p>
<p><img src="https://s2.loli.net/2025/06/21/STVkbaQ65rhZLdU.webp" alt="" loading="lazy"></p>
<p>比如说下面一个例子：对于输入数据<span class="math inline">\(x_0=[1,2]\)</span> 于此同时假设我们的采样噪声 <span class="math inline">\(\epsilon \in[0.5, -0.3]\)</span>并且进行500次加噪声处理，假设<span class="math inline">\(\bar{\alpha}_{500} = 0.8\)</span>那么计算500次加噪得到结果为：</p>
<p></p><div class="math display">\[x_t=\sqrt{\bar{\alpha_t}}x_0+ \sqrt{1-\bar{\alpha_t}}\epsilon=\sqrt{0.8}\times[1,2]+\sqrt{0.2}[0.5, -0.3]≈[1.118,1.654]
\]</div><p></p><p><strong>关键在于损失函数</strong>，通过上面简化过程可以直接通过模型预测噪声因此可以直接计算<span class="math inline">\(\epsilon_\theta(x_t,t)=[0.48，-0.28]\)</span>然后去计算loss即可。<strong>直接上代码</strong>，代码实现上面过程可以自定义实现/使用<code>diffusers</code><sup class="footnote-ref"><a href="#fn4" id="fnref4" rel="noopener nofollow">[4]</a></sup><br>
<strong>diffusers</strong>实现简易demo</p>
<pre><code class="language-python">from diffusers import DDPMScheduler

# 直接加载训练好的调度器
# scheduler = DDPMScheduler.from_pretrained("google/ddpm-cat-256")
# 初始化调度器
scheduler = DDPMScheduler(num_train_timesteps=1000) #添加噪声步数
...
for image in train_dataloader:
    # 假设 image为 32，3，128，128
    noise = torch.randn(image.shape, device=image.device)
    timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, 
                                      (image.shape[0],), device=image.device, dtype=torch.int64)
    noisy_images = scheduler.add_noise(image, noise, timesteps) # 32 3 128 128
    ...
    noise_pred = model(noisy_images)
    loss = F.mse_loss(noise_pred, noise)
    ...

</code></pre>
<h2 id="conditional-diffusion-model">Conditional Diffusion Model</h2>
<p>条件扩散模型（Conditional Diffusion Model）<sup class="footnote-ref"><a href="#fn5" id="fnref5" rel="noopener nofollow">[5]</a></sup>顾名思义就是在使用DF过程中添加一个 <em>限定条件</em>（文本、图像等）来指导模型的生成（原理很简单，而且 <em>条件扩散模型</em>这个概念比较广泛，只要在生成图片过程中加上一个“条件”），这里主要介绍OpenAI的论文来解释 <em>条件扩散模型</em>。<br>
<img src="https://s2.loli.net/2025/06/21/wWsalLZAjrkq719.webp" alt="image.png" loading="lazy"></p>
<p>在论文里面提到了一点： <strong>可以通过文本来提升模型的生成质量</strong>。主要了解一下对于条件如何嵌入到模型中：<br>
1、直接相加范式：这类主要就是将文本、标签进行编码之后直接和 <strong>噪声</strong>/ <strong>时间步</strong>进行相加而后进行后续实验；<br>
2、注意力融合范式：比如下面的Stable Diffusion直接将文本编码之后融入到注意力里面进行计算</p>
<h2 id="latent-diffusion-model">Latent Diffusion Model</h2>
<p>对于Latent Diffusion Model（LDM）<sup class="footnote-ref"><a href="#fn6" id="fnref6" rel="noopener nofollow">[6]</a></sup>主要出发点就是：最开始的DF模型在像素空间（高纬）进行评估这是消耗计算的，因此LDF就是直接通过对 <strong>autoencoding model</strong>得到的 <em>潜在空间</em>（低维）进行建模。整个思路就比较简单，用降低维度的潜在空间来进行建模，整个模型结构为（<a href="#unet%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84" rel="noopener nofollow">代码操作</a>）：<br>
<img src="https://s2.loli.net/2025/06/21/5eTfQwG6tLDpycv.webp" alt="image.png" loading="lazy"></p>
<p>对于上述过程，输入图像为<span class="math inline">\(x=[3,H,W]\)</span>而后通过encoder将其转化为 潜在空间（<span class="math inline">\(z=\varepsilon(x)\)</span>）而后直接在潜在空间 <span class="math inline">\(z\)</span>进行扩散处理得到<span class="math inline">\(z_T\)</span>直接对这个<span class="math inline">\(z_T\)</span>通过U-Net进行建模，整个过程比较简单。不过值得注意的是在U-Net里面因为可能实际使用DF时候会有一些特殊输入（文本、图像等）因此会对这些内容通过一个encoder进行编码得到：<span class="math inline">\(\tau_\theta(y)\in R^{M\times d_\tau}\)</span>，而后直接进行注意力计算：</p>
<p></p><div class="math display">\[\text{Attention}(Q,K,V)=\text{softmax}(\frac{QK^T}{\sqrt{d}})V
\]</div><p></p><p>其中：<span class="math inline">\(Q=W_{Q}^{(i)}\cdot\varphi_{i}(z_{t}),K=W_{K}^{(i)}\cdot\tau_{\theta}(y),V=W_{V}^{(i)}\cdot\tau_{\theta}(y)\)</span>并且各个参数维度为：<span class="math inline">\(W_V^{i}\in R^{d\times d_\epsilon^i},W_Q^i\in R^{d\times d_\tau},W_k^i\in R^{d\times d_\tau}\)</span></p>
<h2 id="df模型生成">DF模型生成</h2>
<h3 id="ddpm">DDPM</h3>
<p>最开始上面有介绍如何使用DF模型来进行生成，比如说在DDPM中生成范式为：<br>
<img src="https://s2.loli.net/2025/06/21/STVkbaQ65rhZLdU.webp" alt="" loading="lazy"></p>
<p>也就是说DDPM生成为：</p>
<p></p><div class="math display">\[x_{t-1}=\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{1- \alpha_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_\theta(x_t,t)\right)+\sigma_tz,\quad z\sim\mathcal{N}(0,I)
\]</div><p></p><p>但是这种生成范式存在问题，比如说T=1000那就意味着一张“合格”图片就需要进行1000次去噪如果1次是为为0.1s那么总共时间大概是100s如果要生产1000张图片那就是：1000x1000x0.1/60≈27h。这样时间花销就会比较大</p>
<h3 id="ddim">DDIM</h3>
<p>最开始在介绍DDPM中将图像的采样过程定义为马尔科夫链过程，而DDIM<sup class="footnote-ref"><a href="#fn7" id="fnref7" rel="noopener nofollow">[7]</a></sup>则是相反直接定义为：非马尔科夫链过程<br>
<img src="https://s2.loli.net/2025/06/21/pwIndituAKX4kjh.webp" alt="" loading="lazy"></p>
<p>并且定义图像生成过程为：</p>
<p></p><div class="math display">\[x_{t-1}=\sqrt{\alpha_{t-1}}\left(\frac{x_t-\sqrt{1-\alpha_t}\epsilon_\theta(x_t,t)}{\sqrt{\alpha_t}}\right)+\sqrt{1-\alpha_{t-1}-\sigma_t^2}\epsilon_\theta(x_t,t)+\sigma_tz
\]</div><p></p><h2 id="代码操作">代码操作</h2>
<h3 id="df模型结构">DF模型结构</h3>
<blockquote>
<p>通过上面分析，知道对于 <span class="math inline">\(x_T=\sqrt{\bar{\alpha_T}}x_0+ \sqrt{1-\bar{\alpha_T}}\epsilon\)</span>通过这个方式添加噪声，但是实际因为时间是一个标量，就像是最开始的位置编码一样，对于这些内容都会通过“类似位置编码”操作一样将其<strong>进行embedding处理然后在模型里面</strong>一般输入的参数也就是这三部分：<code>noise_image</code>, <code>time_step</code>, <code>class_label</code></p>
</blockquote>
<h4 id="dit模型">Dit模型</h4>
<p>将Transformer使用到Diffusion Model中，而Dit<sup class="footnote-ref"><a href="#fn8" id="fnref8" rel="noopener nofollow">[8]</a></sup>在论文中进行的操作：通过一个autoencoder来将图像压缩为低维度的latent，扩散模型用来生成latent，然后再采用autoencoder来重建出图像，比如说在Dit中使用KL-f8对于输入图像维度为：256x256x3那么压缩得到的latent为32x32x4。Dit的模型结构为：<br>
<img src="https://s2.loli.net/2025/06/21/d2MSPBkTt9aIlZi.webp" alt="image.png" loading="lazy"></p>
<p>模型输入参数3个分别为：1、低纬度的latent；2、标签label；3、时间步t。对于latent直接通过一个patch embed来得到不同的patch（得到一系列的token）而后将其和位置编码进行相加得到最后的embedding内容，直接结合代码<sup class="footnote-ref"><a href="#fn9" id="fnref9" rel="noopener nofollow">[9]</a></sup>来解释模型：<br>
假设模型的输入为：</p>
<pre><code class="language-python">#Dit参数为：DiT(depth=12, hidden_size=384, patch_size=4, num_heads=6)
batch_size= 16
image = torch.randn(batch_size, 4, 32, 32).to(device)
t = torch.randint(0, 1000, (batch_size,)).to(device)
y = torch.randint(0, 1000, (batch_size,)).to(device)
</code></pre>
<p>那么对与输入分别都进行embedding处理：1、<strong>Latent Embedding：得到（8，64，384）</strong>，因为patchembedding直接就是假设我们的patch size为4那么每个patch大小为：4x4x4=64并且得到32/4* 32/4=64个patches，而后通过线linear处理将64映射为hidden_size=384；2、<strong>Time Embedding和Label Embedding：得到（8，384）（8，384）</strong>，因为对于t直接通过sin进行编码，对于label在论文里面提到使用 <em>classifier-free guidance</em>方式，具体操作就是在<strong>训练过程中</strong>通过<code>dropout_prob</code>来将输入标签<strong>随机</strong>替换为无标签来生成无标签的向量，在 <strong>推理过程</strong>可以通过 <code>force_drop_ids</code>来指定某些例子为无条件标签。将所有编码后的内容都通过补充位置编码信息（latent embedding直接加全是1，而label直接加time embedding），补充完位置编码之后就直接丢到 <code>DitBlock</code>中进行处理，对于<code>DitBlock</code>结构：</p>
<pre><code class="language-python">def forward(self, x, c):
    shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(c).chunk(6, dim=1)
    x = x + gate_msa.unsqueeze(1) * self.attn(modulate(self.norm1(x), shift_msa, scale_msa))
    x = x + gate_mlp.unsqueeze(1) * self.mlp(modulate(self.norm2(x), shift_mlp, scale_mlp))
    return x
</code></pre>
<p>在这个代码中不是直接使用注意力而是使用通过一个 <code>modulate</code>这个为了实现将传统的layer norm（<span class="math inline">\(\gamma{\frac{x- \mu}{\sigma}}+ \beta\)</span>）改为动态的<span class="math inline">\(\text{scale}{\frac{x- \mu}{\sigma}}+ \text{shift}\)</span>，直接使用动态是为了允许模型根据时间步和类标签调整 Transformer 的行为，使生成过程更灵活和条件相关，除此之外将传统的残差连接改为 权重条件连接 <span class="math inline">\(x+cf(x)\)</span>。再通过线性层进行处理类似的也是使用上面提到的正则化进行处理，处理之后结果通过<code>unpatchify</code>处理（将channels扩展2倍而后还原到最开始的输入状态）</p>
<h4 id="unet模型结构">Unet模型结构</h4>
<p><a href="https://www.big-yellow-j.top/posts/2025/01/18/CV-Backbone.html#:~:text=%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E9%97%AE%E9%A2%98%E3%80%82-,2.Unet%E7%B3%BB%E5%88%97,-Unet%E4%B8%BB%E8%A6%81%E4%BB%8B%E7%BB%8D" target="_blank" rel="noopener nofollow">Unet模型</a>在前面有介绍过了就是通过下采样和上采用并且同层级之间通过特征拼接来补齐不同采用过程之间的“信息”损失。如果直接使用stable diffusion model（<em>封装不多</em>），假设参数如下进行代码操作：</p>
<pre><code class="language-python">{
    'ch': 64,
    'out_ch': 3,
    'ch_mult': (1, 2, 4), # 通道增加倍数 in: 2,3,128,128 第一层卷积：2,64,128,128 通过这个参数直接结合 num_res_blocks来判断通道数量增加 ch_mut*num_res_blocks=(1, 1, 2, 2, 4, 4)
    'num_res_blocks': 2,  # 残差模块数量
    'attn_resolutions': (16,),
    'dropout': 0.1,
    'resamp_with_conv': True,
    'in_channels': 3,
    'resolution': 128,
    'use_timestep': True,
    'use_linear_attn': False,
    'attn_type': "vanilla"
}
</code></pre>
<blockquote>
<p>基本模块</p>
</blockquote>
<p><strong>1、残差模块</strong>：<br>
<img src="https://s2.loli.net/2025/06/21/J6RDxyPk17CVIOo.webp" alt="image.png" loading="lazy"></p>
<p><strong>2、time embedding</strong>：直接使用attention的sin位置编码</p>
<blockquote>
<p>具体过程</p>
</blockquote>
<p><img src="https://s2.loli.net/2025/06/21/ZAWCEJSKmMjVuvt.webp" alt="image.png" loading="lazy"></p>
<p>在得到的分辨率=attn_resolutions时候就会直接进行注意力计算（直接用卷积处理得到q，k，v然后进行计算attention），整个[结构]({{ site.baseurl }}/Dio.drawio)。如果这里直接使用<code>diffuser</code>里面的<a href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unets/unet_2d_blocks.py" target="_blank" rel="noopener nofollow">UNet模型</a>进行解释（使用UNet2DModel模型解释），整个Unet模型就是3部分：1、下采样；2、中间层；3、上采样。假设模型参数为：</p>
<pre><code class="language-python">model = UNet2DModel(
    sample_size= 128,
    in_channels=3,
    out_channels=3,
    layers_per_block=2,
    block_out_channels=(128, 128, 256, 256, 512, 512),
    down_block_types=("DownBlock2D", "DownBlock2D", "DownBlock2D", "DownBlock2D", "DownBlock2D", "DownBlock2D"),
    up_block_types=("UpBlock2D", "UpBlock2D", "UpBlock2D", "UpBlock2D", "UpBlock2D", "UpBlock2D")
).to(device)
</code></pre>
<p>整个过程维度变化，假设输入为：image:(32,3,128,128), time_steps: (32, )：<br>
<strong>首先通过第一层卷积</strong>：(32,128,128,128)与此同时会将时间步进行编码得到：(32, 512)（如果有label数据也是(32,)那么会将其加入到time_steps中）<br>
<strong>下采样处理</strong>：总共6层下采样，得到结果为：<br>
Down-0: torch.Size([32, 128, 128, 128])<br>
Down-1: torch.Size([32, 128, 64, 64])<br>
Down-2: torch.Size([32, 256, 32, 32])<br>
Down-3: torch.Size([32, 256, 16, 16])<br>
Down-4: torch.Size([32, 512, 8, 8])<br>
Down-5: torch.Size([32, 512, 4, 4])<br>
<strong>中间层处理</strong>：torch.Size([32, 512, 4, 4])<br>
<strong>上采样处理</strong>：总共6层上采样，得到结果为：<br>
Up-0 torch.Size([32, 512, 8, 8])<br>
Up-1 torch.Size([32, 512, 16, 16])<br>
Up-2 torch.Size([32, 256, 32, 32])<br>
Up-3 torch.Size([32, 256, 64, 64])<br>
Up-4 torch.Size([32, 128, 128, 128])<br>
Up-5 torch.Size([32, 128, 128, 128])<br>
<strong>输出</strong>：输出就直接通过groupnorm以及silu激活之后直接通过一层卷积进行处理得到：torch.Size([32, 128, 128, 128])</p>
<h3 id="df训练">DF训练</h3>
<blockquote>
<p><strong>生成具有随机性展示图像效果不能很好说明模型生成能力</strong></p>
</blockquote>
<h4 id="1ddpm">1、DDPM</h4>
<p>对于传统的DF训练（前向+反向）比较简单，直接通过输入图像而后不断添加噪声而后解噪。以huggingface<sup class="footnote-ref"><a href="#fn10" id="fnref10" rel="noopener nofollow">[10]</a></sup>上例子为例（测试代码: <a href="'Big-Yellow-J.github.io/code/Unet2Model.py.txt'" target="_blank" rel="noopener nofollow">Unet2Model.py</a>），<strong>首先</strong>、对图像进行添加噪声。<strong>而后</strong>、直接去对添加噪声后的模型进行训练“去噪”（也就是预测图像中的噪声）。<strong>最后</strong>、计算loss反向传播。</p>
<blockquote>
<p>对于加噪声等过程可以直接借助 <code>diffusers</code>来进行处理，对于diffuser：<br>
1、schedulers：调度器<br>
主要实现功能：1、图片的前向过程添加噪声（也就是上面的<span class="math inline">\(x_T=\sqrt{\bar{\alpha_T}}x_0+ \sqrt{1-\bar{\alpha_T}}\epsilon\)</span>）；2、图像的反向过程去噪；3、时间步管理等。如果不是用这个调度器也可以自己设计一个只需要：1、前向加噪过程（需要：使用固定的<span class="math inline">\(\beta\)</span>还是变化的、加噪就比较简单直接进行矩阵计算）；2、采样策略</p>
</blockquote>
<p>测试得到结果为（因为HF官方提供了很好的参数去训练模型，因此测试新的数据集可能就没有那么效果好，只是做一个效果展示<strong>调参可能可以改善模型最后生成效果</strong>）：<br>
<img src="https://cdn.z.wiki/autoupload/20250520/CHJj/1000X200/Generate-image.gif" alt="" loading="lazy"></p>
<blockquote>
<p><strong>Stable Diffusion Model</strong>代码测试。数据集："saitsharipov/CelebA-HQ"</p>
</blockquote>
<p><img src="https://cdn.z.wiki/autoupload/20250523/jJT2/1000X200/SD-Generate-image-normal.gif" alt="SD-Generate-image-normal.gif" loading="lazy"></p>
<blockquote>
<p><strong>Hugging Face</strong>代码测试。数据集："saitsharipov/CelebA-HQ"</p>
</blockquote>
<p><img src="https://cdn.z.wiki/autoupload/20250523/OZmN/1000X200/HF-Generate-image-normal.gif" alt="Generate-image-normal.gif" loading="lazy"></p>
<blockquote>
<p><strong>Dit</strong>代码测试。数据集："saitsharipov/CelebA-HQ"</p>
</blockquote>
<p><img src="https://cdn.z.wiki/autoupload/20250525/Cex6/2000X200/Generate-image.gif" alt="image.gif" loading="lazy"></p>
<h2 id="总结">总结</h2>
<p>上面介绍了各类DF以及具体的代码操作，总的来说在DF训练过程中（从代码角度）基本上就是这个公式：<span class="math inline">\(x_t=\sqrt{\bar{\alpha_t}}x_0+ \sqrt{1-\bar{\alpha_t}}\epsilon\)</span> 加噪过程得到<span class="math inline">\(x_t\)</span>/去噪过程通过<span class="math inline">\(x_t\)</span>通过去预测 <strong>噪声</strong>来优化模型的参数。于此同时训练过程中发现：扩散模型（如DDPM）确实倾向于<strong>先学习图像的低频信息</strong>（大致轮廓），<strong>再逐步学习高频信息</strong>（细节），这是由于模型的去噪过程和损失函数设计，因此<strong>扩散模型需要大量迭代（通常数千到数十万步）才能生成高质量图像，尤其在细节上需要长时间优化</strong>。比如下面为实际迭代过程中生成.</p>
<blockquote>
<p>从左到右，从上到下，保存频率为每20个epoch保存一次，使用的<strong>模型为dit模型</strong></p>
</blockquote>
<p><img src="https://s2.loli.net/2025/05/25/TnWy4JkVSMivQ1H.png" alt="" loading="lazy"></p>
<h2 id="参考">参考</h2>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p><a href="https://arxiv.org/pdf/2102.09672" target="_blank" rel="noopener nofollow">https://arxiv.org/pdf/2102.09672</a> <a href="#fnref1" class="footnote-backref" rel="noopener nofollow">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p><a href="https://arxiv.org/pdf/2208.11970" target="_blank" rel="noopener nofollow">https://arxiv.org/pdf/2208.11970</a> <a href="#fnref2" class="footnote-backref" rel="noopener nofollow">↩︎</a> <a href="#fnref2:1" class="footnote-backref" rel="noopener nofollow">↩︎</a></p>
</li>
<li id="fn3" class="footnote-item"><p><a href="https://arxiv.org/abs/2006.11239" target="_blank" rel="noopener nofollow">https://arxiv.org/abs/2006.11239</a> <a href="#fnref3" class="footnote-backref" rel="noopener nofollow">↩︎</a> <a href="#fnref3:1" class="footnote-backref" rel="noopener nofollow">↩︎</a></p>
</li>
<li id="fn4" class="footnote-item"><p><a href="https://huggingface.co/docs/diffusers/en/index" target="_blank" rel="noopener nofollow">https://huggingface.co/docs/diffusers/en/index</a> <a href="#fnref4" class="footnote-backref" rel="noopener nofollow">↩︎</a></p>
</li>
<li id="fn5" class="footnote-item"><p><a href="https://arxiv.org/pdf/2204.06125" target="_blank" rel="noopener nofollow">https://arxiv.org/pdf/2204.06125</a> <a href="#fnref5" class="footnote-backref" rel="noopener nofollow">↩︎</a></p>
</li>
<li id="fn6" class="footnote-item"><p><a href="https://arxiv.org/abs/2112.10752" target="_blank" rel="noopener nofollow">https://arxiv.org/abs/2112.10752</a> <a href="#fnref6" class="footnote-backref" rel="noopener nofollow">↩︎</a></p>
</li>
<li id="fn7" class="footnote-item"><p><a href="https://arxiv.org/pdf/2010.02502" target="_blank" rel="noopener nofollow">https://arxiv.org/pdf/2010.02502</a> <a href="#fnref7" class="footnote-backref" rel="noopener nofollow">↩︎</a></p>
</li>
<li id="fn8" class="footnote-item"><p><a href="https://arxiv.org/abs/2212.09748" target="_blank" rel="noopener nofollow">https://arxiv.org/abs/2212.09748</a> <a href="#fnref8" class="footnote-backref" rel="noopener nofollow">↩︎</a></p>
</li>
<li id="fn9" class="footnote-item"><p><a href="https://github.com/facebookresearch/DiT" target="_blank" rel="noopener nofollow">https://github.com/facebookresearch/DiT</a> <a href="#fnref9" class="footnote-backref" rel="noopener nofollow">↩︎</a></p>
</li>
<li id="fn10" class="footnote-item"><p><a href="https://huggingface.co/docs/diffusers/en/tutorials/basic_training" target="_blank" rel="noopener nofollow">https://huggingface.co/docs/diffusers/en/tutorials/basic_training</a> <a href="#fnref10" class="footnote-backref" rel="noopener nofollow">↩︎</a></p>
</li>
</ol>
</section>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0" data-date-updated="2025-06-21 15:24">2025-06-21 15:23</span>&nbsp;
<a href="https://www.cnblogs.com/Big-Yellow">Big-Yellow-J</a>&nbsp;
阅读(<span id="post_view_count">29</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18940170);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18940170', targetLink: 'https://www.cnblogs.com/Big-Yellow/p/18940170', title: '深入浅出了解生成模型-3：Diffusion模型原理以及代码' })">举报</a>
</div>
        