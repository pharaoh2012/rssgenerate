
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/leviatan/p/18742291" title="发布于 2025-02-27 23:12">
    <span role="heading" aria-level="2">使用 kubeadm 创建高可用 Kubernetes 及外部 etcd 集群</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p>博客链接：<a href="https://leviatan.cn/archives/PXNwhmiu" target="_blank" rel="noopener nofollow">使用 kubeadm 创建高可用 Kubernetes 及外部 etcd 集群</a></p>
<h2 id="前言">前言</h2>
<p>Kubernetes 的官方中文文档内容全面，表达清晰，有大量示例和解析</p>
<p>无论任何情况下都推荐先花几个小时通读官方文档，来了解配置过程中的可选项，以及可能会遇到哪些问题</p>
<p>本文基于官方文档中 <code>入门 - 生产环境</code> 一章来整理部署流程</p>
<p><a href="https://kubernetes.io/zh-cn/docs/home/" target="_blank" rel="noopener nofollow">Kubernetes 文档 | Kubernetes</a></p>
<h2 id="架构">架构</h2>
<ul>
<li>OS: Debian 12</li>
<li>CGroup Driver: systemd</li>
<li>Container Runtime: containerd</li>
<li>CNI: Calico</li>
<li>Kubernetes: <code>v1.32.0</code></li>
</ul>
<blockquote>
<p><strong>注意</strong><br>
所有节点服务器都需要关闭 swap</p>
</blockquote>
<ul>
<li>Other
<ul>
<li><strong>说明</strong>
<ul>
<li>该服务器运行 K8S 外部应用，包括 Nginx、Nexus 等</li>
<li>该服务器运行的所有业务通过 docker-compose 管理</li>
<li>与 K8S 自身配置相关的步骤说明中的“所有节点”不包括该服务器</li>
</ul>
</li>
<li>Server
<ul>
<li>vCPU: <code>2</code></li>
<li>Memory: <code>4G</code></li>
</ul>
</li>
<li>Network: <code>192.168.1.100</code> <code>2E:7E:86:3A:A5:20</code></li>
<li>Port:
<ul>
<li><code>8443/tcp</code>: 向集群提供 Kubernetes APIServer 负载均衡</li>
</ul>
</li>
</ul>
</li>
<li>Etcd
<ul>
<li>Server
<ul>
<li>vCPU: <code>1</code></li>
<li>Memory: <code>1G</code></li>
</ul>
</li>
<li>Network
<ul>
<li>Etcd-01: <code>192.168.1.101</code> <code>2E:7E:86:3A:A5:21</code></li>
<li>Etcd-02: <code>192.168.1.102</code> <code>2E:7E:86:3A:A5:22</code></li>
<li>Etcd-03: <code>192.168.1.103</code> <code>2E:7E:86:3A:A5:23</code></li>
</ul>
</li>
<li>Port:
<ul>
<li><code>2379/tcp</code>: etcd HTTP API</li>
<li><code>2380/tcp</code>: etcd peer 通信</li>
</ul>
</li>
</ul>
</li>
<li>Master
<ul>
<li>Server
<ul>
<li>vCPU: <code>4</code></li>
<li>Memory: <code>8G</code></li>
</ul>
</li>
<li>Network
<ul>
<li>Master-01: <code>192.168.1.104</code> <code>2E:7E:86:3A:A5:24</code></li>
<li>Master-02: <code>192.168.1.105</code> <code>2E:7E:86:3A:A5:25</code></li>
<li>Master-03: <code>192.168.1.106</code> <code>2E:7E:86:3A:A5:26</code></li>
</ul>
</li>
<li>Port:
<ul>
<li><code>179/tcp</code>: Calico BGP</li>
<li><code>6443/tcp</code>: Kubernetes APIServer</li>
<li><code>10250/tcp</code>: kubelet API</li>
</ul>
</li>
</ul>
</li>
<li>Node
<ul>
<li>Server
<ul>
<li>vCPU: <code>4</code></li>
<li>Memory: <code>8G</code></li>
</ul>
</li>
<li>Network
<ul>
<li>Node-01: <code>192.168.1.107</code> <code>2E:7E:86:3A:A5:27</code></li>
<li>Node-02: <code>192.168.1.108</code> <code>2E:7E:86:3A:A5:28</code></li>
<li>Node-03: <code>192.168.1.109</code> <code>2E:7E:86:3A:A5:29</code></li>
</ul>
</li>
<li>Port:
<ul>
<li><code>179/tcp</code>: Calico BGP</li>
<li><code>10250/tcp</code>: kubelet API</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="配置基础环境">配置基础环境</h2>
<blockquote>
<p><strong>说明</strong><br>
所有节点</p>
</blockquote>
<pre><code class="language-bash">apt update
apt upgrade
apt install curl apt-transport-https ca-certificates gnupg2 software-properties-common vim

curl -fsSL https://mirrors.ustc.edu.cn/kubernetes/core:/stable:/v1.32/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://mirrors.ustc.edu.cn/kubernetes/core:/stable:/v1.32/deb/ /" | tee /etc/apt/sources.list.d/kubernetes.list

curl -fsSL https://mirrors.ustc.edu.cn/docker-ce/linux/debian/gpg -o /etc/apt/keyrings/docker.asc
chmod a+r /etc/apt/keyrings/docker.asc
echo "deb [signed-by=/etc/apt/keyrings/docker.asc] https://mirrors.ustc.edu.cn/docker-ce/linux/debian bookworm stable" | tee /etc/apt/sources.list.d/docker.list

apt update

apt install containerd.io

mkdir -p /etc/containerd
containerd config default | tee /etc/containerd/config.toml

systemctl restart containerd

apt install kubelet kubeadm kubectl
apt-mark hold kubelet kubeadm kubectl
</code></pre>
<p>开启 ipv4 转发</p>
<p>编辑 <code>/etc/sysctl.conf</code>，找到下方配置并取消注释</p>
<pre><code class="language-bash">net.ipv4.ip_forward=1
</code></pre>
<p>执行 <code>sysctl -p</code> 应用配置</p>
<p>创建 crictl 配置</p>
<pre><code class="language-bash">cat &lt;&lt; EOF &gt; /etc/crictl.yaml
runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
timeout: 10
debug: false
EOF
</code></pre>
<p>如果需要通过代理服务器访问容器仓库，需要为 <code>containerd</code> 配置代理服务</p>
<pre><code class="language-bash">mkdir -p /etc/systemd/system/containerd.service.d
cat &lt;&lt; EOF &gt; /etc/systemd/system/containerd.service.d/http-proxy.conf
[Service]
Environment="HTTP_PROXY=http://username:password@proxy-server-ip:port"
Environment="HTTPS_PROXY=http://username:password@proxy-server-ip:port"
Environment="NO_PROXY=localhost,127.0.0.0/8,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16"
EOF

systemctl daemon-reload
systemctl restart containerd.service
</code></pre>
<h3 id="已知问题">已知问题</h3>
<p>使用 <code>systemd</code> 作为 CGroup Driver 且使用 <code>containerd</code> 作为 CRI 运行时</p>
<p>需要修改 <code>/etc/containerd/config.toml</code>，添加如下配置</p>
<p>相关文章：<a href="https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#containerd-systemd" target="_blank" rel="noopener nofollow">配置 systemd cgroup 驱动 | Kubernetes</a></p>
<pre><code class="language-toml">[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
  ...
  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
    SystemdCgroup = true
</code></pre>
<p>执行 <code>systemctl restart containerd</code></p>
<p><strong>或</strong>参照另一篇文章的解决方案</p>
<p>相关文章：<a href="https://discuss.kubernetes.io/t/why-does-etcd-fail-with-debian-bullseye-kernel/19696" target="_blank" rel="noopener nofollow">Why does etcd fail with Debian/bullseye kernel? - General Discussions - Discuss Kubernetes</a></p>
<pre><code class="language-bash">cat /etc/default/grub

# Source:
# GRUB_CMDLINE_LINUX_DEFAULT="quiet"

# Modify:
GRUB_CMDLINE_LINUX_DEFAULT="quiet systemd.unified_cgroup_hierarchy=0"
</code></pre>
<p>执行 <code>update-grub</code> 并重启</p>
<h2 id="配置-etcd-节点">配置 etcd 节点</h2>
<h3 id="将-kubelet-配置为-etcd-的服务管理器">将 kubelet 配置为 etcd 的服务管理器</h3>
<blockquote>
<p><strong>说明</strong><br>
所有 etcd 节点</p>
</blockquote>
<pre><code class="language-bash">apt update
apt install etcd-client

mkdir -p /etc/systemd/system/kubelet.service.d

cat &lt;&lt; EOF &gt; /etc/systemd/system/kubelet.service.d/kubelet.conf
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: false
authorization:
  mode: AlwaysAllow
cgroupDriver: systemd
address: 127.0.0.1
containerRuntimeEndpoint: unix:///var/run/containerd/containerd.sock
staticPodPath: /etc/kubernetes/manifests
EOF

cat &lt;&lt; EOF &gt; /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf
[Service]
Environment="KUBELET_CONFIG_ARGS=--config=/etc/systemd/system/kubelet.service.d/kubelet.conf"
ExecStart=
ExecStart=/usr/bin/kubelet $KUBELET_CONFIG_ARGS
Restart=always
EOF

systemctl daemon-reload
systemctl restart kubelet
</code></pre>
<h3 id="为-kubeadm-创建配置文件">为 kubeadm 创建配置文件</h3>
<blockquote>
<p><strong>说明</strong><br>
Etcd-01 节点，由该节点向其他节点分发证书及配置<br>
该节点同时作为 CA</p>
</blockquote>
<p>生成 CA</p>
<pre><code class="language-bash">kubeadm init phase certs etcd-ca
</code></pre>
<p>生成如下文件</p>
<ul>
<li><code>/etc/kubernetes/pki/etcd/ca.crt</code></li>
<li><code>/etc/kubernetes/pki/etcd/ca.key</code></li>
</ul>
<p>为方便接下来的步骤操作，先将 etcd 节点信息导出为环境变量</p>
<pre><code class="language-bash">export HOST0=192.168.1.101
export HOST1=192.168.1.102
export HOST2=192.168.1.103

export NAME0="etcd-01"
export NAME1="etcd-02"
export NAME2="etcd-03"
</code></pre>
<p>为 etcd 成员生成 kubeadm 配置</p>
<pre><code class="language-bash">HOSTS=(${HOST0} ${HOST1} ${HOST2})
NAMES=(${NAME0} ${NAME1} ${NAME2})

for i in "${!HOSTS[@]}"; do

HOST=${HOSTS[$i]}
NAME=${NAMES[$i]}

mkdir -p /tmp/${HOST}

cat &lt;&lt; EOF &gt; /tmp/${HOST}/kubeadmcfg.yaml
---
apiVersion: "kubeadm.k8s.io/v1beta4"
kind: InitConfiguration
nodeRegistration:
    name: ${NAME}
localAPIEndpoint:
    advertiseAddress: ${HOST}
---
apiVersion: "kubeadm.k8s.io/v1beta4"
kind: ClusterConfiguration
etcd:
    local:
        serverCertSANs:
        - "${HOST}"
        peerCertSANs:
        - "${HOST}"
        extraArgs:
        - name: initial-cluster
          value: ${NAMES[0]}=https://${HOSTS[0]}:2380,${NAMES[1]}=https://${HOSTS[1]}:2380,${NAMES[2]}=https://${HOSTS[2]}:2380
        - name: initial-cluster-state
          value: new
        - name: name
          value: ${NAME}
        - name: listen-peer-urls
          value: https://${HOST}:2380
        - name: listen-client-urls
          value: https://${HOST}:2379
        - name: advertise-client-urls
          value: https://${HOST}:2379
        - name: initial-advertise-peer-urls
          value: https://${HOST}:2380
EOF
done
</code></pre>
<p>为每个成员创建证书</p>
<pre><code class="language-bash">kubeadm init phase certs etcd-server --config=/tmp/${HOST2}/kubeadmcfg.yaml
kubeadm init phase certs etcd-peer --config=/tmp/${HOST2}/kubeadmcfg.yaml
kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST2}/kubeadmcfg.yaml
kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST2}/kubeadmcfg.yaml
cp -R /etc/kubernetes/pki /tmp/${HOST2}/
# Clear useless cert
find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete

kubeadm init phase certs etcd-server --config=/tmp/${HOST1}/kubeadmcfg.yaml
kubeadm init phase certs etcd-peer --config=/tmp/${HOST1}/kubeadmcfg.yaml
kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST1}/kubeadmcfg.yaml
kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST1}/kubeadmcfg.yaml
cp -R /etc/kubernetes/pki /tmp/${HOST1}/
find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete

kubeadm init phase certs etcd-server --config=/tmp/${HOST0}/kubeadmcfg.yaml
kubeadm init phase certs etcd-peer --config=/tmp/${HOST0}/kubeadmcfg.yaml
kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST0}/kubeadmcfg.yaml
kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST0}/kubeadmcfg.yaml

# Clear ca key from member
find /tmp/${HOST2} -name ca.key -type f -delete
find /tmp/${HOST1} -name ca.key -type f -delete
</code></pre>
<p>将证书移动到对应的成员服务器</p>
<pre><code class="language-bash">scp -r /tmp/${HOST2}/pki root@${HOST2}:/etc/kubernetes/
scp /tmp/${HOST2}/kubeadmcfg.yaml root@${HOST2}:~/

scp -r /tmp/${HOST1}/pki root@${HOST1}:/etc/kubernetes/
scp /tmp/${HOST1}/kubeadmcfg.yaml root@${HOST1}:~/

mv /tmp/${HOST0}/kubeadmcfg.yaml ~/

rm -rf /tmp/${HOST2}
rm -rf /tmp/${HOST1}
rm -rf /tmp/${HOST0}
</code></pre>
<p>此时在三台 etcd 节点中的文件结构均应如下</p>
<pre><code class="language-bash">/root
└── kubeadmcfg.yaml
---
/etc/kubernetes/pki
├── apiserver-etcd-client.crt
├── apiserver-etcd-client.key
└── etcd
    ├── ca.crt
    ├── ca.key # 仅 CA 节点既 etcd-01
    ├── healthcheck-client.crt
    ├── healthcheck-client.key
    ├── peer.crt
    ├── peer.key
    ├── server.crt
    └── server.key
</code></pre>
<h3 id="创建静态-pod-清单">创建静态 Pod 清单</h3>
<blockquote>
<p><strong>说明</strong><br>
所有 etcd 节点</p>
</blockquote>
<pre><code class="language-bash">kubeadm init phase etcd local --config=/root/kubeadmcfg.yaml
</code></pre>
<h3 id="检查集群运行情况">检查集群运行情况</h3>
<p>将 <code>${HOST0}</code> 替换为想要检查的节点 ip</p>
<pre><code class="language-bash">ETCDCTL_API=3 etcdctl \
--cert /etc/kubernetes/pki/etcd/peer.crt \
--key /etc/kubernetes/pki/etcd/peer.key \
--cacert /etc/kubernetes/pki/etcd/ca.crt \
--endpoints https://${HOST0}:2379 endpoint health
</code></pre>
<h2 id="使用-kubeadm-创建高可用集群">使用 kubeadm 创建高可用集群</h2>
<h3 id="说明">说明</h3>
<p>配置过程中需要完全重置控制平面节点的配置时，需要有至少一台能够访问集群的节点，在该节点上按如下流程操作</p>
<pre><code class="language-bash">kubectl delete pods,nodes,namespaces,deployments,services --all --all-namespaces --force
kubectl delete -f tigera-operator.yaml --force
kubectl delete -f custom-resources.yaml --force
kubeadm reset --cleanup-tmp-dir -f
rm -rf /etc/cni/net.d/*
rm -rf ~/.kube
systemctl restart kubelet containerd
</code></pre>
<h3 id="为-kube-apiserver-创建负载均衡">为 kube-apiserver 创建负载均衡</h3>
<blockquote>
<p><strong>说明</strong><br>
本文中负载均衡使用 Nginx</p>
</blockquote>
<p>Nginx 配置</p>
<pre><code class="language-conf">http {
    ...
}

stream {
    upstream apiserver {
        server 192.168.1.104:6443 weight=5 max_fails=3 fail_timeout=30s; # Master-01
        server 192.168.1.105:6443 weight=5 max_fails=3 fail_timeout=30s; # Master-02
        server 192.168.1.106:6443 weight=5 max_fails=3 fail_timeout=30s; # Master-03
    }

    server {
        listen 8443;
        proxy_pass apiserver;
    }
}
</code></pre>
<h3 id="为控制平面节点配置外部-etcd-节点">为控制平面节点配置外部 etcd 节点</h3>
<blockquote>
<p><strong>说明</strong><br>
任一 etcd 节点与主控制平面节点，本文中为 <code>Etcd-01</code> 与 <code>Master-01</code></p>
</blockquote>
<p>从集群中任一 etcd 节点复制到主控制平面节点</p>
<pre><code class="language-bash">scp /etc/kubernetes/pki/etcd/ca.crt /etc/kubernetes/pki/apiserver-etcd-client.crt /etc/kubernetes/pki/apiserver-etcd-client.key root@192.168.1.104:~
</code></pre>
<p>在主控制平面节点中将文件移动到指定位置</p>
<pre><code class="language-bash">mkdir -p /etc/kubernetes/pki/etcd
mv ~/ca.crt /etc/kubernetes/pki/etcd/
mv ~/apiserver-etcd-client.crt /etc/kubernetes/pki/
mv ~/apiserver-etcd-client.key /etc/kubernetes/pki/
</code></pre>
<p>创建 <code>kubeadm-config.yaml</code>，内容如下</p>
<ul>
<li><code>controlPlaneEndpoint</code>: 负载均衡服务器</li>
<li><code>etcd</code>
<ul>
<li><code>external</code>
<ul>
<li><code>endpoints</code>: etcd 节点列表</li>
</ul>
</li>
</ul>
</li>
<li><code>networking</code>
<ul>
<li><code>podSubnet</code>: pod ip cidr</li>
</ul>
</li>
</ul>
<pre><code class="language-yaml">---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
kubernetesVersion: v1.32.0
controlPlaneEndpoint: 192.168.1.100:8443
etcd:
  external:
    endpoints:
      - https://192.168.1.101:2379
      - https://192.168.1.102:2379
      - https://192.168.1.103:2379
    caFile: /etc/kubernetes/pki/etcd/ca.crt
    certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt
    keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key
networking:
  dnsDomain: cluster.local
  podSubnet: 10.244.0.0/24
  serviceSubnet: 10.96.0.0/16
</code></pre>
<h3 id="初始化主控制平面">初始化主控制平面</h3>
<blockquote>
<p><strong>说明</strong><br>
主控制平面节点，本文中为 <code>Master-01</code></p>
</blockquote>
<ul>
<li><code>--upload-certs</code>: 将控制平面间的共享证书上传到 <code>kubeadm-certs</code> Secret
<ul>
<li><code>kubeadm-certs</code> Secret 和解密密钥将在两小时后失效</li>
<li>如果要重新上传证书并生成新的解密密钥，需要在已加入集群的控制平面节点上执行 <code>kubeadm init phase upload-certs --upload-certs</code></li>
</ul>
</li>
</ul>
<pre><code class="language-bash">kubeadm init --config kubeadm-config.yaml --upload-certs
</code></pre>
<p>等待运行完成后应输出类似如下内容</p>
<pre><code class="language-bash">...

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes running the following command on each as root:

  kubeadm join 192.168.1.100:8443 --token 7r34LU.iLiRgu2qHdAeeanS --discovery-token-ca-cert-hash sha256:9f86d081884c7d659a2feaa0c55ad015a3bf4f1b2b0b822cd15d6c15b0f00a08 --control-plane --certificate-key 03d66dd08835c1ca3f128cceacd1f31ac94163096b20f445ae84285bc0832d72

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
"kubeadm init phase upload-certs --upload-certs" to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.1.100:8443 --token 7r34LU.iLiRgu2qHdAeeanS --discovery-token-ca-cert-hash sha256:9f86d081884c7d659a2feaa0c55ad015a3bf4f1b2b0b822cd15d6c15b0f00a08
</code></pre>
<p>先将控制台输出的以上内容保存，稍后将使用这些命令来将其他控制平面节点和工作节点加入集群</p>
<p>根据输出的提示，复制 kubeconfig 用于 <code>kubectl</code></p>
<pre><code class="language-bash">mkdir -p ~/.kube
cp /etc/kubernetes/admin.conf ~/.kube/config
</code></pre>
<p>应用 CNI 插件</p>
<p>由于该清单过大，<code>kubectl apply</code> 会产生如下报错，使用 <code>kubectl create</code> 或 <code>kubectl replace</code></p>
<blockquote>
<p><strong>注意</strong><br>
确认 <code>custom-resources.yaml</code> 中 <code>calicoNetwork</code> 配置的 ip cidr 与集群 <code>podSubnet</code> 配置一致</p>
</blockquote>
<pre><code class="language-bash"># kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/refs/heads/release-v3.29/manifests/tigera-operator.yaml
# The CustomResourceDefinition "installations.operator.tigera.io" is invalid: metadata.annotations: Too long: may not be more than 262144 bytes
wget https://raw.githubusercontent.com/projectcalico/calico/refs/heads/release-v3.29/manifests/tigera-operator.yaml
wget https://raw.githubusercontent.com/projectcalico/calico/refs/heads/release-v3.29/manifests/custom-resources.yaml
kubectl create -f tigera-operator.yaml
kubectl create -f custom-resources.yaml
</code></pre>
<p>输入以下内容查看控制平面组件 pod 启动状态</p>
<pre><code class="language-bash">kubectl get pod -A
</code></pre>
<h3 id="初始化其他控制平面">初始化其他控制平面</h3>
<blockquote>
<p><strong>说明</strong><br>
除主控制平面节点外的其他控制平面节点，本文中为 <code>Master-02</code> <code>Master-03</code><br>
使用 <code>kubeadm join</code> 命令加入集群的节点会将 KubeConfig 同步到 <code>/etc/kubernetes/admin.conf</code></p>
</blockquote>
<p>依照上面输出的命令，分别在其他控制平面节点中执行</p>
<ul>
<li><code>--control-plane</code>: 通知 <code>kubeadm join</code> 创建一个新控制平面</li>
<li><code>--certificate-key xxx</code>: 从集群 <code>kubeadm-certs</code> Secret 下载控制平面证书并使用给定的密钥解密</li>
</ul>
<pre><code class="language-bash">kubeadm join 192.168.1.100:8443 --token 7r34LU.iLiRgu2qHdAeeanS --discovery-token-ca-cert-hash sha256:9f86d081884c7d659a2feaa0c55ad015a3bf4f1b2b0b822cd15d6c15b0f00a08 --control-plane --certificate-key 03d66dd08835c1ca3f128cceacd1f31ac94163096b20f445ae84285bc0832d72
</code></pre>
<p>根据输出的提示，复制 kubeconfig 用于 <code>kubectl</code></p>
<pre><code class="language-bash">mkdir -p ~/.kube
cp /etc/kubernetes/admin.conf ~/.kube/config
</code></pre>
<h3 id="初始化负载节点">初始化负载节点</h3>
<blockquote>
<p><strong>说明</strong><br>
所有负载节点<br>
使用 <code>kubeadm join</code> 命令加入集群的节点会将 KubeConfig 同步到 <code>/etc/kubernetes/kubelet.conf</code></p>
</blockquote>
<p>依照上面输出的命令，分别在负载节点中执行</p>
<pre><code class="language-bash">kubeadm join 192.168.1.100:8443 --token 7r34LU.iLiRgu2qHdAeeanS --discovery-token-ca-cert-hash sha256:9f86d081884c7d659a2feaa0c55ad015a3bf4f1b2b0b822cd15d6c15b0f00a08
</code></pre>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.546145658605324" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-02-27 23:13">2025-02-27 23:12</span>&nbsp;
<a href="https://www.cnblogs.com/leviatan">WindSpirit</a>&nbsp;
阅读(<span id="post_view_count">68</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18742291" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18742291);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18742291', targetLink: 'https://www.cnblogs.com/leviatan/p/18742291', title: '使用 kubeadm 创建高可用 Kubernetes 及外部 etcd 集群' })">举报</a>
</div>
        