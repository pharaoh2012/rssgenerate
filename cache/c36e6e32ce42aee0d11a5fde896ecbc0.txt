
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/bricheersz/p/18964675" title="发布于 2025-07-03 23:04">
    <span role="heading" aria-level="2">你应该懂得AI大模型（十二）之 QLoRA</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body blogpost-body-html">
<h1 id="一-显存和算力" data-source-line="1">一、显存和算力</h1>
<h2 id="1-基本概念" data-source-line="3">1. 基本概念</h2>
<h3 id="显存-memory" data-source-line="5">显存 (Memory)</h3>
<ul data-source-line="7">
<li>
<p>定义：GPU 用于临时存储数据的高速内存，类似于计算机的 RAM。</p>
</li>
<li>
<p>作用：</p>
<ul>
<li>存储模型权重、中间激活值、梯度和优化器状态。</li>
<li>数据在显存与 GPU 核心之间快速传输以支持计算。</li>
</ul>
</li>
<li>
<p>衡量单位：GB (如 8GB、24GB)。</p>
</li>
</ul>
<h3 id="算力-computational-power" data-source-line="14">算力 (Computational Power)</h3>
<ul data-source-line="16">
<li>
<p>定义：GPU 执行计算操作的能力，由 GPU 核心数量和频率决定。</p>
</li>
<li>
<p>作用：</p>
<ul>
<li>执行矩阵乘法、卷积等计算密集型操作。</li>
</ul>
</li>
<li>
<p>衡量单位：</p>
<ul>
<li>TFLOPS (万亿次浮点运算 / 秒)，如 NVIDIA A100 的 312 TFLOPS (FP16)。</li>
<li>CUDA 核心数或 Tensor 核心数。</li>
</ul>
</li>
</ul>
<h2 id="2-关键区别" data-source-line="25">2. 关键区别</h2>
<table data-source-line="27">
<thead>
<tr><th>维度</th><th>显存</th><th>算力</th></tr>
</thead>
<tbody>
<tr>
<td>瓶颈表现</td>
<td>训练时出现 "CUDA out of memory" 错误</td>
<td>训练速度慢，GPU 利用率低</td>
</tr>
<tr>
<td>优化方式</td>
<td>量化模型、梯度检查点、减少 batch size</td>
<td>使用更快的 GPU、优化算法复杂度</td>
</tr>
<tr>
<td>典型场景</td>
<td>大模型微调（如 13B 参数模型）</td>
<td>高分辨率图像处理、大 batch 训练</td>
</tr>
<tr>
<td>资源竞争</td>
<td>模型权重 vs 激活值 vs 优化器状态</td>
<td>计算密集型操作（如矩阵乘法）</td>
</tr>
</tbody>
</table>
<h2 id="3-相互关系" data-source-line="34">3. 相互关系</h2>
<blockquote data-source-line="36">
<h3 id="显存决定模型规模">显存决定模型规模</h3>
</blockquote>
<ul data-source-line="38">
<li>
<p>模型越大（参数越多），所需显存越多。</p>
<ul>
<li>例如：7B 参数模型在 FP16 精度下需约 14GB 显存存储权重。</li>
</ul>
</li>
<li>
<p>量化技术（如 QLoRA 的 4-bit 量化）通过降低精度减少显存需求。</p>
</li>
</ul>
<h3 id="算力决定计算速度" data-source-line="43">算力决定计算速度</h3>
<ul data-source-line="45">
<li>
<p>算力越高，单位时间内处理的数据量越大。</p>
<ul>
<li>例如：A100 的算力约为 RTX 3090 的 3 倍，相同任务速度快约 3 倍。</li>
</ul>
</li>
<li>
<p>并行计算（如 Data Parallelism）依赖算力提升效率。</p>
</li>
</ul>
<h3 id="显存与算力的平衡" data-source-line="50">显存与算力的平衡</h3>
<ul data-source-line="52">
<li>小显存 + 高算力：适合小模型高速推理（如手机端 AI）。</li>
<li>大显存 + 低算力：适合训练大模型但速度较慢。</li>
<li>大显存 + 高算力：理想配置（如 A100、H100），支持大规模训练和推理。</li>
</ul>
<h2 id="4-实际应用中的影响" data-source-line="56">4. 实际应用中的影响</h2>
<h3 id="训练阶段" data-source-line="58">训练阶段</h3>
<ul data-source-line="60">
<li>
<p>显存不足：</p>
<ul>
<li>无法加载完整模型或 batch，需使用梯度累积、模型并行等技术。</li>
</ul>
</li>
<li>
<p>算力不足：</p>
<ul>
<li>训练时间过长，即使显存充足也无法充分利用数据。</li>
</ul>
</li>
</ul>
<h3 id="推理阶段" data-source-line="67">推理阶段</h3>
<ul data-source-line="69">
<li>
<p>显存限制部署规模：</p>
<ul>
<li>边缘设备（如车载 GPU）需压缩模型以适配有限显存。</li>
</ul>
</li>
<li>
<p>算力影响响应速度：</p>
<ul>
<li>实时应用（如自动驾驶）需高算力 GPU 保证低延迟。</li>
</ul>
</li>
</ul>
<h2 id="5-优化策略" data-source-line="76">5. 优化策略</h2>
<h3 id="显存优化" data-source-line="78">显存优化</h3>
<ol data-source-line="80">
<li>量化：FP16 → INT8 → 4-bit/2-bit。</li>
<li>梯度检查点：牺牲计算速度换取显存。</li>
<li>模型架构优化：使用参数效率更高的模型（如 LLaMA 比 GPT-3 参数量少 50%）。</li>
</ol>
<h3 id="算力优化" data-source-line="84">算力优化</h3>
<ol data-source-line="86">
<li>算法优化：使用 FlashAttention、TensorRT 等加速库。</li>
<li>硬件升级：从 RTX 3090 (35 TFLOPS) 升级到 A100 (312 TFLOPS)。</li>
<li>并行策略：数据并行、张量并行或流水线并行。</li>
</ol>
<h2 id="6-常见误区" data-source-line="90">6. 常见误区</h2>
<ol data-source-line="92">
<li>
<p>"显存越大越好"：</p>
<ul>
<li>若算力不足，大显存无法充分发挥作用（如训练小模型时）。</li>
</ul>
</li>
<li>
<p>"算力高就能训练大模型"：</p>
<ul>
<li>显存不足时，高算力 GPU 仍无法加载大模型。</li>
</ul>
</li>
<li>
<p>"量化只影响精度"：</p>
<ul>
<li>4-bit 量化不仅减少显存，还能加速计算（如 A100 的 4-bit Tensor Core）。</li>
</ul>
</li>
</ol>
<h2 id="总结" data-source-line="102">总结</h2>
<ul data-source-line="104">
<li>
<p>显存是模型运行的 "空间"，决定了你能处理多大的模型。</p>
</li>
<li>
<p>算力是模型运行的 "速度"，决定了你能多快完成计算。</p>
</li>
<li>
<p>理想的配置需要两者平衡，例如：</p>
<ul>
<li>微调 7B 模型：至少 16GB 显存 + 中等算力（如 RTX 4090）。</li>
<li>训练 70B 模型：80GB 显存 + 高算力（如 A100/A800）。</li>
</ul>
<p><br>在资源有限时，需根据任务需求优先优化瓶颈资源（显存或算力）。</p>

</li>

</ul>
<h1 id="二-部署量化打包量化与训练量化" data-source-line="114">二、部署量化（打包量化）与训练量化</h1>
<table data-source-line="116">
<thead>
<tr><th>维度</th><th>训练量化</th><th>部署量化（打包量化）</th></tr>

</thead>
<tbody>
<tr>
<td>目标</td>
<td>在训练过程中减少显存和计算量</td>
<td>在推理时减小模型体积、加速推理速度</td>

</tr>
<tr>
<td>应用阶段</td>
<td>模型训练阶段</td>
<td>模型部署阶段</td>

</tr>
<tr>
<td>技术重点</td>
<td>保持训练稳定性和模型精度</td>
<td>最大化推理效率，最小化精度损失</td>

</tr>
<tr>
<td>典型场景</td>
<td>QLoRA 微调大模型</td>
<td>在手机、边缘设备上部署模型 或者 需要极致推理速度（如实时对话系统）。</td>

</tr>
<tr>
<td>精度损失处理</td>
<td>通过技术补偿（如双重量化）减少损失</td>
<td>通过校准或微调恢复精度</td>

</tr>

</tbody>

</table>
<p data-source-line="124">部署量化会节约算例但不会节约显存，因为模型推理中间态对显存的占用不会因为量化而变小。但是如果我们想在24G显存的服务器上训练一个8B的模型通过训练量化就可以让开启训练。</p>
<ul data-source-line="126">
<li>
<p>训练量化是以量化方式训练模型，核心是在低精度下保持训练稳定性（一般情况下我们建议使用8位QLoRA训练，这时的精度损失是很小的）。</p>

</li>
<li>
<p>部署量化是对训练好的模型进行压缩加速，核心是在最小精度损失下提升推理效率。</p>

</li>
<li>
<p>两者可结合使用：例如用 QLoRA 训练，再用 GPTQ 进一步量化部署。选择哪种方法取决于你的具体需求：</p>
<ul>
<li>需要高精度微调&nbsp;→ 训练量化（如 QLoRA）</li>
<li>需要极致部署效率&nbsp;→ 部署量化（如 GPTQ+llama.cpp）</li>

</ul>

</li>

</ul>
<h1 id="三-qlora" data-source-line="133">三、QLoRA</h1>
<p data-source-line="135">QLoRA 是 2023 年提出的一种参数高效微调技术，通过将大语言模型量化与 LoRA 低秩适应相结合，大幅降低了微调所需的显存，让普通人也能在消费级 GPU 上微调 7B 甚至 70B 规模的大模型。</p>
<h1 id="四-如何使qlora训练效果超越lora" data-source-line="139">四、如何使QLoRA训练效果超越LoRA</h1>
<p data-source-line="141">以LLamaFactory为例，我们在训练时选择QLoRA，那么我们可以在LoRA参数的配置中提升LoRA秩（一般LoRA缩放参数是秩的两倍）。</p>
<p data-source-line="143">在相同显存限制下，高秩 QLoRA 可以达到比 LoRA 更高的准确率。QLoRA 的训练时间更长，但性价比更高（例如 r=32 的 QLoRA 用 12GB 显存达到了 LoRA 需要 24GB 才能达到的效果）。</p>
<p data-source-line="145"><img alt="" data-src="https://img2024.cnblogs.com/blog/1462902/202507/1462902-20250703230353421-1498989745.png" class="lazyload"></p>
<p data-source-line="147">LoRA 秩 (rank)&nbsp;是控制可训练参数数量和模型表达能力的关键参数。理论上，调高 LoRA 秩可以增强 QLoRA 的表达能力。</p>
<p data-source-line="149">通过QLoRA量化大幅降低基础模型的显存占用，从而允许使用更高的 LoRA 秩。<br>例如：</p>
<ul data-source-line="152">
<li>传统 LoRA（FP16）在 24GB GPU 上最多使用 r=16（否则显存溢出）。</li>
<li>QLoRA（4-bit）在同样 GPU 上可使用 r=32 甚至更高，获得更强表达能力。</li>

</ul>
<h2 id="1如何通过高秩-qlora-获得更好效果" data-source-line="155">1.如何通过高秩 QLoRA 获得更好效果？</h2>
<h3 id="1-硬件与参数配置" data-source-line="157">(1) 硬件与参数配置</h3>
<ul data-source-line="159">
<li>
<p>GPU 显存：</p>
<ul>
<li>24GB GPU：推荐 r≤32</li>
<li>48GB GPU：可尝试 r=64</li>

</ul>

</li>

</ul>
<h3 id="2-训练策略优化" data-source-line="164">(2) 训练策略优化</h3>
<ul data-source-line="166">
<li>学习率调整：<br>高秩 LoRA 需要更高学习率，推荐范围 5e-5 至 1e-4。</li>
<li>梯度累积：<br>使用较大的梯度累积步数（如 8-16），模拟更大 batch size。</li>
<li>更长训练时间：<br>高秩模型需要更多训练步数收敛，可将 max_steps 增加 50-100%。</li>

</ul>
<h3 id="3-量化技术选择" data-source-line="173">(3) 量化技术选择</h3>
<ul data-source-line="175">
<li>双重量化：<br>启用&nbsp;<code>bnb_4bit_use_double_quant=True</code>&nbsp;以节省额外显存。</li>
<li>NF4 量化：<br>使用&nbsp;<code>bnb_4bit_quant_type="nf4"</code>&nbsp;而非默认的 FP4，减少精度损失。
<h2 id="2-注意事项">2. 注意事项</h2>
<ol>
<li>
<p>并非秩越高越好：<br>对于大多数任务，r=16-32 已足够，过高的秩可能导致过拟合。</p>

</li>
<li>
<p>量化误差累积：<br>4-bit 量化会引入一定误差，可通过以下方式缓解：</p>
<ul>
<li>使用&nbsp;<code>compute_dtype=torch.float16</code>&nbsp;保持梯度计算的高精度。</li>
<li>在关键层（如注意力机制）保留 FP16 精度。</li>

</ul>

</li>
<li>
<p>推理部署：<br>高秩 LoRA 在推理时会增加计算量，可通过以下方式优化：</p>
<ul>
<li>将 LoRA 参数合并到基础模型中（需更多显存）。</li>
<li>使用 INT8 量化进行推理。</li>

</ul>

</li>

</ol></li>

</ul>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0" data-date-updated="2025-07-03 23:05">2025-07-03 23:04</span>&nbsp;
<a href="https://www.cnblogs.com/bricheersz">BricheersZ</a>&nbsp;
阅读(<span id="post_view_count">0</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18964675);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18964675', targetLink: 'https://www.cnblogs.com/bricheersz/p/18964675', title: '你应该懂得AI大模型（十二）之 QLoRA' })">举报</a>
</div>
        