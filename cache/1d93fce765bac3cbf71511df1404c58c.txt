
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/MrVolleyball/p/19031555" title="发布于 2025-08-11 15:07">
    <span role="heading" aria-level="2">彩笔运维勇闯机器学习--一元线性回归</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h2 id="前言">前言</h2>
<p>在运维职业生涯中，qps是一个绕不开的话题，leader经常在问，我们的qps是多少，系统能不能抗住啊？？？老板在问，我们的qps是多少，有没有降本的空间啊？？？面试的时候，面试官问，你们的qps是多少啊。。。。</p>
<p>如果我能预测qps与系统压力之间的关系，那一定很不错吧？关于leader，我们的qps是100w，系统完全能够扛得住；关于老板，我们的qps是1000w，系统经过优化之后，依然可以降本20%；关于面试官，我们的qps是1亿，并且系统 完全没有问题</p>
<p>为了在能够吹牛逼的情况下，把当前的工作（系统稳定，探索系统极限）做好，所以有必要搞一搞</p>
<h2 id="算法">算法</h2>
<p>机器学习中，一元线性回归是较为简单的算法，并且特征只需要一个，就可以预测结果，我们就用它来开始学习，由于系统压力有很多指标，cpu、内存、io、带宽等等等等，为了简化，我们就选cpu作为压力的代表，详细讨论一下，qps与cpu的变化特征</p>
<h2 id="开始探索">开始探索</h2>
<p>一元线性回归，就是分析自变量与因变量之间的线性关系，由于自变量只有1个，那线性关系就会容易得多，就是一次函数。简而言之，寻找一条直线，最大程度地去拟合样本特征(qps)和样本(cpu)输出标记之间的关系</p>
<p>我们先来看一看怎么使用一元线性回归</p>
<h4 id="1-scikit-learn包的使用">1. scikit-learn包的使用</h4>
<p>先不管什么鸡r原理，我目前也不想懂，我就需要看到效果，怎么进行一元回归分析。好的，请使用使用scikit-learn包，帮助我们快速上手</p>
<p>安装</p>
<pre><code>pip3 install -U scikit-learn
</code></pre>
<p>使用</p>
<pre><code>from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import pandas as pd

# 准备数据
data = {
    'result': [0.63, 0.72, 0.72, 0.63, 0.57, 0.52, 0.48, 0.47],
    'feature1': [22.48, 19.50, 18.02, 16.97, 15.78, 15.11, 14.02, 13.24]
}
df = pd.DataFrame(data)

X = df[['feature1']]
y = df['result']

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 创建模型并训练
model = LinearRegression()
model.fit(X_train, y_train)

# 模型评估
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"MSE: {mse}, R²: {r2}")

# 预测
new_data = pd.DataFrame({
    "feature1": [10.53, 20.81]
})
predicted_result = model.predict(new_data)
print("预测的 result:", predicted_result)

</code></pre>
<p>脚本！启动:</p>
<p><img alt="linear_regression_1_1" loading="lazy" data-src="https://img2024.cnblogs.com/blog/1416773/202508/1416773-20250811102435309-2110665836.png" class="lazyload"></p>
<h4 id="2-报告解读">2. 报告解读</h4>
<ul>
<li>MSE：均方误差，用于衡量模型预测值与真实值之间的差异，越趋于0越好</li>
<li>R²：决定系数，用于评估线性回归模型拟合优度的重要指标，其取值范围为[0, 1]，result能够被feature1解释的比例，简而言之，该模型能够通过给出的feature1预测出63.93%的result</li>
<li>给出的新的feature1： [10.53, 20.81]，该模型预测出的result是：[0.45440291 0.66743341]</li>
</ul>
<p>简单解读：这个模型不行！</p>
<ul>
<li>它解释不了所有的数据，只有63.93%，所以想要让他预测其他的数据，有大约三分之一的概率是错的</li>
<li>并且误差较大，误差是0.0036，那位大哥说，0.0036这还叫误差大吗？MSE的计算公式有平方计算（这个后面会说），所以0.0036开根号就是0.06，而result是0.xx的两位小数，这已经是10%--20%的误差了，太大了</li>
<li>模型不行有多方面的原因
<ul>
<li>数据量不够</li>
<li>特征不足或者特征不够，在本例由于是探索一元线性回归，所以不存在这个问题</li>
<li>需要处理异常值，就是看上去就不合理的result，在本例中，由于没给几个result，所以没法剔除异常值！</li>
<li>其他，交叉验证、超参数调优、同方差性。。。停停停，不要再说了，不要一开始就上强度！！！</li>
</ul>
</li>
</ul>
<p>好的，综上所述，本例造成模型不行的原因，就是数据量不够</p>
<h2 id="拟合与泛化">拟合与泛化</h2>
<p>下面介绍两个重要的概念</p>
<p>拟合：模型能够解释训练数据的程度</p>
<ul>
<li>过拟合：模型在训练数据上表现非常好，但在未知数据（测试数据或真实数据）上表现很差</li>
<li>欠拟合：模型在训练数据和未知数据上都表现不佳</li>
</ul>
<p>泛化：指模型在未知数据上的表现能力。一个好的模型不仅要在训练数据上表现良好，还要能够对未见过的数据做出准确的预测</p>
<p>在上一小节，我们训练的模型属于<code>欠拟合</code>，并且<code>泛化能力弱</code></p>
<h2 id="深入理解一元线性回归">深入理解一元线性回归</h2>
<p>一元线性回归就是在一堆有规律的散列点当中，找到一条能够解释这些散列点的线</p>
<p><img alt="linear_regression_1_2" loading="lazy" data-src="https://img2024.cnblogs.com/blog/1416773/202508/1416773-20250811102449800-69424329.png" class="lazyload"></p>
<h4 id="1-数学模型">1. 数学模型</h4>
<p></p><div class="math display">\[y=β_0+β_1x_1+ϵ
\]</div><p></p><ul>
<li><span class="math inline">\(β_0\)</span> 叫做截距，在模型中起到了“基准值”的作用，就是当自变量为0的时候，因变量的基准值</li>
<li><span class="math inline">\(β_1\)</span> 叫做自变量系数或者回归系数，描述了自变量对结果的影响方向和大小<br>
<img alt="linear_regression_1_3" loading="lazy" data-src="https://img2024.cnblogs.com/blog/1416773/202508/1416773-20250811102507991-1246115582.png" class="lazyload">
<ul>
<li>回归系数是非常重要的，合理的回归系数，决定了模型是否能够解释大部分数据。计算回归系数的算法，常见的有最小二乘法、梯度下降法等，其中最小二乘法是<code>scikit-learn</code>包的默认算法</li>
</ul>
</li>
<li>ϵ是误差项，代表了模型未能解释的部分</li>
</ul>
<h4 id="2-损失函数">2. 损失函数</h4>
<p>线性回归通常使用均方误差（MSE）来作为损失函数，衡量测试值与真实值之间的差异</p>
<p></p><div class="math display">\[\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\]</div><p></p><p>其中<span class="math inline">\(y_i\)</span>是真实值，<span class="math inline">\(\hat{y}_i\)</span>是预测值</p>
<p>正如前文提到，MSE中真实值与预测值，有平方计算，那就会放大误差，所以MSE可以非常有效的检测误差项</p>
<h4 id="3-最小二乘法">3. 最小二乘法</h4>
<p>下面我们来详细解释一下最小二乘法的原理，最小二乘法的核心思想是找到一组参数，使得模型预测值与实际观测值之间的误差平方和最小。细心的各位看这个定义，就发现这不就是上面的MSE的定义吗？只不过区别就是MSE取了样本平均值</p>
<p></p><div class="math display">\[\text{L} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\]</div><p></p><p>其中<span class="math inline">\(y_i\)</span>是真实值，<span class="math inline">\(\hat{y}_i\)</span>是预测值</p>
<p>这里来八卦一下最小二乘法。该算法是由法国大佬<code>勒让德</code>提出来的，误差是没有办法避免的，但是通过最小二乘法，使误差平方和达到最小，在各方程的误差之间建立了一种平衡，而这有助于揭示系统的更接近真实的状态。后面由<code>高斯</code>大佬证明了，误差服从了标准正态分布。所以误差平方就成为了在回归分析中计算误差的最佳准则</p>
<h4 id="4-决定系数">4. 决定系数</h4>
<p>用于评估线性回归模型拟合优度的重要指标，其取值范围为 [0, 1]</p>
<p></p><div class="math display">\[R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
\]</div><p></p><p>其中<span class="math inline">\(y_i\)</span>是真实值，<span class="math inline">\(\hat{y}_i\)</span>是预测值，<span class="math inline">\(\bar{y}\)</span>是均值</p>
<h2 id="scikit-learn中的常用参数"><code>scikit-learn</code>中的常用参数</h2>
<ul>
<li><code>test_size</code>：是用于划分数据集的关键参数，用于指定测试集的比例或大小，常见的值为 0.2 或 0.3</li>
<li><code>random_state</code>：用于控制随机过程的随机性
<ul>
<li>如果设置为整数，每次运行代码时，随机过程的结果将相同</li>
<li>如果设置为 None，每次运行代码时，随机过程的结果将不同</li>
</ul>
</li>
</ul>
<h2 id="联系我">联系我</h2>
<ul>
<li>联系我，做深入的交流<br>
<img alt="" width="500" height="200" loading="lazy" data-src="https://img2024.cnblogs.com/blog/1416773/202411/1416773-20241121135740959-1907948957.png#" class="lazyload"></li>
</ul>
<hr>
<p>至此，本文结束<br>
在下才疏学浅，有撒汤漏水的，请各位不吝赐教...</p>

</div>
<div id="MySignature" role="contentinfo">
    <p>本文来自博客园，作者：<a href="https://www.cnblogs.com/MrVolleyball/" target="_blank">it排球君</a>，转载请注明原文链接：<a href="https://www.cnblogs.com/MrVolleyball/p/19031555" target="_blank">https://www.cnblogs.com/MrVolleyball/p/19031555</a></p>
<div>本文版权归作者和博客园共有，欢迎转载，但未经作者同意必须在文章页面给出原文连接，否则保留追究法律责任的权利。 </div>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0" data-date-updated="2025-08-11 15:08">2025-08-11 15:07</span>&nbsp;
<a href="https://www.cnblogs.com/MrVolleyball">it排球君</a>&nbsp;
阅读(<span id="post_view_count">86</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(19031555);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '19031555', targetLink: 'https://www.cnblogs.com/MrVolleyball/p/19031555', title: '彩笔运维勇闯机器学习--一元线性回归' })">举报</a>
</div>
        