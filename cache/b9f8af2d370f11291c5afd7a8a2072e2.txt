
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/dechinphy/p/18728222/share-memory" title="发布于 2025-02-21 10:53">
    <span role="heading" aria-level="2">DeepSeek本地性能调优</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                    <div id="cnblogs_post_description" style="display: none">
        <img src="https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250221105032184-1618636620.png" alt="DeepSeek本地性能调优" class="desc_img">
        对于本地模型的加载来说，除了使用KTransformer等工具进行指令集层面的优化之外，还可以调整模型加载层数，做一个简单的优化。这里提供了一个num_gpu和num_ctx参数调整的策略，实测Tokens性能最大可优化10倍左右。
    </div>
<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h1 id="技术背景">技术背景</h1>
<p>大模型性能调优是一个很复杂的工程，涉及到很多细节，如果真要对模型动刀子，还需要对模型和数据集本身有非常好的理解。这里我们仅仅考虑模型加载过程中，可以优化的一些参数配置。关于DeepSeek在本地的部署，以及PageAssist插件的使用，可以参考<a href="https://www.cnblogs.com/dechinphy/collections/25319" target="_blank">DeepSeek合集</a>。</p>
<h1 id="调优思路">调优思路</h1>
<p>一句话总结一下调优思路，如果你已经尝试过了，那么可以略过本文：</p>
<pre><code class="language-bash">GPU内存 &gt; 多GPU内存 &gt; CPU &gt; 共享内存
</code></pre>
<p>遵循这个逻辑，对模型的加载层数进行配置，从而达到本地性能优化的目的。</p>
<h1 id="deepseek-r1-32b-q4_0测试">DeepSeek-R1-32B-Q4_0测试</h1>
<p>首先尝试用命令行启动，作为一个基线：</p>
<pre><code class="language-bash">PS A:\DeepSeek\models&gt; ollama run deepseek-r1-32B-Q4:latest --verbose
&gt;&gt;&gt; 简述拉格朗日乘子法在生物学中的应用场景。

&lt;/think&gt;

拉格朗日乘子法是一种优化技术，用于在有约束条件下寻找函数的极值。在生物学中，该方法可以应用于以下场景：

1. **生态模型**：在研究生态系统时，可能需要在资源有限的情况下最大化物种数量或最小化灭绝风险。拉格朗日乘子法可以帮助
找到这些条件下的最优解。

2. **基因表达分析**：在基因调控网络中，优化基因表达水平以满足特定的生物目标（如蛋白质产量最大化）时，可以使用拉格朗
日乘子法来处理约束条件。

3. **资源分配问题**：例如，在种群动态模型中，如何在有限的食物资源下分配给不同物种以达到某种平衡状态，拉格朗日乘子法
可以帮助找到最优的资源分配方案。

4. **进化生物学中的适应性优化**：研究生物体在环境压力下的最佳适应策略时，可以利用拉格朗日乘子法来求解在特定约束条件
下的最优适应度。

总之，拉格朗日乘子法为生物学中涉及优化和约束的问题提供了一种有力的工具。

total duration:       54.8643183s
load duration:        16.1375ms
prompt eval count:    21 token(s)
prompt eval duration: 5.23s
prompt eval rate:     4.02 tokens/s
eval count:           241 token(s)
eval duration:        49.611s
eval rate:            4.86 tokens/s
</code></pre>
<p>速度大约是4.86t。默认参数下的内存占用情况如下：</p>
<pre><code class="language-bash">PS C:\Users\dechin&gt; ollama ps
NAME                         ID              SIZE     PROCESSOR          UNTIL
deepseek-r1-32B-Q4:latest    d4a2da196dc7    19 GB    23%/77% CPU/GPU    4 minutes from now
</code></pre>
<p>Windows平台还可以用任务管理器查看资源占用：</p>
<div align="center">
	<img src="https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250221085819500-637631995.png">
</div>
<p>这里我们发现一个<code>问题</code>：Ollama默认的加载配置是<code>GPU &gt; CPU</code>的策略。也就是说：</p>
<pre><code class="language-bash">GPU内存 &gt; 多GPU内存 &gt; 共享内存 &gt; CPU
</code></pre>
<p>如果说，你的本地有众多的显卡，可以完全的把模型加载到显存里面，那么毫无疑问这个策略是对的。问题就在于，现在是大语言模型有小型化、本地化的趋势，所以如何让大模型在本地有限的硬件条件下跑起来，能用，才是最关键的。我们尝试去修改参数num_gpu:</p>
<div align="center">
	<img src="https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250221101432884-1436570954.png">
</div>
<p>这里使用的是PageAssist来进行加载了，可以用终端查看一下资源分配的情况：</p>
<pre><code class="language-bash">PS C:\Users\dechin&gt; ollama ps
NAME                         ID              SIZE     PROCESSOR          UNTIL
deepseek-r1-32B-Q4:latest    d4a2da196dc7    20 GB    23%/77% CPU/GPU    4 minutes from now
</code></pre>
<p>需要提醒的是，不论是内存还是共享内存，在Ollama这里就是属于<code>CPU</code>那一列。虽然跟默认配置都是<code>23%/77% CPU/GPU</code>，但是如果我们用系统资源管理器来查看，就会发现此时没有占用共享内存：</p>
<div align="center">
	<img src="https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250221101541836-528516837.png">
</div>
<p>tokens表现：</p>
<div align="center">
	<img src="https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250221101602347-1253225190.png">
</div>
<p>在这个配置下，tokens速度达到了6.45t，比默认配置的速度4.86t要优化了一些。再者考虑到这里显存并没有占满，所以还有优化的空间，可以再把层数拉大：</p>
<div align="center">
	<img src="https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250221101656492-359687543.png">
</div>
<p>再次运行起来，我们可以看到显存资源基本上是占满了，再加大有可能报OOM错误：</p>
<div align="center">
	<img src="https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250221101613260-1549461580.png">
</div>
<p>tokens表现：</p>
<div align="center">
	<img src="https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250221101627210-208713815.png">
</div>
<p>此时tokens速度来到了7.54，进一步优化了模型的性能。此时模型加载的比例有一点点变化：</p>
<pre><code class="language-bash">PS C:\Users\dechin&gt; ollama ps
NAME                         ID              SIZE     PROCESSOR          UNTIL
deepseek-r1-32B-Q4:latest    d4a2da196dc7    20 GB    22%/78% CPU/GPU    4 minutes from now
</code></pre>
<p>虽然CPU的比例只是降了1%，但是因为不涉及到共享内存的使用，性能反而得到了优化。</p>
<h1 id="deepseek-r1-70b-q2_k测试">DeepSeek-R1-70B-Q2_K测试</h1>
<p>其实原本以为70B-Q2K已经是我的本地可以运行的模型的极限了，所以甚至没有使用Q4_0。首先我们使用70B num_gpu=64进行测试：</p>
<div align="center">
	<img src="https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250221101728863-1395359468.png">
</div>
<p>此时资源的占用比例为：</p>
<pre><code class="language-bash">PS C:\Users\dechi&gt; ollama ps
NAME                          ID              SIZE     PROCESSOR          UNTIL
deepseek-r1-70B-Q2K:latest    eb199ae0e147    28 GB    45%/55% CPU/GPU    4 minutes from now
</code></pre>
<p>tokens表现：</p>
<div align="center">
	<img src="https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250221101743317-867619134.png">
</div>
<p>这里tokens速度仅有0.28，说实话已经是一个不太能接受的性能了，不如直接用API。不过我们这里以优化性能为主要考量，继续考虑调参方向。如果设置num_gpu=56：</p>
<pre><code class="language-bash">PS C:\Users\dechi&gt; ollama ps
NAME                          ID              SIZE     PROCESSOR          UNTIL
deepseek-r1-70B-Q2K:latest    eb199ae0e147    29 GB    46%/54% CPU/GPU    4 minutes from now
</code></pre>
<p>比例有一点点微小的变化，但是主要还是得从资源管理器里面去查看共享内存的使用情况：</p>
<div align="center">
	<img src="https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250221101805027-1990255494.png">
</div>
<p>可以看到GPU内存基本占满，共享内存接近于0，这里把CPU也拉起来了：</p>
<div align="center">
	<img src="https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250221102055479-911417241.png">
</div>
<p>tokens表现：</p>
<div align="center">
	<img src="https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250221102109492-106414979.png">
</div>
<p>这里我们优化后的tokens性能在3.09t，说实话吐字速度勉强是可以接受的，比默认配置优化了10倍的性能。不过这里Q2_K模型的语言能力实在是有点一言难尽，建议还是使用Q4_0以上的模型。最后再补充一个70B的Q4_0量化版本的资源分配和运行数据：</p>
<pre><code class="language-bash">PS A:\DeepSeek\models&gt; ollama ps
NAME                         ID              SIZE     PROCESSOR          UNTIL
deepseek-r1-70B-Q4:latest    7ea384772955    42 GB    62%/38% CPU/GPU    4 minutes from now
</code></pre>
<div align="center">
	<img src="https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250221145320683-865596394.png">
</div>
<p>在GPU中放了32层，tokens的速度大约在2t左右，1秒吐2个字这样，能不能接受取决于个人接受度和应用场景了。</p>
<h1 id="总结概要">总结概要</h1>
<p>对于本地模型的加载来说，除了使用KTransformer等工具进行指令集层面的优化之外，还可以调整模型加载层数，做一个简单的优化。这里提供了一个num_gpu和num_ctx参数调整的策略，实测Tokens性能最大可优化10倍左右。</p>
<h1 id="版权声明">版权声明</h1>
<p>本文首发链接为：<a href="https://www.cnblogs.com/dechinphy/p/share-memory.html" target="_blank">https://www.cnblogs.com/dechinphy/p/share-memory.html</a></p>
<p>作者ID：DechinPhy</p>
<p>更多原著文章：<a href="https://www.cnblogs.com/dechinphy/" target="_blank">https://www.cnblogs.com/dechinphy/</a></p>
<p>请博主喝咖啡：<a href="https://www.cnblogs.com/dechinphy/gallery/image/379634.html" target="_blank">https://www.cnblogs.com/dechinphy/gallery/image/379634.html</a></p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.8795313159849537" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-02-21 15:11">2025-02-21 10:53</span>&nbsp;
<a href="https://www.cnblogs.com/dechinphy">DECHIN</a>&nbsp;
阅读(<span id="post_view_count">569</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18728222" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18728222);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18728222', targetLink: 'https://www.cnblogs.com/dechinphy/p/18728222/share-memory', title: 'DeepSeek本地性能调优' })">举报</a>
</div>
        