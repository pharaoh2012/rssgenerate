
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/johnnyzen/p/18699940" title="发布于 2025-02-05 18:08">
    <span role="heading" aria-level="2">[文件格式/数据存储] Parquet：开源、高效的列式存储文件格式协议</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h1 id="序缘起--用-java-读取-parquet-文件">序:缘起 =&gt; 用 java 读取 parquet 文件</h1>
<ul>
<li>生产环境有设备出重大事故，又因一关键功能无法使用，亟需将生产环境的原始MQTT报文（以 parquet 文件格式 + zstd 压缩格式 落盘）DOWN到本地，读取并解析。</li>
</ul>
<blockquote>
<p>本文案例中，聚焦在 本地电脑，用 java 读取 parquet 文件<br>
相当多网络文档的读取代码无法正常运行，有必要记录一二，后续还需进一步研究 parquet 和 zstd 算法。</p>
</blockquote>
<h1 id="概述parquet">概述：Parquet</h1>
<ul>
<li>摘要：</li>
</ul>
<blockquote>
<p>Apache Parquet是一种高效、灵活且可扩展的列式存储格式，专为大规模数据处理而设计。<br>
它通过列式存储、数据压缩和编码优化，显著提高了数据的读取和写入性能。<br>
Parquet与多种数据处理框架和数据模型兼容，使其成为大数据生态系统中不可或缺的一部分。</p>
</blockquote>
<h2 id="什么是-parquet-apache-hadoop社区发起的开源的大数据量场景的高效的列式存储压缩与传输的文件格式协议">什么是 Parquet? Apache Hadoop社区发起的、开源的、大数据量场景的、高效的列式存储、压缩与传输的文件格式协议</h2>
<ul>
<li><code>Parquet</code> 是一个开源的、用于<strong>存储和传输</strong>大数据的、<strong>高效</strong>的、基于<strong>列式存储</strong>的文件格式，它使用一种称为 <code>Columnar</code> 的<strong>数据压缩算法</strong>来优化数据压缩和传输。</li>
</ul>
<blockquote>
<ul>
<li>缘起。Parquet 的灵感来源于<code>2010</code>年 <code>Google</code> 发表的 Dremel 论文，该论文介绍了一种<strong>支持嵌套结构</strong>的<strong>列式存储格式</strong>。</li>
<li>创始与开发团队。最初由Twitter和Cloudera联合开发，并于<code>2013</code>年捐赠给Apache软件基金会，于2015年5月从 Apache 孵化器毕业，成为 Apache 顶级项目。</li>
</ul>
<blockquote>
<p>由 <code>Apache</code> 开源项目 <code>Hadoop</code> 创建，并作为 <code>Hadoop</code> 的一部分进行维护。</p>
</blockquote>
<ul>
<li>应用领域。Parquet专为<strong>大规模数据处理</strong>而设计，支持多种数据处理框架，如： <code>Apache Spark</code>、<code>Hadoop MapReduce</code>、<code>Presto</code>、<code>Apache Flink</code>等。</li>
<li>优势。Parquet文件格式的优势在于<strong>高效的压缩</strong>和<strong>高效的列式存储</strong>，使得它在大数据处理中具有很高的性能。</li>
</ul>
</blockquote>
<h2 id="核心特点">核心特点</h2>
<ul>
<li><strong>列式存储</strong>：Parquet以列式格式存储数据。</li>
</ul>
<blockquote>
<p>这意味着每一列的数据被连续存储。这种存储方式使得对特定列的读取操作更加高效，因为只需要读取相关的列数据，而无需扫描整个行。这在处理大规模数据集时尤其有用，尤其是在执行聚合查询和分析时。</p>
</blockquote>
<ul>
<li><strong>数据压缩</strong>：Parquet支持多种<strong>压缩算法</strong>，如：<code>zstd</code>、<code>Snappy</code>、<code>Gzip</code>、<code>LZO</code>等。</li>
</ul>
<blockquote>
<p>压缩可以显著减少存储空间的占用，并提高数据的读写速度。通过压缩，数据在磁盘上的存储效率更高，同时在读取时可以更快地加载到内存中。</p>
</blockquote>
<ul>
<li>
<p><strong>数据编码</strong>：Parquet提供了多种<strong>数据编码方式</strong>，如字典编码、RLE（Run-Length Encoding）等。这些编码方式可以进一步优化数据的存储和读取性能，尤其是在<strong>处理重复数据</strong>时。</p>
</li>
<li>
<p><strong>可扩展性</strong>：Parquet文件可以<strong>被分割成多个块</strong>（<code>Row Groups</code>），每个块可以独立读取和处理。</p>
</li>
</ul>
<blockquote>
<p>这种设计使得Parquet文件非常适合<strong>分布式处理框架</strong>，如Apache Spark和Hadoop MapReduce，因为它们可以并行处理文件的不同部分。</p>
</blockquote>
<ul>
<li><strong>与多种数据模型兼容</strong>：Parquet支持多种<strong>数据模型</strong>，包括<code>Avro</code>、<code>Thrift</code>和<code>Protobuf</code>。</li>
</ul>
<blockquote>
<p>这意味着您可以使用不同的数据模型来定义和存储数据，而Parquet能够无缝地处理这些数据。</p>
</blockquote>
<h2 id="应用场景">应用场景</h2>
<blockquote>
<p>Parquet广泛应用于以下领域：</p>
</blockquote>
<ul>
<li>大数据分析：由于其高效的存储和读取性能，Parquet非常适合用于大规模数据分析，如数据仓库、数据湖等。</li>
<li>机器学习：在机器学习中，Parquet可以用于存储和处理训练数据，支持快速的数据加载和特征提取。</li>
<li>实时数据处理：Parquet的列式存储和压缩特性使其在实时数据处理中表现出色，能够快速读取和处理数据。</li>
</ul>
<h2 id="官方文献">官方文献</h2>
<ul>
<li><a href="https://parquet.apache.org/" target="_blank" rel="noopener nofollow">Parquet - Apache</a></li>
</ul>
<blockquote>
<ul>
<li><a href="https://parquet.apache.org/docs/file-format/" target="_blank" rel="noopener nofollow">https://parquet.apache.org/docs/file-format/</a></li>
</ul>
</blockquote>
<h1 id="原理分析parquet文件格式-file-format">原理分析：Parquet文件格式( File Format)</h1>
<p><img src="https://img2024.cnblogs.com/blog/1173617/202502/1173617-20250205173656280-147712957.png" alt="" loading="lazy"></p>
<ul>
<li>文件格式:</li>
</ul>
<blockquote>
<ul>
<li><strong>Block（HDFS块）</strong>：指的是HDFS中的一个块，对于描述这种文件格式，其含义不变。该文件格式设计得能够在HDFS上良好运作。</li>
<li><strong>文件(File)</strong>：一个必须包含<strong>文件元数据</strong>的HDFS文件。实际上，它不需要包含数据本身。</li>
<li><strong>行组(Row group)</strong>：数据在行方向的逻辑分区。对于行组，没有保证其存在<strong>物理结构</strong>。一个<strong>行组</strong>由数据集中每个<strong>列</strong>的<strong>列块</strong>组成。</li>
<li><strong>列块(Column chunk)</strong>：一个特定列的数据块。它们存在于特定的行组中，并且在文件中保证是<strong>连续</strong>的。</li>
<li><strong>页面(Page)</strong>：<strong>列块</strong>被划分成<strong>页面</strong>。页面在概念上是一个<strong>不可分割的最小单元</strong>（就<strong>压缩</strong>和<strong>编码</strong>而言）。在一个<strong>列块</strong>中可以有多个页面类型，它们交织在一起。</li>
</ul>
</blockquote>
<ul>
<li>从层次上看，一个<strong>文件</strong>包含一个或多个<strong>行组</strong>。一个<strong>行组</strong>包含每列恰好一个<strong>列块</strong>。<strong>列块</strong>包含一个或多个<strong>页面</strong>。</li>
</ul>
<blockquote>
<p><code>Parquet文件</code> - <code>行组</code> - <code>列块</code> - <code>页面</code></p>
</blockquote>
<p><img src="https://img2024.cnblogs.com/blog/1173617/202502/1173617-20250205174131176-882953408.png" alt="" loading="lazy"></p>
<h1 id="实现框架与客户端">实现框架与客户端</h1>
<h2 id="parquetviewer--开源的-parquet-文件查看器">ParquetViewer : 开源的 Parquet 文件查看器</h2>
<ul>
<li>ParquetViewer 项目</li>
</ul>
<blockquote>
<ul>
<li><a href="https://github.com/mukunku/ParquetViewer/releases" target="_blank" rel="noopener nofollow">https://github.com/mukunku/ParquetViewer/releases</a></li>
<li><a href="https://github.com/mukunku/ParquetViewer/releases/download/v3.0.0.1/ParquetViewer_SelfContained.exe" target="_blank" rel="noopener nofollow">https://github.com/mukunku/ParquetViewer/releases/download/v3.0.0.1/ParquetViewer_SelfContained.exe</a></li>
</ul>
</blockquote>
<p><img src="https://img2024.cnblogs.com/blog/1173617/202502/1173617-20250205175704021-577516673.png" alt="" loading="lazy"></p>
<p><img src="https://img2024.cnblogs.com/blog/1173617/202502/1173617-20250205175217493-261620060.png" alt="" loading="lazy"></p>
<h2 id="vscode--parquet-viewer插件">VSCode + <code>Parquet-Viewer</code>插件</h2>
<ul>
<li>url</li>
</ul>
<blockquote>
<ul>
<li><a href="https://marketplace.visualstudio.com/items?itemName=dvirtz.parquet-viewer" target="_blank" rel="noopener nofollow">https://marketplace.visualstudio.com/items?itemName=dvirtz.parquet-viewer</a></li>
</ul>
</blockquote>
<p><img src="https://img2024.cnblogs.com/blog/1173617/202502/1173617-20250205175553909-1937549578.png" alt="" loading="lazy"></p>
<p><img src="https://img2024.cnblogs.com/blog/1173617/202502/1173617-20250205175911316-768337267.png" alt="" loading="lazy"></p>
<h2 id="parquet-viewer--parquet文件在线浏览网站">Parquet-Viewer : Parquet文件在线浏览网站</h2>
<ul>
<li>url</li>
</ul>
<blockquote>
<ul>
<li><a href="https://www.parquet-viewer.com/" target="_blank" rel="noopener nofollow">https://www.parquet-viewer.com/</a></li>
</ul>
</blockquote>
<h2 id="orgapacheparquet-java-sdk"><code>org.apache.parquet:*</code>: Java SDK</h2>
<blockquote>
<p>详情参见本文档：【案例实践】章节，"<code>org.apache.parquet</code>"</p>
</blockquote>
<h2 id="pandas--集成了-parquet-的-python-data-analysis-library">Pandas : 集成了 Parquet 的 Python Data Analysis Library</h2>
<ul>
<li>python pandas 支持Parquet 格式，与csv 一下简单对比，首先我们测试存储情况</li>
</ul>
<pre><code class="language-python">import pandas as pd
df = pd.read_csv('results.csv')

# df.to_parquet('df_test.parquet.zstd', compression='zstd')  
df.to_parquet('df_test.parquet.gzip', compression='gzip')  
df.to_parquet('df_test.parquet.snappy', compression='snappy')  
df.to_parquet('df_test.parquet', compression=None)  

import os
file_size = os.path.getsize('results.csv')
print("results.csv size:",  f'{file_size:,}', "bytes")


file_size = os.path.getsize('df_test.parquet.gzip')
print("df_test.parquet.gzip size:",  f'{file_size:,}', "bytes")

file_size = os.path.getsize('df_test.parquet.snappy')
print("df_test.parquet.snappy size:",  f'{file_size:,}', "bytes")

file_size = os.path.getsize('df_test.parquet')
print("df_test.parquet size:",  f'{file_size:,}', "bytes")
</code></pre>
<p><img src="https://img2024.cnblogs.com/blog/1173617/202502/1173617-20250205174405620-1554537088.png" alt="" loading="lazy"></p>
<blockquote>
<p>parquet格式的存储，相比 csv 减少80%，结合压缩后是 csv 的 10%，可以节约90% 的存储空间。</p>
</blockquote>
<ul>
<li>为数据加载测试，我们将数据放大100倍。</li>
</ul>
<p><img src="https://img2024.cnblogs.com/blog/1173617/202502/1173617-20250205174531624-978200702.png" alt="" loading="lazy"></p>
<pre><code class="language-python">%time df = pd.read_csv('df_test.csv')
%time df = pd.read_parquet('df_test.parquet.gzip')
%time df = pd.read_parquet('df_test.parquet.snappy')
%time df = pd.read_parquet('df_test.parquet')
</code></pre>
<p><img src="https://img2024.cnblogs.com/blog/1173617/202502/1173617-20250205174552482-1892698722.png" alt="" loading="lazy"></p>
<blockquote>
<p>使用pandas 加载 dataframe 也有大幅度提升，提升了2-3倍。</p>
</blockquote>
<h1 id="案例实践">案例实践</h1>
<h2 id="case--基于-java-sdk-读取parquet文件">CASE : 基于 Java SDK 读取Parquet文件</h2>
<h3 id="引入依赖">引入依赖</h3>
<pre><code class="language-xml">&lt;properties&gt;
	&lt;!-- 1.13.1 / 1.12.0 --&gt;
	&lt;parquet.version&gt;1.13.1&lt;/parquet.version&gt;
	&lt;avro.version&gt;1.10.2&lt;/avro.version&gt;
	&lt;!-- 3.2.1 / 2.7.3 --&gt;
	&lt;hadoop.version&gt;3.2.1&lt;/hadoop.version&gt;
	&lt;!-- 1.5.0-1 / 1.5.5-5 / 与 kafka-clients:1.4-xxx 版本冲突 --&gt;
	&lt;zstd.version&gt;1.5.0-1&lt;/zstd.version&gt;
&lt;/properties&gt;

&lt;dependency&gt;
	&lt;groupId&gt;org.apache.parquet&lt;/groupId&gt;
	&lt;artifactId&gt;parquet-avro&lt;/artifactId&gt;
	&lt;version&gt;${parquet.version}&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
	&lt;groupId&gt;org.apache.parquet&lt;/groupId&gt;
	&lt;artifactId&gt;parquet-hadoop&lt;/artifactId&gt;
	&lt;version&gt;${parquet.version}&lt;/version&gt;
&lt;/dependency&gt;

&lt;dependency&gt;
	&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
	&lt;artifactId&gt;hadoop-mapreduce-client-core&lt;/artifactId&gt;
	&lt;version&gt;${hadoop.version}&lt;/version&gt;
	&lt;exclusions&gt;
		&lt;exclusion&gt;
			&lt;artifactId&gt;commons-compress&lt;/artifactId&gt;
			&lt;groupId&gt;org.apache.commons&lt;/groupId&gt;
		&lt;/exclusion&gt;
	&lt;/exclusions&gt;
&lt;/dependency&gt;

&lt;dependency&gt;
	&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
	&lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;
	&lt;version&gt;${hadoop.version}&lt;/version&gt;
	&lt;exclusions&gt;
		&lt;exclusion&gt;
			&lt;artifactId&gt;commons-compress&lt;/artifactId&gt;
			&lt;groupId&gt;org.apache.commons&lt;/groupId&gt;
		&lt;/exclusion&gt;
	&lt;/exclusions&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
	&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
	&lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;
	&lt;version&gt;${hadoop.version}&lt;/version&gt;
&lt;/dependency&gt;

&lt;!-- ztsd
	Zstd 的 RecyclingBufferPool‌ 是一种内存管理机制，用于高效地重用缓冲区，减少内存分配和释放的开销。
	Zstd库中的BufferPool接口提供了多种实现，其中RecyclingBufferPool是一种常见的实现方式。 --&gt;
&lt;dependency&gt;
	&lt;groupId&gt;com.github.luben&lt;/groupId&gt;
	&lt;artifactId&gt;zstd-jni&lt;/artifactId&gt;
	&lt;!-- 1.5.5-5 / kafka-clients 包冲突 --&gt;
	&lt;version&gt;${zstd.version}&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<h3 id="readparquetformatmqttmessagerawdatademo">ReadParquetFormatMQTTMessageRawDataDemo</h3>
<pre><code class="language-java">import com.xx.yy.common.utils.DatetimeUtil;
import com.xx.yy.common.utils.FileUtil;
import com.alibaba.fastjson2.JSON;
import lombok.SneakyThrows;
import lombok.extern.slf4j.Slf4j;
import org.apache.commons.io.FileUtils;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.parquet.hadoop.ParquetFileReader;
import org.apache.parquet.hadoop.ParquetReader;
import org.apache.parquet.example.data.Group;
import org.apache.parquet.hadoop.example.GroupReadSupport;
import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;
import org.apache.parquet.hadoop.metadata.ParquetMetadata;

import java.io.IOException;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

/**
 * 本地读取 Parquet 文件
 * @reference-doc
 *  [1] java读取parquet文件 - https://blog.csdn.net/letterss/article/details/131417952
 *  [2] datax读取Parquet格式文件总列数 - https://blog.csdn.net/letterss/article/details/131189471
 */
@Slf4j
public class ReadParquetFormatMQTTMessageRawDataDemo {
    @SneakyThrows
    public static void main(String[] args) {
        String baseDir = "file:///E:/tmp_data/parquet_mqtt_messages/ods_raw_data_history/";
        String parquetFilePathStr = baseDir + "20081-XXXX-XXXXXXXXXX/e7db5c81e70131d55d4fb4a1752b90f2-1.parquet"; //"output.parquet"

        try {
            // 指定 Parquet 文件路径
            //Path parquetFilePath = new Path("C:\\\\Users\\\\Administrator\\\\Desktop\\\\07fe7433-99c2-41d8-a91d-27a77d99f690-0_4-8-0_20230510103931772.parquet");
            Path parquetFilePath = new Path(parquetFilePathStr);

            //查询总列数，参考博客: https://blog.csdn.net/letterss/article/details/131189471
            int allColumnsCount = getParquetAllColumnsCount(parquetFilePath);
            int columnIndexMax = -1;
            columnIndexMax = allColumnsCount - 1;
            // 创建 ParquetReader.Builder 实例
            ParquetReader.Builder&lt;Group&gt; builder = ParquetReader.builder(new GroupReadSupport(), parquetFilePath);
            // 创建 ParquetReader 实例
            ParquetReader&lt;Group&gt; reader = builder.build();
            // 循环读取 Parquet 文件中的记录
            Group record;

            List&lt;Map&lt;String, Object&gt;&gt; records = new ArrayList&lt;&gt;();

            while ((record = reader.read()) != null) {
                Map&lt;String, Object&gt; recordMap = new HashMap&lt;&gt;();
                // 处理每个记录的逻辑
                for (int i = 0; i &lt;= columnIndexMax; i++) {
                    String fieldKey = record.getType().getType(i).getName(); //record.getType().getFieldName(i);
                    Object fieldValue = record.getValueToString(i, 0);
                    recordMap.put( fieldKey , fieldValue);
                    System.out.println(fieldValue);
                }
                records.add( recordMap );

                //writeMqttMessageRawDataToLocal(recordMap);
            }
            System.out.println(JSON.toJSONString( records ));
            // 关闭读取器
            reader.close();
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    @SneakyThrows
    public static void writeMqttMessageRawDataToLocal(Map&lt;String, Object&gt; recordMap){
        String targetBaseDir = "E:\\tmp_data\\batch_parse_raw_mqtt_messages\\";
        String hexMqttMessageRawData = (String) recordMap.get("raw_data");
        String deviceId = (String) recordMap.get("device_id");
        Long collectTime = Long.valueOf( (String) recordMap.get("collect_time") );

        String mqttMessageRawDataFile = String.format("%s.%d(%s).hex-bin", deviceId, collectTime, DatetimeUtil.longToString(collectTime, DatetimeUtil.MILLISECOND_TIME_WITH_NUMBER_FORMAT) );
        FileUtil.writeToFile(hexMqttMessageRawData, targetBaseDir + mqttMessageRawDataFile);
        //FileUtils.write();
    }

    /**
	 * 获取 parquet 的 总列数
	 * @reference-doc
	 *   [1] datax读取Parquet格式文件总列数 - https://blog.csdn.net/letterss/article/details/131189471
	 */
    public static int getParquetAllColumnsCount(Path path){
        int columnCount = -1;
        Configuration configuration = new Configuration();
        //Path path = new Path(parquetFilePath);
        try {
            ParquetMetadata metadata = ParquetFileReader.readFooter(configuration, path);
            List&lt;ColumnChunkMetaData&gt; columns = metadata.getBlocks().get(0).getColumns();
            columnCount = columns.size();
            System.out.println("Total column count: " + columnCount);
        } catch (Exception e) {
            e.printStackTrace();
        }
        return columnCount;
    }
}
</code></pre>
<h2 id="case--python-pands-on-parquet">CASE : Python Pands On Parquet</h2>
<blockquote>
<p>参见本文档： Pandas 的 描述</p>
</blockquote>
<h2 id="case--duckdb-数据库-on-parquet">CASE : DuckDB 数据库 On Parquet</h2>
<ul>
<li>
<p>DuckDB 与 Parquet 的集成是无缝的，使得轻松使用SQL进行Parquet 文件上数据进行分析、查询和统计。</p>
</li>
<li>
<p>DuckDB 最大的优势在于能够对庞大的数据集运行查询，无论是具有大量小文件还是一个巨大的文件。使用熟悉的 SQL 语法在个人电脑/笔记本上执行探索性数据分析（EDA）任务，为分析大量数据带来了新的视角。</p>
</li>
</ul>
<pre><code class="language-SQL">select 
    home_team    ,count(*),avg(home_score) 
from read_parquet('/home/df_test.parquet.snappy') 
group by home_team
</code></pre>
<p><img src="https://img2024.cnblogs.com/blog/1173617/202502/1173617-20250205174738245-1127548382.png" alt="" loading="lazy"></p>
<h1 id="y-faq">Y FAQ</h1>
<h2 id="q-apache-parquetapache-avro-和-zstd-之间的关系和区别是什么">Q: Apache Parquet、Apache Avro 和 ZSTD 之间的关系和区别是什么？</h2>
<blockquote>
<p>Apache Parquet、Apache Avro 和 ZSTD 在大数据处理中各自扮演着不同的角色，它们之间的关系和区别如下：</p>
</blockquote>
<ul>
<li>Parquet 与 Avro</li>
</ul>
<blockquote>
<ul>
<li>存储格式：</li>
</ul>
<blockquote>
<ul>
<li>Parquet：列式存储格式，数据按列存储，适合读取密集型操作和分析查询。它在处理大规模数据集时效率更高，尤其是在需要对特定列进行聚合或过滤时。</li>
<li>Avro：行式存储格式，数据按行存储，适合写入密集型操作和需要快速序列化/反序列化的场景。它在处理事务性系统或消息序列化时表现更好。</li>
</ul>
</blockquote>
</blockquote>
<blockquote>
<ul>
<li>模式管理：</li>
</ul>
<blockquote>
<ul>
<li>Parquet：模式存储在文件尾部，使用Thrift格式，模式演变相对不那么灵活。</li>
<li>Avro：模式以JSON格式存储，支持强大的模式演变，包括向后、向前和完全兼容。</li>
</ul>
</blockquote>
</blockquote>
<blockquote>
<ul>
<li>压缩与性能：</li>
</ul>
<blockquote>
<ul>
<li>Parquet：支持列级压缩（如Snappy、Gzip、ZSTD等），在分析查询中效率更高。</li>
<li>Avro：支持块级压缩（如Snappy、Deflate等），在处理整行数据时效率更高。</li>
</ul>
</blockquote>
</blockquote>
<blockquote>
<ul>
<li>使用场景：</li>
</ul>
<blockquote>
<ul>
<li>Parquet：适合数据仓库、大数据分析和批处理。</li>
<li>Avro：适合流处理系统（如Apache Kafka）、消息序列化和需要频繁写入的场景。</li>
</ul>
</blockquote>
</blockquote>
<ul>
<li>Parquet 与 ZSTD</li>
</ul>
<blockquote>
<blockquote>
<ul>
<li>ZSTD 是一种高效的压缩算法，以高压缩率和快速解压缩速度著称。</li>
<li>Parquet 是一种文件格式协议，支持多种压缩算法，包括 ZSTD。使用 ZSTD 压缩可以显著减少 Parquet 文件的存储空间，同时保持较高的读取速度。</li>
<li>在实际应用中，ZSTD 压缩在 Parquet 文件中表现出色，尤其是在需要高效存储和快速读取的场景中。</li>
</ul>
</blockquote>
</blockquote>
<ul>
<li>总结</li>
</ul>
<blockquote>
<ul>
<li>Parquet 是一种列式存储格式，适合分析和读取密集型操作，支持多种压缩算法（包括ZSTD）。</li>
<li>Avro 是一种行式存储格式，适合写入密集型操作和消息序列化，支持灵活的模式演变。</li>
<li>ZSTD 是一种高效的压缩算法，可以与 Parquet 结合使用，以提高存储效率和读取速度。</li>
</ul>
</blockquote>
<blockquote>
<p>选择哪种格式取决于您的具体需求：如果需要高效读取和分析数据，Parquet 是更好的选择；如果需要快速写入和序列化数据，Avro 更适合。而 ZSTD 压缩算法可以在 Parquet 文件中提供高效的存储和读取性能。</p>
</blockquote>
<h2 id="q-parquet-与-avro-如何结合使用">Q: Parquet 与 Avro 如何结合使用？</h2>
<blockquote>
<p>Parquet和Avro可以结合使用，以充分利用两者的优势。以下是它们结合使用的方式和场景：</p>
</blockquote>
<ol>
<li>
<p>数据存储与读取<br>
Avro用于数据写入：Avro是一种基于行的存储格式，适合写入密集型操作，尤其是在需要快速序列化和反序列化数据的场景中（如Kafka）。它支持灵活的模式演变，能够轻松处理数据结构的变化。<br>
Parquet用于数据读取：Parquet是一种基于列的存储格式，适合读取密集型操作，尤其是在需要对数据进行分析和查询的场景中。它支持高效的列式存储和压缩，能够显著减少I/O操作。</p>
</li>
<li>
<p>数据转换<br>
从Avro到Parquet：在数据湖或数据仓库中，可以将Avro格式的数据转换为Parquet格式。这样可以在保留Avro的写入性能的同时，利用Parquet的高效读取性能。例如，可以使用Apache Spark或Hadoop MapReduce来完成这种转换。<br>
从Parquet到Avro：在某些需要快速写入的场景中，可以将Parquet格式的数据转换为Avro格式，以便更好地支持流处理。</p>
</li>
<li>
<p>混合使用<br>
结合使用场景：在实际应用中，可以将Avro和Parquet结合使用。例如，可以将较新的数据存储为Avro文件，以便快速写入；而将历史数据转换为Parquet文件，以便高效读取。这种混合使用方式可以在不同的数据处理阶段发挥各自的优势。</p>
</li>
<li>
<p>工具支持<br>
Apache Spark：Apache Spark支持对Avro和Parquet文件的读写操作。可以使用Spark的API将Avro文件转换为Parquet文件，或者在需要时将Parquet文件转换为Avro文件。<br>
Hadoop MapReduce：Hadoop MapReduce也支持对这两种格式的处理，可以编写MapReduce作业来完成数据格式的转换。</p>
</li>
</ol>
<ul>
<li>总结</li>
</ul>
<blockquote>
<ul>
<li>Parquet和Avro的结合使用可以充分发挥两者的优势。</li>
<li><code>Avro</code>适合<strong>快速写入</strong>和<strong>数据序列化</strong>，而<code>Parquet</code>适合<strong>高效读取</strong>和<strong>数据分析</strong>。通过在不同的数据处理阶段选择合适的格式，可以优化数据存储和处理的性能。</li>
</ul>
</blockquote>
<h1 id="x-参考文献">X 参考文献</h1>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/689305554" target="_blank" rel="noopener nofollow">一文带您理解Apache Parquet：高效存储和处理大数据的利器 - zhihu</a></li>
<li><a href="https://blog.csdn.net/letterss/article/details/131417952" target="_blank" rel="noopener nofollow">java读取parquet文件 - CSDN</a> 【参考/推荐】</li>
<li><a href="https://blog.csdn.net/letterss/article/details/131189471" target="_blank" rel="noopener nofollow">datax读取Parquet格式文件总列数 - CSDN</a> 【参考/推荐】</li>
<li><a href="https://blog.csdn.net/weixin_43369296/article/details/132837895" target="_blank" rel="noopener nofollow">Java读取本地Parquet文件 - CSDN</a></li>
<li><a href="https://blog.csdn.net/m0_37739193/article/details/136300647" target="_blank" rel="noopener nofollow">Parquet 文件生成和读取 - CSDN</a></li>
<li><a href="https://cloud.tencent.com/developer/ask/sof/1472416/answer/2015623" target="_blank" rel="noopener nofollow">org.apache.parquet.io.InputFile的S3实现？ - 腾讯云</a></li>
</ul>
<pre><code class="language-java">Configuration conf = new Configuration();
conf.set(Constants.ENDPOINT, "https://s3.eu-central-1.amazonaws.com/");
conf.set(Constants.AWS_CREDENTIALS_PROVIDER,
    DefaultAWSCredentialsProviderChain.class.getName());
// maybe additional configuration properties depending on the credential provider


URI uri = URI.create("s3a://bucketname/path");
org.apache.hadoop.fs.Path path = new Path(uri);

ParquetFileReader pfr = ParquetFileReader.open(HadoopInputFile.fromPath(path, conf))
</code></pre>
<ul>
<li><a href="https://blog.csdn.net/trayvontang/article/details/102909127" target="_blank" rel="noopener nofollow">Parquet文件读写与合并小Parquet文件 - CSDN</a></li>
<li><a href="https://blog.51cto.com/u_16213363/12808368" target="_blank" rel="noopener nofollow">java 解析 parquet文件怎么打开 - 51CTO</a></li>
</ul>

</div>
<div id="MySignature" role="contentinfo">
    <div class="essaySuffix-box">
    <div class="essaySuffix-box-left" style=" margin: 6px auto; ">
        <img src="https://blog-static.cnblogs.com/files/johnnyzen/cnblogs-qq-group-qrcode.gif?t=1679679148" alt="QQ沟通交流群" onload="changeImg(this,200,100)">
    </div>
<div class="essaySuffix-box-right">
    <span class="essaySuffix-right-title">本文作者</span>：
        <strong><span><a href="https://github.com/Johnny-ZTSD" target="_blank">千千寰宇</a></span></strong>
    <br>
    <span style="font-weight: bold; white-space:nowrap;">本文链接</span>：
        <a href="https://www.cnblogs.com/johnnyzen" target="_blank" id="articleLinkElement"> https://www.cnblogs.com/johnnyzen</a>
    <br>
    <span class="essaySuffix-right-title">关于博文</span>：评论和私信会在第一时间回复，或<a href="https://msg.cnblogs.com/msg/send/johnnyzen" target="_blank">直接私信</a>我。
    <br>
    <span class="essaySuffix-right-title">版权声明</span>：本博客所有文章除特别声明外，均采用 <a title="https://creativecommons.org/licenses/by-nc-nd/4.0/" href="http://blog.sina.com.cn/s/blog_896327b90102y6c6.html" alt="BY-NC-SA" target="_blank">BY-NC-SA</a> 
    许可协议。转载请注明出处！<br>
    <span class="essaySuffix-right-title">日常交流</span>：大数据与软件开发-QQ交流群: 774386015<strong>
        <span style="color: #ff0000; font-size: 12pt;">【<a id="post-up" onclick="votePost(getArticleNumber(),'Digg')" href="javascript:void(0);">入群二维码</a>】</span></strong>参见左下角。您的支持、鼓励<span style="color: #ff0000; font-size: 12pt;"></span>是博主技术写作的重要动力！
    <br>
</div>
<div style="clear: both;">
</div>
</div>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.006559326761574074" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-02-05 18:08">2025-02-05 18:08</span>&nbsp;
<a href="https://www.cnblogs.com/johnnyzen">千千寰宇</a>&nbsp;
阅读(<span id="post_view_count">0</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18699940" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18699940);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18699940', targetLink: 'https://www.cnblogs.com/johnnyzen/p/18699940', title: '[文件格式/数据存储] Parquet：开源、高效的列式存储文件格式协议' })">举报</a>
</div>
        