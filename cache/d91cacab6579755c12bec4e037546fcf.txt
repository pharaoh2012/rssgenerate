
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/zhaoweiwei/p/19048895/NsightSystems" title="发布于 2025-08-22 17:23">
    <span role="heading" aria-level="2">NVIDIA系统级性能分析工具Nsight Systems入门详解</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                    <div id="cnblogs_post_description" style="display: none">
        
        本文首先对NVIDIA GPU程序系统级性能分析工具Nsight Systems进行简单的介绍，然后说明了工具适用平台及安装，最后通过cuda-samples示例程序的分析，说明了如何在实际应用中使用该功能强大的工具。
    </div>
<div id="cnblogs_post_body" class="blogpost-body blogpost-body-html">
<h1>1 基本情况</h1>
<h2>1.1 简介</h2>
<p>Nsight Systems（简称nsys）是NVIDIA推出的一款系统级性能分析工具，主要用于优化 GPU 加速应用程序（尤其是基于 CUDA、OpenCL、DirectX、Vulkan 等 API 开发的程序）的性能，帮助开发者定位和解决计算、内存、通信等环节的瓶颈。所谓系统层面的分析工具，除了分析GPU的使用，还要分析CPU的使用，以及CPU和GPU的交互情况，可以捕捉CPU和GPU的各种事件，发现CPU和GPU上的等待以及不必要的同步，可以通过Nsight systems将任务均匀的分配到CPU和GPU上。做为NVIDIA核心分析工具之一，它和Compute以及Graphics有各自的侧重点：</p>
<div class="cnblogs_code">
<pre>Nsight Systems: A system-<span style="color: rgba(0, 0, 0, 1)">wide performance analysis tool
Nsight Compute: An interactive kernel profiler </span><span style="color: rgba(0, 0, 255, 1)">for</span> CUDA applications<br>Nsight Graphics: A standalone developer tool to debug, profile, and export frames built with Direct3D, Vulkan, OpenGL, OpenVR</pre>
</div>
<p>下图给出了CUDA程序整理优化流程，对于Nsight Systems侧重点在CPU&amp;GPU同步、数据拷贝以及处理重叠同步运行等方面，优化后再分别用Compute完成Kernel层或者用Graphics完成图像层优化，这之后再重新进行系统层分析及优化，不断迭代最终完成应用程序优化。</p>
<p><img src="https://img2024.cnblogs.com/blog/465567/202508/465567-20250822145326657-11836966.png" alt="image" width="674" height="422" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<p>接下来用下面的图来初步了解下Nsight Systems的强大功能，图中matrix_mul_test是一个GPU程序，在它执行过程中ntoskernl.exe会以线程形式参与到内存管理和设备驱动交互等环节，红色、橙色、绿色分别表示CPU占用率、当前CPU核（不同颜色表示线程运行在不同CPU核上）、线程状态（不同颜色表示不同状态Running，Ready to run，Blocked），图中标记1处于Running状态，标记2处于Ready to run状态，这时可能是ntoskernl线程在当前CPU核上时间片运行结束，CPU核已经调度给其他线程，比如标记3表示的ToDesk程序的相应线程。</p>
<p><img src="https://img2024.cnblogs.com/blog/465567/202508/465567-20250822154413250-1578405040.png" alt="image" width="1046" height="672" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<p>通过上图可以看到，借助Nsight Systems的图形显示可以直观方便的对GPU程序的运行过程进行分析。</p>
<h2>1.2 安装</h2>
<p>Nsight Systems是一款跨平台应用工具，根据主机平台的不同，它支持本地或远程Profiling目标程序，简单来说就是在Linxu和Windows下既支持本地分析有支持远程分析目标程序，对于Mac系统来说，只支持远程分析目标程序，以下是完整的支持列表：</p>
<p><img src="https://img2024.cnblogs.com/blog/465567/202508/465567-20250822170056335-1893148885.png" alt="image" width="635" height="497" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<p>不同平台的用户，可从最后参考1地址获得相应平台的安装程序，工具当前最新版本为2025.5.1。</p>
<p><img src="https://img2024.cnblogs.com/blog/465567/202508/465567-20250822171022151-871066949.png" alt="image" width="804" height="483" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<p>以Windows平台为例，下载以后直接安装NsightSystems-2025xxxxx.msi文件即可，另外如果之前安装toolkit，其实已经在在其中包含Nsight Systems，当然可能不是最新版本。其他平台看着请参考官网说明进行安装。</p>
<h1>2 使用说明</h1>
<h2>2.1 图形界面</h2>
<h3>1. 创建工程</h3>
<p>打开NsightSystems后，工具中默认会创建一个Project 1的工程，可以将该工程进行重命名，如改为HelloNsight，之后需要在右侧指定一个分析目标，工具支持多种方式的目标，以下进行详细解释：</p>
<p><img src="https://img2024.cnblogs.com/blog/465567/202508/465567-20250821105247747-1235193384.png" alt="image" width="468" height="240" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<p>（1）Localhost connection</p>
<p>表示本地主机连接，即对运行 Nsight Systems 的本机进行性能分析、调试。可监测和分析本机上运行的程序，收集其性能数据，像 CPU、GPU 的使用情况，线程活动等。图图中&nbsp;“zwv” 是已识别我的电脑名称。</p>
<p>（2）USB connections</p>
<p>指通过 USB 连接的调试目标。可用于分析连接到本机 USB 接口的设备上运行的程序性能，比如Jetson和Rive平台等，只要设备支持且正确配置，就能通过该连接进行性能数据采集。</p>
<p>（3）SSH connections</p>
<p>基于 SSH（Secure Shell）协议的远程连接。借助 SSH 安全通道，可连接到远程主机（如另一台服务器、开发机等），对远程主机上运行的程序进行性能分析和调试，方便管理不在本地的设备或集群环境中的程序性能。</p>
<p>（4）SSH connection groups</p>
<p>是 SSH 连接组，可将多个 SSH 连接进行分组管理。比如有多个远程服务器需要调试，可把它们的 SSH 连接归为一组，便于批量操作、统一管理和快速切换不同远程目标，提升对多远程环境调试的效率。<br>（5）Configure targets：<br>用于配置调试目标相关参数，比如添加新的调试目标、设置目标连接的认证信息（SSH 连接的用户名密码、密钥等 ）、调整连接超时时间等，对调试目标进行个性化设置和管理，确保能正确连接、识别和调试各类目标设备或程序 。</p>
<p>这里以本地可执行程序vectorAdd.exe为例进行解析，该程序是NVIDIA官方提供cuda-samples中的示例程序，可以从<a href="https://github.com/NVIDIA/cuda-samples" target="_blank" rel="noopener nofollow">官方Github主页</a>上下载，主要功能就是计算两个数组之和，其源码如下：</p>
<div class="cnblogs_code"><img src="https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif" id="code_img_closed_326a41ef-831c-4e27-b0f8-e7e2ecf4f843" class="code_img_closed"><img src="https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif" id="code_img_opened_326a41ef-831c-4e27-b0f8-e7e2ecf4f843" class="code_img_opened" style="display: none">
<div id="cnblogs_code_open_326a41ef-831c-4e27-b0f8-e7e2ecf4f843" class="cnblogs_code_hide">
<pre><span style="color: rgba(0, 128, 128, 1)">  1</span> <span style="color: rgba(0, 128, 0, 1)">/*</span><span style="color: rgba(0, 128, 0, 1)"> Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
</span><span style="color: rgba(0, 128, 128, 1)">  2</span> <span style="color: rgba(0, 128, 0, 1)"> *
</span><span style="color: rgba(0, 128, 128, 1)">  3</span> <span style="color: rgba(0, 128, 0, 1)"> * Redistribution and use in source and binary forms, with or without
</span><span style="color: rgba(0, 128, 128, 1)">  4</span> <span style="color: rgba(0, 128, 0, 1)"> * modification, are permitted provided that the following conditions
</span><span style="color: rgba(0, 128, 128, 1)">  5</span> <span style="color: rgba(0, 128, 0, 1)"> * are met:
</span><span style="color: rgba(0, 128, 128, 1)">  6</span> <span style="color: rgba(0, 128, 0, 1)"> *  * Redistributions of source code must retain the above copyright
</span><span style="color: rgba(0, 128, 128, 1)">  7</span> <span style="color: rgba(0, 128, 0, 1)"> *    notice, this list of conditions and the following disclaimer.
</span><span style="color: rgba(0, 128, 128, 1)">  8</span> <span style="color: rgba(0, 128, 0, 1)"> *  * Redistributions in binary form must reproduce the above copyright
</span><span style="color: rgba(0, 128, 128, 1)">  9</span> <span style="color: rgba(0, 128, 0, 1)"> *    notice, this list of conditions and the following disclaimer in the
</span><span style="color: rgba(0, 128, 128, 1)"> 10</span> <span style="color: rgba(0, 128, 0, 1)"> *    documentation and/or other materials provided with the distribution.
</span><span style="color: rgba(0, 128, 128, 1)"> 11</span> <span style="color: rgba(0, 128, 0, 1)"> *  * Neither the name of NVIDIA CORPORATION nor the names of its
</span><span style="color: rgba(0, 128, 128, 1)"> 12</span> <span style="color: rgba(0, 128, 0, 1)"> *    contributors may be used to endorse or promote products derived
</span><span style="color: rgba(0, 128, 128, 1)"> 13</span> <span style="color: rgba(0, 128, 0, 1)"> *    from this software without specific prior written permission.
</span><span style="color: rgba(0, 128, 128, 1)"> 14</span> <span style="color: rgba(0, 128, 0, 1)"> *
</span><span style="color: rgba(0, 128, 128, 1)"> 15</span> <span style="color: rgba(0, 128, 0, 1)"> * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
</span><span style="color: rgba(0, 128, 128, 1)"> 16</span> <span style="color: rgba(0, 128, 0, 1)"> * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
</span><span style="color: rgba(0, 128, 128, 1)"> 17</span> <span style="color: rgba(0, 128, 0, 1)"> * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
</span><span style="color: rgba(0, 128, 128, 1)"> 18</span> <span style="color: rgba(0, 128, 0, 1)"> * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
</span><span style="color: rgba(0, 128, 128, 1)"> 19</span> <span style="color: rgba(0, 128, 0, 1)"> * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
</span><span style="color: rgba(0, 128, 128, 1)"> 20</span> <span style="color: rgba(0, 128, 0, 1)"> * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
</span><span style="color: rgba(0, 128, 128, 1)"> 21</span> <span style="color: rgba(0, 128, 0, 1)"> * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
</span><span style="color: rgba(0, 128, 128, 1)"> 22</span> <span style="color: rgba(0, 128, 0, 1)"> * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
</span><span style="color: rgba(0, 128, 128, 1)"> 23</span> <span style="color: rgba(0, 128, 0, 1)"> * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
</span><span style="color: rgba(0, 128, 128, 1)"> 24</span> <span style="color: rgba(0, 128, 0, 1)"> * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
</span><span style="color: rgba(0, 128, 128, 1)"> 25</span> <span style="color: rgba(0, 128, 0, 1)"> * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
</span><span style="color: rgba(0, 128, 128, 1)"> 26</span>  <span style="color: rgba(0, 128, 0, 1)">*/</span>
<span style="color: rgba(0, 128, 128, 1)"> 27</span> 
<span style="color: rgba(0, 128, 128, 1)"> 28</span> <span style="color: rgba(0, 128, 0, 1)">/*</span><span style="color: rgba(0, 128, 0, 1)">*
</span><span style="color: rgba(0, 128, 128, 1)"> 29</span> <span style="color: rgba(0, 128, 0, 1)"> * Vector addition: C = A + B.
</span><span style="color: rgba(0, 128, 128, 1)"> 30</span> <span style="color: rgba(0, 128, 0, 1)"> *
</span><span style="color: rgba(0, 128, 128, 1)"> 31</span> <span style="color: rgba(0, 128, 0, 1)"> * This sample is a very basic sample that implements element by element
</span><span style="color: rgba(0, 128, 128, 1)"> 32</span> <span style="color: rgba(0, 128, 0, 1)"> * vector addition. It is the same as the sample illustrating Chapter 2
</span><span style="color: rgba(0, 128, 128, 1)"> 33</span> <span style="color: rgba(0, 128, 0, 1)"> * of the programming guide with some additions like error checking.
</span><span style="color: rgba(0, 128, 128, 1)"> 34</span>  <span style="color: rgba(0, 128, 0, 1)">*/</span>
<span style="color: rgba(0, 128, 128, 1)"> 35</span> 
<span style="color: rgba(0, 128, 128, 1)"> 36</span> #include &lt;stdio.h&gt;
<span style="color: rgba(0, 128, 128, 1)"> 37</span> 
<span style="color: rgba(0, 128, 128, 1)"> 38</span> <span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)"> For the CUDA runtime routines (prefixed with "cuda_")</span>
<span style="color: rgba(0, 128, 128, 1)"> 39</span> #include &lt;cuda_runtime.h&gt;
<span style="color: rgba(0, 128, 128, 1)"> 40</span> 
<span style="color: rgba(0, 128, 128, 1)"> 41</span> #include &lt;helper_cuda.h&gt;
<span style="color: rgba(0, 128, 128, 1)"> 42</span> <span style="color: rgba(0, 128, 0, 1)">/*</span><span style="color: rgba(0, 128, 0, 1)">*
</span><span style="color: rgba(0, 128, 128, 1)"> 43</span> <span style="color: rgba(0, 128, 0, 1)"> * CUDA Kernel Device code
</span><span style="color: rgba(0, 128, 128, 1)"> 44</span> <span style="color: rgba(0, 128, 0, 1)"> *
</span><span style="color: rgba(0, 128, 128, 1)"> 45</span> <span style="color: rgba(0, 128, 0, 1)"> * Computes the vector addition of A and B into C. The 3 vectors have the same
</span><span style="color: rgba(0, 128, 128, 1)"> 46</span> <span style="color: rgba(0, 128, 0, 1)"> * number of elements numElements.
</span><span style="color: rgba(0, 128, 128, 1)"> 47</span>  <span style="color: rgba(0, 128, 0, 1)">*/</span>
<span style="color: rgba(0, 128, 128, 1)"> 48</span> __global__ <span style="color: rgba(0, 0, 255, 1)">void</span> vectorAdd(<span style="color: rgba(0, 0, 255, 1)">const</span> <span style="color: rgba(0, 0, 255, 1)">float</span> *A, <span style="color: rgba(0, 0, 255, 1)">const</span> <span style="color: rgba(0, 0, 255, 1)">float</span> *B, <span style="color: rgba(0, 0, 255, 1)">float</span> *<span style="color: rgba(0, 0, 0, 1)">C,
</span><span style="color: rgba(0, 128, 128, 1)"> 49</span>                           <span style="color: rgba(0, 0, 255, 1)">int</span><span style="color: rgba(0, 0, 0, 1)"> numElements) {
</span><span style="color: rgba(0, 128, 128, 1)"> 50</span>   <span style="color: rgba(0, 0, 255, 1)">int</span> i = blockDim.x * blockIdx.x +<span style="color: rgba(0, 0, 0, 1)"> threadIdx.x;
</span><span style="color: rgba(0, 128, 128, 1)"> 51</span> 
<span style="color: rgba(0, 128, 128, 1)"> 52</span>   <span style="color: rgba(0, 0, 255, 1)">if</span> (i &lt;<span style="color: rgba(0, 0, 0, 1)"> numElements) {
</span><span style="color: rgba(0, 128, 128, 1)"> 53</span>     C[i] = A[i] + B[i] + <span style="color: rgba(128, 0, 128, 1)">0.0f</span><span style="color: rgba(0, 0, 0, 1)">;
</span><span style="color: rgba(0, 128, 128, 1)"> 54</span> <span style="color: rgba(0, 0, 0, 1)">  }
</span><span style="color: rgba(0, 128, 128, 1)"> 55</span> <span style="color: rgba(0, 0, 0, 1)">}
</span><span style="color: rgba(0, 128, 128, 1)"> 56</span> 
<span style="color: rgba(0, 128, 128, 1)"> 57</span> <span style="color: rgba(0, 128, 0, 1)">/*</span><span style="color: rgba(0, 128, 0, 1)">*
</span><span style="color: rgba(0, 128, 128, 1)"> 58</span> <span style="color: rgba(0, 128, 0, 1)"> * Host main routine
</span><span style="color: rgba(0, 128, 128, 1)"> 59</span>  <span style="color: rgba(0, 128, 0, 1)">*/</span>
<span style="color: rgba(0, 128, 128, 1)"> 60</span> <span style="color: rgba(0, 0, 255, 1)">int</span> main(<span style="color: rgba(0, 0, 255, 1)">void</span><span style="color: rgba(0, 0, 0, 1)">) {
</span><span style="color: rgba(0, 128, 128, 1)"> 61</span>   <span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)"> Error code to check return values for CUDA calls</span>
<span style="color: rgba(0, 128, 128, 1)"> 62</span>   cudaError_t err =<span style="color: rgba(0, 0, 0, 1)"> cudaSuccess;
</span><span style="color: rgba(0, 128, 128, 1)"> 63</span> 
<span style="color: rgba(0, 128, 128, 1)"> 64</span>   <span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)"> Print the vector length to be used, and compute its size</span>
<span style="color: rgba(0, 128, 128, 1)"> 65</span>   <span style="color: rgba(0, 0, 255, 1)">int</span> numElements = <span style="color: rgba(128, 0, 128, 1)">50000</span><span style="color: rgba(0, 0, 0, 1)">;
</span><span style="color: rgba(0, 128, 128, 1)"> 66</span>   size_t size = numElements * <span style="color: rgba(0, 0, 255, 1)">sizeof</span>(<span style="color: rgba(0, 0, 255, 1)">float</span><span style="color: rgba(0, 0, 0, 1)">);
</span><span style="color: rgba(0, 128, 128, 1)"> 67</span>   printf(<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">[Vector addition of %d elements]\n</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">, numElements);
</span><span style="color: rgba(0, 128, 128, 1)"> 68</span> 
<span style="color: rgba(0, 128, 128, 1)"> 69</span>   <span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)"> Allocate the host input vector A</span>
<span style="color: rgba(0, 128, 128, 1)"> 70</span>   <span style="color: rgba(0, 0, 255, 1)">float</span> *h_A = (<span style="color: rgba(0, 0, 255, 1)">float</span> *)<span style="color: rgba(0, 0, 255, 1)">malloc</span><span style="color: rgba(0, 0, 0, 1)">(size);
</span><span style="color: rgba(0, 128, 128, 1)"> 71</span> 
<span style="color: rgba(0, 128, 128, 1)"> 72</span>   <span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)"> Allocate the host input vector B</span>
<span style="color: rgba(0, 128, 128, 1)"> 73</span>   <span style="color: rgba(0, 0, 255, 1)">float</span> *h_B = (<span style="color: rgba(0, 0, 255, 1)">float</span> *)<span style="color: rgba(0, 0, 255, 1)">malloc</span><span style="color: rgba(0, 0, 0, 1)">(size);
</span><span style="color: rgba(0, 128, 128, 1)"> 74</span> 
<span style="color: rgba(0, 128, 128, 1)"> 75</span>   <span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)"> Allocate the host output vector C</span>
<span style="color: rgba(0, 128, 128, 1)"> 76</span>   <span style="color: rgba(0, 0, 255, 1)">float</span> *h_C = (<span style="color: rgba(0, 0, 255, 1)">float</span> *)<span style="color: rgba(0, 0, 255, 1)">malloc</span><span style="color: rgba(0, 0, 0, 1)">(size);
</span><span style="color: rgba(0, 128, 128, 1)"> 77</span> 
<span style="color: rgba(0, 128, 128, 1)"> 78</span>   <span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)"> Verify that allocations succeeded</span>
<span style="color: rgba(0, 128, 128, 1)"> 79</span>   <span style="color: rgba(0, 0, 255, 1)">if</span> (h_A == NULL || h_B == NULL || h_C ==<span style="color: rgba(0, 0, 0, 1)"> NULL) {
</span><span style="color: rgba(0, 128, 128, 1)"> 80</span>     fprintf(stderr, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">Failed to allocate host vectors!\n</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">);
</span><span style="color: rgba(0, 128, 128, 1)"> 81</span> <span style="color: rgba(0, 0, 0, 1)">    exit(EXIT_FAILURE);
</span><span style="color: rgba(0, 128, 128, 1)"> 82</span> <span style="color: rgba(0, 0, 0, 1)">  }
</span><span style="color: rgba(0, 128, 128, 1)"> 83</span> 
<span style="color: rgba(0, 128, 128, 1)"> 84</span>   <span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)"> Initialize the host input vectors</span>
<span style="color: rgba(0, 128, 128, 1)"> 85</span>   <span style="color: rgba(0, 0, 255, 1)">for</span> (<span style="color: rgba(0, 0, 255, 1)">int</span> i = <span style="color: rgba(128, 0, 128, 1)">0</span>; i &lt; numElements; ++<span style="color: rgba(0, 0, 0, 1)">i) {
</span><span style="color: rgba(0, 128, 128, 1)"> 86</span>     h_A[i] = rand() / (<span style="color: rgba(0, 0, 255, 1)">float</span><span style="color: rgba(0, 0, 0, 1)">)RAND_MAX;
</span><span style="color: rgba(0, 128, 128, 1)"> 87</span>     h_B[i] = rand() / (<span style="color: rgba(0, 0, 255, 1)">float</span><span style="color: rgba(0, 0, 0, 1)">)RAND_MAX;
</span><span style="color: rgba(0, 128, 128, 1)"> 88</span> <span style="color: rgba(0, 0, 0, 1)">  }
</span><span style="color: rgba(0, 128, 128, 1)"> 89</span> 
<span style="color: rgba(0, 128, 128, 1)"> 90</span>   <span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)"> Allocate the device input vector A</span>
<span style="color: rgba(0, 128, 128, 1)"> 91</span>   <span style="color: rgba(0, 0, 255, 1)">float</span> *d_A =<span style="color: rgba(0, 0, 0, 1)"> NULL;
</span><span style="color: rgba(0, 128, 128, 1)"> 92</span>   err = cudaMalloc((<span style="color: rgba(0, 0, 255, 1)">void</span> **)&amp;<span style="color: rgba(0, 0, 0, 1)">d_A, size);
</span><span style="color: rgba(0, 128, 128, 1)"> 93</span> 
<span style="color: rgba(0, 128, 128, 1)"> 94</span>   <span style="color: rgba(0, 0, 255, 1)">if</span> (err !=<span style="color: rgba(0, 0, 0, 1)"> cudaSuccess) {
</span><span style="color: rgba(0, 128, 128, 1)"> 95</span>     fprintf(stderr, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">Failed to allocate device vector A (error code %s)!\n</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">,
</span><span style="color: rgba(0, 128, 128, 1)"> 96</span> <span style="color: rgba(0, 0, 0, 1)">            cudaGetErrorString(err));
</span><span style="color: rgba(0, 128, 128, 1)"> 97</span> <span style="color: rgba(0, 0, 0, 1)">    exit(EXIT_FAILURE);
</span><span style="color: rgba(0, 128, 128, 1)"> 98</span> <span style="color: rgba(0, 0, 0, 1)">  }
</span><span style="color: rgba(0, 128, 128, 1)"> 99</span> 
<span style="color: rgba(0, 128, 128, 1)">100</span>   <span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)"> Allocate the device input vector B</span>
<span style="color: rgba(0, 128, 128, 1)">101</span>   <span style="color: rgba(0, 0, 255, 1)">float</span> *d_B =<span style="color: rgba(0, 0, 0, 1)"> NULL;
</span><span style="color: rgba(0, 128, 128, 1)">102</span>   err = cudaMalloc((<span style="color: rgba(0, 0, 255, 1)">void</span> **)&amp;<span style="color: rgba(0, 0, 0, 1)">d_B, size);
</span><span style="color: rgba(0, 128, 128, 1)">103</span> 
<span style="color: rgba(0, 128, 128, 1)">104</span>   <span style="color: rgba(0, 0, 255, 1)">if</span> (err !=<span style="color: rgba(0, 0, 0, 1)"> cudaSuccess) {
</span><span style="color: rgba(0, 128, 128, 1)">105</span>     fprintf(stderr, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">Failed to allocate device vector B (error code %s)!\n</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">,
</span><span style="color: rgba(0, 128, 128, 1)">106</span> <span style="color: rgba(0, 0, 0, 1)">            cudaGetErrorString(err));
</span><span style="color: rgba(0, 128, 128, 1)">107</span> <span style="color: rgba(0, 0, 0, 1)">    exit(EXIT_FAILURE);
</span><span style="color: rgba(0, 128, 128, 1)">108</span> <span style="color: rgba(0, 0, 0, 1)">  }
</span><span style="color: rgba(0, 128, 128, 1)">109</span> 
<span style="color: rgba(0, 128, 128, 1)">110</span>   <span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)"> Allocate the device output vector C</span>
<span style="color: rgba(0, 128, 128, 1)">111</span>   <span style="color: rgba(0, 0, 255, 1)">float</span> *d_C =<span style="color: rgba(0, 0, 0, 1)"> NULL;
</span><span style="color: rgba(0, 128, 128, 1)">112</span>   err = cudaMalloc((<span style="color: rgba(0, 0, 255, 1)">void</span> **)&amp;<span style="color: rgba(0, 0, 0, 1)">d_C, size);
</span><span style="color: rgba(0, 128, 128, 1)">113</span> 
<span style="color: rgba(0, 128, 128, 1)">114</span>   <span style="color: rgba(0, 0, 255, 1)">if</span> (err !=<span style="color: rgba(0, 0, 0, 1)"> cudaSuccess) {
</span><span style="color: rgba(0, 128, 128, 1)">115</span>     fprintf(stderr, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">Failed to allocate device vector C (error code %s)!\n</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">,
</span><span style="color: rgba(0, 128, 128, 1)">116</span> <span style="color: rgba(0, 0, 0, 1)">            cudaGetErrorString(err));
</span><span style="color: rgba(0, 128, 128, 1)">117</span> <span style="color: rgba(0, 0, 0, 1)">    exit(EXIT_FAILURE);
</span><span style="color: rgba(0, 128, 128, 1)">118</span> <span style="color: rgba(0, 0, 0, 1)">  }
</span><span style="color: rgba(0, 128, 128, 1)">119</span> 
<span style="color: rgba(0, 128, 128, 1)">120</span>   <span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)"> Copy the host input vectors A and B in host memory to the device input
</span><span style="color: rgba(0, 128, 128, 1)">121</span>   <span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)"> vectors in
</span><span style="color: rgba(0, 128, 128, 1)">122</span>   <span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)"> device memory</span>
<span style="color: rgba(0, 128, 128, 1)">123</span>   printf(<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">Copy input data from the host memory to the CUDA device\n</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">);
</span><span style="color: rgba(0, 128, 128, 1)">124</span>   err =<span style="color: rgba(0, 0, 0, 1)"> cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);
</span><span style="color: rgba(0, 128, 128, 1)">125</span> 
<span style="color: rgba(0, 128, 128, 1)">126</span>   <span style="color: rgba(0, 0, 255, 1)">if</span> (err !=<span style="color: rgba(0, 0, 0, 1)"> cudaSuccess) {
</span><span style="color: rgba(0, 128, 128, 1)">127</span> <span style="color: rgba(0, 0, 0, 1)">    fprintf(stderr,
</span><span style="color: rgba(0, 128, 128, 1)">128</span>             <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">Failed to copy vector A from host to device (error code %s)!\n</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">,
</span><span style="color: rgba(0, 128, 128, 1)">129</span> <span style="color: rgba(0, 0, 0, 1)">            cudaGetErrorString(err));
</span><span style="color: rgba(0, 128, 128, 1)">130</span> <span style="color: rgba(0, 0, 0, 1)">    exit(EXIT_FAILURE);
</span><span style="color: rgba(0, 128, 128, 1)">131</span> <span style="color: rgba(0, 0, 0, 1)">  }
</span><span style="color: rgba(0, 128, 128, 1)">132</span> 
<span style="color: rgba(0, 128, 128, 1)">133</span>   err =<span style="color: rgba(0, 0, 0, 1)"> cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);
</span><span style="color: rgba(0, 128, 128, 1)">134</span> 
<span style="color: rgba(0, 128, 128, 1)">135</span>   <span style="color: rgba(0, 0, 255, 1)">if</span> (err !=<span style="color: rgba(0, 0, 0, 1)"> cudaSuccess) {
</span><span style="color: rgba(0, 128, 128, 1)">136</span> <span style="color: rgba(0, 0, 0, 1)">    fprintf(stderr,
</span><span style="color: rgba(0, 128, 128, 1)">137</span>             <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">Failed to copy vector B from host to device (error code %s)!\n</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">,
</span><span style="color: rgba(0, 128, 128, 1)">138</span> <span style="color: rgba(0, 0, 0, 1)">            cudaGetErrorString(err));
</span><span style="color: rgba(0, 128, 128, 1)">139</span> <span style="color: rgba(0, 0, 0, 1)">    exit(EXIT_FAILURE);
</span><span style="color: rgba(0, 128, 128, 1)">140</span> <span style="color: rgba(0, 0, 0, 1)">  }
</span><span style="color: rgba(0, 128, 128, 1)">141</span> 
<span style="color: rgba(0, 128, 128, 1)">142</span>   <span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)"> Launch the Vector Add CUDA Kernel</span>
<span style="color: rgba(0, 128, 128, 1)">143</span>   <span style="color: rgba(0, 0, 255, 1)">int</span> threadsPerBlock = <span style="color: rgba(128, 0, 128, 1)">256</span><span style="color: rgba(0, 0, 0, 1)">;
</span><span style="color: rgba(0, 128, 128, 1)">144</span>   <span style="color: rgba(0, 0, 255, 1)">int</span> blocksPerGrid = (numElements + threadsPerBlock - <span style="color: rgba(128, 0, 128, 1)">1</span>) /<span style="color: rgba(0, 0, 0, 1)"> threadsPerBlock;
</span><span style="color: rgba(0, 128, 128, 1)">145</span>   printf(<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">CUDA kernel launch with %d blocks of %d threads\n</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">, blocksPerGrid,
</span><span style="color: rgba(0, 128, 128, 1)">146</span> <span style="color: rgba(0, 0, 0, 1)">         threadsPerBlock);
</span><span style="color: rgba(0, 128, 128, 1)">147</span>   vectorAdd&lt;&lt;&lt;blocksPerGrid, threadsPerBlock&gt;&gt;&gt;<span style="color: rgba(0, 0, 0, 1)">(d_A, d_B, d_C, numElements);
</span><span style="color: rgba(0, 128, 128, 1)">148</span>   err =<span style="color: rgba(0, 0, 0, 1)"> cudaGetLastError();
</span><span style="color: rgba(0, 128, 128, 1)">149</span> 
<span style="color: rgba(0, 128, 128, 1)">150</span>   <span style="color: rgba(0, 0, 255, 1)">if</span> (err !=<span style="color: rgba(0, 0, 0, 1)"> cudaSuccess) {
</span><span style="color: rgba(0, 128, 128, 1)">151</span>     fprintf(stderr, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">Failed to launch vectorAdd kernel (error code %s)!\n</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">,
</span><span style="color: rgba(0, 128, 128, 1)">152</span> <span style="color: rgba(0, 0, 0, 1)">            cudaGetErrorString(err));
</span><span style="color: rgba(0, 128, 128, 1)">153</span> <span style="color: rgba(0, 0, 0, 1)">    exit(EXIT_FAILURE);
</span><span style="color: rgba(0, 128, 128, 1)">154</span> <span style="color: rgba(0, 0, 0, 1)">  }
</span><span style="color: rgba(0, 128, 128, 1)">155</span> 
<span style="color: rgba(0, 128, 128, 1)">156</span>   <span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)"> Copy the device result vector in device memory to the host result vector
</span><span style="color: rgba(0, 128, 128, 1)">157</span>   <span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)"> in host memory.</span>
<span style="color: rgba(0, 128, 128, 1)">158</span>   printf(<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">Copy output data from the CUDA device to the host memory\n</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">);
</span><span style="color: rgba(0, 128, 128, 1)">159</span>   err =<span style="color: rgba(0, 0, 0, 1)"> cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);
</span><span style="color: rgba(0, 128, 128, 1)">160</span> 
<span style="color: rgba(0, 128, 128, 1)">161</span>   <span style="color: rgba(0, 0, 255, 1)">if</span> (err !=<span style="color: rgba(0, 0, 0, 1)"> cudaSuccess) {
</span><span style="color: rgba(0, 128, 128, 1)">162</span> <span style="color: rgba(0, 0, 0, 1)">    fprintf(stderr,
</span><span style="color: rgba(0, 128, 128, 1)">163</span>             <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">Failed to copy vector C from device to host (error code %s)!\n</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">,
</span><span style="color: rgba(0, 128, 128, 1)">164</span> <span style="color: rgba(0, 0, 0, 1)">            cudaGetErrorString(err));
</span><span style="color: rgba(0, 128, 128, 1)">165</span> <span style="color: rgba(0, 0, 0, 1)">    exit(EXIT_FAILURE);
</span><span style="color: rgba(0, 128, 128, 1)">166</span> <span style="color: rgba(0, 0, 0, 1)">  }
</span><span style="color: rgba(0, 128, 128, 1)">167</span> 
<span style="color: rgba(0, 128, 128, 1)">168</span>   <span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)"> Verify that the result vector is correct</span>
<span style="color: rgba(0, 128, 128, 1)">169</span>   <span style="color: rgba(0, 0, 255, 1)">for</span> (<span style="color: rgba(0, 0, 255, 1)">int</span> i = <span style="color: rgba(128, 0, 128, 1)">0</span>; i &lt; numElements; ++<span style="color: rgba(0, 0, 0, 1)">i) {
</span><span style="color: rgba(0, 128, 128, 1)">170</span>     <span style="color: rgba(0, 0, 255, 1)">if</span> (fabs(h_A[i] + h_B[i] - h_C[i]) &gt; 1e-<span style="color: rgba(128, 0, 128, 1)">5</span><span style="color: rgba(0, 0, 0, 1)">) {
</span><span style="color: rgba(0, 128, 128, 1)">171</span>       fprintf(stderr, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">Result verification failed at element %d!\n</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">, i);
</span><span style="color: rgba(0, 128, 128, 1)">172</span> <span style="color: rgba(0, 0, 0, 1)">      exit(EXIT_FAILURE);
</span><span style="color: rgba(0, 128, 128, 1)">173</span> <span style="color: rgba(0, 0, 0, 1)">    }
</span><span style="color: rgba(0, 128, 128, 1)">174</span> <span style="color: rgba(0, 0, 0, 1)">  }
</span><span style="color: rgba(0, 128, 128, 1)">175</span> 
<span style="color: rgba(0, 128, 128, 1)">176</span>   printf(<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">Test PASSED\n</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">);
</span><span style="color: rgba(0, 128, 128, 1)">177</span> 
<span style="color: rgba(0, 128, 128, 1)">178</span>   <span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)"> Free device global memory</span>
<span style="color: rgba(0, 128, 128, 1)">179</span>   err =<span style="color: rgba(0, 0, 0, 1)"> cudaFree(d_A);
</span><span style="color: rgba(0, 128, 128, 1)">180</span> 
<span style="color: rgba(0, 128, 128, 1)">181</span>   <span style="color: rgba(0, 0, 255, 1)">if</span> (err !=<span style="color: rgba(0, 0, 0, 1)"> cudaSuccess) {
</span><span style="color: rgba(0, 128, 128, 1)">182</span>     fprintf(stderr, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">Failed to free device vector A (error code %s)!\n</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">,
</span><span style="color: rgba(0, 128, 128, 1)">183</span> <span style="color: rgba(0, 0, 0, 1)">            cudaGetErrorString(err));
</span><span style="color: rgba(0, 128, 128, 1)">184</span> <span style="color: rgba(0, 0, 0, 1)">    exit(EXIT_FAILURE);
</span><span style="color: rgba(0, 128, 128, 1)">185</span> <span style="color: rgba(0, 0, 0, 1)">  }
</span><span style="color: rgba(0, 128, 128, 1)">186</span> 
<span style="color: rgba(0, 128, 128, 1)">187</span>   err =<span style="color: rgba(0, 0, 0, 1)"> cudaFree(d_B);
</span><span style="color: rgba(0, 128, 128, 1)">188</span> 
<span style="color: rgba(0, 128, 128, 1)">189</span>   <span style="color: rgba(0, 0, 255, 1)">if</span> (err !=<span style="color: rgba(0, 0, 0, 1)"> cudaSuccess) {
</span><span style="color: rgba(0, 128, 128, 1)">190</span>     fprintf(stderr, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">Failed to free device vector B (error code %s)!\n</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">,
</span><span style="color: rgba(0, 128, 128, 1)">191</span> <span style="color: rgba(0, 0, 0, 1)">            cudaGetErrorString(err));
</span><span style="color: rgba(0, 128, 128, 1)">192</span> <span style="color: rgba(0, 0, 0, 1)">    exit(EXIT_FAILURE);
</span><span style="color: rgba(0, 128, 128, 1)">193</span> <span style="color: rgba(0, 0, 0, 1)">  }
</span><span style="color: rgba(0, 128, 128, 1)">194</span> 
<span style="color: rgba(0, 128, 128, 1)">195</span>   err =<span style="color: rgba(0, 0, 0, 1)"> cudaFree(d_C);
</span><span style="color: rgba(0, 128, 128, 1)">196</span> 
<span style="color: rgba(0, 128, 128, 1)">197</span>   <span style="color: rgba(0, 0, 255, 1)">if</span> (err !=<span style="color: rgba(0, 0, 0, 1)"> cudaSuccess) {
</span><span style="color: rgba(0, 128, 128, 1)">198</span>     fprintf(stderr, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">Failed to free device vector C (error code %s)!\n</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">,
</span><span style="color: rgba(0, 128, 128, 1)">199</span> <span style="color: rgba(0, 0, 0, 1)">            cudaGetErrorString(err));
</span><span style="color: rgba(0, 128, 128, 1)">200</span> <span style="color: rgba(0, 0, 0, 1)">    exit(EXIT_FAILURE);
</span><span style="color: rgba(0, 128, 128, 1)">201</span> <span style="color: rgba(0, 0, 0, 1)">  }
</span><span style="color: rgba(0, 128, 128, 1)">202</span> 
<span style="color: rgba(0, 128, 128, 1)">203</span>   <span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)"> Free host memory</span>
<span style="color: rgba(0, 128, 128, 1)">204</span>   <span style="color: rgba(0, 0, 255, 1)">free</span><span style="color: rgba(0, 0, 0, 1)">(h_A);
</span><span style="color: rgba(0, 128, 128, 1)">205</span>   <span style="color: rgba(0, 0, 255, 1)">free</span><span style="color: rgba(0, 0, 0, 1)">(h_B);
</span><span style="color: rgba(0, 128, 128, 1)">206</span>   <span style="color: rgba(0, 0, 255, 1)">free</span><span style="color: rgba(0, 0, 0, 1)">(h_C);
</span><span style="color: rgba(0, 128, 128, 1)">207</span> 
<span style="color: rgba(0, 128, 128, 1)">208</span>   printf(<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">Done\n</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">);
</span><span style="color: rgba(0, 128, 128, 1)">209</span>   <span style="color: rgba(0, 0, 255, 1)">return</span> <span style="color: rgba(128, 0, 128, 1)">0</span><span style="color: rgba(0, 0, 0, 1)">;
</span><span style="color: rgba(0, 128, 128, 1)">210</span> }</pre>
</div>
<span class="cnblogs_code_collapse">vectorAdd.cu</span></div>
<p>配置待分析的可执行程序，以及设置相应的工作目录是进行分析的必选项，其他选项都是可以定制，如下所示：</p>
<p><img src="https://img2024.cnblogs.com/blog/465567/202508/465567-20250821113246972-899804092.png" alt="image" width="929" height="641" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<h3>2. 配置采集项</h3>
<p>上图中采集配置项主要可以分为6个方面：</p>
<p>（1）CPU相关采集项</p>
<ul>
<li>Collect CPU IP/backtrace samples（CPU 指令指针 / 回溯采样）</li>
</ul>
<p>以设置的采样率（如 1kHz）记录 CPU 指令执行位置，结合调用栈，帮你定位程序耗时在哪些函数、代码段，分析 CPU 计算瓶颈。</p>
<p><strong>Sampling rate：</strong>控制采样频率，频率越高数据越细但采集开销越大，按需平衡精度和性能。</p>
<p><strong>Collect call stacks of executing threads：</strong>记录 “活跃线程” 的调用栈，明确 CPU 周期到底用在哪些函数调用里。</p>
<ul>
<li>Collect CPU context switch trace（CPU 上下文切换追踪）</li>
</ul>
<p>记录线程在 CPU 核心间切换、阻塞 / 唤醒等事件，分析线程调度效率，排查因频繁切换导致的性能损耗。</p>
<p><strong>Collect call stacks of blocked threads：</strong>记录 “阻塞线程” 的调用栈，找到线程等待资源（锁、IO 等）的原因，优化同步逻辑。</p>
<p>（2）异构计算相关采集（GPU及特定API）</p>
<ul>
<li>Collect CUDA trace</li>
</ul>
<p>深度追踪 CUDA 调用（如核函数启动、内存分配），分析 GPU 任务调度、内存传输瓶颈，是 CUDA 程序性能优化核心配置。</p>
<p><strong>Flush data periodically：</strong>定期将采集的CUDA追踪数据从内存刷写到磁盘/存储，避免因程序崩溃、异常退出导致采集数据丢失，例如10.00 seconds表示每10秒执行依次存盘操作。</p>
<p><strong>Flush CUDA trace buffers on cudaProfilerStop()：</strong>调用cudaProfilerStop() API时，强制将CUDA追踪缓冲区的数据存盘，</p>
<p><strong>Skip some API calls：</strong>跳过部分低价值、高频次的CUDA API调用追踪，可以降低采集开销简化分析数据，让你更专注kernel launch、memory copy等关键操作。</p>
<p><strong>CUDA Event trace mode：</strong>控制CUDA Event的追踪模式，Off表示关闭，如果开启，可追踪cudaEventRecord/cudaEventSynchronize等事件，分析GPU任务同步、耗时依赖。</p>
<p><strong>CUDA graph trace granularity：</strong>设置CUDA Graph（任务图）的追踪粒度，如Graph（图级），Graph表示把整个CUDA Graph当作一个整体追踪，Node会细化到Graph内部的每个Node。</p>
<p><strong>Trace CUDA graphs launched from device codes：</strong>追踪由设备端代码启动的CUDA Graphs。</p>
<p><strong>Collect GPU memory usage：</strong>采集GPU内存的使用数据，配合核函数、内存传输的追踪，分析“内存占用”与“计算性能”的关联（如现存带宽瓶颈时，内存分配策略是否可优化）。</p>
<p><strong>Collect hardware-based trace：</strong>启动硬件级追踪，采集更底层的硬件指标（如SM利用率、warp调度效率、指令吞吐率），适合深度优化核函数。</p>
<ul>
<li>Collect GPU context switch trace</li>
</ul>
<p>追踪 GPU 上下文切换事件（如不同 CUDA 上下文切换），排查 GPU 资源竞争、调度延迟问题。</p>
<ul>
<li>Collect GPU Metrics</li>
</ul>
<p>采集 GPU 硬件指标（如 SM 利用率、显存带宽），从硬件层面定位性能瓶颈，需 GPU 支持对应 metrics 采集。</p>
<ul>
<li>Collect NVTX trace</li>
</ul>
<p>识别程序中通过 NVTX（NVIDIA 工具扩展）标记的自定义事件 / 区间，方便关联业务逻辑（如 “模型推理阶段”“数据预处理”），分析各阶段耗时。</p>
<ul>
<li>Collect WDDM trace（Windows 环境）</li>
</ul>
<p>采集 WDDM（Windows 显示驱动模型）相关事件，分析 GPU 与系统显示子系统交互性能，对图形渲染、游戏等程序有帮助。</p>
<p>（3）图形API追踪（按需启用）</p>
<ul>
<li>Collect DX11/DX12/OpenGL/Vulkan trace</li>
</ul>
<p>针对对应图形 API 的程序，采集 API 调用、渲染管线事件，分析图形渲染性能，优化画面卡顿、帧率低等问题。</p>
<ul>
<li>Collect OpenXR trace</li>
</ul>
<p>追踪 OpenXR（跨平台 AR/VR 标准）相关事件，分析 VR/AR 应用在设备交互、场景渲染的性能表现。</p>
<p>（4）其他特殊采集</p>
<ul>
<li>Collect ISR/DPCC trace</li>
</ul>
<p>采集 ISR（中断服务程序）、DPCC（特定 GPU 调试事件），一般用于深度系统级调试，普通应用优化较少用到。</p>
<ul>
<li>Collect custom ETW trace（Windows 环境）</li>
</ul>
<p>采集自定义 ETW（Event Tracing for Windows）事件，可整合系统级或其他工具的自定义性能数据。</p>
<ul>
<li>Collect Reflex SDK events</li>
</ul>
<p>追踪 NVIDIA Reflex（低延迟优化 SDK）相关事件，分析游戏等应用的输入延迟、渲染同步性能。</p>
<p>（5）Python profiling options</p>
<p>针对 Python 程序，可配置 Python 性能分析参数（如采集 Python 函数调用栈、追踪解释器事件 ），分析 Python 脚本在 CPU/GPU 交互、第三方库调用的性能问题。</p>
<p>（6）右侧采集触发控制</p>
<p><strong>Start profiling manually：</strong>手动点击开始 / 停止采集，灵活控制采集区间（如只抓 “程序关键流程”）。<br><strong>Start profiling after/ Limit profiling to（seconds/frames）：</strong>设置延迟启动、限制采集时长 / 帧数，避免采集无关数据，聚焦目标阶段（如 “前 10 秒初始化不采集，只抓后续 600 帧渲染” ）。<br><strong>Hotkey Start/Stop：</strong>设置快捷键（如 F12）控制采集，方便运行程序时快速触发，无需切回界面操作。</p>
<p>由于这些选项覆盖了从 CPU 基础分析→GPU 异构计算→图形 / VR 渲染→特殊场景（如 Python、AR/VR ） 的全链路性能采集能力，在进行配置时，按需勾选配置，以最大限度地减少跟踪开销。</p>
<h3>3. 结果分析</h3>
<p>首先看看在只选择“Collect CUDA trace”时的report结果：</p>
<p><img src="https://img2024.cnblogs.com/blog/465567/202508/465567-20250821150233044-891676765.png" alt="image" width="1143" height="348" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<p>在图中的timeline视图中可以清楚的看到主要函数调用时间线，其中三个cudaMalloc调用中，第一个调用所花费的时间要远远大于剩余两个图中标记1，这是因为在第一次调用任何CUDA API（包括cudaMalloc）时，CUDA runtime 会进行：CUDA 上下文创建（Context Creation），驱动和设备初始化，内核模块加载等操作，这部分开销通常是毫秒级，所以第一次cudaMalloc看起来很慢，图中标记2是HostToDevice内存拷贝，标记3是DeviceToHost内存拷贝，由于程序比较简单可见这两部分内存拷贝占用绝大部分的运行时间，而真正的Kernel函数调用仅仅占用3.8%，见标记4。</p>
<p>Report中的分析汇总给出了解析过程依赖的主要硬件资源：</p>
<p><img src="https://img2024.cnblogs.com/blog/465567/202508/465567-20250821151331738-988441454.png" alt="image" width="747" height="439" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<p>诊断汇总给出了解析过程中的主要事件线索：</p>
<p><img src="https://img2024.cnblogs.com/blog/465567/202508/465567-20250821151608703-270162869.png" alt="image" width="792" height="331" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<p>最后，Files给出了标准错误输出log信息，标准输出log信息，以及配置信息。</p>
<p><img src="https://img2024.cnblogs.com/blog/465567/202508/465567-20250821151730648-2055414372.png" alt="e61f9414afdf5887296dfd9e7bb426e4" width="787" height="155" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<h2>2.2 命令行&nbsp;</h2>
<h3>1.&nbsp;Nsight Systems 进行分析</h3>
<p>命令行方式主要应用在Linux操作系统下，仍使用之前的vectorAdd作为分析对象，直接使用如下命令进行分析：</p>
<div class="cnblogs_code">
<pre>nsys profile -o report ./vectorAdd</pre>
</div>
<p>命令执行后会生成report.nsys-rep分析文件，后续可以通过命令行获取相关信息或者Nsight Systems GUI工具直接打开该文件进行分析。</p>
<p><img src="https://img2024.cnblogs.com/blog/465567/202508/465567-20250821155632926-1495541944.png" alt="image" width="543" height="322" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<p>需要注意的是图中在输出结果前还有两个警告信息，它表明在当前环境下的某些功能不可用（CPU相关），所以它自动关掉了，出现这个警告，可能是和当前CPU架构或者操作系统内核等相关，可以使用nsys status --environment命令查看当前环境对 Nsight Systems 各种功能的支持情况，如输出信息如下：</p>
<div class="cnblogs_code">
<pre><span style="color: rgba(0, 128, 128, 1)"> 1</span> <span style="color: rgba(0, 0, 0, 1)">Timestamp counter supported: Yes
</span><span style="color: rgba(0, 128, 128, 1)"> 2</span> 
<span style="color: rgba(0, 128, 128, 1)"> 3</span> <span style="color: rgba(0, 0, 0, 1)">CPU Profiling Environment Check
</span><span style="color: rgba(0, 128, 128, 1)"> 4</span> <span style="color: rgba(0, 0, 0, 1)">Root privilege: disabled
</span><span style="color: rgba(0, 128, 128, 1)"> 5</span> Linux Kernel Paranoid Level = <span style="color: rgba(128, 0, 128, 1)">4</span>
<span style="color: rgba(0, 128, 128, 1)"> 6</span> Linux Distribution =<span style="color: rgba(0, 0, 0, 1)"> Ubuntu
</span><span style="color: rgba(0, 128, 128, 1)"> 7</span> Linux Kernel Version = <span style="color: rgba(128, 0, 128, 1)">6.8</span>.<span style="color: rgba(128, 0, 128, 1)">0</span>-<span style="color: rgba(128, 0, 128, 1)">65</span>-<span style="color: rgba(0, 0, 0, 1)">generic: OK
</span><span style="color: rgba(0, 128, 128, 1)"> 8</span> <span style="color: rgba(0, 0, 0, 1)">Linux perf_event_open syscall available: Fail
</span><span style="color: rgba(0, 128, 128, 1)"> 9</span> Sampling trigger <span style="color: rgba(0, 0, 255, 1)">event</span><span style="color: rgba(0, 0, 0, 1)"> available: Fail
</span><span style="color: rgba(0, 128, 128, 1)">10</span> <span style="color: rgba(0, 0, 0, 1)">Intel(c) Last Branch Record support: Not Available
</span><span style="color: rgba(0, 128, 128, 1)">11</span> CPU Profiling Environment (process-<span style="color: rgba(0, 0, 0, 1)">tree): Fail
</span><span style="color: rgba(0, 128, 128, 1)">12</span> CPU Profiling Environment (system-<span style="color: rgba(0, 0, 0, 1)">wide): Fail
</span><span style="color: rgba(0, 128, 128, 1)">13</span> 
<span style="color: rgba(0, 128, 128, 1)">14</span> See the product documentation at https:<span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)">docs.nvidia.com/nsight-systems for more information,</span>
<span style="color: rgba(0, 128, 128, 1)">15</span> including information on how to <span style="color: rgba(0, 0, 255, 1)">set</span> the Linux Kernel Paranoid Level.</pre>
</div>
<p>可见在9~12行，有好几处Fail所以导致了相关CPU profiling不支持，后搜索解决方案发现，进行如下操作可能解决该问题：</p>
<div class="cnblogs_code">
<pre><span style="color: rgba(0, 0, 0, 1)"># 安装 libunwind
sudo apt install libunwind</span>-<span style="color: rgba(0, 0, 0, 1)">dev
# 确认用户有权限访问 perf_event
sudo sysctl </span>-w kernel.perf_event_paranoid=<span style="color: rgba(128, 0, 128, 1)">1</span></pre>
</div>
<p>进行上述操作后，再次检测环境，虽然仍有一个错误提示，但是再次运行nsys分析时已经不再显示两个警告信息：</p>
<p><img src="https://img2024.cnblogs.com/blog/465567/202508/465567-20250821163832031-613597746.png" alt="image" width="577" height="361" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<h3>2. 分析结果</h3>
<p>可使用如下命令分析结果：</p>
<div class="cnblogs_code">
<pre>nsys stats report.nsys-rep</pre>
</div>
<p>关键输出信息类似：</p>
<div class="cnblogs_code">
<pre><span style="color: rgba(0, 128, 128, 1)"> 1</span>  **<span style="color: rgba(0, 0, 0, 1)"> CUDA API Summary (cuda_api_sum):
</span><span style="color: rgba(0, 128, 128, 1)"> 2</span> 
<span style="color: rgba(0, 128, 128, 1)"> 3</span>  Time (%<span style="color: rgba(0, 0, 0, 1)">)  Total Time (ns)  Num Calls    Avg (ns)    Med (ns)  Min (ns)   Max (ns)   StdDev (ns)         Name      
</span><span style="color: rgba(0, 128, 128, 1)"> 4</span>  --------  ---------------  ---------  ------------  --------  --------  ----------  ------------  ----------------
<span style="color: rgba(0, 128, 128, 1)"> 5</span>      <span style="color: rgba(128, 0, 128, 1)">99.6</span>       <span style="color: rgba(128, 0, 128, 1)">86</span>,<span style="color: rgba(128, 0, 128, 1)">755</span>,<span style="color: rgba(128, 0, 128, 1)">035</span>          <span style="color: rgba(128, 0, 128, 1)">3</span>  <span style="color: rgba(128, 0, 128, 1)">28</span>,<span style="color: rgba(128, 0, 128, 1)">918</span>,<span style="color: rgba(128, 0, 128, 1)">345.0</span>   <span style="color: rgba(128, 0, 128, 1)">5</span>,<span style="color: rgba(128, 0, 128, 1)">369.0</span>     <span style="color: rgba(128, 0, 128, 1)">3</span>,<span style="color: rgba(128, 0, 128, 1)">688</span>  <span style="color: rgba(128, 0, 128, 1)">86</span>,<span style="color: rgba(128, 0, 128, 1)">745</span>,<span style="color: rgba(128, 0, 128, 1)">978</span>  <span style="color: rgba(128, 0, 128, 1)">50</span>,<span style="color: rgba(128, 0, 128, 1)">080</span>,<span style="color: rgba(128, 0, 128, 1)">199.2</span><span style="color: rgba(0, 0, 0, 1)">  cudaMalloc      
</span><span style="color: rgba(0, 128, 128, 1)"> 6</span>       <span style="color: rgba(128, 0, 128, 1)">0.3</span>          <span style="color: rgba(128, 0, 128, 1)">223</span>,<span style="color: rgba(128, 0, 128, 1)">020</span>          <span style="color: rgba(128, 0, 128, 1)">3</span>      <span style="color: rgba(128, 0, 128, 1)">74</span>,<span style="color: rgba(128, 0, 128, 1)">340.0</span>  <span style="color: rgba(128, 0, 128, 1)">43</span>,<span style="color: rgba(128, 0, 128, 1)">770.0</span>    <span style="color: rgba(128, 0, 128, 1)">39</span>,<span style="color: rgba(128, 0, 128, 1)">025</span>     <span style="color: rgba(128, 0, 128, 1)">140</span>,<span style="color: rgba(128, 0, 128, 1)">225</span>      <span style="color: rgba(128, 0, 128, 1)">57</span>,<span style="color: rgba(128, 0, 128, 1)">107.4</span><span style="color: rgba(0, 0, 0, 1)">  cudaMemcpy      
</span><span style="color: rgba(0, 128, 128, 1)"> 7</span>       <span style="color: rgba(128, 0, 128, 1)">0.1</span>           <span style="color: rgba(128, 0, 128, 1)">92</span>,<span style="color: rgba(128, 0, 128, 1)">533</span>          <span style="color: rgba(128, 0, 128, 1)">3</span>      <span style="color: rgba(128, 0, 128, 1)">30</span>,<span style="color: rgba(128, 0, 128, 1)">844.3</span>  <span style="color: rgba(128, 0, 128, 1)">21</span>,<span style="color: rgba(128, 0, 128, 1)">988.0</span>     <span style="color: rgba(128, 0, 128, 1)">4</span>,<span style="color: rgba(128, 0, 128, 1)">057</span>      <span style="color: rgba(128, 0, 128, 1)">66</span>,<span style="color: rgba(128, 0, 128, 1)">488</span>      <span style="color: rgba(128, 0, 128, 1)">32</span>,<span style="color: rgba(128, 0, 128, 1)">143.9</span><span style="color: rgba(0, 0, 0, 1)">  cudaFree        
</span><span style="color: rgba(0, 128, 128, 1)"> 8</span>       <span style="color: rgba(128, 0, 128, 1)">0.0</span>           <span style="color: rgba(128, 0, 128, 1)">21</span>,<span style="color: rgba(128, 0, 128, 1)">241</span>          <span style="color: rgba(128, 0, 128, 1)">1</span>      <span style="color: rgba(128, 0, 128, 1)">21</span>,<span style="color: rgba(128, 0, 128, 1)">241.0</span>  <span style="color: rgba(128, 0, 128, 1)">21</span>,<span style="color: rgba(128, 0, 128, 1)">241.0</span>    <span style="color: rgba(128, 0, 128, 1)">21</span>,<span style="color: rgba(128, 0, 128, 1)">241</span>      <span style="color: rgba(128, 0, 128, 1)">21</span>,<span style="color: rgba(128, 0, 128, 1)">241</span>           <span style="color: rgba(128, 0, 128, 1)">0.0</span><span style="color: rgba(0, 0, 0, 1)">  cudaLaunchKernel
</span><span style="color: rgba(0, 128, 128, 1)"> 9</span> 
<span style="color: rgba(0, 128, 128, 1)">10</span> Processing [report.sqlite] with [/opt/nvidia/nsight-systems/<span style="color: rgba(128, 0, 128, 1)">2023.1</span>.<span style="color: rgba(128, 0, 128, 1)">2</span>/host-linux-x64/reports/<span style="color: rgba(0, 0, 0, 1)">cuda_gpu_kern_sum.py]... 
</span><span style="color: rgba(0, 128, 128, 1)">11</span> 
<span style="color: rgba(0, 128, 128, 1)">12</span>  **<span style="color: rgba(0, 0, 0, 1)"> CUDA GPU Kernel Summary (cuda_gpu_kern_sum):
</span><span style="color: rgba(0, 128, 128, 1)">13</span> 
<span style="color: rgba(0, 128, 128, 1)">14</span>  Time (%<span style="color: rgba(0, 0, 0, 1)">)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)                          Name                         
</span><span style="color: rgba(0, 128, 128, 1)">15</span>  --------  ---------------  ---------  --------  --------  --------  --------  -----------  -----------------------------------------------------
<span style="color: rgba(0, 128, 128, 1)">16</span>     <span style="color: rgba(128, 0, 128, 1)">100.0</span>            <span style="color: rgba(128, 0, 128, 1)">4</span>,<span style="color: rgba(128, 0, 128, 1)">128</span>          <span style="color: rgba(128, 0, 128, 1)">1</span>   <span style="color: rgba(128, 0, 128, 1)">4</span>,<span style="color: rgba(128, 0, 128, 1)">128.0</span>   <span style="color: rgba(128, 0, 128, 1)">4</span>,<span style="color: rgba(128, 0, 128, 1)">128.0</span>     <span style="color: rgba(128, 0, 128, 1)">4</span>,<span style="color: rgba(128, 0, 128, 1)">128</span>     <span style="color: rgba(128, 0, 128, 1)">4</span>,<span style="color: rgba(128, 0, 128, 1)">128</span>          <span style="color: rgba(128, 0, 128, 1)">0.0</span>  vectorAdd(<span style="color: rgba(0, 0, 255, 1)">const</span> <span style="color: rgba(0, 0, 255, 1)">float</span> *, <span style="color: rgba(0, 0, 255, 1)">const</span> <span style="color: rgba(0, 0, 255, 1)">float</span> *, <span style="color: rgba(0, 0, 255, 1)">float</span> *, <span style="color: rgba(0, 0, 255, 1)">int</span><span style="color: rgba(0, 0, 0, 1)">)
</span><span style="color: rgba(0, 128, 128, 1)">17</span> 
<span style="color: rgba(0, 128, 128, 1)">18</span> Processing [report.sqlite] with [/opt/nvidia/nsight-systems/<span style="color: rgba(128, 0, 128, 1)">2023.1</span>.<span style="color: rgba(128, 0, 128, 1)">2</span>/host-linux-x64/reports/<span style="color: rgba(0, 0, 0, 1)">cuda_gpu_mem_time_sum.py]... 
</span><span style="color: rgba(0, 128, 128, 1)">19</span> 
<span style="color: rgba(0, 128, 128, 1)">20</span>  **<span style="color: rgba(0, 0, 0, 1)"> CUDA GPU MemOps Summary (by Time) (cuda_gpu_mem_time_sum):
</span><span style="color: rgba(0, 128, 128, 1)">21</span> 
<span style="color: rgba(0, 128, 128, 1)">22</span>  Time (%<span style="color: rgba(0, 0, 0, 1)">)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)      Operation     
</span><span style="color: rgba(0, 128, 128, 1)">23</span>  --------  ---------------  -----  --------  --------  --------  --------  -----------  ------------------
<span style="color: rgba(0, 128, 128, 1)">24</span>      <span style="color: rgba(128, 0, 128, 1)">67.5</span>           <span style="color: rgba(128, 0, 128, 1)">33</span>,<span style="color: rgba(128, 0, 128, 1)">122</span>      <span style="color: rgba(128, 0, 128, 1)">2</span>  <span style="color: rgba(128, 0, 128, 1)">16</span>,<span style="color: rgba(128, 0, 128, 1)">561.0</span>  <span style="color: rgba(128, 0, 128, 1)">16</span>,<span style="color: rgba(128, 0, 128, 1)">561.0</span>    <span style="color: rgba(128, 0, 128, 1)">16</span>,<span style="color: rgba(128, 0, 128, 1)">513</span>    <span style="color: rgba(128, 0, 128, 1)">16</span>,<span style="color: rgba(128, 0, 128, 1)">609</span>         <span style="color: rgba(128, 0, 128, 1)">67.9</span><span style="color: rgba(0, 0, 0, 1)">  [CUDA memcpy HtoD]
</span><span style="color: rgba(0, 128, 128, 1)">25</span>      <span style="color: rgba(128, 0, 128, 1)">32.5</span>           <span style="color: rgba(128, 0, 128, 1)">15</span>,<span style="color: rgba(128, 0, 128, 1)">937</span>      <span style="color: rgba(128, 0, 128, 1)">1</span>  <span style="color: rgba(128, 0, 128, 1)">15</span>,<span style="color: rgba(128, 0, 128, 1)">937.0</span>  <span style="color: rgba(128, 0, 128, 1)">15</span>,<span style="color: rgba(128, 0, 128, 1)">937.0</span>    <span style="color: rgba(128, 0, 128, 1)">15</span>,<span style="color: rgba(128, 0, 128, 1)">937</span>    <span style="color: rgba(128, 0, 128, 1)">15</span>,<span style="color: rgba(128, 0, 128, 1)">937</span>          <span style="color: rgba(128, 0, 128, 1)">0.0</span>  [CUDA memcpy DtoH]</pre>
</div>
<p>输出信息第1~8行是CPU API 调用耗时统计，其中Avg是平均时间，例如cudaMalloc函数在程序中一共被调用了3次，第1次由于包含GPU初始化相关内容运行时间最长，为Max值为86,745,978ns，第2次调用时间为中位数时间Med，值为5,369ns，第3次调用用时最少为Min，值为3,688ns，3次调用的平均时间Avg为28,918,345ns，StdDev为运行时间的标准偏差，其计算公式为sqrt(Σpow((x<sub>i</sub>-x<sub>avg</sub>), 2)/(n -1))，该值用于衡量数据的离散程度，即各数据点与平均值之间的平均偏离程度，其大小直接反映API调用时长的稳定性。由第8行可知，在Linux平台下整个cudaLaunchKernel调用时间仅有21,241 ns，在整个程序运行时间中的占比基本可以被忽略。输出信息第10~16行是Kernel函数vectorAdd的运行时间为4128ns，输出信息第20~25行是设备和主机内存互拷所用时间，分别是33123ns和15937ns，这三部分时间中，Kernel运行时间只占4128.0/(4128+33122+15937)=0.0776，即7.8%左右。</p>
<p>此外在命令行输出的report文件也可以在Nsight system GUI里直接打开，图中分别框出了Kernels运行占比、第3次cudaMalloc调用及Kernel函数vectorAdd运行时间，显示值也是和命令行中的输出信息及分析结果一致的：</p>
<p><img src="https://img2024.cnblogs.com/blog/465567/202508/465567-20250821181625598-406462619.png" alt="image" width="1177" height="636" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<h3>3. 其他内容</h3>
<p>在 Nsight Systems 中使用nsys profile命令时，加上--stats=true参数会产生.sqlite文件，参数会使 Nsight Systems 在收集性能数据的过程中，额外收集并整理统计信息， 而.sqlite格式在存储结构化数据和统计信息方面有较好的支持和优势。开启该参数后，工具为了更方便地存储和组织这些额外的统计数据，会生成.sqlite文件，以充分利用 SQLite 数据库在数据管理和查询上的特性，便于后续对性能统计信息进行分析、检索 。用sqlite工具打开相应文件，内容如下，由于本人对数据库操作不熟悉对此不再详细分析。</p>
<p><img src="https://img2024.cnblogs.com/blog/465567/202508/465567-20250821183809450-39323617.png" alt="image" width="675" height="407" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<p>更多命令行参数请参考<a href="https://docs.nvidia.com/nsight-systems/UserGuide/index.html" target="_blank" rel="noopener nofollow">官方文档</a>。</p>
<p>&nbsp;</p>
<h1>参考</h1>
<p>1.&nbsp;<a href="https://developer.nvidia.com/nsight-systems/get-started" target="_blank" rel="noopener nofollow">https://developer.nvidia.com/nsight-systems/get-started</a></p>
<p>2.&nbsp;<a href="https://docs.nvidia.com/nsight-systems/index.html" target="_blank" rel="noopener nofollow">https://docs.nvidia.com/nsight-systems/index.html</a></p>
<p>3.&nbsp;<a href="https://docs.nvidia.com/nsight-systems/UserGuide/index.html" target="_blank" rel="noopener nofollow">https://docs.nvidia.com/nsight-systems/UserGuide/index.html</a></p>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0" data-date-updated="2025-08-22 17:24">2025-08-22 17:23</span>&nbsp;
<a href="https://www.cnblogs.com/zhaoweiwei">weiwei22844</a>&nbsp;
阅读(<span id="post_view_count">109</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(19048895);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '19048895', targetLink: 'https://www.cnblogs.com/zhaoweiwei/p/19048895/NsightSystems', title: 'NVIDIA系统级性能分析工具Nsight Systems入门详解' })">举报</a>
</div>
        