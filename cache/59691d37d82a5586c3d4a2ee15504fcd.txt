
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/hiit/p/18757849" title="发布于 2025-03-10 12:36">
    <span role="heading" aria-level="2">超详细：普通电脑也行Windows部署deepseek R1训练数据并当服务器共享给他人</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body blogpost-body-html">
<h3 class="auto-hide-last-sibling-br paragraph-JOTKXA paragraph-element br-paragraph-space">一、<strong>Windows 版 DeepSeek-R1、Ollama 与 AnythingLLM 介绍及核心使用场景</strong>‌</h3>
<h4>‌<strong>一、组件功能与定位</strong>‌</h4>
<ol>
<li>
<p class="marklang-paragraph">‌<strong>DeepSeek-R1</strong>‌</p>
<ul>
<li>‌<strong>模型特性</strong>‌：支持 ‌<strong>FP16 计算</strong>‌ 和 ‌<strong>CUDA 加速</strong>‌，提供 1.5B 至 671B 参数量级版本，适用于本地部署的逻辑推理、文本生成、数据分析等场景‌。</li>
<li>‌<strong>优势</strong>‌：开源免费、响应速度快（本地低延迟），支持中文复杂任务处理‌。</li>
</ul>
</li>
<li>
<p class="marklang-paragraph">‌<strong>Ollama</strong>‌</p>
<ul>
<li>‌<strong>核心功能</strong>‌：简化大语言模型本地部署流程，支持一键下载、运行和管理模型（如 DeepSeek-R1），提供命令行界面和灵活的环境变量配置‌。</li>
<li>‌<strong>特性</strong>‌：支持自定义安装路径（需修改系统变量）、多模型切换、离线运行‌。</li>
</ul>
</li>
<li>
<p class="marklang-paragraph">‌<strong>AnythingLLM</strong>‌</p>
<ul>
<li>‌<strong>定位</strong>‌：全栈 AI 应用，集成本地大模型（通过 Ollama）与知识库，支持文档/音视频/网页内容转换为上下文数据，供 LLM 调用‌。</li>
<li>‌<strong>功能</strong>‌：私有化知识库构建、多工作区管理、模型与数据关联式问答‌。</li>
</ul>
</li>
</ol><hr>
<h4>‌<strong>二、核心使用场景</strong>‌</h4>
<ol>
<li>
<p class="marklang-paragraph">‌<strong>企业内部知识库与智能客服</strong>‌</p>
<ul>
<li>‌<strong>场景</strong>‌：通过 AnythingLLM 上传企业文档（如产品手册、合同），结合 DeepSeek-R1 实现精准问答，替代传统人工客服‌。</li>
<li>‌<strong>优势</strong>‌：数据本地化存储（避免云端泄露）、支持多格式文件解析‌。</li>
</ul>
</li>
<li>
<p class="marklang-paragraph">‌<strong>专业领域研究与分析</strong>‌</p>
<ul>
<li>‌<strong>场景</strong>‌：科研人员使用 DeepSeek-R1 处理长文本（如论文、报告），生成摘要或提取核心结论；结合 AnythingLLM 训练领域专属模型‌。</li>
<li>‌<strong>案例</strong>‌：法律条文分析、医学文献结构化处理‌。</li>
</ul>
</li>
<li>
<p class="marklang-paragraph">‌<strong>个人效率工具</strong>‌</p>
<ul>
<li>‌<strong>场景</strong>‌：
<ul>
<li>‌<strong>周报生成</strong>‌：输入工作记录，由 DeepSeek-R1 自动整理成结构化周报‌。</li>
<li>‌<strong>实时翻译</strong>‌：本地部署模型实现无网络环境下的多语言互译‌。</li>
<li><strong>个性化知识助手</strong>‌:导入个人笔记、电子书等资料，构建专属知识库，辅助日常学习决策‌。</li>
</ul>
</li>
</ul>
</li>
<li>
<p class="marklang-paragraph">‌<strong>开发与测试环境</strong>‌</p>
<ul>
<li>‌<strong>场景</strong>‌：开发者通过 Ollama 快速切换不同参数量级的 DeepSeek-R1 版本，测试模型性能或调试应用兼容性‌。</li>
</ul>
</li>
</ol>
<h3 class="auto-hide-last-sibling-br paragraph-JOTKXA paragraph-element br-paragraph-space">二、本地下载安装部署</h3>
<h4><strong>1、安装 CUDA Toolkit 步骤如下</strong>：<a title="CUDA Toolkit " href="https://developer.nvidia.com/cuda-downloads?target_os=Windows&amp;target_arch=x86_64" rel="noopener nofollow" target="_blank">https://developer.nvidia.com/cuda-downloads?target_os=Windows&amp;target_arch=x86_64</a></h4>
<ol class="auto-hide-last-sibling-br">
<li>不用显卡跳过此步骤</li>
<li>访问指定链接下载 CUDA Toolkit。请务必留意针对 Windows 系统的版本选项，如 Windows 10、Windows 11 等，并选择 “local” 本地安装类型，以获取最佳安装体验。</li>
<li>下载过程需登录 NVIDIA 账号。若您尚无账号，请提前完成注册流程，确保下载顺利进行。</li>
<li>完成 CUDA Toolkit 安装后，请重启计算机，以使所有配置更改生效，从而确保 CUDA Toolkit 能够正常运行。<img src="https://img2024.cnblogs.com/blog/785716/202503/785716-20250307141342085-733882947.png"></li>
</ol>
<h4><span style="font-size: 14px">2、Ollama 安装：<a href="https://ollama.com/" rel="noopener nofollow" target="_blank">https://ollama.com/</a></span></h4>
<ol class="auto-hide-last-sibling-br">
<li><strong>下载与安装</strong>：前往 Ollama 官方网站，获取 Ollama 安装程序。整个下载与安装流程十分简易，按照系统提示逐步操作即可轻松完成，在此便不展开详述。</li>
<li><strong>后台运行检查</strong>：安装完成并启动 Ollama 后，该程序将在后台持续运行。您可通过查看电脑右下角的系统托盘区域，确认是否出现 Ollama 的 Logo 图标，以此判断 Ollama 是否已成功在后台启动并正常运行 。</li>
</ol>
<p><img src="https://img2024.cnblogs.com/blog/785716/202503/785716-20250307140959674-1816146356.png"></p>
<h4>3、<strong> 通过Ollama安装模型</strong>：<a href="https://ollama.com/search" rel="noopener nofollow" target="_blank">https://ollama.com/search</a></h4>
<ol class="auto-hide-last-sibling-br">
<li><strong>选择模型参数与获取安装命令</strong>：Ollama 的模型库涵盖了丰富多样的大语言模型，通过 Ollama 安装 DeepSeek - R1 模型的操作十分便捷。其模型库中的可用模型包括但不限于 DeepSeek - R1、Lamma3.3、qwq 等，用户可按需选择安装 。进入 DeepSeek - R1 模型集合，依照下方图示的顺序，选择合适的参数数量，然后复制对应的安装命令。<img src="https://img2024.cnblogs.com/blog/785716/202503/785716-20250307141038398-108665489.png"></li>
<li><strong>考虑硬件限制</strong>：需特别注意，GPU 显存大小（若仅使用 CPU，则考虑本地主机的内存）会限制可使用的模型大小。例如，拥有 16GB 显存的 GPU 能够运行 14B 的模型，而 24GB 显存的 GPU 则可以运行 32B 的模型。<img src="https://img2024.cnblogs.com/blog/785716/202503/785716-20250307141132308-667036559.png"></li>
<li><strong>执行安装命令</strong>：<span style="color: rgba(224, 62, 45, 1)">普通电脑运行1.5b就可以，不用显卡</span>。以本人为例，我选择了 14B 的模型。复制好命令后，打开 Powershell 并执行该命令，Ollama 会自动开始安装相应版本的 DeepSeek - R1 模型。你只需耐心等待命令运行结束，安装完成后即可使用该模型。</li>
</ol>
<pre class="language-python highlighter-hljs"><code>ollama run deepseek-r1:1.5b</code></pre>
<p><img src="https://img2024.cnblogs.com/blog/785716/202503/785716-20250307142938444-90790737.png"></p>
<p>&nbsp;Powershell 的终端中直接运行了，但是存在诸多不便之处。比如，难以对对话记录进行保存、搜索与管理，无法读取附件，并且无法集成本地知识库来实现检索增强生成等功能，极大地影响了用户体验。因此，强烈推荐大家安装一款本地 AI 应用用户界面，本文以 AnythingLLM 为例进行示范。</p>
<h4 class="header-vfC6AV auto-hide-last-sibling-br"><span style="font-size: 14px">4、AnythingLLM 安装与配置：<a href="https://anythingllm.com/desktop" rel="noopener nofollow" target="_blank">https://anythingllm.com/desktop</a></span></h4>
<ol class="auto-hide-last-sibling-br">
<li><strong>下载与安装</strong>：前往 AnythingLLM 官方网站获取安装程序并进行安装。<img src="https://img2024.cnblogs.com/blog/785716/202503/785716-20250307145501376-844204615.png"></li>
<li><strong>启动与设置</strong>：安装完成并启动 AnythingLLM 后，在设置中选择 “Ollama”（注意不是 “DeepSeek”）。<img src="https://img2024.cnblogs.com/blog/785716/202503/785716-20250307150034268-1329073474.png"></li>
<li>在设置中选择 “Ollama”（注意不是 “DeepSeek”）作为 LLM Provider。此时，AnythingLLM 会自动检测本地部署的大语言模型，从中选择 “deepseek - r1:14b”。然后一直点击右键。<img src="https://img2024.cnblogs.com/blog/785716/202503/785716-20250307150343865-973690151.png"></li>
<li><strong>创建工作区并开始对话</strong>：完成上述步骤后，创建一个 Workspace，随后即可开启与模型的对话之旅。<img src="https://img2024.cnblogs.com/blog/785716/202503/785716-20250307161437501-74548815.png" alt=""></li>
<li>进行设置
<p><img src="https://img2024.cnblogs.com/blog/785716/202503/785716-20250307161402183-679398563.png" alt="">&nbsp;</p>
</li>
<li>保存设置
<p><img src="https://img2024.cnblogs.com/blog/785716/202503/785716-20250307161637053-1151304714.png" alt=""></p>
<p>&nbsp;</p>
<h4 class="header-vfC6AV auto-hide-last-sibling-br">5、下载 embedding 模型</h4>
<h4 class="header-vfC6AV auto-hide-last-sibling-br">步骤一：下载模型</h4>
<ol class="auto-hide-last-sibling-br">
<li>打开命令行工具。</li>
<li>在命令行中输入指令&nbsp;<code>ollama pull nomic-embed-text</code>，然后回车执行，等待模型下载完成。</li>
<li><img src="https://img2024.cnblogs.com/blog/785716/202503/785716-20250307163933729-979564789.png"></li>
</ol>
<h4 class="header-vfC6AV auto-hide-last-sibling-br">步骤二：切换并保存模型设置</h4>
<ol class="auto-hide-last-sibling-br"><ol class="auto-hide-last-sibling-br">
<li>找到系统左下角的相关操作入口（扳手）。</li>
<li><img src="https://img2024.cnblogs.com/blog/785716/202503/785716-20250307163157650-2033287569.png"></li>
<li>切换到新下载的&nbsp;<code>nomic-embed-text</code>&nbsp;embedding 模型。</li>
<li><img src="https://img2024.cnblogs.com/blog/785716/202503/785716-20250307163435483-41473718.png"></li>
<li>切换完成后，点击 “保存更改” 按钮，确保设置生效。</li>
<li><img src="https://img2024.cnblogs.com/blog/785716/202503/785716-20250307164024284-499338744.png"></li>
</ol></ol></li>
</ol>
<h4 class="header-vfC6AV auto-hide-last-sibling-br">6. 上传本地资料构建本地数据库</h4>
<ol class="auto-hide-last-sibling-br">
<li>在工作空间页面中，找到并点击 “上传” 按钮。</li>
<li><img src="https://img2024.cnblogs.com/blog/785716/202503/785716-20250307164350752-1283461136.png"></li>
<li>依据实际需求选择上传方式：你既可以直接上传本地文件，也能够选择连接数据库以获取所需资料。<img src="https://img2024.cnblogs.com/blog/785716/202503/785716-20250307164717941-1248439679.png"></li>
<li>资料上传或数据库连接完成后，点击 “保存” 按钮，系统将自动对上传的资料进行向量化处理。</li>
<li>然后新建对话进行测试。<img src="https://img2024.cnblogs.com/blog/785716/202503/785716-20250307164959542-542797908.png"></li>
</ol>
<h3>三、Windows 系统 Ollama 当服务器开放共享给其他人</h3>
<hr>
<h4>‌<strong>一、配置环境变量开放访问权限</strong>‌</h4>
<ol>
<li>
<p class="marklang-paragraph">‌<strong>设置&nbsp;<code>OLLAMA_HOST</code>&nbsp;环境变量</strong>‌</p>
<ul>
<li>右键点击「此电脑」→「属性」→「高级系统设置」→「环境变量」→「系统变量」→「新建」：
<ul>
<li>‌<strong>变量名</strong>‌：<code>OLLAMA_HOST</code></li>
<li>‌<strong>变量值</strong>‌：<code>0.0.0.0</code>（允许所有网络接口监听请求）‌</li>
</ul>
</li>
<li>若需同时解决跨域问题，可添加变量&nbsp;<code>OLLAMA_ORIGINS</code>，值为&nbsp;<code>*</code>&nbsp;‌。</li>
<li><img src="https://img2024.cnblogs.com/blog/785716/202503/785716-20250310092719366-1188089114.png"></li>
</ul>
</li>
<li>
<p class="marklang-paragraph">‌<strong>重启 Ollama 服务</strong>‌</p>
<ul>
<li>退出任务栏的 Ollama 程序（右键图标→「Quit Ollama」），重新启动 Ollama ‌。</li>
<li>若配置未生效，建议重启系统 ‌。</li>
</ul>
</li>
</ol><hr>
<h4>‌<strong>二、开放防火墙端口</strong>‌</h4>
<ol>
<li>
<p class="marklang-paragraph">‌<strong>通过命令行添加防火墙规则</strong>‌</p>
<ul>
<li>以管理员身份运行命令提示符，执行以下命令：
<div class="code-wrapper">
<div class="code-right"><code class="hljs language-cmd">netsh advfirewall firewall add rule name="Ollama" dir=in action=allow protocol=TCP localport=11434 </code></div>
</div>
<em>（放行 Ollama 默认端口&nbsp;<code>11434</code>）‌</em></li>
</ul>
</li>
<li>
<p class="marklang-paragraph">‌<strong>验证防火墙规则</strong>‌</p>
<ul>
<li>检查端口是否放行成功：
<div class="code-wrapper">
<div class="code-right"><code class="hljs language-cmd">netsh advfirewall firewall show rule name="Ollama" </code></div>
</div>
<em>（输出需包含&nbsp;<code>Enabled=Yes</code>&nbsp;和&nbsp;<code>Action=Allow</code>）‌</em></li>
</ul>
</li>
</ol><hr>
<h4>‌<strong>三、验证非本机访问</strong>‌</h4>
<ol>
<li>
<p class="marklang-paragraph">‌<strong>局域网设备测试</strong>‌</p>
<ul>
<li>执行命令&nbsp;<code>ipconfig</code>&nbsp;查看本机 IPv4 地址（如&nbsp;<code>192.168.1.100</code>）。</li>
<li>在其他设备的浏览器或命令行中访问：
<div class="code-wrapper">
<div class="code-right"><code class="hljs language-bash">curl http://[Windows主机IP]:11434/api/tags </code></div>
</div>
<em>（若返回模型列表，则配置成功）‌</em></li>
</ul>
</li>
<li>
<p class="marklang-paragraph">‌<strong>可视化工具验证（可选）</strong>‌</p>
<ul>
<li>使用 Open WebUI 或 LobeChat 等工具，输入&nbsp;<code>http://[Windows主机IP]:11434</code>&nbsp;作为 Ollama 服务地址进行连接 ‌。</li>
</ul>
</li>
</ol><hr>
<h4>‌<strong>四、常见问题解决</strong>‌</h4>
<ul>
<li>‌<strong>报错&nbsp;<code>Server connection failed</code></strong>‌：<br>检查环境变量&nbsp;<code>OLLAMA_HOST</code>&nbsp;和&nbsp;<code>OLLAMA_ORIGINS</code>&nbsp;是否配置正确，并重启服务 ‌。</li>
<li>‌<strong>端口占用或冲突</strong>‌：<br>使用&nbsp;<code>netstat -ano | findstr 11434</code>&nbsp;确认端口未被其他进程占用 ‌。</li>

</ul>
<hr>
<h4><strong>五、公网共享（可选）</strong>‌</h4>
<ol>
<li>
<p class="marklang-paragraph">‌<strong>内网穿透工具部署</strong>‌</p>
<ul>
<li>使用&nbsp;<code>ngrok</code>&nbsp;将本地服务映射至公网：
<div class="code-wrapper">
<div class="code-right"><code class="hljs language-cmd">ngrok http 11434  # 映射 Ollama 服务  
</code></div>

</div>

</li>
<li>生成公网链接（如&nbsp;<code>https://xxx.ngrok.io</code>）共享给外部用户。</li>

</ul>

</li>
<li>
<p class="marklang-paragraph">‌<strong>域名绑定（高级）</strong>‌</p>
<ul>
<li>若已备案域名，可通过路由器或云服务商配置端口转发，将域名指向服务器 IP。</li>

</ul>

</li>

</ol>
<p class="marklang-paragraph">‌<strong>注意事项</strong>‌</p>
<ul>
<li>开放&nbsp;<code>0.0.0.0</code>&nbsp;会暴露服务至公网，建议内网使用时结合 IP 白名单或 VPN 提升安全性 ‌。</li>
<li>若修改了模型存储路径（<code>OLLAMA_MODELS</code>），需确保目录权限允许网络访问 ‌。</li>
<li>内网穿透需谨慎暴露公网端口，建议配置 HTTPS 加密和 IP 白名单‌。</li>
<li>定期清理 AnythingLLM 的无效文档，避免存储空间占用过高‌</li>

</ul>
<h3>&nbsp;</h3>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.5321994035347222" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-03-10 13:33">2025-03-10 12:36</span>&nbsp;
<a href="https://www.cnblogs.com/hiit">HIIT</a>&nbsp;
阅读(<span id="post_view_count">349</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18757849" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18757849);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18757849', targetLink: 'https://www.cnblogs.com/hiit/p/18757849', title: '超详细：普通电脑也行Windows部署deepseek R1训练数据并当服务器共享给他人' })">举报</a>
</div>
        