
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/Big-Yellow/p/18666364" title="发布于 2025-01-13 20:08">
    <span role="heading" aria-level="2">深度学习基础理论————DeepSpeed</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h2 id="deepspeed原理"><code>DeepSpeed</code>原理</h2>
<p>DeepSpeed 是由微软开发的一种深度学习优化库，专为高性能训练和推理而设计，尤其适用于大规模深度学习模型（如 GPT 系列、BERT 等）。它通过一系列技术和优化策略，帮助研究者和开发者高效利用硬件资源，实现快速训练、降低内存使用以及提升推理速度。<br>
正如其官方描述那样：</p>
<div align="center"><img src="https://img2023.cnblogs.com/blog/3395559/202501/3395559-20250111152607535-1429941566.png" alt="Deepspeed" style="zoom: 0%"></div>
<blockquote>
<p>Image From: <a href="https://github.com/microsoft/DeepSpeed" target="_blank" rel="noopener nofollow">https://github.com/microsoft/DeepSpeed</a></p>
</blockquote>
<hr>
<p><code>Deepspeed</code>作为一种显存优化技术，那么就会有一个问题：<strong>模型训练显存都被谁占用了？</strong><br>
参考论文（<a href="https://arxiv.org/pdf/1910.02054%EF%BC%89%E4%B8%AD%E7%9A%84%E6%8F%8F%E8%BF%B0%E5%9C%A8%E4%B8%80%E4%B8%AA" target="_blank" rel="noopener nofollow">https://arxiv.org/pdf/1910.02054）中的描述在一个</a><code>1.5B</code>的<code>GPT-2</code>模型参数量为<code>3G</code>（半精度）但是一块32G的显卡可能无法训练下来，这是因为显存都被 <strong>模型状态</strong> 以及 <strong>剩余状态</strong>（<code>Residual Memory Consumption</code>）</p>
<p><strong>模型状态</strong>显存占用<br>
主要指的是：<em>优化器状态，梯度，模型参数</em>。比如说在训练过程中一般都会选择使用<code>Adam</code>作为一种优化器进行使用，而在<code>Adam</code>计算过程中就会存储两部分内容：<strong>1、动量（上一轮梯度累计）；2、二阶动量（存储梯度平方的滑动平均值）</strong>。如何去避免这部分结果对显存占用的影响，就提出了 <em>混合精度训练</em>（用<code>FP16</code>存储和计算梯度及优化器状态）<br>
比如说：用<code>Adam</code>作为优化器在混合精度下训练参数量为<span class="math inline">\(\Phi\)</span>的模型显存占用：1、一部分用来存储<code>FP16</code>的参数以及梯度：<span class="math inline">\(2\Phi, 2\Phi\)</span>；2、另外一部分需要存储优化器状态（<code>FP32</code>存储：模型参数，动量，二阶动量）：<span class="math inline">\(4\Phi, 4\Phi, 4\Phi\)</span>。那么显存占用上就有：<span class="math inline">\(2+ 2+ 4+ 4+ 4=16\Phi\)</span>。那么回到上面提到的<code>1.5B</code>的<code>GPT-2</code>至少需要：<span class="math inline">\(1.5 \times 16=24G\)</span></p>
<p><strong>剩余状态</strong>显存占用<br>
这部分主要指的是： 除了模型状态之外的显存占用，包括<strong>激活值（activation）</strong>（可以通过<code>Activation checkpointing</code>减少）、<strong>各种临时缓冲区（buffer）</strong>以及无法使用的<strong>显存碎片（fragmentation）</strong></p>
<hr>
<p>了解模型训练过程中显存占用之后再去了解<code>DeepSpeed</code>中核心内容<code>ZeRO</code>（按照论文中表述作者是分了两部分介绍：<code>ZeRO-DP</code>和<code>ZeRO-R</code>分别去优化上面两部分显存占用）</p>
<blockquote>
<p><code>ZeRO-DP</code>原理</p>
</blockquote>
<p>主要是通过<strong>切分</strong>（<code>partitioning</code>）的方式来减少 <strong>模型状态</strong>显存占用</p>
<div align="center"><img src="https://img2023.cnblogs.com/blog/3395559/202501/3395559-20250111165122863-1423604415.png" alt="ZeRO-DP解释" style="zoom: 80%"></div>
<p>第一种方式为<span class="math inline">\(P_{OS}\)</span>：对优化器的状态进行切分，将<span class="math inline">\(N\)</span>块GPU上每块只存储<span class="math inline">\(\frac{1}{N}\)</span>，那么最后显存占用（按上面的显存分析为例）就为：<span class="math inline">\(4\Phi+ \frac{12\times \Phi}{N}\)</span></p>
<p>第二种方式为<span class="math inline">\(P_{OS+g}\)</span>也就是在对优化器切分的基础上补充一个对梯度的切分，那么显存占用上就变成为：<span class="math inline">\(2\Phi+ \frac{(2+ 12)\times \Phi}{N}\)</span></p>
<p>第三种方式为<span class="math inline">\(P_{OS+g+p}\)</span>也就是对模型状态三个都进行切分，显存占用为：<span class="math inline">\(\frac{4\Phi+ 12\Phi}{N}\)</span></p>
<p>对于上面3种方式显存减少上分别为：<span class="math inline">\(4\text{x}, 8\text{x}, N\)</span>（其中N表示的为设备数量）</p>
<hr>
<p>进一步理解上面3个操作</p>
<blockquote>
<p>Image From: <a href="https://zhuanlan.zhihu.com/p/618865052" target="_blank" rel="noopener nofollow">https://zhuanlan.zhihu.com/p/618865052</a></p>
</blockquote>
<p><strong>第一种方式<span class="math inline">\(P_{OS}\)</span></strong></p>
<p>因为会将优化器状态切分，那么在3个不同设备上分别存储<strong>3分优化器状态</strong>（o1, o2, o3）,对于这3部分优化器（因为优化器最后还是去“作用”到梯度上），分别对各自的梯度进行优化，但是会有一个问题：每块GPU上存储的是 <strong>一部分优化器状态</strong>，那么对于每份优化器也只能去优化各自的参数，每次更新需要通过 <strong>All-Gather</strong> 操作合并梯度，完成优化器状态更新</p>
<div align="center"><img src="https://img2023.cnblogs.com/blog/3395559/202501/3395559-20250111180835911-1295467906.jpg" alt="第一种方式" style="zoom: 30%"></div>
<p><strong>第二种方式<span class="math inline">\(P_{OS+g}\)</span></strong></p>
<div align="center"><img src="https://img2023.cnblogs.com/blog/3395559/202501/3395559-20250111180945132-231170935.jpg" alt="第二种方式" style="zoom: 30%"></div>
<p>在进行前向+反向传播之后，<strong>得到完整的梯度</strong>，因为要实现梯度拆分，那么就对梯度进行<code>reduce-scatter</code>对于不同的GPU就会存储不同的梯度（g1, g2, g3白色的就会剔除掉）前向和反向传播需要通过 <strong>All-Gather</strong> 和 <strong>All-Reduce</strong> 操作同步梯度和参数</p>
<p><strong>第三种方式为<span class="math inline">\(P_{OS+g+p}\)</span></strong></p>
<div align="center"><img src="https://img2023.cnblogs.com/blog/3395559/202501/3395559-20250111181435075-697401832.jpg" alt="第二种方式" style="zoom: 30%"></div>
<p>通过 <strong>All-Gather</strong>和 <strong>Reduce-Scatter</strong> 高效完成参数同步和更新。</p>
<p>总的来说：<code>ZeRO-DP</code>是一种 <em>用完就丢</em> 的套路，<strong>计算时候是完整内容，但是使用完之后就丢掉</strong></p>
<blockquote>
<p><strong>补充1</strong>：<code>All-Gather</code>, <code>All-Reduce</code>, <code>reduce-scatter</code>什么意思？</p>
</blockquote>
<blockquote>
<p><code>All-Gather</code>：将每个设备上的数据片段收集起来并广播到所有设备上。最终，每个设备都会拥有所有设备的数据片段</p>
<p>比如说4个GPU分别存储不同的值：<span class="math inline">\(GPU_i: i(i=1,2,3,4)\)</span>通过 <code>all-gather</code>那么不同GPU值为<span class="math inline">\(GPU_i: [1,2,3,4]\)</span></p>
</blockquote>
<blockquote>
<p><code>reduce-scatter</code>：将数据分片并执行 <strong>聚合</strong>，然后将结果分发给每个设备。每个设备最终只保留聚合后的部分结果</p>
<p>比如说4个GPU分别存储不同的值：<span class="math inline">\(GPU_i: [i_1, i_2, i_3, i_4](i=1,2,3,4)\)</span>通过 <code>reduce-scatter</code>（假设为按照加法聚合）那么不同GPU值为<span class="math inline">\(GPU_1: [1_1, 2_1, 3_1, 4_1]...\)</span></p>
</blockquote>
<blockquote>
<p><code>All-Reduce</code>：用于在所有设备之间对数据进行 <strong>聚合（Reduce）</strong> 和 <strong>广播（Broadcast）</strong>。每个设备都会执行相同的聚合操作，并最终持有相同的聚合结果</p>
<p>比如说4个GPU分别存储不同的值：<span class="math inline">\(GPU_i: i(i=1,2,3,4)\)</span>通过 <code>all-reduce</code>（假设为<code>sum</code>）那么不同GPU值为<span class="math inline">\(GPU_i: 10\)</span><br>
<code>Ring-ALLReduce</code>操作：<br>
<strong>第一阶段</strong>，通过<code>reduce-sactter</code>传递参数</p>
<div align="center"><img src="https://img2023.cnblogs.com/blog/3395559/202501/3395559-20250111225810355-560580627.png" alt="第二种方式" style="zoom: 100%"></div>
<p>通过3次参数更新之后，这样就会出现不同设备上都会有一个都具有参数<span class="math inline">\(a_i+ b_i+ c_i+ d_i\)</span>那么下一阶段就是通过<code>all-gather</code>将不同设备上参数广播到不同设备最后实现参数都实现更新。</p>
<div align="center"><img src="https://img2023.cnblogs.com/blog/3395559/202501/3395559-20250111230332745-1814167802.png" alt="第二种方式" style="zoom: 100%"></div>
<p><strong>补充2</strong>：通信量和传统的数据并行之间有无区别？<br>
这部分描述来自论文（<a href="https://arxiv.org/pdf/1910.02054%EF%BC%89%E4%B8%AD%E7%9A%84%E6%8F%8F%E8%BF%B0%EF%BC%9A" target="_blank" rel="noopener nofollow">https://arxiv.org/pdf/1910.02054）中的描述：</a><br>
<strong>传统的数据并行方式</strong>：传统的<code>DDP</code>主要使用的是<code>Ring AllReduce</code>在通信量上为：<span class="math inline">\(2\Phi\)</span>（主要来自两部分：）<br>
<span class="math inline">\(P_{OS} \text{和} P_{OS+g}\)</span>通信量：<span class="math inline">\(2\Phi\)</span>，以后者为例：因为每部分设备只保留了部分梯度信息，因此首先需要通过<code>reduce-scatter</code>操作（<span class="math inline">\(\Phi\)</span>）在梯度都通一之后需要对所有的参数进行更新（<span class="math inline">\(\Phi\)</span>）<br>
<span class="math inline">\(P_{OS+g+p}\)</span>：<span class="math inline">\(3\Phi\)</span>。<strong>前向传播</strong>过程中每个设备都只保存了部分参数，因此需要对设备之间进行一次参数广播，在前向操作结束之后，将其他参数删除掉（比如<span class="math inline">\(GPU_i\)</span>接受了<span class="math inline">\(i+1,...,n\)</span>的参数，那么就将这部分参数删除）此部分通信量为：<span class="math inline">\(\frac{\Phi \times N}{N}=\Phi\)</span>，类似的<strong>反向传播</strong>还需要再来一次，梯度还需要进行<code>reduce-scatter</code></p>
</blockquote>
<hr>
<blockquote>
<p><code>ZeRO-R</code>原理</p>
</blockquote>
<p>1、对于激活值的占用。通过<span class="math inline">\(P_a\)</span>：<code>Partitioned Activation Checkpointing</code>通过分区+checkpointing方式<br>
2、对于临时缓冲区。模型训练过程中经常会创建一些大小不等的临时缓冲区，比如对梯度进行AllReduce，解决办法就是预先创建一个固定的缓冲区，训练过程中不再动态创建，如果要传输的数据较小，则多组数据bucket后再一次性传输，提高效率<br>
3、对于显存碎片。显存出现碎片的一大原因是时候gradient checkpointing后，不断地创建和销毁那些不保存的激活值，解决方法是预先分配一块连续的显存，将常驻显存的模型状态和checkpointed activation存在里面，剩余显存用于动态创建和销毁discarded activation</p>
<h2 id="deepspeed代码"><code>DeepSpeed</code>代码</h2>
<p><code>Deepspeed</code>代码也比较简单，首先安装<code>deepspeed</code>:<code>pip install deepspeed</code>。使用<code>deepspeed</code>之前一般先去初始化，<a href="https://github.com/microsoft/DeepSpeed/blob/fa8db5cf2f9cf724fd2703353d40e3b37a8e7310/deepspeed/__init__.py#L68" target="_blank" rel="noopener nofollow">代码</a>如下：</p>
<pre><code class="language-python">def initialize(args=None,
               model: torch.nn.Module = None,
               optimizer: Optional[Union[Optimizer, DeepSpeedOptimizerCallable]] = None,
               model_parameters: Optional[torch.nn.Module] = None,
               training_data: Optional[torch.utils.data.Dataset] = None,
               lr_scheduler: Optional[Union[_LRScheduler, DeepSpeedSchedulerCallable]] = None,
               distributed_port: int = TORCH_DISTRIBUTED_DEFAULT_PORT,
               mpu=None,
               dist_init_required: Optional[bool] = None,
               collate_fn=None,
               config=None,
               mesh_param=None,
               config_params=None):
    """初始化 DeepSpeed 引擎。

    参数:
        args: 一个包含 `local_rank` 和 `deepspeed_config` 字段的对象。
            如果提供了 `config`，此参数是可选的。

        model: 必填项：在应用任何包装器之前的 nn.Module 类。

        optimizer: 可选：用户定义的 Optimizer 或返回 Optimizer 对象的 Callable。
            如果提供，将覆盖 DeepSpeed JSON 配置中的任何优化器定义。

        model_parameters: 可选：torch.Tensors 或字典的可迭代对象。
            指定需要优化的张量。

        training_data: 可选：torch.utils.data.Dataset 类型的数据集。

        lr_scheduler: 可选：学习率调度器对象或一个 Callable，接收一个 Optimizer 并返回调度器对象。
            调度器对象应定义 `get_lr()`、`step()`、`state_dict()` 和 `load_state_dict()` 方法。

        distributed_port: 可选：主节点（rank 0）用于分布式训练期间通信的空闲端口。

        mpu: 可选：模型并行单元对象，需实现以下方法：
            `get_{model,data}_parallel_{rank,group,world_size}()`。

        dist_init_required: 可选：如果为 None，将根据需要自动初始化 torch 分布式；
            否则用户可以通过布尔值强制初始化或不初始化。

        collate_fn: 可选：合并样本列表以形成一个小批量的张量。
            在从 map-style 数据集中使用批量加载时使用。

        config: 可选：可以作为路径或字典传递的 DeepSpeed 配置，
            用于替代 `args.deepspeed_config`。

        config_params: 可选：与 `config` 相同，为了向后兼容保留。

    返回值:
        返回一个包含 `engine`, `optimizer`, `training_dataloader`, `lr_scheduler` 的元组。

        * `engine`: DeepSpeed 运行时引擎，用于包装客户端模型以进行分布式训练。

        * `optimizer`: 如果提供了用户定义的 `optimizer`，返回包装后的优化器；
          如果在 JSON 配置中指定了优化器也会返回；否则为 `None`。

        * `training_dataloader`: 如果提供了 `training_data`，则返回 DeepSpeed 数据加载器；
          否则为 `None`。

        * `lr_scheduler`: 如果提供了用户定义的 `lr_scheduler`，或在 JSON 配置中指定了调度器，
          返回包装后的学习率调度器；否则为 `None`。
    """

</code></pre>
<p><code>deepspeed</code>具体案例可以查看其官方示例：<a href="https://github.com/microsoft/DeepSpeedExamples" target="_blank" rel="noopener nofollow">https://github.com/microsoft/DeepSpeedExamples</a><br>
具体使用也很简单,因为<code>Deepspeed</code>将各种功能都封装好了，可以直接使用，一个建议<code>Demo</code>如下：</p>
<pre><code># 首先初始化
model_engine, optimizer, train_loader, _ = deepspeed.initialize(
        model=model,
        model_parameters=model.parameters(),
        training_data=train_dataset,
        config=config['deepspeed_config'] # 这里的话是直接将deepspeed的设置都存储到一个json文件里面了
    )
def train(model_engine, optimizer, train_loader, ...):
    ...
    image = image.to(model_engine.local_rank)
    out = model_engine(..)
    ...
    model_engine.backward()
    model_engine.step()
    ...
</code></pre>
<p>值得注意的是：</p>
<ul>
<li>1、如果需要访问设备，可以直接用：<code>model_engine.local_ranl()</code>进行访问即可</li>
<li>2、如果再<code>deepspeed</code>参数（更加多的参数可以参考官方文档：<a href="https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training" target="_blank" rel="noopener nofollow">1</a>，<a href="https://deepspeed.readthedocs.io/en/latest/zero3.html#deepspeed.runtime.zero.config.DeepSpeedZeroConfig.contiguous_gradients" target="_blank" rel="noopener nofollow">2</a>）中设置了 <em>半精度</em> 训练，在数据里面要设定：<code>images.to(model.local_rank).half()</code></li>
</ul>
<hr>
<pre><code class="language-json">{
  "train_batch_size": 512,
  "gradient_accumulation_steps": 1,
  "fp16": {
    "enabled": true,
    "loss_scale": 0,
    "loss_scale_window": 1000,
    "hysteresis": 2,
    "min_loss_scale": 1
  }, //开启半精度训练
  "optimizer": {
    "type": "Adam",
    "params": {
      "lr": 0.001,
      "betas": [0.9, 0.999],
      "eps": 1e-8,
      "weight_decay": 0.01
    }
  }, // 设置优化器
  "zero_optimization": {
    "stage": 2
  } // 指定zero的方式：1，2，3
}
</code></pre>
<hr>
<ul>
<li>3、理论上分析，在显存占用上是 <span class="math inline">\(P_{OS}&lt;P_{OS+g}&lt;P_{OS+g+p}\)</span> 但是实验过程中会出现相反的情况，参考<a href="https://github.com/microsoft/DeepSpeed/issues/1302" target="_blank" rel="noopener nofollow">这部分讨论</a>：1、在使用<code>deepspeed</code>中的<code>zero</code>设定时，<strong>需要保证模型的大小足够大（大小&gt;1B的参数）</strong>。于此同时在使用<code>stage=2</code>或者<code>stage=3</code>的时候可以分别指定下面参数：1、<code>reduce_bucket_size</code>，<code>allgather_bucket_size</code>；2、<code>stage3_max_live_parameters</code>， <code>stage3_max_reuse_distance</code></li>
<li>4、对于<code>zero</code>中<code>stage</code>设定，通过结合github上的<a href="https://zhuanlan.zhihu.com/p/630734624" target="_blank" rel="noopener nofollow">讨论</a>：</li>
</ul>
<p>1、<code>stage=2</code>时：</p>
<pre><code class="language-json">{
"zero_optimization": {
    "stage": 2,
    "offload_optimizer": {
        "device": "cpu",
        "pin_memory": true
    },
    "allgather_partitions": true,
    "allgather_bucket_size": 2e8,
    "overlap_comm": true,
    "reduce_scatter": true,
    "reduce_bucket_size": 2e8,
    "contiguous_gradients": true
},
}
</code></pre>
<table>
<thead>
<tr>
<th style="text-align: center">参数</th>
<th style="text-align: center">含义</th>
<th style="text-align: center">当前值</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center"><code>stage</code></td>
<td style="text-align: center"><code>1</code>: 仅优化优化器状态。<code>2</code>: 优化优化器状态和梯度。<code>3</code>: 优化优化器状态、梯度和模型参数。<code>0</code>:普通DDP</td>
<td style="text-align: center"><code>2</code></td>
</tr>
<tr>
<td style="text-align: center"><code>offload_optimizer</code></td>
<td style="text-align: center">是否将优化器状态迁移到其他设备（如 CPU 或 NVMe）</td>
<td style="text-align: center"><code>{ "device": "cpu", "pin_memory": true }</code></td>
</tr>
<tr>
<td style="text-align: center"><code>allgather_partitions</code></td>
<td style="text-align: center">在每个step结束时，选择用allgather集合通信操作还是一系列的broadcast从所有GPUs收集更新后的参数，一般不需要修改，论文中在分析集合通讯开销时就用了allgather</td>
<td style="text-align: center"><code>true</code></td>
</tr>
<tr>
<td style="text-align: center"><code>allgather_bucket_size</code></td>
<td style="text-align: center">动态收集参数时的最大通信块大小（字节）。<strong>较大值</strong>：提高效率但增加显存压力。<strong>较小值</strong>：减少显存压力但增加通信次数。</td>
<td style="text-align: center"><code>2e8</code> (200MB)</td>
</tr>
<tr>
<td style="text-align: center"><code>overlap_comm</code></td>
<td style="text-align: center">尝试在反向传播期间并行进行梯度通信</td>
<td style="text-align: center"><code>true</code></td>
</tr>
<tr>
<td style="text-align: center"><code>reduce_scatter</code></td>
<td style="text-align: center">是否启用 reduce-scatter 操作，将梯度分片和通信合并以降低显存需求和通信负担</td>
<td style="text-align: center"><code>true</code></td>
</tr>
<tr>
<td style="text-align: center"><code>reduce_bucket_size</code></td>
<td style="text-align: center">reduce-scatter 操作的最大通信块大小（字节）。<strong>较大值</strong>：提高效率但增加显存压力。<strong>较小值</strong>：减少显存压力但增加通信次数</td>
<td style="text-align: center"><code>2e8</code> (200MB)</td>
</tr>
<tr>
<td style="text-align: center"><code>contiguous_gradients</code></td>
<td style="text-align: center">是否将梯度存储为连续内存块，以减少显存碎片并提升梯度更新效率</td>
<td style="text-align: center"><code>true</code></td>
</tr>
</tbody>
</table>
<p>2、<code>stage=3</code>时：</p>
<pre><code class="language-json">{
"zero_optimization": {
      "stage": 3,
      "offload_optimizer": {
          "device": "cpu",
          "pin_memory": true
      }, //是否将优化器状态迁移到CPU
      "offload_param": {
          "device": "cpu",
          "pin_memory": true
      },
      "overlap_comm": true,
      "contiguous_gradients": true,
      "sub_group_size": 1e9,
      "reduce_bucket_size": "auto",
      "stage3_prefetch_bucket_size": "auto",
      "stage3_param_persistence_threshold": "auto",
      "stage3_max_live_parameters": 1e9,
      "stage3_max_reuse_distance": 1e9,
      "stage3_gather_16bit_weights_on_model_save": true
  },
}
</code></pre>
<table>
<thead>
<tr>
<th style="text-align: center">参数</th>
<th style="text-align: center">含义</th>
<th style="text-align: left">当前值</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center"><code>stage</code></td>
<td style="text-align: center"><code>1</code>: 仅优化优化器状态。<code>2</code>: 优化优化器状态和梯度。<code>3</code>: 优化优化器状态、梯度和模型参数。<code>0</code>:普通DDP</td>
<td style="text-align: left"><code>3</code></td>
</tr>
<tr>
<td style="text-align: center"><code>offload_optimizer</code></td>
<td style="text-align: center">是否将<strong>优化器状态</strong>迁移到其他设备（如 CPU 或 NVMe）。优化器状态的存储可以迁移到 CPU 以释放显存。</td>
<td style="text-align: left"><code>{ "device": "cpu", "pin_memory": true }</code></td>
</tr>
<tr>
<td style="text-align: center"><code>offload_param</code></td>
<td style="text-align: center">是否将<strong>模型参数</strong>迁移到其他设备（如 CPU）。类似于优化器状态，模型参数可以迁移到 CPU 以降低显存压力。</td>
<td style="text-align: left"><code>{ "device": "cpu", "pin_memory": true }</code></td>
</tr>
<tr>
<td style="text-align: center"><code>overlap_comm</code></td>
<td style="text-align: center">尝试在反向传播期间并行进行梯度通信</td>
<td style="text-align: left"><code>true</code></td>
</tr>
<tr>
<td style="text-align: center"><code>contiguous_gradients</code></td>
<td style="text-align: center">是否将梯度存储为连续的内存块，<strong>启用后减少显存碎片，提高梯度更新效率</strong>。</td>
<td style="text-align: left"><code>true</code></td>
</tr>
<tr>
<td style="text-align: center"><code>sub_group_size</code></td>
<td style="text-align: center">设置参数分组大小，用于分配和通信的优化。<strong>大的值可以减少通信次数，适用于更大规模的模型</strong></td>
<td style="text-align: left"><code>1e9</code></td>
</tr>
<tr>
<td style="text-align: center"><code>reduce_bucket_size</code></td>
<td style="text-align: center">设置 reduce-scatter 操作的最大通信块大小（字节）。如果设置为 <code>auto</code>，DeepSpeed 会自动调整。</td>
<td style="text-align: left"><code>auto</code></td>
</tr>
<tr>
<td style="text-align: center"><code>stage3_prefetch_bucket_size</code></td>
<td style="text-align: center">为 stage 3 优化中的预取操作设置桶大小。如果设置为 <code>auto</code>，DeepSpeed 会自动调整。</td>
<td style="text-align: left"><code>auto</code></td>
</tr>
<tr>
<td style="text-align: center"><code>stage3_param_persistence_threshold</code></td>
<td style="text-align: center">在 stage 3 中设置模型参数持久化的阈值。如果设置为 <code>auto</code>，DeepSpeed 会自动调整。</td>
<td style="text-align: left"><code>auto</code></td>
</tr>
<tr>
<td style="text-align: center"><code>stage3_max_live_parameters</code></td>
<td style="text-align: center">保留在 GPU 上的完整参数数量的上限</td>
<td style="text-align: left"><code>1e9</code></td>
</tr>
<tr>
<td style="text-align: center"><code>stage3_max_reuse_distance</code></td>
<td style="text-align: center">是指将来何时再次使用参数的指标，从而决定是丢弃参数还是保留参数。 如果一个参数在不久的将来要再次使用（小于 <code>stage3_max_reuse_distance</code>），可以保留以减少通信开销。 使用<code>activation checkpointing</code>时，这一点非常有用</td>
<td style="text-align: left"><code>1e9</code></td>
</tr>
<tr>
<td style="text-align: center"><code>stage3_gather_16bit_weights_on_model_save</code></td>
<td style="text-align: center">在保存模型时是否收集 16 位权重。启用时可以将权重收集为 16 位格式，降低存储开销。</td>
<td style="text-align: left"><code>true</code></td>
</tr>
</tbody>
</table>
<p>3、其他<br>
实际参数过程中，可能还需要设置<code>train_batch_size</code>，<code>gradient_accumulation_steps</code>（梯度累计次数），<code>optimizer</code>（优化器选择）</p>
<h1 id="代码">代码</h1>
<p><a href="https://gitee.com/a-ha-a/deep-learning-note/tree/master/DeepLearning-Summary/Computer-Vision/deepspeed" target="_blank" rel="noopener nofollow">https://gitee.com/a-ha-a/deep-learning-note/tree/master/DeepLearning-Summary/Computer-Vision/deepspeed</a></p>
<h1 id="参考">参考</h1>
<p>1、<a href="https://arxiv.org/pdf/1910.02054" target="_blank" rel="noopener nofollow">https://arxiv.org/pdf/1910.02054</a><br>
2、<a href="https://zhuanlan.zhihu.com/p/513571706" target="_blank" rel="noopener nofollow">https://zhuanlan.zhihu.com/p/513571706</a><br>
3、<a href="https://zhuanlan.zhihu.com/p/618865052" target="_blank" rel="noopener nofollow">https://zhuanlan.zhihu.com/p/618865052</a><br>
4、<a href="https://zhuanlan.zhihu.com/p/504957661" target="_blank" rel="noopener nofollow">https://zhuanlan.zhihu.com/p/504957661</a><br>
5、<a href="https://deepspeed.readthedocs.io/en/latest/initialize.html#" target="_blank" rel="noopener nofollow">https://deepspeed.readthedocs.io/en/latest/initialize.html#</a><br>
6、<a href="https://www.deepspeed.ai/docs/config-json/#batch-size-related-parameters" target="_blank" rel="noopener nofollow">https://www.deepspeed.ai/docs/config-json/#batch-size-related-parameters</a><br>
7、<a href="https://zhuanlan.zhihu.com/p/630734624" target="_blank" rel="noopener nofollow">https://zhuanlan.zhihu.com/p/630734624</a></p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.6061410823541666" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-01-13 20:08">2025-01-13 20:08</span>&nbsp;
<a href="https://www.cnblogs.com/Big-Yellow">Big-Yellow-J</a>&nbsp;
阅读(<span id="post_view_count">29</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18666364" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18666364);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18666364', targetLink: 'https://www.cnblogs.com/Big-Yellow/p/18666364', title: '深度学习基础理论————DeepSpeed' })">举报</a>
</div>
        