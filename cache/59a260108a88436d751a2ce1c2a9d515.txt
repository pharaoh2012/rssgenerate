
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/12lisu/p/18780370" title="发布于 2025-03-19 09:31">
    <span role="heading" aria-level="2">系统高可用的 10 条军规</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h2 id="前言">前言</h2>
<p>系统高可用是非常经典的问题，无论在面试，还是实际工作中，都经常会遇到。</p>
<p>这篇文章跟大家一起聊聊，保证系统高可用的10个小技巧，希望对你会有所帮助。</p>
<p><img src="https://files.mdnice.com/user/5303/9dede5a1-2f09-42e5-94f7-d886dc47d4ec.png" alt="" loading="lazy"></p>
<h2 id="1-冗余部署">1 冗余部署</h2>
<p><strong>场景</strong>：某电商大促期间，数据库主节点突然宕机，导致全站交易瘫痪。</p>
<p><strong>问题</strong>：单节点部署的系统，一旦关键组件（如数据库、消息队列）故障，业务直接归零。</p>
<p><strong>解决方案</strong>：通过主从复制、集群化部署实现冗余。例如MySQL主从同步，Redis Sentinel哨兵机制。</p>
<p><img src="https://files.mdnice.com/user/5303/aa1493fd-ffb1-4693-8b0b-c8c1eaf6762f.png" alt="" loading="lazy"></p>
<p>MySQL主从配置如下：</p>
<pre><code class="language-sql">-- 主库配置
CHANGE MASTER TO 
MASTER_HOST='master_host',
MASTER_USER='replica_user',
MASTER_PASSWORD='password',
MASTER_LOG_FILE='mysql-bin.000001',
MASTER_LOG_POS=154;

-- 从库启动复制
START SLAVE;
</code></pre>
<p><strong>效果</strong>：主库宕机时，从库自动切换为可读写状态，业务无感知。</p>
<h2 id="2-服务熔断">2 服务熔断</h2>
<p><strong>场景</strong>：支付服务响应延迟，导致订单服务线程池耗尽，引发连锁故障。</p>
<p><strong>问题</strong>：服务依赖链中某个环节异常，会像多米诺骨牌一样拖垮整个系统。</p>
<p><strong>解决方案</strong>：引入熔断器模式，例如Hystrix或Resilience4j。</p>
<p>Resilience4j熔断配置如下：</p>
<pre><code class="language-java">CircuitBreakerConfig config = CircuitBreakerConfig.custom()
    .failureRateThreshold(50)  // 失败率超过50%触发熔断
    .waitDurationInOpenState(Duration.ofMillis(1000))
    .build();
CircuitBreaker circuitBreaker = CircuitBreaker.of("paymentService", config);

// 调用支付服务
Supplier&lt;String&gt; supplier = () -&gt; paymentService.call();
Supplier&lt;String&gt; decoratedSupplier = CircuitBreaker
    .decorateSupplier(circuitBreaker, supplier);
</code></pre>
<p><strong>效果</strong>：当支付服务失败率飙升时，自动熔断并返回降级结果（如“系统繁忙，稍后重试”）。</p>
<h2 id="3-流量削峰">3 流量削峰</h2>
<p><strong>场景</strong>：秒杀活动开始瞬间，10万QPS直接击穿数据库连接池。</p>
<p><strong>问题</strong>：突发流量超过系统处理能力，导致资源耗尽。</p>
<p><strong>解决方案</strong>：引入消息队列（如Kafka、RocketMQ）做异步缓冲。</p>
<p>用户下单的系统流程图如下：</p>
<p><img src="https://files.mdnice.com/user/5303/26908169-30c0-406d-b78c-e2d6575cfeab.png" alt="" loading="lazy"></p>
<p>RocketMQ生产者的示例代码：</p>
<pre><code class="language-java">DefaultMQProducer producer = new DefaultMQProducer("seckill_producer");
producer.setNamesrvAddr("127.0.0.1:9876");
producer.start();
Message msg = new Message("seckill_topic", "订单数据".getBytes());
producer.send(msg);
</code></pre>
<p><strong>效果</strong>：将瞬时10万QPS的请求平滑处理为数据库可承受的2000 TPS。</p>
<h2 id="4-动态扩容">4 动态扩容</h2>
<p><strong>场景</strong>：日常流量100台服务器足够，但大促时需要快速扩容到500台。</p>
<p><strong>问题</strong>：固定资源无法应对业务波动，手动扩容效率低下。</p>
<p><strong>解决方案</strong>：基于Kubernetes的HPA（Horizontal Pod Autoscaler）。</p>
<p>K8s HPA 的配置如下：</p>
<pre><code class="language-yml">apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: order-service-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: order-service
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 60
</code></pre>
<p><strong>效果</strong>：CPU利用率超过60%时自动扩容，低于30%时自动缩容。</p>
<h2 id="5-灰度发布">5 灰度发布</h2>
<p><strong>场景</strong>：新版本代码存在内存泄漏，全量发布导致线上服务崩溃。</p>
<p><strong>问题</strong>：一次性全量发布风险极高，可能引发全局故障。</p>
<p><strong>解决方案</strong>：基于流量比例的灰度发布策略。</p>
<p>Istio流量染色配置如下：</p>
<pre><code class="language-yml">apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: bookinfo
spec:
  hosts:
  - bookinfo.com
  http:
  - route:
    - destination:
        host: reviews
        subset: v1
      weight: 90  # 90%流量走老版本
    - destination:
        host: reviews
        subset: v2
      weight: 10  # 10%流量走新版本
</code></pre>
<p><strong>效果</strong>：新版本异常时，仅影响10%的用户，快速回滚无压力。</p>
<h2 id="6-降级开关">6 降级开关</h2>
<p><strong>场景</strong>：推荐服务超时导致商品详情页加载时间从200ms飙升到5秒。</p>
<p><strong>问题</strong>：非核心功能异常影响核心链路用户体验。</p>
<p><strong>解决方案</strong>：配置中心增加降级开关，如果遇到紧急情况，能   动态降级非关键服务。</p>
<p>Apollo配置中心的示例代码如下：</p>
<pre><code class="language-java">@ApolloConfig
private Config config;

public ProductDetail getDetail(String productId) {
    if(config.getBooleanProperty("recommend.switch", true)) {
        // 调用推荐服务
    }
    // 返回基础商品信息
}
</code></pre>
<p><strong>效果</strong>：关闭推荐服务后，详情页响应时间恢复至200ms以内。</p>
<h2 id="7-全链路压测">7 全链路压测</h2>
<p><strong>场景</strong>：某金融系统在真实流量下暴露出数据库死锁问题。</p>
<p><strong>问题</strong>：测试环境无法模拟真实流量特征，线上隐患难以发现。</p>
<p><strong>解决方案</strong>：基于流量录制的全链路压测。</p>
<p><strong>实施步骤</strong>：</p>
<ol>
<li>线上流量录制（如Jmeter+TCPCopy）</li>
<li>影子库隔离（压测数据写入隔离存储）</li>
<li>压测数据脱敏</li>
<li>执行压测并监控系统瓶颈</li>
</ol>
<p><strong>效果</strong>：提前发现数据库连接池不足、缓存穿透等问题。</p>
<h2 id="8-数据分片">8 数据分片</h2>
<p><strong>场景</strong>：用户表达到10亿行，查询性能断崖式下降。</p>
<p><strong>问题</strong>：单库单表成为性能瓶颈。</p>
<p><strong>解决方案</strong>：基于ShardingSphere的分库分表。</p>
<p>分库分表的配置如下：</p>
<pre><code class="language-yml">sharding:
  tables:
    user:
      actualDataNodes: ds_${0..1}.user_${0..15}
      tableStrategy:
        standard:
          shardingColumn: user_id
          preciseAlgorithmClassName: HashModShardingAlgorithm
          preciseAlgorithmType: HASH_MOD
          shardingCount: 16
</code></pre>
<p><strong>效果</strong>：10亿数据分散到16个物理表，查询性能提升20倍。</p>
<h2 id="9-混沌工程">9 混沌工程</h2>
<p><strong>场景</strong>：某次机房网络抖动导致服务不可用3小时。</p>
<p><strong>问题</strong>：系统健壮性不足，故障恢复能力弱。</p>
<p><strong>解决方案</strong>：使用ChaosBlade模拟故障。</p>
<p><strong>示例命令</strong>：</p>
<pre><code class="language-sql"># 模拟网络延迟
blade create network delay --time 3000 --interface eth0

# 模拟数据库节点宕机
blade create docker kill --container-id mysql-node-1
</code></pre>
<p><strong>效果</strong>：提前发现缓存穿透导致DB负载过高的问题，优化缓存击穿防护策略。</p>
<h2 id="10-立体化监控">10 立体化监控</h2>
<p><strong>场景</strong>：磁盘IOPS突增导致订单超时，但运维人员2小时后才发现。</p>
<p><strong>问题</strong>：监控维度单一，无法快速定位根因。</p>
<p><strong>解决方案</strong>：构建Metrics-Log-Trace三位一体监控体系。</p>
<p><strong>技术栈组合</strong>：</p>
<ul>
<li>Metrics：Prometheus + Grafana（资源指标）</li>
<li>Log：ELK（日志分析）</li>
<li>Trace：SkyWalking（调用链追踪）</li>
</ul>
<p>定位问题流程如下 ：</p>
<pre><code class="language-plain">CPU利用率 &gt; 80% → 关联日志检索 → 定位到GC频繁 → 
追踪调用链 → 发现某个DAO层SQL未走索引
</code></pre>
<p><strong>效果</strong>：故障定位时间从小时级缩短到分钟级。</p>
<h2 id="总结">总结</h2>
<p>系统高可用建设就像打造一艘远洋巨轮。</p>
<p>冗余部署是双发动机，熔断降级是救生艇，监控体系是雷达系统。</p>
<p>但真正的关键在于：</p>
<ol>
<li><strong>故障预防</strong>比故障处理更重要（如混沌工程）</li>
<li><strong>自动化</strong>是应对复杂性的唯一出路（如K8s弹性扩缩）</li>
<li><strong>数据驱动</strong>的优化才是王道（全链路压测+立体监控）</li>
</ol>
<p>没有100%可用的系统，但通过这10个实战技巧，我们可以让系统的可用性从99%提升到99.99%。</p>
<p>这0.99%的提升，可能意味着每年减少8小时的故障时间——而这，正是架构师价值的体现。</p>
<h2 id="最后说一句求关注别白嫖我">最后说一句(求关注，别白嫖我)</h2>
<p>如果这篇文章对您有所帮助，或者有所启发的话，帮忙关注一下我的同名公众号：苏三说技术，我的所有文章都会在公众号上首发，您的支持是我坚持写作最大的动力。</p>
<p>求一键三连：点赞、转发、在看。</p>
<p>关注公众号：【苏三说技术】，在公众号中回复：进大厂，可以免费获取我最近整理的10万字的面试宝典，好多小伙伴靠这个宝典拿到了多家大厂的offer。</p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.3662438732025463" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-03-19 09:31">2025-03-19 09:31</span>&nbsp;
<a href="https://www.cnblogs.com/12lisu">苏三说技术</a>&nbsp;
阅读(<span id="post_view_count">263</span>)&nbsp;
评论(<span id="post_comment_count">1</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18780370" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18780370);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18780370', targetLink: 'https://www.cnblogs.com/12lisu/p/18780370', title: '系统高可用的 10 条军规' })">举报</a>
</div>
        