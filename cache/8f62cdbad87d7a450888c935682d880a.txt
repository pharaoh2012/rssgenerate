
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/dechinphy/p/18748323/cython-cuda-batchgather" title="发布于 2025-03-03 15:54">
    <span role="heading" aria-level="2">Cython与CUDA之BatchGather</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                    <div id="cnblogs_post_description" style="display: none">
        <img src="https://img2024.cnblogs.com/blog/2277440/202503/2277440-20250303155233907-974941602.png" alt="Cython与CUDA之BatchGather" class="desc_img">
        以学习CUDA为目的，接上一篇关于Cython与CUDA架构下的Gather算子实现，这里我们加一个Batch的维度，做一个BatchGather的简单实现。
    </div>
<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h1 id="技术背景">技术背景</h1>
<p>在前面一篇文章中，我们介绍过<a href="https://www.cnblogs.com/dechinphy/p/18740207/cycuda-gather" target="_blank">Cython+CUDA框架下实现一个简单的Gather算子的方法</a>。这里演示Gather算子的升级版本实现——BatchGather算子。不过这里只是加了一个Batch维度，并没有添加其他的维度，例如Dimension维度，在这里暂不考虑。</p>
<h1 id="cuda头文件">CUDA头文件</h1>
<p>这里我们保留了原本的Gather部分，只添加一个BatchGather的运算，以下为<code>cuda_index.cuh</code>的内容：</p>
<pre><code class="language-c">#include &lt;stdio.h&gt;

extern "C" float Gather(float *source, int *index, float *res, int N, int M);
extern "C" float BatchGather(float *source, int *index, float *res, int N, int M, int B);
</code></pre>
<p>BatchGather只是在Gather的基础上加了一个B的维度。除了CUDA算子本身的头文件之外，这里我们还使用到了异常捕获头文件<code>error.cuh</code>：</p>
<pre><code class="language-c">#pragma once
#include &lt;stdio.h&gt;

#define CHECK(call) do{const cudaError_t error_code = call; if (error_code != cudaSuccess){printf("CUDA Error:\n"); printf("    File:   %s\n", __FILE__); printf("    Line:   %d\n", __LINE__); printf("    Error code: %d\n", error_code); printf("    Error text: %s\n", cudaGetErrorString(error_code)); exit(1);}} while (0)
</code></pre>
<p>其中的宏可用于检测CUDA函数所抛出的异常。另外还有一个用于统计CUDA函数运行时长的头文件：</p>
<pre><code class="language-c">#pragma once
#include &lt;stdio.h&gt;
#include &lt;cuda_runtime.h&gt;

// 宏定义，用于测量CUDA函数的执行时间
#define TIME_CUDA_FUNCTION(func) \
    do { \
        cudaEvent_t start, stop; \
        float elapsedTime; \
        cudaEventCreate(&amp;start); \
        cudaEventCreate(&amp;stop); \
        cudaEventRecord(start, NULL); \
        \
        func; \
        \
        cudaEventRecord(stop, NULL); \
        cudaEventSynchronize(stop); \
        cudaEventElapsedTime(&amp;elapsedTime, start, stop); \
        printf("Time taken by function %s is: %f ms\n", #func, elapsedTime); \
        \
        cudaEventDestroy(start); \
        cudaEventDestroy(stop); \
    } while (0)

// 宏定义，用于测量CUDA函数的执行时间并返回该时间
#define GET_CUDA_TIME(func) \
    ({ \
        cudaEvent_t start, stop; \
        float elapsedTime = 0.0f; \
        cudaEventCreate(&amp;start); \
        cudaEventCreate(&amp;stop); \
        cudaEventRecord(start, NULL); \
        \
        func; \
        \
        cudaEventRecord(stop, NULL); \
        cudaEventSynchronize(stop); \
        cudaEventElapsedTime(&amp;elapsedTime, start, stop); \
        \
        cudaEventDestroy(start); \
        cudaEventDestroy(stop); \
        \
        elapsedTime; \
    })
</code></pre>
<p>可选择直接打印时长，也可以选择返回时长的float值。</p>
<h1 id="cuda文件">CUDA文件</h1>
<p>接下来就是正式的CUDA函数内容<code>cuda_index.cu</code>：</p>
<pre><code class="language-c">// nvcc -shared ./cuda_index.cu -Xcompiler -fPIC -o ./libcuindex.so
#include &lt;stdio.h&gt;
#include "cuda_index.cuh"
#include "error.cuh"
#include "record.cuh"

__global__ void GatherKernel(float *source, int *index, float *res, int N){
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx &lt; N){
        res[idx] = source[index[idx]];
    }
}

extern "C" float Gather(float *source, int *index, float *res, int N, int M){
    float *souce_device, *res_device;
    int *index_device;
    CHECK(cudaMalloc((void **)&amp;souce_device, M * sizeof(float)));
    CHECK(cudaMalloc((void **)&amp;res_device, N * sizeof(float)));
    CHECK(cudaMalloc((void **)&amp;index_device, N * sizeof(int)));
    CHECK(cudaMemcpy(souce_device, source, M * sizeof(float), cudaMemcpyHostToDevice));
    CHECK(cudaMemcpy(res_device, res, N * sizeof(float), cudaMemcpyHostToDevice));
    CHECK(cudaMemcpy(index_device, index, N * sizeof(int), cudaMemcpyHostToDevice));
    int block_size = 1024;
    int grid_size = (N + block_size - 1) / block_size;
    float timeTaken = GET_CUDA_TIME((GatherKernel&lt;&lt;&lt;grid_size, block_size&gt;&gt;&gt;(souce_device, index_device, res_device, N)));
    CHECK(cudaGetLastError());
    CHECK(cudaDeviceSynchronize());
    CHECK(cudaMemcpy(res, res_device, N * sizeof(float), cudaMemcpyDeviceToHost));
    CHECK(cudaFree(souce_device));
    CHECK(cudaFree(index_device));
    CHECK(cudaDeviceSynchronize());
    CHECK(cudaFree(res_device));
    CHECK(cudaDeviceReset());
    return timeTaken;
}

__global__ void BatchGatherKernel(float *source, int *index, float *res, int N, int M, int B){
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx &lt; N*B){
        int batch_idx = idx / N;
        int source_idx = batch_idx * M + index[idx];
        res[idx] = source[source_idx];
    }
}

extern "C" float BatchGather(float *source, int *index, float *res, int N, int M, int B){
    float *souce_device, *res_device;
    int *index_device;
    CHECK(cudaMalloc((void **)&amp;souce_device, B * M * sizeof(float)));
    CHECK(cudaMalloc((void **)&amp;res_device, B * N * sizeof(float)));
    CHECK(cudaMalloc((void **)&amp;index_device, B * N * sizeof(int)));
    CHECK(cudaMemcpy(souce_device, source, B * M * sizeof(float), cudaMemcpyHostToDevice));
    CHECK(cudaMemcpy(res_device, res, B * N * sizeof(float), cudaMemcpyHostToDevice));
    CHECK(cudaMemcpy(index_device, index, B * N * sizeof(int), cudaMemcpyHostToDevice));
    int block_size = 1024;
    int grid_size = (B * N + block_size - 1) / block_size;
    float timeTaken = GET_CUDA_TIME((BatchGatherKernel&lt;&lt;&lt;grid_size, block_size&gt;&gt;&gt;(souce_device, index_device, res_device, N, M, B)));
    CHECK(cudaGetLastError());
    CHECK(cudaDeviceSynchronize());
    CHECK(cudaMemcpy(res, res_device, B * N * sizeof(float), cudaMemcpyDeviceToHost));
    CHECK(cudaFree(souce_device));
    CHECK(cudaFree(index_device));
    CHECK(cudaDeviceSynchronize());
    CHECK(cudaFree(res_device));
    CHECK(cudaDeviceReset());
    return timeTaken;
}
</code></pre>
<p>这里传入到CUDA之前，我们需要在Cython或者Python中把相关的数据压缩为一维，所以传入CUDA函数的是一个一维的指针。相比于单一的Gather操作，BatchGather中的几个输入含义有所变化，例如<code>N</code>表示的是单Batch的Index长度，<code>M</code>表示的是单Batch的源数组长度。</p>
<h1 id="cython文件">Cython文件</h1>
<p>对于一个新的Batch函数来说，我们需要构建一个新的Cython调用函数<code>wrapper.pyx</code>：</p>
<pre><code class="language-python"># cythonize -i -f wrapper.pyx

import numpy as np
cimport numpy as np
cimport cython

cdef extern from "&lt;dlfcn.h&gt;" nogil:
    void *dlopen(const char *, int)
    char *dlerror()
    void *dlsym(void *, const char *)
    int dlclose(void *)
    enum:
        RTLD_LAZY

ctypedef float (*GatherFunc)(float *source, int *index, float *res, int N, int M) noexcept nogil
ctypedef float (*BatchGatherFunc)(float *source, int *index, float *res, int N, int M, int B) noexcept nogil

cdef void* handle = dlopen('/path/to/libcuindex.so', RTLD_LAZY)

@cython.boundscheck(False)
@cython.wraparound(False)
cpdef float[:] cuda_gather(float[:] x, int[:] idx):
    cdef:
        GatherFunc Gather
        float timeTaken
        int N = idx.shape[0]
        int M = x.shape[0]
        float[:] res = np.zeros((N, ), dtype=np.float32)
    Gather = &lt;GatherFunc&gt;dlsym(handle, "Gather")
    timeTaken = Gather(&amp;x[0], &amp;idx[0], &amp;res[0], N, M)
    print (timeTaken)
    return res

@cython.boundscheck(False)
@cython.wraparound(False)
cpdef float[:] batch_cuda_gather(float[:] x, int[:] idx, int B):
    cdef:
        BatchGatherFunc BatchGather
        float timeTaken
        int N = idx.shape[0] // B
        int M = x.shape[0] // B
        float[:] res = np.zeros((B*N, ), dtype=np.float32)
    BatchGather = &lt;BatchGatherFunc&gt;dlsym(handle, "BatchGather")
    timeTaken = BatchGather(&amp;x[0], &amp;idx[0], &amp;res[0], N, M, B)
    print (timeTaken)
    return res

while not True:
    dlclose(handle)
</code></pre>
<p>这里我们还是接受一维的数组，多引入一个Batch维度的参数<code>B</code>，其他的都是一样的。</p>
<h1 id="python调用文件">Python调用文件</h1>
<p>最后是用来调用的最上层Python端的代码<code>test_gather.py</code>：</p>
<pre><code class="language-python">import numpy as np
np.random.seed(0)
from wrapper import batch_cuda_gather

B = 2
M = 1024 * 1024 * 128
N = 1024 * 1024

x = np.random.random((M*B,)).astype(np.float32)
idx = np.random.randint(0, M, (N*B,)).astype(np.int32)

np_res = np.zeros((B, N), dtype=np.float32)
for i in range(B):
    np_res[i] = x.reshape((B,-1))[i][idx.reshape((B, -1))[i]]
np_res = np_res.reshape(-1)

res = np.asarray(batch_cuda_gather(x, idx, B))
print (res.shape)
print ((res==np_res).sum())
</code></pre>
<p>为了方便处理，在构建数据的时候，我们直接在生成数据阶段就生成一维的数据，然后直接调用Cython函数进行CUDA相关运算。</p>
<h1 id="运行方法">运行方法</h1>
<p>首先将CUDA文件编译成动态链接库，使其可以在Cython中被调用。然后将Cython文件编译成动态链接库，使其可以在Python中被调用。最后运行Python代码即可：</p>
<pre><code class="language-bash">$ nvcc -shared ./cuda_index.cu -Xcompiler -fPIC -o ./libcuindex.so
$ cythonize -i -f wrapper.pyx
$ python3 test_gather.py
</code></pre>
<p>运行结果如下：</p>
<pre><code class="language-bash">0.9606080055236816
(2097152,)
2097152
</code></pre>
<p>这表示CUDA核函数部分的运行时长为0.96ms，输入的数组总长度为2097152，跟numpy版本的数组索引实现对比之后，得到2097152个相同的元素。也就是说，计算结果跟numpy的计算结果是一致的，以此来校验CUDA部分的运算结果。</p>
<h1 id="总结概要">总结概要</h1>
<p>以学习CUDA为目的，接上一篇关于Cython与CUDA架构下的Gather算子实现，这里我们加一个Batch的维度，做一个BatchGather的简单实现。</p>
<h1 id="版权声明">版权声明</h1>
<p>本文首发链接为：<a href="https://www.cnblogs.com/dechinphy/p/cython-cuda-batchgather.html" target="_blank">https://www.cnblogs.com/dechinphy/p/cython-cuda-batchgather.html</a></p>
<p>作者ID：DechinPhy</p>
<p>更多原著文章：<a href="https://www.cnblogs.com/dechinphy/" target="_blank">https://www.cnblogs.com/dechinphy/</a></p>
<p>请博主喝咖啡：<a href="https://www.cnblogs.com/dechinphy/gallery/image/379634.html" target="_blank">https://www.cnblogs.com/dechinphy/gallery/image/379634.html</a></p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.7197526182604167" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-03-03 16:11">2025-03-03 15:54</span>&nbsp;
<a href="https://www.cnblogs.com/dechinphy">DECHIN</a>&nbsp;
阅读(<span id="post_view_count">60</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18748323" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18748323);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18748323', targetLink: 'https://www.cnblogs.com/dechinphy/p/18748323/cython-cuda-batchgather', title: 'Cython与CUDA之BatchGather' })">举报</a>
</div>
        