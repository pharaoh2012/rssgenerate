
		<h1 class="postTitle">
			<a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/aser1989/p/18697164" title="发布于 2025-02-02 23:37">
    <span role="heading" aria-level="2">本地部署DeepSeek</span>
    

</a>

		</h1>
		<div class="clear"></div>
		<div class="postBody">
			<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p>没想到新年最热闹的地方之一会是互联网，刷爆朋友圈的除了新年祝福还有DeepSeek。揣着一颗好奇心试了试，竟有一种发现新大路的感觉。估计是围观的人太多，在线的版本有时候会出现连不上的情况，好奇心驱使之下想尝试本地部署。</p>
<h2 id="方案">方案</h2>
<p>本地化方案非常简单：Ollama + DeepSeek-R1 + Enchanted LLM 。</p>
<h3 id="ollama"><a href="https://ollama.com/" target="_blank" rel="noopener nofollow">Ollama</a></h3>
<p><a href="https://ollama.com/" target="_blank" rel="noopener nofollow">Ollama</a>  是一个用于在本地运行大型语言模型（LLMs）的工具，支持多种开源模型，如 Llama 2、Code Llama、Mistral 等。它简化了模型下载、配置和运行的过程，用户可以通过命令行轻松管理模型。使用起来颇有docker的感觉，pull、run、ps...。安装也简单，选择对于系统，下载安装就妥了。<br>
<img src="https://img2024.cnblogs.com/blog/362034/202502/362034-20250202215531156-1690371599.png" alt="https://ollama.com/" loading="lazy"></p>
<h3 id="deepseek-r1">DeepSeek-R1</h3>
<p>在Ollama的网站上（<a href="https://ollama.com/" target="_blank" rel="noopener nofollow">https://ollama.com/</a>）Models页面当前排名第一的就是DeepSeek-R1，点进去选择对应的版本，右边会展示对应的ollama 命令，复制命令粘贴到命令行工具中执行即可。如图：<br>
<img src="https://img2024.cnblogs.com/blog/362034/202502/362034-20250202220326890-1536306454.png" alt="image" loading="lazy"><br>
这种高端操作一帆风顺貌似不太合理，所以我也遇到点问题。不同的版本大小差异会比较大，第一个上手的是<mark>70b</mark>这个版本，总大小43GB。en～下了两个小时左右，终于好了，正准备见证奇迹的时候，ollama抛出这样一个提示：<code>ollama Error: Post "http://127.0.0.1:11434/api/show": read tcp 127.0.0.1:57953-&gt;127.0.0.1:11434: read: connection reset by peer</code> 。后来问了DeepSeek，它思考了半天，它告诉我<code>The server is busy. Please try again later.</code> 最后它让我检查服务状态、端口占用、防火墙、Ollama版本......一顿操作之后突然想起了ollama ps这个命令，一看傻眼了，啥都没有。突然明白是模型没运行起来，估计是电脑配置不够，<mark>70b</mark>这个版本高攀不起。于是开始尝试<mark>1.5b</mark>这个版本，一切顺利，可以通过控制台进行对话了。让它用React实现一个tab组件，好家伙给了我十几个方案，果断换下一个版本。于是选择了<mark>32b</mark>这个版本，20GB竟然成功跑起来了。</p>
<h3 id="enchanted">Enchanted</h3>
<p>控制台对话终究没有UI方便，在<a href="https://github.com/ollama/ollama" target="_blank" rel="noopener nofollow">Ollama README</a>上列举了大量UI工具，如Open WebUI、Enchanted、Hollama、Lollms-Webui.... 因为本地是MacOs，这里选择的是<a href="https://github.com/gluonfield/enchanted" target="_blank" rel="noopener nofollow">Enchanted</a>。<a href="https://github.com/gluonfield/enchanted" target="_blank" rel="noopener nofollow">Enchanted</a>是MacOs下的一个App，也支持IOS和Vision Pro，在AppStore上下载安装即可。<a href="https://github.com/gluonfield/enchanted" target="_blank" rel="noopener nofollow">Enchanted</a>配置非常简单，填写本地服务地址（Ollama本地服务是： localhost:11434）选择对应模型版本即可。至此本地化就完成了。最后附上纪念照：</p>
<p><img src="https://img2024.cnblogs.com/blog/362034/202502/362034-20250202232535530-1422659262.png" alt="example" loading="lazy"></p>

</div>
<div class="clear"></div>

		</div>
		<div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="1.1864213427037038" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-02-02 23:47">2025-02-02 23:37</span>&nbsp;
<a href="https://www.cnblogs.com/aser1989">ASER_1989</a>&nbsp;
阅读(<span id="post_view_count">922</span>)&nbsp;
评论(<span id="post_comment_count">5</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18697164" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18697164);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18697164', targetLink: 'https://www.cnblogs.com/aser1989/p/18697164', title: '本地部署DeepSeek' })">举报</a>
</div>
	