
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/carpell/p/18909515" title="å‘å¸ƒäº 2025-06-04 09:27">
    <span role="heading" aria-level="2">ã€è¯­ä¹‰åˆ†å‰²ä¸“æ ã€‘2ï¼šU-netå®æˆ˜ç¯‡(é™„ä¸Šå®Œæ•´å¯è¿è¡Œçš„ä»£ç pytorch)</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                    <div id="cnblogs_post_description" style="display: none">
        
        æœ¬æ–‡è¯¦ç»†ä»‹ç»äº†å¦‚ä½•æ‰‹åŠ¨å¤ç°U-netï¼ˆå…¨å·ç§¯ç½‘ç»œï¼‰è¯­ä¹‰åˆ†å‰²æ¨¡å‹ï¼Œå¹¶æ·±å…¥è®²è§£äº†ä»£ç å®ç°ã€‚æ–‡ç« æŒ‰ç…§æ¨¡å‹æ„å»ºã€æ•°æ®é›†å¤„ç†ã€è¯„ä»·æŒ‡æ ‡è®¾å®šã€è®­ç»ƒæµç¨‹å’Œæµ‹è¯•äº”ä¸ªéƒ¨åˆ†è¿›è¡Œè®²è§£ã€‚
    </div>
<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p></p><div class="toc"><div class="toc-container-header">ç›®å½•</div><ul><li><a href="#å‰è¨€" rel="noopener nofollow">å‰è¨€</a></li><li><a href="#u-netå…¨æµç¨‹ä»£ç " rel="noopener nofollow">U-netå…¨æµç¨‹ä»£ç </a><ul><li><a href="#æ¨¡å‹æ­å»ºmodel" rel="noopener nofollow">æ¨¡å‹æ­å»º(model)</a></li><li><a href="#æ•°æ®å¤„ç†dataloader" rel="noopener nofollow">æ•°æ®å¤„ç†(dataloader)</a></li><li><a href="#è¯„ä»·æŒ‡æ ‡metric" rel="noopener nofollow">è¯„ä»·æŒ‡æ ‡(metric)</a></li><li><a href="#è®­ç»ƒæµç¨‹train" rel="noopener nofollow">è®­ç»ƒæµç¨‹(train)</a></li><li><a href="#æ¨¡å‹æµ‹è¯•test" rel="noopener nofollow">æ¨¡å‹æµ‹è¯•(test)</a></li></ul></li><li><a href="#æ•ˆæœå›¾" rel="noopener nofollow">æ•ˆæœå›¾</a></li><li><a href="#ç»“è¯­" rel="noopener nofollow">ç»“è¯­</a></li></ul></div><p></p>
<h1 id="å‰è¨€">å‰è¨€</h1>
<blockquote>
<p>U-netåŸç†ç¯‡è®²è§£ï¼š<a href="https://www.cnblogs.com/carpell/p/18908044" target="_blank">ã€è¯­ä¹‰åˆ†å‰²ä¸“æ ã€‘2ï¼šU-netåŸç†ç¯‡(ç”±æµ…å…¥æ·±) - carpell - åšå®¢å›­</a></p>
<p>ä»£ç åœ°å€ï¼Œä¸‹è½½å¯å¤ç°ï¼š<a href="https://github.com/fouen6/unet_semantic-segmentation" target="_blank" rel="noopener nofollow">fouen6/unet_semantic-segmentation</a></p>
</blockquote>
<p>æœ¬ç¯‡æ–‡ç« æ”¶å½•äºè¯­ä¹‰åˆ†å‰²ä¸“æ ï¼Œå¦‚æœå¯¹è¯­ä¹‰åˆ†å‰²é¢†åŸŸæ„Ÿå…´è¶£çš„ï¼Œå¯ä»¥å»çœ‹çœ‹ä¸“æ ï¼Œä¼šå¯¹ç»å…¸çš„æ¨¡å‹ä»¥åŠä»£ç è¿›è¡Œè¯¦ç»†çš„è®²è§£å“¦ï¼å…¶ä¸­ä¼šåŒ…å«å¯å¤ç°çš„ä»£ç ï¼<strong>(æ•°æ®é›†æ–‡ä¸­æä¾›äº†ä¸‹è½½åœ°å€ï¼Œä¸‹è½½ä¸åˆ°å¯åœ¨è¯„è®ºåŒºè¦å–)</strong></p>
<p>ä¸Šç¯‡æ–‡ç« å·²ç»å¸¦å¤§å®¶å­¦ä¹ è¿‡äº†U-netçš„åŸç†ï¼Œç›¸ä¿¡å¤§å®¶å¯¹äºåŸç†åº”è¯¥æœ‰äº†æ¯”è¾ƒæ·±çš„äº†è§£ã€‚æœ¬æ–‡å°†ä¼šå¸¦å¤§å®¶å»æ‰‹åŠ¨å¤ç°å±äºè‡ªå·±çš„ä¸€ä¸ªè¯­ä¹‰åˆ†å‰²æ¨¡å‹ã€‚å°†ä¼šæ·±å…¥ä»£ç è¿›è¡Œè®²è§£ï¼Œå¦‚æœæœ‰è®²é”™çš„åœ°æ–¹æ¬¢è¿å¤§å®¶æ‰¹è¯„æŒ‡æ­£ï¼</p>
<p>å…¶å®æ‰€æœ‰çš„æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ­å»ºæˆ‘è®¤ä¸ºå¯ä»¥æ€»ç»“æˆäº”éƒ¨åˆ†ï¼šæ¨¡å‹çš„æ„å»ºï¼Œæ•°æ®é›†çš„å¤„ç†ï¼Œè¯„ä»·æŒ‡æ ‡çš„è®¾å®šï¼Œè®­ç»ƒæµç¨‹ï¼Œæµ‹è¯•ã€‚å…¶å®æ„Ÿè§‰æœ‰ç‚¹æ·±åº¦å­¦ä¹ ä»£ç å…«è‚¡æ–‡çš„é‚£ç§æ„æ€ã€‚æœ¬ç¯‡åŒæ ·çš„ä¹Ÿä¼šæŒ‰ç…§è¿™æ ·çš„æ–¹å¼è¿›è¡Œè®²è§£ï¼Œå¸Œæœ›å¤§å®¶èƒ½å¤Ÿæ·±å…¥ä»£ç å»è¿›è¡Œäº†è§£å­¦ä¹ ã€‚</p>
<p>è¯·è®°ä½ï¼š<strong>åªæ‡‚åŸç†ä¸æ‡‚ä»£ç ï¼Œä½ å°±ç®—æœ‰äº†å¾ˆå¥½çš„æƒ³æ³•åˆ›æ–°ç‚¹ï¼Œä½ ä¹Ÿéš¾ä»¥å»å®ç°ï¼Œæ‰€ä»¥å¸Œæœ›å¤§å®¶èƒ½å¤Ÿæ·±å…¥å»äº†è§£</strong>ï¼Œæœ€å¥½èƒ½å¤Ÿå‚è€ƒç€æœ¬æ–‡è‡ªå·±å¤ç°ä¸€ä¸‹ã€‚</p>
<p><img src="https://gitee.com/fouen/image/raw/master/image/20250603095814418.png" alt="image-20250603095814153" loading="lazy"></p>
<h1 id="u-netå…¨æµç¨‹ä»£ç ">U-netå…¨æµç¨‹ä»£ç </h1>
<h2 id="æ¨¡å‹æ­å»ºmodel">æ¨¡å‹æ­å»º(model)</h2>
<p>æˆ‘ä»¬å…ˆæ¥çœ‹U-netæ¨¡å‹ä»£ç ï¼Œå½“ç„¶ç»†èŠ‚ä¸Šè·ŸåŸè®ºæ–‡ä¸­çš„U-netä¸æ˜¯å®Œå…¨ä¸€æ ·ï¼ŒåŸæ¥çš„U-netæ¨¡å‹æ˜¯é€‚ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ï¼Œæ‰€ä»¥å…¶æœ‰éƒ¨åˆ†è®¾è®¡ä¹Ÿæ˜¯ä¸ºäº†åŒ»å­¦å›¾åƒåˆ†å‰²è®¾è®¡çš„ï¼Œæˆ‘è¿™é‡Œå¤ç°çš„U-netä»£ç æ›´é€‚åˆæ™®éçš„è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ï¼Œå…¶è¾“å…¥è¾“å‡ºçš„shapeå¤§å°æ˜¯ç›¸åŒçš„ã€‚</p>
<p>é¦–å…ˆæ˜¯æˆ‘å°†æ‰€æœ‰çš„ä¸Šé‡‡æ ·ä¸‹é‡‡æ ·ä¸­çš„å·ç§¯éƒ¨åˆ†é›†æˆåˆ°äº†ä¸€èµ·ï¼Œçœ‹æ¨¡å‹ç»“æ„èƒ½å¤Ÿçœ‹å‡ºï¼Œæ¯ä¸ªéƒ¨åˆ†éƒ½æ˜¯ä¸¤æ¬¡å·ç§¯ï¼Œæ‰€ä»¥ä»£ç å¦‚ä¸‹ï¼Œå°±åœ¨è®¾ç½®ä¸åŒstageçš„æ—¶å€™è®¾ç½®å¥½è¾“å…¥è¾“å‡ºé€šé“å³å¯ã€‚</p>
<pre><code class="language-python">class Down_Up_Conv(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):
        super(Down_Up_Conv, self).__init__()
        self.conv_block = nn.Sequential(
            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
            nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding),
            nn.BatchNorm2d(out_channels),
            nn.ReLU()
        )

    def forward(self, x):
        return self.conv_block(x)
</code></pre>
<p>ç„¶åè¿™æ˜¯è·³è·ƒè¿æ¥çš„ä»£ç ï¼ŒåŒæ—¶æˆ‘ä»¬é‡‡å–äº†cropæ“ä½œã€‚æˆ‘ä»¬é€šè¿‡è·å–ä¸¤ä¸ªfeature mapçš„é•¿å®½ï¼Œç„¶åå†å¯¹é½ä¹‹åè¿›è¡Œå†é€šé“ç»´ä¸Šçš„æ‹¼æ¥ï¼Œä»£ç å¦‚ä¸‹ï¼Œè¿˜æ˜¯æ¯”è¾ƒå¥½ç†è§£çš„ã€‚</p>
<pre><code class="language-python">def crop_and_concat(upsampled, bypass):
    """
    å°†ä¸¤ä¸ª feature map åœ¨ H å’Œ W ä¸Šå¯¹é½åæ‹¼æ¥ï¼ˆdim=1ï¼‰
    - upsampled: è§£ç å™¨ä¸Šé‡‡æ ·åçš„ç‰¹å¾å›¾ (N, C1, H1, W1)
    - bypass: ç¼–ç å™¨ä¼ æ¥çš„ç‰¹å¾å›¾ (N, C2, H2, W2)
    """
    h1, w1 = upsampled.shape[2], upsampled.shape[3]
    h2, w2 = bypass.shape[2], bypass.shape[3]

    # è®¡ç®—å·®å€¼
    delta_h = h2 - h1
    delta_w = w2 - w1

    # å¯¹ encoder è¾“å‡ºè¿›è¡Œä¸­å¿ƒè£å‰ª
    bypass_cropped = bypass[:, :,
                     delta_h // 2: delta_h // 2 + h1,
                     delta_w // 2: delta_w // 2 + w1]

    # æ‹¼æ¥é€šé“ç»´
    return torch.cat([upsampled, bypass_cropped], dim=1)
</code></pre>
<p>ç„¶åå°±æ˜¯æ­å»ºæˆ‘ä»¬çš„U-netæ¨¡å‹äº†ï¼Œè¿™è¿˜æ˜¯æ¯”è¾ƒå®¹æ˜“çš„ï¼Œå°†encoderéƒ¨åˆ†çš„äº”ä¸ªé˜¶æ®µçš„ä¸‹é‡‡æ ·å·ç§¯å®šä¹‰å¥½ï¼Œæ³¨æ„é€šé“æ•°çš„å˜æ¢ï¼Œç„¶åå°±æ˜¯Decoderçš„ä¸Šé‡‡æ ·çš„è¿‡ç¨‹ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯è½¬ç½®å·ç§¯ï¼Œä¸Šé‡‡æ ·åè¿˜æœ‰å·ç§¯è¿‡ç¨‹ï¼Œæ‰€ä»¥æˆ‘ä»¬æŒ‰ç…§U-netçš„æ¨¡å‹å›¾æ­å»ºå³å¯ã€‚æ³¨æ„ï¼Œæˆ‘è¿™é‡Œæ˜¯æŠŠmaxpoolingç»™æ‘˜å‡ºæ¥äº†çš„ï¼Œæ¯ä¸ªä¸‹é‡‡æ ·å·ç§¯ä¹‹åéƒ½ä¼šæœ‰ä¸€ä¸ªmaxpoolingå±‚ï¼Œè¿™ä¸ªå¯åˆ«å¿˜äº†ï¼Œåœ¨forwardé‡Œé¢æœ‰ä½“ç°ã€‚å®šä¹‰å¥½æ¨¡å‹å‚æ•°ä¹‹åå°±æ˜¯æ¨¡å‹å‚æ•°çš„åˆå§‹åŒ–äº†ï¼Œè¿™ä¸ªæ­¥éª¤å¯åƒä¸‡ä¸èƒ½å¿˜ã€‚</p>
<pre><code class="language-python">class UNet(nn.Module):
    def __init__(self, num_classes=2):
        super(UNet, self).__init__()
        self.stage_down1=Down_Up_Conv(3, 64)
        self.stage_down2=Down_Up_Conv(64, 128)
        self.stage_down3=Down_Up_Conv(128, 256)
        self.stage_down4=Down_Up_Conv(256, 512)
        self.stage_down5=Down_Up_Conv(512, 1024)

        self.up4 = nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2,padding=1)
        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2,padding=1)
        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2,padding=1)
        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2,padding=1)

        self.stage_up4=Down_Up_Conv(1024, 512)
        self.stage_up3=Down_Up_Conv(512, 256)
        self.stage_up2=Down_Up_Conv(256, 128)
        self.stage_up1=Down_Up_Conv(128, 64)
        self.stage_out=Down_Up_Conv(64, num_classes)
        self.maxpool = nn.MaxPool2d(kernel_size=2)

        self.initialize_weights()

    def initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        stage1 = self.stage_down1(x)
        x = self.maxpool(stage1)
        stage2 = self.stage_down2(x)
        x = self.maxpool(stage2)
        stage3 = self.stage_down3(x)
        x = self.maxpool(stage3)
        stage4 = self.stage_down4(x)
        x = self.maxpool(stage4)
        stage5 = self.stage_down5(x)

        x = self.up4(stage5)

        x = self.stage_up4(crop_and_concat(x, stage4))
        x = self.up3(x)
        x = self.stage_up3(crop_and_concat(x, stage3))
        x = self.up2(x)
        x = self.stage_up2(crop_and_concat(x, stage2))
        x = self.up1(x)
        x = self.stage_up1(crop_and_concat(x, stage1))
        out = self.stage_out(x)
        return out
</code></pre>
<h2 id="æ•°æ®å¤„ç†dataloader">æ•°æ®å¤„ç†(dataloader)</h2>
<blockquote>
<p>æ•°æ®é›†åç§°ï¼šCamVid</p>
<p>æ•°æ®é›†ä¸‹è½½åœ°å€ï¼š<a href="https://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/" target="_blank" rel="noopener nofollow">Object Recognition in Video Dataset</a></p>
</blockquote>
<p><img src="https://gitee.com/fouen/image/raw/master/image/20250603094547017.png" alt="img" loading="lazy"></p>
<p>åœ¨è¿™é‡Œè¿›è¡Œä¸‹è½½ï¼ŒCamVidæ•°æ®é›†æœ‰ä¸¤ç§ï¼Œä¸€ç§æ˜¯å®˜æ–¹çš„å°±æ˜¯ä¸Šè¿°çš„ä¸‹è½½åœ°å€çš„ï¼Œæ€»å…±æœ‰32ç§ç±»åˆ«ï¼Œåˆ’åˆ†çš„ä¼šæ›´åŠ çš„ç»†è‡´ã€‚ä½†æ˜¯ä¸€èˆ¬å®˜ç½‘çš„å¤ªéš¾æ‰“å¼€äº†ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥é€šè¿‡Kaggleä¸­çš„<a href="https://www.kaggle.com/datasets/carlolepelaars/camvid/data" target="_blank" rel="noopener nofollow">CamVid (Cambridge-Driving Labeled Video Database)</a>è¿›è¡Œä¸‹è½½ã€‚</p>
<p>è¿˜æœ‰ä¸€ç§å°±æ˜¯11ç±»åˆ«çš„(ä¸åŒ…æ‹¬èƒŒæ™¯)ï¼Œä¼šå°†ä¸€äº›è¯­ä¹‰ç›¸è¿‘çš„å†…å®¹è¿›è¡Œåˆå¹¶ï¼Œå°±åˆ’åˆ†çš„æ²¡æœ‰è¿™ä¹ˆç»†è‡´ï¼Œä»»åŠ¡éš¾åº¦ä¹Ÿä¼šæ¯”è¾ƒä½ä¸€äº›ã€‚(å¦‚æœä½ åœ¨ç½‘ä¸Šæ‰¾ä¸åˆ°çš„è¯ï¼Œå¯ä»¥åœ¨è¯„è®ºåŒºå‘è¨€æˆ–æ˜¯ç§èŠæˆ‘è¦å–)</p>
<p>CamVid æ•°æ®é›†ä¸»è¦ç”¨äºè‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸­çš„è¯­ä¹‰åˆ†å‰²ï¼ŒåŒ…å«é©¾é©¶åœºæ™¯ä¸­çš„é“è·¯ã€äº¤é€šæ ‡å¿—ã€è½¦è¾†ç­‰ç±»åˆ«çš„æ ‡æ³¨å›¾åƒã€‚è¯¥æ•°æ®é›†æ—¨åœ¨æ¨åŠ¨è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨é“è·¯åœºæ™¯ä¸­çš„è¡¨ç°ã€‚</p>
<p><strong>æ•°æ®ç‰¹ç‚¹</strong>ï¼š</p>
<ul>
<li><strong>å›¾åƒæ•°é‡</strong>ï¼šåŒ…æ‹¬701å¸§è§†é¢‘åºåˆ—å›¾åƒï¼Œåˆ†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚</li>
<li><strong>ç±»åˆ«</strong>ï¼šåŒ…å«32ä¸ªç±»åˆ«(ä¹Ÿæœ‰åŒ…å«11ä¸ªç±»åˆ«çš„)ï¼ŒåŒ…æ‹¬é“è·¯ã€å»ºç­‘ç‰©ã€è½¦è¾†ã€è¡Œäººç­‰ã€‚</li>
<li><strong>æŒ‘æˆ˜</strong>ï¼šç”±äºæ•°æ®é›†ä¸»è¦æ¥è‡ªåŸå¸‚äº¤é€šåœºæ™¯ï¼Œå› æ­¤é¢ä¸´ç€åŠ¨æ€å˜åŒ–çš„å¤©æ°”ã€å…‰ç…§ã€äº¤é€šå¯†åº¦ç­‰æŒ‘æˆ˜</li>
</ul>
<p>è¿™é‡Œæˆ‘å·²ç»ä¸“é—¨å‘äº†ä¸€ç¯‡åšå®¢å¯¹è¯­ä¹‰åˆ†å‰²ä»»åŠ¡å¸¸ç”¨çš„æ•°æ®é›†åšäº†æ·±å…¥çš„ä»‹ç»ï¼Œå·²ç»å…·ä½“è®²è§£äº†å…¶å®ç°çš„å¤„ç†ä»£ç ã€‚å¦‚æœä½ å¯¹è¯­ä¹‰åˆ†å‰²å¸¸ç”¨æ•°æ®é›†æœ‰ä¸äº†è§£çš„è¯ï¼Œ<strong>å¯ä»¥å…ˆå»æˆ‘çš„è¯­ä¹‰åˆ†å‰²ä¸“æ ä¸­è¿›è¡Œäº†è§£å“¦</strong>ï¼ï¼  æˆ‘è¿™é‡Œå°±ç›´æ¥é™„ä¸Šä»£ç äº†ã€‚</p>
<pre><code class="language-python">import os
from PIL import Image
import albumentations as A
from albumentations.pytorch.transforms import ToTensorV2
from torch.utils.data import Dataset, DataLoader
import numpy as np
import torch
# 11ç±»
Cam_CLASSES = [ "Unlabelled","Sky","Building","Pole",
                "Road","Sidewalk", "Tree","SignSymbol",
                "Fence","Car","Pedestrian","Bicyclist"]

# ç”¨äºåšå¯è§†åŒ–
Cam_COLORMAP = [
    [0, 0, 0],[128, 128, 128],[128, 0, 0],[192, 192, 128],
    [128, 64, 128],[0, 0, 192],[128, 128, 0],[192, 128, 128],
    [64, 64, 128],[64, 0, 128],[64, 64, 0],[0, 128, 192]
]


# è½¬æ¢RGB maskä¸ºç±»åˆ«idçš„å‡½æ•°
def mask_to_class(mask):
    mask_class = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.int64)
    for idx, color in enumerate(Cam_COLORMAP):
        color = np.array(color)
        # æ¯ä¸ªåƒç´ å’Œå½“å‰é¢œè‰²åŒ¹é…
        matches = np.all(mask == color, axis=-1)
        mask_class[matches] = idx
    return mask_class


class CamVidDataset(Dataset):
    def __init__(self, image_dir, label_dir):
        self.image_dir = image_dir
        self.label_dir = label_dir
        self.transform = A.Compose([
            A.Resize(224, 224),
            A.HorizontalFlip(),
            A.VerticalFlip(),
            A.Normalize(),
            ToTensorV2(),
        ])

        self.images = sorted(os.listdir(image_dir))
        self.labels = sorted(os.listdir(label_dir))

        assert len(self.images) == len(self.labels), "Images and labels count mismatch!"

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        img_path = os.path.join(self.image_dir, self.images[idx])
        label_path = os.path.join(self.label_dir, self.labels[idx])

        image = np.array(Image.open(img_path).convert("RGB"))
        label_rgb = np.array(Image.open(label_path).convert("RGB"))

        # RGBè½¬ç±»åˆ«ç´¢å¼•
        mask = mask_to_class(label_rgb)
        #mask = torch.from_numpy(np.array(mask)).long()

        # Albumentations éœ€è¦ (H, W, 3) å’Œ (H, W)
        transformed = self.transform(image=image, mask=mask)

        return transformed['image'], transformed['mask'].long()


def get_dataloader(data_path, batch_size=4, num_workers=4):
    train_dir = os.path.join(data_path, 'train')
    val_dir = os.path.join(data_path, 'val')
    trainlabel_dir = os.path.join(data_path, 'train_labels')
    vallabel_dir = os.path.join(data_path, 'val_labels')
    train_dataset = CamVidDataset(train_dir, trainlabel_dir)
    val_dataset = CamVidDataset(val_dir, vallabel_dir)

    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, pin_memory=True, num_workers=num_workers)
    val_loader = DataLoader(val_dataset, shuffle=False, batch_size=batch_size, pin_memory=True, num_workers=num_workers)
    return train_loader, val_loader

</code></pre>
<h2 id="è¯„ä»·æŒ‡æ ‡metric">è¯„ä»·æŒ‡æ ‡(metric)</h2>
<p>æˆ‘ä»¬è¿™é‡Œè¯­ä¹‰åˆ†å‰²é‡‡ç”¨çš„è¯„ä»·æŒ‡æ ‡ä¸ºï¼šPA(åƒç´ å‡†ç¡®ç‡)ï¼ŒCPA(ç±»åˆ«åƒç´ å‡†ç¡®ç‡)ï¼ŒMPA(ç±»åˆ«å¹³å‡åƒç´ å‡†ç¡®ç‡)ï¼ŒIoU(äº¤å¹¶æ¯”)ï¼ŒmIoU(å¹³å‡äº¤å¹¶æ¯”)ï¼ŒFWIoU(é¢‘ç‡åŠ æƒäº¤å¹¶æ¯”)ï¼ŒmF1(å¹³å‡F1åˆ†æ•°)ã€‚</p>
<p>è¿™é‡Œæˆ‘å·²ç»ä¸“é—¨å‘äº†ä¸€ç¯‡åšå®¢å¯¹è¿™äº›å¹³å‡æŒ‡æ ‡åšäº†æ·±å…¥çš„ä»‹ç»ï¼Œå·²ç»å…·ä½“è®²è§£äº†å…¶å®ç°çš„ä»£ç ã€‚å¦‚æœä½ å¯¹è¿™äº›è¯„ä»·æŒ‡æ ‡æœ‰ä¸äº†è§£çš„è¯ï¼Œ<strong>å¯ä»¥å…ˆå»æˆ‘çš„è¯­ä¹‰åˆ†å‰²ä¸“æ ä¸­è¿›è¡Œäº†è§£å“¦</strong>ï¼ï¼  æˆ‘è¿™é‡Œå°±ç›´æ¥é™„ä¸Šä»£ç äº†ã€‚</p>
<pre><code class="language-python">import numpy as np

__all__ = ['SegmentationMetric']


class SegmentationMetric(object):
    def __init__(self, numClass):
        self.numClass = numClass
        self.confusionMatrix = np.zeros((self.numClass,) * 2)

    def genConfusionMatrix(self, imgPredict, imgLabel):
        mask = (imgLabel &gt;= 0) &amp; (imgLabel &lt; self.numClass)
        label = self.numClass * imgLabel[mask] + imgPredict[mask]
        count = np.bincount(label, minlength=self.numClass ** 2)
        confusionMatrix = count.reshape(self.numClass, self.numClass)
        return confusionMatrix

    def addBatch(self, imgPredict, imgLabel):
        assert imgPredict.shape == imgLabel.shape
        self.confusionMatrix += self.genConfusionMatrix(imgPredict, imgLabel)
        return self.confusionMatrix

    def pixelAccuracy(self):
        acc = np.diag(self.confusionMatrix).sum() / self.confusionMatrix.sum()
        return acc

    def classPixelAccuracy(self):
        denominator = self.confusionMatrix.sum(axis=1)
        denominator = np.where(denominator == 0, 1e-12, denominator)
        classAcc = np.diag(self.confusionMatrix) / denominator
        return classAcc

    def meanPixelAccuracy(self):
        classAcc = self.classPixelAccuracy()
        meanAcc = np.nanmean(classAcc)
        return meanAcc

    def IntersectionOverUnion(self):
        intersection = np.diag(self.confusionMatrix)
        union = np.sum(self.confusionMatrix, axis=1) + np.sum(self.confusionMatrix, axis=0) - np.diag(
            self.confusionMatrix)
        union = np.where(union == 0, 1e-12, union)
        IoU = intersection / union
        return IoU

    def meanIntersectionOverUnion(self):
        mIoU = np.nanmean(self.IntersectionOverUnion())
        return mIoU

    def Frequency_Weighted_Intersection_over_Union(self):
        denominator1 = np.sum(self.confusionMatrix)
        denominator1 = np.where(denominator1 == 0, 1e-12, denominator1)
        freq = np.sum(self.confusionMatrix, axis=1) / denominator1
        denominator2 = np.sum(self.confusionMatrix, axis=1) + np.sum(self.confusionMatrix, axis=0) - np.diag(
            self.confusionMatrix)
        denominator2 = np.where(denominator2 == 0, 1e-12, denominator2)
        iu = np.diag(self.confusionMatrix) / denominator2
        FWIoU = (freq[freq &gt; 0] * iu[freq &gt; 0]).sum()
        return FWIoU

    def classF1Score(self):
        tp = np.diag(self.confusionMatrix)
        fp = self.confusionMatrix.sum(axis=0) - tp
        fn = self.confusionMatrix.sum(axis=1) - tp

        precision = tp / (tp + fp + 1e-12)
        recall = tp / (tp + fn + 1e-12)

        f1 = 2 * precision * recall / (precision + recall + 1e-12)
        return f1

    def meanF1Score(self):
        f1 = self.classF1Score()
        mean_f1 = np.nanmean(f1)
        return mean_f1

    def reset(self):
        self.confusionMatrix = np.zeros((self.numClass, self.numClass))

    def get_scores(self):
        scores = {
            'Pixel Accuracy': self.pixelAccuracy(),
            'Class Pixel Accuracy': self.classPixelAccuracy(),
            'Intersection over Union': self.IntersectionOverUnion(),
            'Class F1 Score': self.classF1Score(),
            'Frequency Weighted Intersection over Union': self.Frequency_Weighted_Intersection_over_Union(),
            'Mean Pixel Accuracy': self.meanPixelAccuracy(),
            'Mean Intersection over Union(mIoU)': self.meanIntersectionOverUnion(),
            'Mean F1 Score': self.meanF1Score()
        }
        return scores

</code></pre>
<h2 id="è®­ç»ƒæµç¨‹train">è®­ç»ƒæµç¨‹(train)</h2>
<p>åˆ°è¿™é‡Œï¼Œæ‰€æœ‰çš„å‰æœŸå‡†å¤‡éƒ½å·²ç»å°±ç»ªï¼Œæˆ‘ä»¬å°±è¦å¼€å§‹è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹äº†ã€‚</p>
<pre><code class="language-python">def parse_arguments():
    parser = argparse.ArgumentParser()
    parser.add_argument('--data_root', type=str, default='../../data/CamVid/CamVid(11)', help='Dataset root path')
    parser.add_argument('--data_name', type=str, default='CamVid', help='Dataset class names')
    parser.add_argument('--model', type=str, default='unet', help='Segmentation model')
    parser.add_argument('--num_classes', type=int, default=12, help='Number of classes')
    parser.add_argument('--epochs', type=int, default=50, help='Epochs')
    parser.add_argument('--lr', type=float, default=0.005, help='Learning rate')
    parser.add_argument('--momentum', type=float, default=0.9, help='Momentum')
    parser.add_argument('--weight-decay', type=float, default=1e-4, help='Weight decay')
    parser.add_argument('--batch_size', type=int, default=8, help='Batch size')
    parser.add_argument('--checkpoint', type=str, default='./checkpoint', help='Checkpoint directory')
    parser.add_argument('--resume', type=str, default=None, help='Resume checkpoint path')
    return parser.parse_args()
</code></pre>
<p>é¦–å…ˆæ¥çœ‹çœ‹æˆ‘ä»¬çš„ä¸€äº›å‚æ•°çš„è®¾å®šï¼Œä¸€èˆ¬æˆ‘ä»¬éƒ½æ˜¯è¿™æ ·æ”¾åœ¨æœ€å‰é¢ï¼Œèƒ½å¤Ÿè®©äººæ›´åŠ å¿«é€Ÿçš„äº†è§£å…¶ä»£ç çš„ä¸€äº›æ ¸å¿ƒå‚æ•°è®¾ç½®ã€‚é¦–å…ˆå°±æ˜¯æˆ‘ä»¬çš„æ•°æ®é›†ä½ç½®(data_root)ï¼Œç„¶åå°±æ˜¯æˆ‘ä»¬çš„æ•°æ®é›†åç§°(classes_name)ï¼Œè¿™ä¸ªæš‚æ—¶æ²¡ä»€ä¹ˆç”¨ï¼Œå› ä¸ºæˆ‘ä»¬ç›®å‰åªç”¨äº†CamVidæ•°æ®é›†ï¼Œç„¶åå°±æ˜¯æ£€æµ‹æ¨¡å‹çš„é€‰æ‹©(model)ï¼Œæˆ‘ä»¬é€‰æ‹©unetæ¨¡å‹ï¼Œæ•°æ®é›†çš„ç±»åˆ«æ•°(num_classes)ï¼Œè®­ç»ƒepochæ•°ï¼Œè¿™ä¸ªä½ è®¾ç½®å¤§ä¸€ç‚¹ä¹Ÿè¡Œï¼Œå› ä¸ºæˆ‘ä»¬ä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿å­˜æœ€å¥½ç»“æœçš„æ¨¡å‹çš„ã€‚å­¦ä¹ ç‡(lr)ï¼ŒåŠ¨é‡(momentum)ï¼Œæƒé‡è¡°å‡(weight-decay)ï¼Œè¿™äº›éƒ½å±äºæ¨¡å‹è¶…å‚æ•°ï¼Œå¤§å®¶å¯ä»¥å°è¯•ä¸åŒçš„æ•°å€¼ï¼Œå¤šè¯•è¯•ï¼Œå°±ä¼šæœ‰ä¸ªå¤§è‡´çš„äº†è§£çš„ï¼Œæ‰¹é‡å¤§å°(batch_size)æ ¹æ®è‡ªå·±ç”µè„‘æ€§èƒ½æ¥è®¾ç½®ï¼Œä¸€èˆ¬éƒ½æ˜¯ä¸º2çš„å€æ•°ï¼Œä¿å­˜æƒé‡çš„æ–‡ä»¶å¤¹(checkpoint)ï¼Œæ˜¯å¦ç»§ç»­è®­ç»ƒ(resume)ã€‚</p>
<pre><code class="language-python">def train(args):
    if not os.path.exists(args.checkpoint):
        os.makedirs(args.checkpoint)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    n_gpu = torch.cuda.device_count()
    print(f"Device: {device}, GPUs available: {n_gpu}")

    # Dataloader
    train_loader, val_loader = get_dataloader(args.data_root, batch_size=args.batch_size)
    train_dataset_size = len(train_loader.dataset)
    val_dataset_size = len(val_loader.dataset)
    print(f"Train samples: {train_dataset_size}, Val samples: {val_dataset_size}")

    # Model
    model = get_model(num_classes=args.num_classes)
    model.to(device)

    # Loss + Optimizer + Scheduler
    criterion = nn.CrossEntropyLoss(ignore_index=0)
    #optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)
    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)
    scaler = torch.cuda.amp.GradScaler()

    # Resume
    start_epoch = 0
    best_miou = 0.0
    if args.resume and os.path.isfile(args.resume):
        print(f"Loading checkpoint '{args.resume}'")
        checkpoint = torch.load(args.resume)
        start_epoch = checkpoint['epoch']
        best_miou = checkpoint['best_miou']
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        print(f"Loaded checkpoint (epoch {start_epoch})")

    # Training history
    history = {
        'train_loss': [],
        'val_loss': [],
        'pixel_accuracy': [],
        'miou': []
    }

    print(f"ğŸš€ Start training ({args.model})")
    for epoch in range(start_epoch, args.epochs):
        model.train()
        train_loss = 0.0
        t0 = time.time()
        for images, masks in tqdm(train_loader, desc=f'Epoch {epoch+1}/{args.epochs} [Train]'):
            images = images.to(device)
            masks = masks.to(device)


            optimizer.zero_grad()
            with torch.cuda.amp.autocast():
                outputs = model(images)
                loss = criterion(outputs, masks)
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            train_loss += loss.item() * images.size(0)

        train_loss /= train_dataset_size
        history['train_loss'].append(train_loss)

        # Validation
        model.eval()
        val_loss = 0.0
        evaluator = SegmentationMetric(args.num_classes)
        with torch.no_grad():
            for images, masks in tqdm(val_loader, desc=f'Epoch {epoch+1}/{args.epochs} [Val]'):
                images = images.to(device)
                masks = masks.to(device)


                outputs = model(images)
                loss = criterion(outputs, masks)
                val_loss += loss.item() * images.size(0)

                predictions = torch.argmax(outputs, dim=1)
                if isinstance(predictions, torch.Tensor):
                    predictions = predictions.cpu().numpy()
                if isinstance(masks, torch.Tensor):
                    masks = masks.cpu().numpy()

                evaluator.addBatch(predictions, masks)

        val_loss /= val_dataset_size
        history['val_loss'].append(val_loss)

        scores = evaluator.get_scores()
        print(f"\nğŸ“ˆ Validation Epoch {epoch+1}:")
        for k, v in scores.items():
            if isinstance(v, np.ndarray):
                print(f"{k}: {np.round(v, 3)}")
            else:
                print(f"{k}: {v:.4f}")

        history['pixel_accuracy'].append(scores['Pixel Accuracy'])
        history['miou'].append(scores['Mean Intersection over Union(mIoU)'])

        # Save best
        if scores['Mean Intersection over Union(mIoU)'] &gt; best_miou:
            best_miou = scores['Mean Intersection over Union(mIoU)']
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'best_miou': best_miou,
            }, os.path.join(args.checkpoint, f'{args.model}_best.pth'))
            print(f"Saved best model! mIoU: {best_miou:.4f}")

        scheduler.step()

        print(f"ğŸ•’ Epoch time: {time.time() - t0:.2f}s\n")

    print("ğŸ‰ Training complete!")
</code></pre>
<p>ç„¶åå°±æ˜¯æˆ‘ä»¬çš„è®­ç»ƒæµç¨‹äº†ã€‚è®­ç»ƒæµç¨‹ä¹Ÿæ˜¯æœ‰å¥—è·¯çš„å“¦ï¼Œæˆ‘ä»¬è¯¥æ€ä¹ˆå»æ­å»ºä¸€ä¸ªæ›´å¥½çš„è®­ç»ƒæµç¨‹ï¼Œå¯ä»¥ä»å¤šæ–¹é¢å…¥æ‰‹çš„ã€‚</p>
<p>é¦–å…ˆæˆ‘ä»¬ç¡®å®šæˆ‘ä»¬çš„ä»£ç è¿è¡Œè®¾å¤‡ï¼ŒåŸºæœ¬ä¸Šéƒ½æ˜¯è¦GPUçš„ã€‚ç„¶åå°±æ˜¯åŠ è½½æˆ‘ä»¬å¤„ç†å¥½çš„æ•°æ®ï¼Œè¿™é‡Œå°±æ˜¯dataloaderçš„é‚£éƒ¨åˆ†äº†ï¼ŒåŠ è½½å¥½æ•°æ®ä¹‹åï¼Œæˆ‘ä»¬åŠ è½½æˆ‘ä»¬æ„å»ºå¥½çš„æ¨¡å‹ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬åœ¨modelé‚£éƒ¨åˆ†åšçš„å·¥ä½œã€‚ç„¶åå°±æ˜¯losså‡½æ•°ï¼ŒOptimizer å’Œ Schedulerï¼Œè¿™æ˜¯æˆ‘ä»¬æ¯”è¾ƒé‡è¦çš„å‡ ä¸ªéƒ¨åˆ†ã€‚losså‡½æ•°çš„é€‰æ‹©æœ‰å¾ˆå¤šï¼Œä¸åŒçš„losså‡½æ•°åœ¨ä¸€å®šç¨‹åº¦ä¸Šä¼šå†³å®šæˆ‘ä»¬çš„æ¨¡å‹æ”¶æ•›å¥½åï¼Œåƒè¯­ä¹‰åˆ†å‰²ä»»åŠ¡å°±åŸºæœ¬ä¸Šéƒ½æ˜¯ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°äº†ã€‚Optimizer ä¹Ÿæœ‰å¾ˆå¤šï¼ŒSGDï¼ŒAdamä¹‹ç±»çš„ï¼Œéƒ½å¯ä»¥å»å°è¯•ä¸‹ã€‚Schedulerå°±æ˜¯æˆ‘ä»¬çš„å­¦ä¹ ç­–ç•¥ï¼Œå­¦ä¹ ç‡çš„æ›´æ–°ï¼Œå¸Œæœ›ä¸€å¼€å§‹å­¦ä¹ ç‡å¤§ï¼Œè®­ç»ƒåˆ°åæœŸå­¦ä¹ ç‡å°ï¼Œè¿™æ ·åŠ é€Ÿæ”¶æ•›ï¼Œé¿å…éœ‡è¡ã€‚ç„¶åè¿˜æœ‰ä¸ªscalerï¼Œè¿™æ˜¯AMP(è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒ)ï¼Œèƒ½å¤ŸèŠ‚çœæˆ‘ä»¬çš„å†…å­˜ï¼Œè®©æˆ‘ä»¬çš„å°ç”µè„‘ä¹Ÿèƒ½è·‘èµ·æ¥æ¨¡å‹ã€‚</p>
<p>è¿˜æœ‰ä¸ªæ–­ç‚¹é‡è®­åŠŸèƒ½ï¼Œä¸ºäº†é¿å…å› ä¸ºä¸€äº›æ„å¤–çš„æƒ…å†µå¯¼è‡´è®­ç»ƒä¸­æ–­ï¼Œå¯èƒ½è¿™æ˜¯æˆ‘ä»¬è®­ç»ƒå¥½ä¹…çš„ç»“æœï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥é€šè¿‡è¿™ä¸ªåŠŸèƒ½ç»§ç»­ä»æ–­ç‚¹è¿›è¡Œè®­ç»ƒã€‚ç„¶åå°±æ˜¯è®­ç»ƒäº†ï¼Œæˆ‘ä»¬åŠ è½½æ•°æ®ï¼Œé€šè¿‡æ¨¡å‹çš„é¢„æµ‹ä¸maskå¾—åˆ°æŸå¤±ï¼Œç„¶åæ¢¯åº¦è¯¯å·®åä¼ ï¼Œæ›´æ–°æ¨¡å‹å‚æ•°ã€‚å½“ä¸€ä¸ªepochä¸­çš„æ•°æ®éƒ½è®­ç»ƒç»“æŸä¹‹åï¼Œæˆ‘ä»¬å°±éœ€è¦è¯„ä¼°ä¸‹æˆ‘ä»¬çš„æ¨¡å‹æ€ä¹ˆæ ·äº†ï¼Œè¿™é‡Œå°±æ˜¯æ ¹æ®æˆ‘ä»¬çš„è¯„ä»·æŒ‡æ ‡è¿›è¡Œè¯„ä»·ï¼Œå…¶ä¸­æˆ‘ä»¬æ ‡è®°best_mIoUï¼Œå½“æ›´å¥½çš„æ—¶å€™å°±é‡æ–°ä¿å­˜æ¨¡å‹æ–‡ä»¶ã€‚</p>
<p>æœ€åå½“è®­ç»ƒç»“æŸåæˆ‘ä»¬å°±ä¼šè·å¾—æœ€å¥½çš„æ¨¡å‹å‚æ•°çš„æ–‡ä»¶äº†ã€‚</p>
<p>å®Œæ•´ä»£ç ï¼š</p>
<pre><code class="language-python">import argparse
import os
import time
from tqdm import tqdm
import numpy as np
import torch
import torch.nn as nn
from datasets.CamVid_dataloader11 import get_dataloader
from model import get_model
from metric import SegmentationMetric
os.environ['NO_ALBUMENTATIONS_UPDATE'] = '1'


def parse_arguments():
    parser = argparse.ArgumentParser()
    parser.add_argument('--data_root', type=str, default='../../data/CamVid/CamVid(11)', help='Dataset root path')
    parser.add_argument('--data_name', type=str, default='CamVid', help='Dataset class names')
    parser.add_argument('--model', type=str, default='unet', help='Segmentation model')
    parser.add_argument('--num_classes', type=int, default=12, help='Number of classes')
    parser.add_argument('--epochs', type=int, default=50, help='Epochs')
    parser.add_argument('--lr', type=float, default=0.005, help='Learning rate')
    parser.add_argument('--momentum', type=float, default=0.9, help='Momentum')
    parser.add_argument('--weight-decay', type=float, default=1e-4, help='Weight decay')
    parser.add_argument('--batch_size', type=int, default=8, help='Batch size')
    parser.add_argument('--checkpoint', type=str, default='./checkpoint', help='Checkpoint directory')
    parser.add_argument('--resume', type=str, default=None, help='Resume checkpoint path')
    return parser.parse_args()

def train(args):
    if not os.path.exists(args.checkpoint):
        os.makedirs(args.checkpoint)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    n_gpu = torch.cuda.device_count()
    print(f"Device: {device}, GPUs available: {n_gpu}")

    # Dataloader
    train_loader, val_loader = get_dataloader(args.data_root, batch_size=args.batch_size)
    train_dataset_size = len(train_loader.dataset)
    val_dataset_size = len(val_loader.dataset)
    print(f"Train samples: {train_dataset_size}, Val samples: {val_dataset_size}")

    # Model
    model = get_model(num_classes=args.num_classes)
    model.to(device)

    # Loss + Optimizer + Scheduler
    criterion = nn.CrossEntropyLoss(ignore_index=0)
    #optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)
    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)
    scaler = torch.cuda.amp.GradScaler()

    # Resume
    start_epoch = 0
    best_miou = 0.0
    if args.resume and os.path.isfile(args.resume):
        print(f"Loading checkpoint '{args.resume}'")
        checkpoint = torch.load(args.resume)
        start_epoch = checkpoint['epoch']
        best_miou = checkpoint['best_miou']
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        print(f"Loaded checkpoint (epoch {start_epoch})")

    # Training history
    history = {
        'train_loss': [],
        'val_loss': [],
        'pixel_accuracy': [],
        'miou': []
    }

    print(f"ğŸš€ Start training ({args.model})")
    for epoch in range(start_epoch, args.epochs):
        model.train()
        train_loss = 0.0
        t0 = time.time()
        for images, masks in tqdm(train_loader, desc=f'Epoch {epoch+1}/{args.epochs} [Train]'):
            images = images.to(device)
            masks = masks.to(device)


            optimizer.zero_grad()
            with torch.cuda.amp.autocast():
                outputs = model(images)
                loss = criterion(outputs, masks)
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            train_loss += loss.item() * images.size(0)

        train_loss /= train_dataset_size
        history['train_loss'].append(train_loss)

        # Validation
        model.eval()
        val_loss = 0.0
        evaluator = SegmentationMetric(args.num_classes)
        with torch.no_grad():
            for images, masks in tqdm(val_loader, desc=f'Epoch {epoch+1}/{args.epochs} [Val]'):
                images = images.to(device)
                masks = masks.to(device)


                outputs = model(images)
                loss = criterion(outputs, masks)
                val_loss += loss.item() * images.size(0)

                predictions = torch.argmax(outputs, dim=1)
                if isinstance(predictions, torch.Tensor):
                    predictions = predictions.cpu().numpy()
                if isinstance(masks, torch.Tensor):
                    masks = masks.cpu().numpy()

                evaluator.addBatch(predictions, masks)

        val_loss /= val_dataset_size
        history['val_loss'].append(val_loss)

        scores = evaluator.get_scores()
        print(f"\nğŸ“ˆ Validation Epoch {epoch+1}:")
        for k, v in scores.items():
            if isinstance(v, np.ndarray):
                print(f"{k}: {np.round(v, 3)}")
            else:
                print(f"{k}: {v:.4f}")

        history['pixel_accuracy'].append(scores['Pixel Accuracy'])
        history['miou'].append(scores['Mean Intersection over Union(mIoU)'])

        # Save best
        if scores['Mean Intersection over Union(mIoU)'] &gt; best_miou:
            best_miou = scores['Mean Intersection over Union(mIoU)']
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'best_miou': best_miou,
            }, os.path.join(args.checkpoint, f'{args.model}_best.pth'))
            print(f"Saved best model! mIoU: {best_miou:.4f}")

        scheduler.step()

        print(f"ğŸ•’ Epoch time: {time.time() - t0:.2f}s\n")

    print("ğŸ‰ Training complete!")

if __name__ == '__main__':
    args = parse_arguments()
    train(args)

</code></pre>
<h2 id="æ¨¡å‹æµ‹è¯•test">æ¨¡å‹æµ‹è¯•(test)</h2>
<p>è¿™é‡Œå°±åˆ°äº†æˆ‘ä»¬çš„æœ€åä¸€æ­¥äº†ï¼Œæµ‹è¯•æˆ‘ä»¬çš„æ¨¡å‹ã€‚</p>
<pre><code class="language-python">def parse_arguments():
    parser = argparse.ArgumentParser()
    parser.add_argument('--image_dir', type=str, default='./datasets/test', help='Input image or folder')
    parser.add_argument('--checkpoint', type=str, default='./checkpoint/unet_best.pth', help='Checkpoint path')
    parser.add_argument('--model', type=str, default='unet', help='Segmentation head')
    parser.add_argument('--num_classes', type=int, default=12, help='Number of classes')
    parser.add_argument('--save_dir', type=str, default='./predictions', help='Directory to save results')
    parser.add_argument('--overlay', type=bool, default=True, help='Save overlay image')
    return parser.parse_args()
</code></pre>
<p>åŒæ ·çš„æ¥çœ‹ï¼Œæˆ‘ä»¬æ‰€éœ€è¦çš„ä¸€äº›å‚æ•°è®¾å®šå“ˆï¼æˆ‘ä»¬æ‰€éœ€è¦è¿›è¡Œæµ‹è¯•çš„å›¾ç‰‡æ–‡ä»¶å¤¹(image_dir)ï¼Œæˆ‘ä»¬è®­ç»ƒæ—¶å€™æ‰€ä¿å­˜çš„æƒé‡æ–‡ä»¶å¤¹(checkpoint)ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ£€æµ‹æ¨¡å‹(model)ï¼Œè¿˜æœ‰æ•°æ®é›†çš„ç±»åˆ«æ•°(num_classes)ï¼Œä¿æŒçš„ç»“æœçš„æ–‡ä»¶å¤¹(save_dir)ï¼Œè¿˜è¦ä¸ªéå¸¸é‡è¦çš„å‚æ•°ï¼Œæ˜¯å¦å°†é¢„æµ‹å›¾è¦†ç›–åœ¨åŸå›¾ä¸Š(overlay)ï¼Œé€šè¿‡è¿™ä¸ªæˆ‘ä»¬å¯ä»¥æ›´å¥½çš„çœ‹è¯­ä¹‰åˆ†å‰²çš„æ•ˆæœæ€ä¹ˆæ ·ã€‚</p>
<pre><code class="language-python">def load_image(image_path):
    image = Image.open(image_path).convert('RGB')
    transform = T.Compose([
        #T.Resize((224, 224)),
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406],
                    std=[0.229, 0.224, 0.225])
    ])
    return transform(image).unsqueeze(0), image  # tensor, PIL image

#æŠŠç±»åˆ«mask â” å½©è‰²å›¾ (ç”¨VOC_COLORMAP)
def mask_to_color(mask):
    color_mask = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)
    for label in range(len(Cam_COLORMAP)):
        color_mask[mask == label] = Cam_COLORMAP[label]
    return color_mask

def save_mask(mask, save_path):
    color_mask = mask_to_color(mask)
    Image.fromarray(color_mask).save(save_path)

def overlay_mask_on_image(raw_image, mask, alpha=0.6):
    mask_color = mask_to_color(mask)
    mask_pil = Image.fromarray(mask_color)
    mask_pil = mask_pil.resize(raw_image.size, resample=Image.NEAREST)
    blended = Image.blend(raw_image, mask_pil, alpha=alpha)
    return blended
</code></pre>
<p>ç„¶åæ¥çœ‹æµ‹è¯•è¿‡ç¨‹ä¸­ä¼šç”¨åˆ°çš„ä¸€äº›å‡½æ•°ï¼Œå½“ç„¶æµ‹è¯•é¦–å…ˆè‚¯å®šè¦åŠ è½½æˆ‘ä»¬çš„å›¾ç‰‡å‘ã€‚æ³¨æ„çœ‹è¿™é‡Œæœ‰ä¸ªç»†èŠ‚ï¼ŒåŠ è½½å›¾ç‰‡çš„æ—¶å€™æˆ‘ä»¬è¿›è¡Œäº†æ ‡å‡†åŒ–çš„ï¼Œä¸ºä»€ä¹ˆè¿™ä¹ˆåšï¼Ÿå› ä¸ºæˆ‘ä»¬åœ¨è®­ç»ƒæ¨¡å‹çš„æ—¶å€™ï¼Œå›¾ç‰‡å°±è¿›è¡Œäº†æ ‡å‡†åŒ–çš„æ“ä½œï¼Œæ‰€æœ‰æµ‹è¯•å›¾ç‰‡ï¼Œæˆ‘ä»¬è‚¯å®šè¦ä¿æŒå›¾ç‰‡å’Œè®­ç»ƒæ—¶å€™çš„æ¡ä»¶ä¸€æ ·ã€‚ç„¶åä¸ºäº†æ›´å¥½çš„å¯è§†åŒ–ï¼Œæˆ‘ä»¬éœ€è¦å°†é¢„æµ‹çš„maskå›¾è½¬æ¢ä¸ºå½©è‰²å›¾ã€‚æ ¹æ®VOC_COLORMAPçš„é¢œè‰²è¿›è¡Œè½¬æ¢å³å¯ã€‚è¿˜æœ‰ä¸ªoverlay_mask_on_imageå‡½æ•°ï¼Œé€šè¿‡å°†é¢„æµ‹çš„å¯è§†åŒ–å›¾ä¸åŸå›¾è¿›è¡Œå åŠ æ··åˆèƒ½å¤Ÿè®©æˆ‘ä»¬æ›´åŠ ç›´è§‚ã€‚</p>
<pre><code class="language-python">def predict(args):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Device: {device}")

    # æ¨¡å‹
    model = get_model(num_classes=args.num_classes)
    checkpoint = torch.load(args.checkpoint, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()

    os.makedirs(args.save_dir, exist_ok=True)

    # é¢„æµ‹å•å¼  or æ‰¹é‡
    if os.path.isdir(args.image_dir):
        image_list = [os.path.join(args.image_dir, f) for f in os.listdir(args.image_dir) if f.lower().endswith(('jpg', 'png', 'jpeg'))]
    else:
        image_list = [args.image]

    print(f"ğŸ” Found {len(image_list)} images to predict.")

    for img_path in tqdm(image_list):
        img_tensor, raw_img = load_image(img_path)
        img_tensor = img_tensor.to(device)

        with torch.no_grad():
            output = model(img_tensor)
            pred = torch.argmax(output.squeeze(), dim=0).cpu().numpy()

        # ä¿å­˜ mask
        base_name = os.path.basename(img_path).split('.')[0]
        mask_save_path = os.path.join(args.save_dir, f"{base_name}_mask.png")
        save_mask(pred, mask_save_path)

        # ä¿å­˜ overlay
        if args.overlay:
            overlay_img = overlay_mask_on_image(raw_img, pred)
            overlay_save_path = os.path.join(args.save_dir, f"{base_name}_overlay.png")
            overlay_img.save(overlay_save_path)

        print(f"Saved: {mask_save_path}")
        if args.overlay:
            print(f"Saved overlay: {overlay_save_path}")

    print("ğŸ‰ Prediction done!")
</code></pre>
<p>ç„¶åå°±åˆ°äº†é¢„æµ‹ç¯èŠ‚ï¼Œå…¶å®æµç¨‹è·Ÿtrainçš„æµç¨‹å·®ä¸å¤šï¼Œä½†æ˜¯ä¸åœ¨éœ€è¦åƒtrainçš„æ—¶å€™ä»€ä¹ˆæ¢¯åº¦åä¼ æ›´æ–°å‚æ•°äº†ï¼Œç›´æ¥é¢„æµ‹å¾—å‡ºç»“æœç„¶åä¿å­˜å³å¯ã€‚</p>
<p>é¦–å…ˆç¡®å®šè®¾å¤‡å“ˆï¼Œä¸€èˆ¬éƒ½æ˜¯GPUçš„ï¼Œç„¶åå°±æ˜¯å°±æ˜¯åŠ è½½æ•°æ®å’Œæ¨¡å‹äº†ï¼Œæœ€åé¢„æµ‹ä¿å­˜ç»“æœå³å¯ï¼Œè¿™äº›ä»£ç åº”è¯¥è¿˜æ˜¯æ¯”è¾ƒå®¹æ˜“ç†è§£çš„ï¼Œç›´æ¥çœ‹ä»£ç å³å¯ã€‚</p>
<p>å®Œæ•´ä»£ç ï¼š</p>
<pre><code class="language-python">import argparse
import os
import torch
import numpy as np
from PIL import Image
from tqdm import tqdm
from model import get_model
import torchvision.transforms as T
from datasets.CamVid_dataloader11 import *



def parse_arguments():
    parser = argparse.ArgumentParser()
    parser.add_argument('--image_dir', type=str, default='./datasets/test', help='Input image or folder')
    parser.add_argument('--checkpoint', type=str, default='./checkpoint/unet_best.pth', help='Checkpoint path')
    parser.add_argument('--model', type=str, default='unet', help='Segmentation head')
    parser.add_argument('--num_classes', type=int, default=12, help='Number of classes')
    parser.add_argument('--save_dir', type=str, default='./predictions', help='Directory to save results')
    parser.add_argument('--overlay', type=bool, default=True, help='Save overlay image')
    return parser.parse_args()

def load_image(image_path):
    image = Image.open(image_path).convert('RGB')
    transform = T.Compose([
        #T.Resize((224, 224)),
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406],
                    std=[0.229, 0.224, 0.225])
    ])
    return transform(image).unsqueeze(0), image  # tensor, PIL image

#æŠŠç±»åˆ«mask â” å½©è‰²å›¾ (ç”¨VOC_COLORMAP)
def mask_to_color(mask):
    color_mask = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)
    for label in range(len(Cam_COLORMAP)):
        color_mask[mask == label] = Cam_COLORMAP[label]
    return color_mask

def save_mask(mask, save_path):
    color_mask = mask_to_color(mask)
    Image.fromarray(color_mask).save(save_path)

def overlay_mask_on_image(raw_image, mask, alpha=0.6):
    mask_color = mask_to_color(mask)
    mask_pil = Image.fromarray(mask_color)
    mask_pil = mask_pil.resize(raw_image.size, resample=Image.NEAREST)
    blended = Image.blend(raw_image, mask_pil, alpha=alpha)
    return blended

def predict(args):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Device: {device}")

    # æ¨¡å‹
    model = get_model(num_classes=args.num_classes)
    checkpoint = torch.load(args.checkpoint, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()

    os.makedirs(args.save_dir, exist_ok=True)

    # é¢„æµ‹å•å¼  or æ‰¹é‡
    if os.path.isdir(args.image_dir):
        image_list = [os.path.join(args.image_dir, f) for f in os.listdir(args.image_dir) if f.lower().endswith(('jpg', 'png', 'jpeg'))]
    else:
        image_list = [args.image]

    print(f"ğŸ” Found {len(image_list)} images to predict.")

    for img_path in tqdm(image_list):
        img_tensor, raw_img = load_image(img_path)
        img_tensor = img_tensor.to(device)

        with torch.no_grad():
            output = model(img_tensor)
            pred = torch.argmax(output.squeeze(), dim=0).cpu().numpy()

        # ä¿å­˜ mask
        base_name = os.path.basename(img_path).split('.')[0]
        mask_save_path = os.path.join(args.save_dir, f"{base_name}_mask.png")
        save_mask(pred, mask_save_path)

        # ä¿å­˜ overlay
        if args.overlay:
            overlay_img = overlay_mask_on_image(raw_img, pred)
            overlay_save_path = os.path.join(args.save_dir, f"{base_name}_overlay.png")
            overlay_img.save(overlay_save_path)

        print(f"Saved: {mask_save_path}")
        if args.overlay:
            print(f"Saved overlay: {overlay_save_path}")

    print("ğŸ‰ Prediction done!")

if __name__ == '__main__':
    args = parse_arguments()
    predict(args)

</code></pre>
<h1 id="æ•ˆæœå›¾">æ•ˆæœå›¾</h1>
<p>æˆ‘å°±è®­ç»ƒäº†50ä¸ªepochï¼Œæ•ˆæœè¿˜è¡Œï¼Œæ•ˆæœå›¾å¦‚ä¸‹æ‰€ç¤º</p>
<p><img src="https://gitee.com/fouen/image/raw/master/image/20250603100146559.png" alt="image-20250603100146319" loading="lazy"></p>
<h1 id="ç»“è¯­">ç»“è¯­</h1>
<p>å¸Œæœ›ä¸Šåˆ—æ‰€è¿°å†…å®¹å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œå¦‚æœæœ‰é”™è¯¯çš„åœ°æ–¹æ¬¢è¿å¤§å®¶æ‰¹è¯„æŒ‡æ­£ï¼</p>
<p>å¹¶ä¸”å¦‚æœå¯ä»¥çš„è¯å¸Œæœ›å¤§å®¶èƒ½å¤Ÿä¸‰è¿é¼“åŠ±ä¸€ä¸‹ï¼Œè°¢è°¢å¤§å®¶ï¼</p>
<p>å¦‚æœä½ è§‰å¾—è®²çš„è¿˜ä¸é”™æƒ³è½¬è½½ï¼Œå¯ä»¥ç›´æ¥è½¬è½½ï¼Œä¸è¿‡éº»çƒ¦æŒ‡å‡ºæœ¬æ–‡æ¥æºå‡ºå¤„å³å¯ï¼Œè°¢è°¢ï¼</p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.29309732594560184" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-06-04 13:29">2025-06-04 09:27</span>&nbsp;
<a href="https://www.cnblogs.com/carpell">carpell</a>&nbsp;
é˜…è¯»(<span id="post_view_count">69</span>)&nbsp;
è¯„è®º(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18909515);return false;">æ”¶è—</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18909515', targetLink: 'https://www.cnblogs.com/carpell/p/18909515', title: 'ã€è¯­ä¹‰åˆ†å‰²ä¸“æ ã€‘2ï¼šU-netå®æˆ˜ç¯‡(é™„ä¸Šå®Œæ•´å¯è¿è¡Œçš„ä»£ç pytorch)' })">ä¸¾æŠ¥</a>
</div>
        