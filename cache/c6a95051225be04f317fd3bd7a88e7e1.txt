
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/myleaf/p/18727733" title="发布于 2025-02-20 20:50">
    <span role="heading" aria-level="2">【H2O系列】包括人形机器人WBC相关论文小结</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h1 id="1-前言">1. 前言</h1>
<p>这篇博客主要用于记录包括人形机器人WBC或locomotion相关论文小结。<br>
一方面便于日后自己的温故学习，另一方面也便于大家的学习和交流。<br>
如有不对之处，欢迎评论区指出错误，你我共同进步学习！<br>
PS：主要是备忘，不然看过就忘了。。。（汗</p>
<h1 id="2-正文">2. 正文</h1>
<p>先看数据集或者说动捕数据：</p>
<h2 id="21-smpl-skinned-multi-person-linear-smpl-model">2.1 SMPL Skinned Multi-Person Linear (SMPL) Model</h2>
<p>用数据来构建人体和mesh的，在这个上，后面的AMASS有所拓展，引入了motions，<s>赋予了尸体灵魂（bushi</s><br>
详细查看：<a href="https://blog.csdn.net/IanYue/article/details/127206953" target="_blank" rel="noopener nofollow">https://blog.csdn.net/IanYue/article/details/127206953</a><br>
<strong>一个3D人体mesh由6890个网格顶点和23个关节点组成</strong></p>
<p>Skinned表示这个模型不仅仅是骨架点了，其是有蒙皮的，其蒙皮通过3D mesh表示，3D mesh如图所示，指的是在立体空间里面用三个点表示一个面，可以视为是对真实几何的采样，其中采样的点越多，3D mesh就越密，建模的精确度就越高（这里的由三个点组成的面称之为三角面片）。Multi-person表示的是这个模型是可以表示不同的人的，是通用的。Linear就很容易理解了，其表示人体的不同姿态或者不同升高，胖瘦（我们都称之为形状shape）是一个线性的过程，是可以控制和解释的（线性系统是可以解释和易于控制的）<br>
<img src="https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250207133903358-427516087.png" alt="image" loading="lazy"></p>
<h3 id="211姿态参数">2.1.1姿态参数</h3>
<p>pose parameters，含有<span class="math inline">\(24\times3\)</span>个参数，24个点，每个点含有相对于父节点的axis-angle 表达，也就是相对父节点的旋转角度：<br>
<img src="https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250207134113886-446633494.png" alt="image" loading="lazy"><br>
<img src="https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250207135104585-234034865.png" alt="image" loading="lazy"><br>
影响动作姿势：θ，72个参数，后69个值在-1到1之间，3*23 + 3，影响23个关节点+1个root orientation的旋转。前三个控制root orientation，后面每连续三个控制一个关节点</p>
<h3 id="212-形状参数">2.1.2 形状参数</h3>
<p>一组形状参数有着10个维度的数值去描述一个人的形状，每一个维度的值都可以解释为人体形状的某个指标，比如高矮，胖瘦等。<br>
<img src="https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250207135041056-733097786.png" alt="image" loading="lazy"></p>
<p><img src="https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250207145605662-156476251.png" alt="image" loading="lazy"></p>
<h2 id="22-amass-archive-of-motion-capture-as-surface-shapes">2.2 AMASS: Archive of Motion Capture as Surface Shapes</h2>
<p>2019.4.5</p>
<blockquote>
<p>介绍了AMASS，这是一个庞大而多样的人体运动数据库，通过在一个共同的框架和参数化中表示它们，统一了15种不同的基于光学标记的运动捕捉数据集</p>
</blockquote>
<ol>
<li>首先，我们开发了一种从标准动作捕捉（mocap）标记数据中准确恢复运动中的人的形状和姿势的方法。</li>
<li>创建最大的公共人类运动数据库，使机器学习能够应用于动画和计算机视觉</li>
</ol>
<p>只要知道这个是一个包含人体motions的数据集就好了，里面会有很多的dataset提供给我们下载。</p>
<p>见我的另一篇博客：<a href="https://i.cnblogs.com/posts/edit;postId=18715051#postBody" target="_blank">https://i.cnblogs.com/posts/edit;postId=18715051#postBody</a></p>
<p>何泰然大佬的连续三篇工作：</p>
<h2 id="23-h2olearning-human-to-humanoid-real-time-whole-body-teleoperation">2.3 H2O：Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation</h2>
<p>IROS 2024.3.7<br>
CMU</p>
<blockquote>
<p>何泰然大佬，b站也很有名！</p>
</blockquote>
<p>个人总结主要贡献点包括：<br>
<img src="https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250207141509132-797736365.png" alt="image" loading="lazy"></p>
<h3 id="231-retargetinga">2.3.1 Retargeting(a)</h3>
<p>将机器人的关节点映射，和SMPL的数据集的人体模型作距离的剃度下降，以最小化二者间的距离，这时人体的shape参数就需要改变了：<br>
<img src="https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250207140934131-1816468315.png" alt="image" loading="lazy"><br>
shape-fitted的过程<br>
<img src="https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250207141112253-565904694.png" alt="image" loading="lazy"><br>
作者还对比了一下如果不这么做的结果：如果直接把人体的关节点般过去，就会导致机器人的脚部距离过小 ，走路可能会绊倒。<br>
<img src="https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250207141154543-1050745446.png" alt="image" loading="lazy"><br>
有一些动作比较特殊，机器人完成不了，所以需要去除：<br>
<img src="https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250207142651510-630932729.png" alt="image" loading="lazy"></p>
<h3 id="232-sim-to-data">2.3.2 Sim-to-data</h3>
<p>将2.1.1部分的得到的运动机器人数据集（H1）的作为输入，输入到ISSAC GYM中进行训练，让机器人可以跟踪数据点进行模仿学习</p>
<ul>
<li>本体状态：</li>
</ul>
<p><img src="https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250207143113204-1239813148.png" alt="image" loading="lazy"></p>
<ul>
<li>目标状态：</li>
</ul>
<p><img src="https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250207143145050-609815498.png" alt="image" loading="lazy"></p>
<ul>
<li>奖励函数：</li>
</ul>
<p><img src="https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250207143633153-1886385433.png" alt="image" loading="lazy"></p>
<h3 id="233-real-time-teleoperation">2.3.3 Real-time Teleoperation</h3>
<p>通过RGB相机输入人体动作，通过<strong>HybrIK</strong>进行3D人体姿态估计。</p>
<blockquote>
<p>说到<strong>HybrIK</strong>，这里进行简要的记录：<br>
之前一直比较好奇他是如何通过RGB相机得到人体的3D姿态分析的：<br>
<img src="https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250207145126811-637615164.png" alt="image" loading="lazy"><br>
关于HybrIK，详细查看L：<a href="https://zhuanlan.zhihu.com/p/461640390" target="_blank" rel="noopener nofollow">https://zhuanlan.zhihu.com/p/461640390</a></p>
</blockquote>
<h2 id="24-omnih2o-universal-and-dexterous-human-tohumanoid-whole-body-teleoperation-and-learnin">2.4 OmniH2O: Universal and Dexterous Human-toHumanoid Whole-Body Teleoperation and Learnin</h2>
<p>2024.6.13<br>
目前我主要也在看这篇工作的开源代码。。。。。</p>
<h2 id="241-abstarct">2.4.1 abstarct</h2>
<p>较之前进行了多种控制方式的扩展，比如：<br>
<img src="https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250207152052918-1899900416.png" alt="image" loading="lazy"><br>
还公布了一个数据集：OmniH2O-6</p>
<h2 id="242-从pipeline得到的对比">2.4.2 从pipeline得到的对比</h2>
<p><img src="https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250207153013563-1391564072.png" alt="image" loading="lazy"><br>
第一个部分几乎没有变化，还是retargeting</p>
<blockquote>
<p>(a) OmniH2O retargets large-scale human motions and filters out infeasible motions for humanoids.</p>
</blockquote>
<p>第二个部分采用了模仿学习，先利用特权观测值训练一轮，然后去掉特权观测值，利用之前的几组历史本体观测值训练得到sim2real的policy网络</p>
<blockquote>
<p>(b) Our sim-to-real policy is distilled through supervised learning from an RL-trained teacher policy using<br>
privileged information.</p>
</blockquote>
<p>第三个部分区别在于，这里又拓展了遥操作的多样性，versital</p>
<blockquote>
<p>(c) The universal design of OmniH2O supports versatile human control interfaces<br>
including VR headset, RGB camera, language, etc. Our system also supports to be controlled by autonomous<br>
agents like GPT-4o or imitation learning policy trained using our dataset collected via teleoperation.</p>
</blockquote>
<p>也就是先用pre obs训一个教师网络，然后再蒸馏给只能看到历史数据的学生网络。</p>
<h2 id="25-hover-versatile-neural-whole-body-controller-for-humanoid-robot">2.5 HOVER: Versatile Neural Whole-Body Controller for Humanoid Robot</h2>
<p>HOVER (Humanoid Versatile Controller）<br>
输入的<br>
<img src="https://img2024.cnblogs.com/blog/3481742/202501/3481742-20250123104036015-1929957410.png" alt="image" loading="lazy"><br>
蒸馏的结构：<br>
<img src="https://img2024.cnblogs.com/blog/3481742/202501/3481742-20250123104003478-35896956.png" alt="image" loading="lazy"><br>
融合了很多的模态输入，引入了掩码。</p>
<h2 id="26-humanplus">2.6 HumanPlus</h2>
<p><a href="https://blog.csdn.net/v_JULY_v/article/details/139702814" title="大佬的解读博客" target="_blank" rel="noopener nofollow">大佬的解读博客</a><br>
Best Paper Award Finalist (top 6) at CoRL 2024<br>
Stanford<br>
<img src="https://img2024.cnblogs.com/blog/3481742/202501/3481742-20250122194058200-1411605501.png" alt="image" loading="lazy"><br>
<a href="https://humanoid-ai.github.io/" target="_blank" rel="noopener nofollow">https://humanoid-ai.github.io/</a><br>
读起来不象正经的科研论文的格式，比较奇怪。。。。。。不过代码很是简洁，就是issac的那套框架改的。<br>
HumanPlus的全栈人型机器人<br>
主要贡献点</p>
<blockquote>
<p>1、一个实时影子系统，允许人类操作员使用单个RGB相机和Humanoid Shadowing Transformer(简称HST)来全身控制人形机器人，该HST是一种low-level策略，基于大量的模拟人体运动的数据进行训练<br>
2、人形模仿Transformer，本质就是模仿学习算法，能够通过40次演示高效学习：双目感知和高自由度控制</p>
</blockquote>
<p>通过影子跟踪模仿施教：具体而言，通过使用上面收集的数据，然后执行监督行为克隆，并使用自我中心视觉训练技能策略，使人形机器人(33自由度、180cm高)通过模仿人类技能自主完成不同的任务<br>
最终，机器人自主完成了穿鞋、站立行走、从仓库货架卸载物品、折叠运动衫、重新排列物品、打字以及向另一台机器人打招呼等任务</p>
<h2 id="261-简单总结下">2.6.1 简单总结下</h2>
<p><code>HST</code>：通过AMASS数据集实现能够shadow人体数据集的动作，使用HST也可以用一个单纯的RGB相机来进行shadow，不过前面训练的shadow是一个低级low level的版本，后面人类的数较有点大模型的微调的那个感觉(bushi)<br>
<code>HIT</code>：机器人可以自主学习技能，通过上面训练的数据，然后执行监督学习进行克隆，并使用以自我为中心的视觉技能策略，是机器人通过模仿人类动作自主完成策略。<br>
<code>代码开源了，连接已经贴上了</code></p>
<h2 id="27-expressive-whole-body-control-for-humanoid-robots">2.7 Expressive Whole-Body Control for Humanoid Robots</h2>
<p><a href="https://expressive-humanoid.github.io/" target="_blank" rel="noopener nofollow">https://expressive-humanoid.github.io/</a><br>
<img src="https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250214172013873-913778650.png" alt="image" loading="lazy"></p>
<p>上下半身解耦，下肢主要是locomotion的任务，上肢是模仿学习的任务<br>
有相关的解读，我这里就补充下：<br>
<a href="https://zhuanlan.zhihu.com/p/684655285" title="解读博客--知乎" target="_blank" rel="noopener nofollow">解读博客--知乎</a></p>
<h2 id="28-asap-aligning-simulation-and-real-world-physics-for-learning-agile-humanoid-whole-body-skills">2.8 ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills</h2>
<p>网站:<a href="https://agile.human2humanoid.com/" target="_blank" rel="noopener nofollow">https://agile.human2humanoid.com/</a><br>
还是何Tairan大佬的文章<br>
下面是整篇文章的pipeline:<br>
<img src="https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250220195006029-416694161.png" alt="image" loading="lazy"></p>
<h3 id="281-尝试解读">2.8.1 尝试解读</h3>
<p>奖励函数有3种类型：<br>
<img src="https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250220201451694-1687079836.png" alt="image" loading="lazy"><br>
为了增强鲁棒性，使用了DR：<br>
<img src="https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250220201606275-971649006.png" alt="image" loading="lazy"></p>
<h3 id="282-总结">2.8.2 总结</h3>
<p>大概浏览了下整体的可结构，我可以得到如下的总结：</p>
<h4 id="阶段1">阶段1</h4>
<ul>
<li>首先通过video数据集得到动作数据，然后retargeted到机器人身上，请注意，这时是不考虑物理引擎的，也居士机器人肯能会飞在控制模仿动作。</li>
<li>然后在虚拟环境中训练一个policy，让机器人学会这些motion。</li>
<li>然后将这个policy部署到实机上，roll out出来一个trajectories，得到一系列的&lt;s,a&gt;数据。</li>
<li>让仿真器得到的下一时刻的state和真实环境的state作loss，这算一个奖励函数了。</li>
</ul>
<h4 id="阶段2">阶段2</h4>
<ul>
<li>训练一个delta action网络根据roll out出来的trajectories训练，也就是基于real环境的训练，这是训练的是<span class="math inline">\(\Delta\)</span>A,经过仿真器得到下一时刻的state，然后以此类推。<br>
我其实感觉就是在真实环境的action上进行了修正<br>
<img src="https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250220204213458-874315341.png" alt="image" loading="lazy"></li>
</ul>
<h4 id="阶段3">阶段3</h4>
<ul>
<li>当delta action网络训练好后，冻结这个网络，训练一个policy，能够根据一个输入s0的状态，推断出action0，其实这个地方用的就是PPO的强化学习算法，然后将aciton送到delte action网络里面，输出delta action，加到输入的aciton上面，经过仿真器输出state,这个是下一时刻的，以此类推<br>
<img src="https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250220204643433-1816396177.png" alt="image" loading="lazy"></li>
</ul>
<h4 id="阶段4">阶段4</h4>
<ul>
<li>在真实环境中部署这个policy，</li>
</ul>
<h1 id="3-后记">3. 后记</h1>
<p>这篇博客暂时记录到这里，日后我会继续补充。</p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.06209492074884259" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-02-20 20:51">2025-02-20 20:50</span>&nbsp;
<a href="https://www.cnblogs.com/myleaf">泪水下的笑靥</a>&nbsp;
阅读(<span id="post_view_count">10</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18727733" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18727733);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18727733', targetLink: 'https://www.cnblogs.com/myleaf/p/18727733', title: '【H2O系列】包括人形机器人WBC相关论文小结' })">举报</a>
</div>
        