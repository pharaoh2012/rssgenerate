
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/xiao987334176/p/18855424" title="发布于 2025-04-30 14:47">
    <span role="heading" aria-level="2">LM Studio本地部署Qwen3</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body blogpost-body-html">
<h1>一、概述</h1>
<p>LM Studio 是一款桌面应用程序，用于在您的计算机本地开发和实验 LLM。</p>
<p>官方地址：<a href="https://lmstudio.ai" target="_blank" rel="noopener nofollow">https://lmstudio.ai</a></p>
<p>官方中文地址：<a href="https://lm-studio.cn" target="_blank" rel="noopener nofollow">https://lm-studio.cn</a></p>
<h2>主要功能</h2>
<ul>
<li>用于运行本地 LLM 的桌面应用程序</li>
<li>熟悉的聊天界面</li>
<li>搜索和下载功能（通过 Hugging Face 🤗）</li>
<li>可以监听类似 OpenAI 端点的本地服务器</li>
<li>用于管理本地模型和配置的系统</li>
</ul>
<p><br><span style="color: rgba(255, 0, 0, 1)"><strong>重点来了，LM Studio可以本地运行Hugging Face上面的所有模型，只要Hugging Face有就行。</strong></span></p>
<p><span style="color: rgba(255, 0, 0, 1)"><strong>即使国内网络，依然可以下载任意模型，速度也很快。</strong></span></p>
<p>Hugging Face地址：<a href="https://huggingface.co" target="_blank" rel="noopener nofollow">https://huggingface.co</a></p>
<p>&nbsp;</p>
<p>Ollama本地我也运行过，就是有一个很大缺点，模型不够丰富，因为很多模型不能适配Ollama。</p>
<p>目前模型最丰富的网站，还是Hugging Face，更新速度非常快。比如Qwen3凌晨发布，第二天，Hugging Face就可以看到了。</p>
<p>&nbsp;</p>
<h2>二、安装</h2>
<p>访问中文网页：https://lm-studio.cn，下载客户端，直接下一步，下一步安装好，就可以了。</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202504/1341090-20250430142130805-943457601.png" alt=""></p>
<h1>三、使用</h1>
<p>中文文档：<a href="https://lm-studio.cn/docs/app" target="_blank" rel="noopener nofollow">https://lm-studio.cn/docs/app</a></p>
<h2>设置语言</h2>
<p>打开客户端，点击右下角的设置按钮</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202504/1341090-20250430142307560-541738772.png" alt="" loading="lazy"></p>
<p>&nbsp;设置为简体中文</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202504/1341090-20250430142328821-402248463.png" alt="" loading="lazy"></p>
<h2>模型目录</h2>
<p>默认目录是C盘，由于C盘空间太小了，所以需要设置为其他盘，比如： E盘</p>
<p>手动创建目录 E:\lmstudio\model<br>然后点击按钮，设置一下即可。</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202504/1341090-20250430142623436-1991917869.png" alt="" loading="lazy"></p>
<h2>模型安装</h2>
<p>默认没有模型，所以需要安装一个，这里以最火的Qwen3为例子。</p>
<p>点击搜索按钮</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202504/1341090-20250430142924449-1864591044.png" alt="" loading="lazy"></p>
<p>&nbsp;</p>
<p>搜索模型，比如：Qwen3-4b</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202504/1341090-20250430142443880-1008939140.png" alt="" loading="lazy"></p>
<p>&nbsp;点击下载</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202504/1341090-20250430142513182-1181751208.png" alt="" loading="lazy"></p>
<p>下载速度还可以，17MB/s</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202504/1341090-20250430153908015-2089792070.png" alt="" loading="lazy"></p>
<p>下载完成后，不要着急运行。</p>
<p>&nbsp;</p>
<p><span style="color: rgba(255, 0, 0, 1)"><strong>注意：LM Studio搜索展示的模型，会自动根据你的电脑配置，显示是否可以正常运行。</strong></span></p>
<p>比如：Qwen3 235B A22B，就会有提示。</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202504/1341090-20250430152432021-981186719.png" alt="" loading="lazy"></p>
<h2>运行模型</h2>
<p>点击按钮，选择加载模型，Qwen3-4b</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202504/1341090-20250430143022293-1921063934.png" alt="" loading="lazy"></p>
<p>&nbsp;点击设置，开启网络和cors</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202504/1341090-20250430143132737-118061092.png" alt="" loading="lazy"></p>
<p><strong>端口暴露</strong>：默认使用 1234 端口，可自定义修改(需注意端口冲突问题)</p>
<p><strong>跨域支持</strong>：启用 CORS 后，可对接网页应用或其他客户端工具</p>
<p><strong>局域网访问</strong>：勾选“在局域网内提供服务”选项后，服务器会监听所有网络接口(0.0.0.0)，允许其他设备通过 IP 地址访问</p>
<p>&nbsp;</p>
<p>&nbsp;右边会展示模型相关信息</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202504/1341090-20250430143242410-1217268430.png" alt="" loading="lazy"></p>
<p>API密钥是：qwen3-4b</p>
<p>注意：这里会进行监听本机ip的1234端口。</p>
<p>&nbsp;</p>
<p><span style="color: rgba(255, 0, 0, 1)"><strong>重点提醒一下，这里最好是内网访问，不要用公网暴露端口，否则很容易受到攻击。</strong></span></p>
<p>&nbsp;</p>
<p>访问api地址：http://127.0.0.1:1234/v1/models</p>
<p>效果如下：</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202504/1341090-20250430143316480-1634705021.png" alt="" loading="lazy"></p>
<h2>测试模型</h2>
<p>回到首页，选择模型，提问</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202504/1341090-20250430143634151-1521067302.png" alt="" loading="lazy"></p>
<h1>四、Cherry Studio测试</h1>
<p>将Cherry Studio更新到最新版本</p>
<p>点击模型服务--&gt;LM Studio</p>
<p>输入API密钥</p>
<p>添加模型：qwen3-4b</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202504/1341090-20250430143828887-1940704030.png" alt="" loading="lazy"></p>
<p>点击检测</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202504/1341090-20250430144117256-596717620.png" alt="" loading="lazy"></p>
<p>&nbsp;检查模型</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202504/1341090-20250430144059767-546328739.png" alt="" loading="lazy"></p>
<p>&nbsp;提示连接成功，就可以了</p>
<p>&nbsp;</p>
<p>回到首页，选择默认助手，选择模型</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202504/1341090-20250430144343880-1699850618.png" alt="" loading="lazy"></p>
<p>&nbsp;</p>
<p>LM Studio基本使用，到这里就结束了，使用还是挺简单的。</p>
<p>Qwen3模型都支持MCP调用，这点挺好的。</p>
<p>&nbsp;</p>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.029320770310185186" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-04-30 15:40">2025-04-30 14:47</span>&nbsp;
<a href="https://www.cnblogs.com/xiao987334176">肖祥</a>&nbsp;
阅读(<span id="post_view_count">99</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18855424);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18855424', targetLink: 'https://www.cnblogs.com/xiao987334176/p/18855424', title: 'LM Studio本地部署Qwen3' })">举报</a>
</div>
        