
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/zhangmingcheng/p/19012622" title="发布于 2025-07-30 17:48">
    <span role="heading" aria-level="2">容器云网络故障深度排查：POD访问SVC超时全解析</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body blogpost-body-html">
<h1>1. 故障背景</h1>
<p>　　单节点Kubernetes集群升级操作系统内核版本、NVIDIA驱动与CUDA后重启服务器，引发容器云管理界面访问异常。核心环境如下：</p>
<ul>
<li>​​组件版本​​：
<ul>
<li>Ubuntu&nbsp;5.19.0-40-generic</li>
<li>Kubernetes 1.21.5, Docker 27.5.1</li>
<li>网络插件：Flannel（Pod网段 10.233.64.0/18、Svc网段10.233.0.0/18）</li>
<li>域名解析：CoreDNS + NodeLocalDNS</li>
<li>代理模式：Kube-Proxy ipvs模式</li>
</ul>
</li>
<li>​​关键现象​​：<br>Pod可互访​​Pod IP​​，宿主机可访问​​Service IP​​与​​Pod IP​​，但Pod内部访问​​Service IP超时​​（如 10.233.36.146:6379）。</li>


</ul>
<h1>2. 问题排查</h1>
<h2>阶段一：基础状态检查​</h2>
<p>（1）防火墙确认：ufw status 显示inactive（排除防火墙拦截）</p>
<p>（2）核心组件状态：</p>
<div class="cnblogs_Highlighter">
<pre class="brush:bash;gutter:true;">kubectl get pods -n=kube-system  # 所有组件Running
kubectl get pods -n=容器云核心组件-system  # 发现apiserver报错
</pre>
</div>
<p>（3）日志线索定位：</p>
<div class="cnblogs_Highlighter">
<pre class="brush:bash;gutter:true;">kubectl logs -f -n=容器云核心组件-system apiserver-68654cdc5-gg88b
</pre>
</div>
<p>关键报错：</p>
<div class="cnblogs_Highlighter">
<pre class="brush:bash;gutter:true;">Error: failed to connect to redis service, please check redis status, error: dial tcp 10.233.36.146:6379: i/o timeout
2025/07/29 17:22:08 failed to connect to redis service, please check redis status, error: dial tcp 10.233.36.146:6379: i/o timeout
</pre>
</div>
<p>结论：DNS解析正常（域名→Service IP），但Service流量不通。</p>
<p>（4）排查redis服务运行情况：</p>
<div class="cnblogs_Highlighter">
<pre class="brush:bash;gutter:true;">kubectl get pods -n=容器云公共组件-system |grrp redis
</pre>
</div>
<p>Redis容器运行状态正常，容器日志也正常，将Redis Svc改成NodePort模式，通过本地Redis客户端工具也能正常连接Redis服务，说明Redis服务是正常的。</p>
<h2>阶段二：网络分层验证</h2>
<ul>
<li>客户端: bubybox容器或者宿主机</li>
<li>服务端: 集群里正常运行的Nginx服务</li>
</ul>
<table>
<thead>
<tr><th>测试类型​​</th><th>​​操作命令​​</th><th>​​结果​​</th><th>​​推断​​</th></tr>
</thead>
<tbody>
<tr>
<td>​​Pod→Pod IP​​</td>
<td><code class="hyc-common-markdown__code__inline">kubectl exec -it busybox -- telnet &lt;PodIP&gt; 80</code></td>
<td>✅ 成功</td>
<td>Flannel底层网络正常
<div class="hyc-common-markdown__ref-list">&nbsp;</div>
</td>
</tr>
<tr>
<td>​​Pod→Service IP​​</td>
<td><code class="hyc-common-markdown__code__inline">kubectl exec -it busybox -- telnet &lt;SvcIP&gt; 80</code></td>
<td>❌ 超时</td>
<td>Service层异常</td>
</tr>
<tr>
<td>​​宿主机→Service​​</td>
<td><code class="hyc-common-markdown__code__inline">telnet &lt;SvcIP&gt; 80</code></td>
<td>✅ 成功</td>
<td>kube-proxy规则对宿主机有效</td>
</tr>
</tbody>
</table>
<p>关键矛盾点​​：</p>
<ul>
<li>IPVS规则存在（ipvsadm -Ln | grep &lt;SvcIP&gt; 显示正常DNAT）</li>
<li>但Pod流量无法穿透Service</li>
</ul>
<p>注意： 如果使用的是iptables规则使用iptables-save | grep &lt;service-name&gt;命令排查Kube-Proxy组件是否生成了SvcIP转PodIP规则。</p>
<h2>阶段三：抓包与内核层深挖​</h2>
<p>（1）​​抓包分析（Pod侧）​​</p>
<p>同时打开2个shell，都进入busybox容器内部，其中一个shell执行tcpdump命令进行抓包，另一个shell执行telnet&nbsp;nginx_svcIp（10.233.42.160） 80命令。</p>
<div class="cnblogs_Highlighter">
<pre class="brush:bash;gutter:true;">tcpdump -i any 'host 10.233.42.160 and tcp port 80' -w svc_capture.pcap
</pre>
</div>
<p>抓包结束后使用kubectl cp或者docker cp命令将抓的包拷贝到宿主机上再使用sftp工具下载到本地用wireshark分析，发现源地址到不了目标svcIP。</p>
<p><img alt="image" width="901" height="123" loading="lazy" data-src="https://img2024.cnblogs.com/blog/624219/202507/624219-20250730144707396-1468750839.png" class="lazyload"></p>
<p>结论：</p>
<p>持续发送SYN包 → ​​零响应​​（无RST/SYN-ACK），<strong>排除目标拒绝，指向​​中间层拦截​​。</strong></p>
<blockquote>
<p>​​源地址​​：Pod IP 10.233.64.250（发送方）</p>
<p>​​目标地址​​：Service IP 10.233.42.160</p>
<p>​​行为​​：</p>
<ol>
<li>发送端发起TCP SYN请求（序号1）</li>
<li>连续​​6次重传SYN包​​（序号2-7），时间间隔指数级增长（1s → 3s → 7s → 15s → 31s → 64s）</li>
</ol>
<p>​​关键缺失​​：​​零响应​​：无SYN-ACK（目标端确认）、无RST（目标拒绝）</p>
</blockquote>
<p>&nbsp;（2）​​抓包分析（服务器侧）​​</p>
<p>同时打开2个shell，一个直接在服务器上执行tcpdump命令进行抓包(直接抓nginx_podIP)，另一个进入busybox容器内部执行telnet&nbsp;nginx_svcIp（10.233.42.160） 80命令。</p>
<div class="cnblogs_Highlighter">
<pre class="brush:bash;gutter:true;">tcpdump -i any 'host 10.233.64.43 and tcp port 80' -w svc_capture.pcap
</pre>
</div>
<p>抓包结束后使用kubectl cp或者docker cp命令将抓的包拷贝到宿主机上再使用sftp工具下载到本地用wireshark分析，根据包信息可以看到pod访问svcIp时也进行了DNAT将svcIP转成podIp了（说明kube-proxy正常），但是源Pod访问不通目标Pod。</p>
<p><img alt="image" width="819" height="529" loading="lazy" data-src="https://img2024.cnblogs.com/blog/624219/202507/624219-20250730145405133-609363486.png" class="lazyload"></p>
<p>（3）内核参数致命错误</p>
<p>这时候怀疑是Conntrack问题，使用sysctl -p命令检查内核参数配置，发现参数加载报错。</p>
<p><img alt="8b96bf6997b136e913d074479c246fe4" loading="lazy" data-src="https://img2024.cnblogs.com/blog/624219/202507/624219-20250730150043680-358593096.png" class="lazyload"></p>
<p>这三个关键内核参数配置没有加载成功，主要是net.bridge.bridge-nf-call-iptables = 1。</p>
<div class="cnblogs_Highlighter">
<pre class="brush:bash;gutter:true;">net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1  
</pre>
</div>
<p>net.bridge.bridge-nf-call-iptables = 1参数失效后果。</p>
<div class="cnblogs_Highlighter">
<pre class="brush:bash;gutter:true;">`net.bridge.bridge-nf-call-iptables=0`  
↓  
网桥流量**绕过iptables规则**  
↓  
kube-proxy的IPVS DNAT规则**失效**</pre>
</div>
<blockquote>
<p>net.bridge.bridge-nf-call-iptables = 1​​</p>
<ol>
<li>​​作用​​：控制通过 Linux 网桥的 ​​IPv4 流量是否经过 iptables 规则链​​（如 FORWARD、INPUT）。</li>
<li>​​启用后（=1）​​：
<ul>
<li>桥接流量（如 Pod 间通信、Service IP 访问）会被提交到 iptables 规则处理。</li>
<li>这是 Kubernetes 的​​必需配置​​，确保 kube-proxy 的 Service 负载均衡规则（DNAT）能生效。</li>
</ul>
</li>
<li>​​禁用后（=0）​​：
<ul>
<li>桥接流量绕过 iptables，导致 Service IP 的 DNAT 规则失效，​​Pod 访问 Service IP 必然失败​​。</li>
</ul>
</li>
</ol>
<p>​​net.bridge.bridge-nf-call-ip6tables = 1​​</p>
<ol>
<li>​​作用​​：控制桥接的 ​​IPv6 流量是否经过 ip6tables 规则链​​。</li>
<li>在 IPv6 环境中需启用，否则 IPv6 Service 无法正常工作。</li>
</ol>
<p>​​net.bridge.bridge-nf-call-arptables = 1</p>
<ol>
<li><code class="hyc-common-markdown__code__inline"></code>作用​​：控制桥接的 ​​ARP 流量是否经过 arptables 规则链​​。</li>
<li>启用后可防止 ARP 欺骗，但对 Service IP 通信影响较小。</li>
</ol></blockquote>
<h1>3. 解决方案：修复内核隔离​</h1>
<h2>步骤一：动态加载模块​</h2>
<p>加载内核模块：</p>
<div class="cnblogs_Highlighter">
<pre class="brush:bash;gutter:true;">modprobe br_netfilter # 临时加载
modprobe nf_conntrack # 临时加载
echo "br_netfilter" &gt;&gt; /etc/modules-load.d/k8s.conf # 永久加载
echo "nf_conntrack" &gt;&gt; /etc/modules-load.d/k8s.conf # 永久加载
</pre>
</div>
<h2>步骤二：永久固化配置​内核参数配置（实际这些参数之前都有，只不过上面那三行加载失败了）</h2>
<div class="cnblogs_Highlighter">
<pre class="brush:bash;gutter:true;">cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt;EOF
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-arptables = 1
net.ipv4.ip_forward = 1
EOF
sysctl -p /etc/sysctl.d/k8s.conf　#执行sysctl -p命令不报错</pre>
</div>
<p><span class="hljs-built_in"><strong>立即生效​​：</strong>Pod访问Service IP恢复</span></p>
<blockquote>
<p><span class="hljs-built_in">安装k8s集群建议内核参数配置：</span></p>
<div class="cnblogs_Highlighter">
<pre class="brush:bash;gutter:true;">net.ipv4.icmp_echo_ignore_broadcasts = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.accept_source_route = 0
net.ipv4.conf.default.accept_source_route = 0
net.ipv4.conf.all.accept_redirects = 0
net.ipv4.conf.default.accept_redirects = 0
net.ipv6.conf.all.accept_redirects = 0
net.ipv6.conf.default.accept_redirects = 0
net.ipv4.conf.all.send_redirects = 0
net.ipv4.conf.default.send_redirects = 0
net.ipv4.icmp_ignore_bogus_error_responses = 1
net.ipv4.conf.all.secure_redirects = 0
net.ipv4.conf.default.secure_redirects = 0
kernel.dmesg_restrict = 1
kernel.sysrq = 0
net.ipv4.tcp_syncookies = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
vm.max_map_count = 262144
vm.swappiness = 1
fs.inotify.max_user_instances = 524288</pre>
</div>
<p>modprobe br_netfilter：​​</p>
<ul>
<li>​​作用​​：动态加载 br_netfilter 内核模块。</li>
<li>​​功能​​：该模块使​​网桥流量经过 Netfilter 框架（iptables/ip6tables）​​，是 Kubernetes 等容器网络的基础。启用后，网桥（如 Docker 的 docker0 或 CNI 网桥）的流量会被 iptables 规则处理，确保 Service DNAT、NodePort 等生效。</li>
<li>​​场景​​：解决同节点 Pod 互访 Service IP 失败问题（因绕过 iptables 导致 DNAT 失效）。</li>
</ul>
<p>​​modprobe ip_conntrack：​​</p>
<ul>
<li>​​作用​​：加载 ip_conntrack 模块（新内核中名为 nf_conntrack）。</li>
<li>​​功能​​：实现​​连接跟踪（Connection Tracking）​​，记录网络连接状态（如 TCP/UDP 会话），是 NAT 和有状态防火墙的核心依赖。例如，Kubernetes Service 的 SNAT/DNAT 依赖此模块。</li>
<li>​​注意​​：新版本内核中该模块已更名为 nf_conntrack，但 modprobe ip_conntrack 会自动加载新模块。</li>
</ul>
<p>​​sysctl -p：​​</p>
<ul>
<li>​​作用​​：重新加载 /etc/sysctl.conf 或 /etc/sysctl.d/*.conf 中的内核参数配置。</li>
<li>​​功能​​：使修改的 net.bridge.bridge-nf-call-iptables、net.ipv4.ip_forward 等参数​​立即生效​​。</li>
</ul>
</blockquote>
<h2>步骤三：残余问题</h2>
<p>向量数据库仍异常 → 定位到​​冗余iptables规则拦截​​：</p>
<p><img alt="img_v3_02om_2e2158f7-df53-41bf-99bd-a7f60cc2ddbg" width="1035" height="43" loading="lazy" data-src="https://img2024.cnblogs.com/blog/624219/202507/624219-20250730151056648-142921292.jpg" class="lazyload">&nbsp;</p>
<div class="cnblogs_Highlighter">
<pre class="brush:bash;gutter:true;">iptables -t filter -L KUBE-EXTERNAL-SERVICES --line-numbers
iptables -t filter -D KUBE-EXTERNAL-SERVICES &lt;行号&gt;  # 删除冲突规则</pre>
</div>
<p><strong>最终状态​​：</strong>所有服务连通性恢复。</p>
<h1>4. 总结</h1>
<table>
<thead>
<tr><th>故障根因​​</th><th>​​防御措施​​</th></tr>
</thead>
<tbody>
<tr>
<td>内核升级后模块未加载</td>
<td>将&nbsp;<code class="hyc-common-markdown__code__inline">br_netfilter</code>、<code class="hyc-common-markdown__code__inline">nf_conntrack</code>&nbsp;加入&nbsp;<code class="hyc-common-markdown__code__inline">/etc/modules-load.d/k8s.conf</code></td>
</tr>
<tr>
<td>桥接流量隔离</td>
<td>部署前验证&nbsp;<code class="hyc-common-markdown__code__inline">sysctl net.bridge.bridge-nf-call-iptables=1</code></td>
</tr>
<tr>
<td>iptables规则残留</td>
<td>定期审计&nbsp;<code class="hyc-common-markdown__code__inline">iptables-save | grep KUBE-</code>&nbsp;清理无效规则</td>
</tr>
<tr>
<td>​​核心启示​​：升级后需全链路测试网络，重点检查 ​​内核参数 → 网桥 → kube-proxy​​ 链条
<div class="hyc-common-markdown__ref-list">&nbsp;</div>
。</td>
<td>&nbsp;</td>
</tr>
</tbody>
</table>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0" data-date-updated="2025-07-30 17:49">2025-07-30 17:48</span>&nbsp;
<a href="https://www.cnblogs.com/zhangmingcheng">人艰不拆_zmc</a>&nbsp;
阅读(<span id="post_view_count">0</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(19012622);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '19012622', targetLink: 'https://www.cnblogs.com/zhangmingcheng/p/19012622', title: '容器云网络故障深度排查：POD访问SVC超时全解析' })">举报</a>
</div>
        