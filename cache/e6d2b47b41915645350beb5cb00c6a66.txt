
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/daimajiangxin/p/18919412" title="发布于 2025-06-08 15:21">
    <span role="heading" aria-level="2">从零开始学Flink：揭开实时计算的神秘面纱</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                    <div id="cnblogs_post_description" style="display: none">
        <img src="https://img2024.cnblogs.com/blog/3365149/202506/3365149-20250608151959061-855532209.png" alt="从零开始学Flink：揭开实时计算的神秘面纱" class="desc_img">
        传统批处理（如Hadoop）像老式火车，必须等所有乘客（数据）到齐才能发车；而流处理（如Flink）如同磁悬浮列车，每个乘客（数据）上车即刻出发。Flink的诞生，让数据从"考古材料"变为"新鲜血液"。
    </div>
<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h2 id="一为什么需要flink">一、为什么需要Flink？</h2>
<p>当你在电商平台秒杀商品时，1毫秒的延迟可能导致交易失败；当自动驾驶汽车遇到障碍物时，10毫秒的计算延迟可能酿成事故。这些场景揭示了一个残酷事实：数据的价值随时间呈指数级衰减。</p>
<p>传统批处理（如Hadoop）像老式火车，必须等所有乘客（数据）到齐才能发车；而流处理（如Flink）如同磁悬浮列车，每个乘客（数据）上车即刻出发。Flink的诞生，让数据从"考古材料"变为"新鲜血液"。</p>
<h2 id="二初识flink">二、初识Flink</h2>
<h3 id="1-定义">1. 定义</h3>
<p>Apache Flink是由德国柏林工业大学于2009年启动的研究项目，2014年进入Apache孵化器，现已成为实时计算领域的事实标准。其核心能力可用一句话概括：对无界和有界数据流进行有状态计算。</p>
<h3 id="2-核心特性">2. 核心特性</h3>
<p>流处理优先：批处理是流处理的特例（有界数据流）<br>
事件时间语义：按数据真实发生时间处理（而非系统接收时间）<br>
精确一次语义：确保计算结果100%准确<br>
亚秒级延迟：处理延迟可控制在毫秒级</p>
<h3 id="3-技术架构">3. 技术架构</h3>
<p>Flink运行时架构包含三个关键角色：</p>
<ul>
<li>JobManager：大脑中枢，负责任务调度与检查点管理</li>
<li>TaskManager：肌肉组织，执行具体计算任务</li>
<li>Dispatcher：网关系统，提供REST接口提交作业</li>
</ul>
<h2 id="三环境搭建">三、环境搭建</h2>
<h3 id="环境要求">环境要求</h3>
<p>​1. ​Windows 10 2004 或更高版本​​（建议使用 Windows 11）<br>
​2. ​已启用 WSL 2​​<br>
3. 存储空间：至少 1GB 可用空间</p>
<h3 id="详细安装步骤">详细安装步骤</h3>
<h4 id="步骤-1启用-wsl">步骤 1：启用 WSL</h4>
<p>在 PowerShell 中以管理员身份运行以下命令：</p>
<pre><code class="language-text">
  # 启用 WSL 功能
  dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart

  # 启用虚拟机平台
  dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart

  # 设置 WSL 2 为默认版本
  wsl --set-default-version 2

  # 重启电脑（必须步骤）

</code></pre>
<h4 id="步骤-2安装-ubuntu">步骤 2：安装 Ubuntu</h4>
<p>​1. 打开 Microsoft Store<br>
​2. 搜索安装 ​​Ubuntu 22.04 LTS​​<br>
3. 启动 Ubuntu 并创建用户名和密码</p>
<h4 id="步骤-3安装-java-17">步骤 3：安装 Java 17</h4>
<p>在 Ubuntu 终端执行：</p>
<pre><code class="language-text">  # 更新软件包列表
  sudo apt update

  # 安装 Java 17
  sudo apt install -y openjdk-17-jdk
  # 设置环境变量
  echo 'export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64' &gt;&gt;  /etc/profile
  echo 'export PATH=$PATH:$JAVA_HOME/bin' &gt;&gt; /etc/profile
  source /etc/profile
  # 验证安装
  java -version
  # 应显示类似：OpenJDK Runtime Environment (build 17.0.14+...)

</code></pre>
<h4 id="步骤-4下载并安装-flink-1201">步骤 4：下载并安装 Flink 1.20.1</h4>
<pre><code class="language-text">  # 下载 Flink
  wget https://archive.apache.org/dist/flink/flink-1.20.1/flink-1.20.1-bin-scala_2.12.tgz

  # 解压安装包
  tar xzf flink-1.20.1-bin-scala_2.12.tgz

  # 移动到安装目录
  sudo mv flink-1.20.1 /opt/flink

  # 设置环境变量

  echo 'export FLINK_HOME=/opt/flink' &gt;&gt;  /etc/profile
  echo 'export PATH=$PATH:$FLINK_HOME/bin' &gt;&gt; /etc/profile
  source /etc/profile

</code></pre>
<h4 id="步骤-5修改内存配置">步骤 5：修改内存配置</h4>
<p>编辑配置文件：</p>
<pre><code class="language-text">vi /opt/flink/conf/conf.yaml
</code></pre>
<p>修改以下关键参数:</p>
<pre><code class="language-text">  jobmanager:
    bind-host: 0.0.0.0
    rpc:
      address: localhost
      port: 6123
    memory:
      process:
        size: 1600m
    execution:
      failover-strategy: region

  taskmanager:
    bind-host: 0.0.0.0
    host: localhost
    numberOfTaskSlots: 2
    memory:
      process:
        size: 2048m
  parallelism:
    default: 2
  
  rest:
    address: localhost
    bind-address: 0.0.0.0
    port: 8081

</code></pre>
<h4 id="步骤-6启动-flink-集群">步骤 6：启动 Flink 集群</h4>
<pre><code class="language-text">
# 启动集群（JobManager + TaskManager）
$FLINK_HOME/bin/start-cluster.sh

# 检查运行状态
jps

</code></pre>
<h4 id="步骤-7访问-web-ui">步骤 7：访问 Web UI</h4>
<p>在 Windows 浏览器中访问：<br>
<a href="http://localhost:8081" target="_blank" rel="noopener nofollow">http://localhost:8081</a></p>
<h2 id="四实战第一个flink程序batchwordcount">四、实战第一个Flink程序：BatchWordCount</h2>
<p>下面将详细介绍如何在Flink环境中创建并运行第一个WordCount程序。这个经典示例将带你从项目创建到代码执行，全面体验Flink开发流程。</p>
<h3 id="项目结构设计">项目结构设计</h3>
<p>采用多模块Gradle项目，结构清晰：</p>
<pre><code class="language-text">  flink-learning/
  ├── build.gradle                 # 根项目构建配置
  ├── settings.gradle              # 多模块配置
  ├── libraries.gradle            # 依赖统一管理
  ├── data/                        # 数据文件夹
  │   ├── input.txt               # 输入文件
  │   └── output.txt              # 输出文件
  └── wordcount/                  # WordCount模块
      ├── build.gradle            # 模块构建配置
      └── src/main/java           # 源代码目录
          └── cn/com/daimajiangxin/flink/wordcount
              └── BatchWordCount.java # 主程序
</code></pre>
<h3 id="核心文件配置">核心文件配置</h3>
<p>详细配置参考代码仓库：<a href="https://gitee.com/daimajiangxin/flink-learning.git" target="_blank" rel="noopener nofollow">https://gitee.com/daimajiangxin/flink-learning.git</a></p>
<h3 id="wordcount代码实现">WordCount代码实现</h3>
<pre><code class="language-text">package cn.com.daimajiangxin.flink.wordcount;

import org.apache.flink.api.common.RuntimeExecutionMode;
import org.apache.flink.api.common.eventtime.WatermarkStrategy;
import org.apache.flink.api.common.functions.FlatMapFunction;
import org.apache.flink.api.common.functions.ReduceFunction;
import org.apache.flink.api.common.typeinfo.TypeHint;
import org.apache.flink.api.common.typeinfo.TypeInformation;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.connector.file.src.FileSource;
import org.apache.flink.connector.file.src.reader.TextLineFormat;
import org.apache.flink.core.fs.Path;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.util.Collector;

import java.nio.charset.StandardCharsets;
import java.time.Duration;
import java.util.Arrays;

public class BatchWordCount {

    public static void main(String[] args) throws Exception {
        // 转换Windows路径格式
        args = convertWindowsPaths(args);
        
        // 参数校验
        if (args.length &lt; 2) {
            System.err.println("Usage: BatchWordCount &lt;input&gt; &lt;output&gt; [--parallelism=N]");
            System.err.println("Example: BatchWordCount input.txt output.txt --parallelism=4");
            System.exit(1);
        }

        final String inputPath = args[0];
        final String outputPath = args[1];
        int parallelism = 1; // 默认并行度
        
        // 1. 创建流批一体执行环境
        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        // 明确指定批处理模式
        env.setRuntimeMode(RuntimeExecutionMode.BATCH);

        // 设置并行度和作业名称
        env.setParallelism(parallelism);
        env.getConfig().enableObjectReuse();

        // 2. 使用最新的FileSource API读取输入数据
        DataStream&lt;String&gt; text = createFileSource(env, inputPath, parallelism);

        // 3. 定义处理逻辑
        SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; counts = text
                .flatMap(new Tokenizer())
                .name("Tokenizer")
                .setParallelism(parallelism)
                .keyBy(value -&gt; value.f0)
                .reduce(new SumReducer())
                .name("SumReducer")
                .setParallelism(parallelism)
                .returns(TypeInformation.of(new TypeHint&lt;Tuple2&lt;String, Integer&gt;&gt;() {}));

        // 4. 输出结果到文件
        counts.writeAsText(outputPath)
                .name("FileSink")
                .setParallelism(1);

        // 5. 执行作业
        try {
            System.out.println("Starting Flink WordCount job...");
            System.out.println("Input path: " + inputPath);
            System.out.println("Output path: " + outputPath);
            System.out.println("Parallelism: " + parallelism);

            env.execute("Flink Batch WordCount Example");
            System.out.println("Job completed successfully!");

        } catch (Exception e) {
            System.err.println("Job execution failed: " + e.getMessage());
            e.printStackTrace();
        }
    }

    // Windows路径转换
    private static String[] convertWindowsPaths(String[] args) {
        if (args.length &gt;= 1) {
            args[0] = "file:///" + args[0]
                .replace("\\", "/")
                .replace(" ", "%20");
        }
        if (args.length &gt;= 2) {
            args[1] = "file:///" + args[1]
                .replace("\\", "/")
                .replace(" ", "%20");
        }
        return args;
    }

    // 创建文件源
    private static DataStream&lt;String&gt; createFileSource(
            StreamExecutionEnvironment env, 
            String path, 
            int parallelism) {
        // 使用file://前缀
        Path filePath = new Path(path);
        
        System.out.println("Loading file from: " + filePath);
        
        TextLineFormat format = new TextLineFormat(StandardCharsets.UTF_8);
        
        FileSource&lt;String&gt; fileSource = FileSource
                .forRecordStreamFormat(format, filePath)
                .build();
        
        WatermarkStrategy&lt;String&gt; watermarkStrategy = WatermarkStrategy
                .&lt;String&gt;forMonotonousTimestamps()
                .withIdleness(Duration.ofSeconds(10));
        
        return env.fromSource(
                fileSource,
                watermarkStrategy,
                "FileSource"
        )
        .name("FileSource")
        .setParallelism(1);
    }

    // 分词器
    public static final class Tokenizer implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; {
        @Override
        public void flatMap(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) {
            // 过滤空行
            if (value == null || value.trim().isEmpty()) return;
            
            // 转换为小写并分割单词
            String[] words = value.toLowerCase().split("\\W+");
            
            for (String word : words) {
                if (!word.isEmpty()) {
                    out.collect(Tuple2.of(word, 1));
                }
            }
        }
    }

    // 累加器
    public static final class SumReducer implements ReduceFunction&lt;Tuple2&lt;String, Integer&gt;&gt; {
        @Override
        public Tuple2&lt;String, Integer&gt; reduce(Tuple2&lt;String, Integer&gt; v1, Tuple2&lt;String, Integer&gt; v2) {
            return Tuple2.of(v1.f0, v1.f1 + v2.f1);
        }
    }
}

</code></pre>
<h3 id="输入文件示例-inputtxt">输入文件示例 (input.txt)</h3>
<p>input.txt参考代码仓库：<a href="https://gitee.com/daimajiangxin/flink-learning.git" target="_blank" rel="noopener nofollow">https://gitee.com/daimajiangxin/flink-learning.git</a></p>
<h3 id="运行flink作业">运行Flink作业</h3>
<p>这里讲述在IDEA中运行刚刚写的BatchWordCount 任务，配置IDEA的APPlication。</p>
<h4 id="vm选项配置">VM选项配置</h4>
<pre><code class="language-text">  --add-exports=java.base/sun.net.util=ALL-UNNAMED
  --add-exports=java.rmi/sun.rmi.registry=ALL-UNNAMED
  --add-exports=jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED
  --add-exports=jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED
  --add-exports=jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED
  --add-exports=jdk.compiler/com.sun.tools.javac.tree=ALL-UNNAMED
  --add-exports=jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED
  --add-exports=java.security.jgss/sun.security.krb5=ALL-UNNAMED
  --add-opens=java.base/java.lang=ALL-UNNAMED
  --add-opens=java.base/java.net=ALL-UNNAMED
  --add-opens=java.base/java.io=ALL-UNNAMED
  --add-opens=java.base/java.nio=ALL-UNNAMED
  --add-opens=java.base/sun.nio.ch=ALL-UNNAMED
  --add-opens=java.base/java.lang.reflect=ALL-UNNAMED
  --add-opens=java.base/java.text=ALL-UNNAMED
  --add-opens=java.base/java.time=ALL-UNNAMED
  --add-opens=java.base/java.util=ALL-UNNAMED
  --add-opens=java.base/java.util.concurrent=ALL-UNNAMED
  --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED
  --add-opens=java.base/java.util.concurrent.locks=ALL-UNNAMED
</code></pre>
<h4 id="程序参数">程序参数</h4>
<pre><code class="language-text"> 代码放置路径\\flink-learning\\data\\input.txt
 代码放置路径\bigdata\\flink-learning\\data\\output.txt
</code></pre>
<h4 id="运行batchwordcount类">运行BatchWordCount类</h4>
<p>Run 或者Debug BatchWordCount的 APPlication.</p>
<p><img src="https://pic-1258258471.cos.ap-nanjing.myqcloud.com/img/sad/20250608143813.png" alt="20250608143813" loading="lazy"></p>
<h3 id="预期输出">预期输出</h3>
<p>运行成功data目录下会生成output的文件。</p>
<pre><code class="language-text">(processing,1)
(batch,2)
(flink,2)
(hello,2)
</code></pre>
<p><img src="https://pic-1258258471.cos.ap-nanjing.myqcloud.com/img/sad/20250608143152.png" alt="20250608143152" loading="lazy"></p>
<h2 id="五技术要点解析">五、技术要点解析</h2>
<ul>
<li>流批一体API：Flink 1.20+使用StreamExecutionEnvironment统一处理批流</li>
<li>文件源：使用FileSource API</li>
<li>精确一次处理：批处理天然支持Exactly-Once语义</li>
<li>并行度控制：通过setParallelism控制任务并行度</li>
<li>Windows路径适配：统一转换为file:///开头的URI格式</li>
</ul>
<h2 id="六学习路线建议">六、学习路线建议</h2>
<p>完成WordCount后，可逐步探索：</p>
<ul>
<li>实时流处理（SocketWordCount）</li>
<li>状态管理（StatefulProcessing）</li>
<li>事件时间处理（EventTimeProcessing）</li>
<li>窗口计算（TumblingWindow、SlidingWindow）</li>
<li>CEP复杂事件处理</li>
<li>Table API和SQL<br>
通过这个完整的BatchWordCount实例，你已经掌握了Flink项目的搭建、编码和运行全流程。随着Flink在实时数据处理领域的广泛应用，这些技能将成为大数据开发的宝贵资产。</li>
</ul>
<hr>
<p>源文来自：<a href="http://blog.daimajiangxin.com.cn" target="_blank" rel="noopener nofollow">http://blog.daimajiangxin.com.cn</a></p>
<p>源码地址：<a href="https://gitee.com/daimajiangxin/flink-learning" target="_blank" rel="noopener nofollow">https://gitee.com/daimajiangxin/flink-learning</a></p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.04008237643171296" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-06-08 15:21">2025-06-08 15:21</span>&nbsp;
<a href="https://www.cnblogs.com/daimajiangxin">代码匠心</a>&nbsp;
阅读(<span id="post_view_count">23</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18919412);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18919412', targetLink: 'https://www.cnblogs.com/daimajiangxin/p/18919412', title: '从零开始学Flink：揭开实时计算的神秘面纱' })">举报</a>
</div>
        