
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/informatics/p/18704799" title="发布于 2025-02-08 17:26">
    <span role="heading" aria-level="2">五分钟搭建属于你的AI助手：Ollama+DeepSeek+AnythingLLM深度整合教程</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h1 id="作者简介">作者简介</h1>
<ul>
<li>微信公众号：密码应用技术实战</li>
<li>博客园首页：<a href="https://www.cnblogs.com/informatics/" target="_blank">https://www.cnblogs.com/informatics/</a></li>
<li>GitHub地址：<a href="https://github.com/warm3snow" target="_blank" rel="noopener nofollow">https://github.com/warm3snow</a></li>
</ul>
<h1 id="引言">引言</h1>
<p>随着人工智能的发展，大模型的应用越来越广泛，它们不仅能够理解自然语言，还能在特定领域内提供专业化的知识服务。为了满足个人或小型团队对于数据隐私保护的需求，本地部署大模型并搭建私有知识库成为了一种趋势。本文将介绍如何使用Ollama、DeepSeek和AnythingLLM来构建这样一个系统，使您能够在自己的计算机上运行强大的AI模型，并利用其进行高效的知识管理和智能问答。</p>
<p>本地部署大模型并搭建私有知识库的好处：</p>
<ul>
<li>数据隐私和安全性：本地部署可以确保敏感数据不被上传到云端，降低数据泄露的风险，特别适合对数据保密有严格要求的企业和个人。</li>
<li>控制权：用户对自己的数据和模型有更大的控制权，可以根据需要进行定制和优化，而不依赖于第三方服务。</li>
<li>低延迟：本地运行模型可以减少网络延迟，提高响应速度，尤其在需要实时处理的应用场景中尤为重要。</li>
<li>离线访问：本地部署允许在没有互联网连接的情况下使用模型和数据，适合在网络不稳定或无法访问的环境中工作。</li>
<li>成本效益：长期来看，尤其是对于大规模使用，避免了云服务的持续费用，可能会降低总体拥有成本。</li>
<li>定制化：用户可以根据特定需求调整和优化模型，进行更深入的定制，满足特定行业或应用的需求。</li>
<li>合规性：某些行业（如医疗、金融等）对数据存储和处理有严格的法律法规要求，本地部署可以帮助企业更好地遵守这些规定。</li>
</ul>
<p>通过本地部署，用户能够享受更高的灵活性和安全性，同时也能更好地满足特定的业务需求。</p>
<h1 id="简介">简介</h1>
<p>本文将手把手教你用 Ollama、DeepSeek 与 AnythingLLM 搭建超强大本地知识库</p>
<h2 id="deepseekai-领域的实力新星">DeepSeek：AI 领域的实力新星</h2>
<p>DeepSeek专注于人工智能技术的研发，尤其在大语言模型方面成果显著。开源发布的DeepSeek R1模型，具备优秀的逻辑推理能力，能够处理复杂多样的自然语言任务。</p>
<h2 id="ollama本地运行-ai-模型的利器">Ollama：本地运行 AI 模型的利器</h2>
<p>Ollama 是一款致力于让用户在本地轻松运行 AI 模型的工具。它提供了简洁易用的命令行界面，极大降低了本地部署模型的门槛。通过 Ollama，用户可以快速拉取并运行各种主流的大语言模型，无需复杂的配置和高昂的云计算成本。其高效的模型管理系统，能帮助用户方便地切换和使用不同模型，满足多样化的需求。</p>
<h2 id="anythingllm知识整合的智能助手">AnythingLLM：知识整合的智能助手</h2>
<p>AnythingLLM专注于知识管理和问答系统，提供了桌面客户端，方便用户使用；能够从多种不同来源获取数据，包括但不限于文档、网页、数据库等，将这些分散的非结构化或半结构化数据进行有效整合，统一处理为可供分析和查询的格式，为知识的全面性和完整性提供了保障。，AnythingLLM 能够理解用户的问题，并在知识图谱中精准检索答案，为用户提供准确、全面的回答。</p>
<h1 id="部署架构">部署架构</h1>
<p>Ollama+DeepSeek+AnythingLLM搭建本地知识库的整体架构如下：</p>
<div class="mermaid">graph TD
    %% 用户接口层
    subgraph 用户接口层
        UI1[AnythingLLM桌面应用]
    end

    %% 应用服务层
    subgraph 应用服务层
        AS1[AnythingLLM前端服务]
        AS2[Ollama &lt;/br&gt; 模型管理服务]
    end

    %% 数据处理层
    subgraph 数据处理层
        DP1[向量数据库&lt;br&gt;LanceDB, Pinecone等测试与优化]
        DP2[嵌入模型&lt;br&gt;用于文本转换为向量]
    end

    %% 核心模型层
    subgraph 核心模型层
        CM1[DeepSeek R1&lt;br&gt;语言模型]
    end

    %% 外部资源层（可选）
    subgraph 外部资源层/可选
        ER1[外部APIs或其他服务]
    end

    %% 连接关系
    UI1 --&gt;|请求查询或上传文档| AS1
    AS1 --&gt;|调用模型推理| AS2
    AS2 --&gt;|加载模型| CM1
    AS1 --&gt;|查询| DP1
    DP1 --&gt;|检索相关文档片段| AS1
    DP2 --&gt;|生成文本向量| DP1
    AS2 -.-&gt;|可能需要的外部数据| ER1

    %% 注释
    style UI1 fill:#f96,stroke:#333,stroke-width:4px
    style AS1 fill:#bbf,stroke:#f66,stroke-width:2px,stroke-dasharray: 5, 5
    style AS2 fill:#ddf,stroke:#f66,stroke-width:2px,stroke-dasharray: 5, 5
    style DP1 fill:#fff,stroke:#000,stroke-width:4px
    style DP2 fill:#eef,stroke:#000,stroke-width:2px
    style CM1 fill:#ddf,stroke:#000,stroke-width:4px
    style ER1 fill:#ccc,stroke:#000,stroke-width:2px
</div><p>Ollama、DeepSeek、AnythingLLM三者整合，搭建本地知识库步骤：</p>
<ul>
<li>准备工作：确保你的设备满足运行要求，安装好 Ollama、DeepSeek 模型（可通过 Ollama 拉取）以及 AnythingLLM。</li>
<li>数据导入：将你想要纳入知识库的文本数据整理好，导入到 AnythingLLM 中，构建知识图谱。</li>
<li>模型连接：通过 Ollama 运行 DeepSeek 模型，并将其与 AnythingLLM 进行连接，使得 DeepSeek 强大的语言处理能力与 AnythingLLM 的知识管理能力相结合。</li>
<li>测试与优化：输入各种问题进行测试，根据结果对知识库和模型参数进行优化，提升回答的准确性和效率。</li>
</ul>
<h1 id="准备工作">准备工作</h1>
<p>配置要求：</p>
<ul>
<li>操作系统：MacOS、Linux、Windows</li>
<li>硬件要求：</li>
</ul>
<table>
<thead>
<tr>
<th>模型规模</th>
<th>CPU核心数</th>
<th>内存</th>
<th>硬盘存储空间</th>
<th>显卡推荐显存</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>1.5B</strong></td>
<td>最低4核</td>
<td>8GB+</td>
<td>3GB+</td>
<td>非必需，可选4GB+</td>
<td>低资源设备部署等场景</td>
</tr>
<tr>
<td><strong>7B</strong></td>
<td>8核以上</td>
<td>16GB+</td>
<td>8GB+</td>
<td>推荐8GB+</td>
<td>本地开发测试等场景</td>
</tr>
<tr>
<td><strong>8B</strong></td>
<td>略高于7B</td>
<td>同上</td>
<td>同上</td>
<td>同上</td>
<td>需更高精度的轻量级任务</td>
</tr>
<tr>
<td><strong>14B</strong></td>
<td>12核以上</td>
<td>32GB+</td>
<td>15GB+</td>
<td>推荐16GB+</td>
<td>企业级复杂任务等场景</td>
</tr>
<tr>
<td><strong>32B</strong></td>
<td>16核以上</td>
<td>64GB+</td>
<td>30GB+</td>
<td>推荐24GB+</td>
<td>高精度专业领域任务等场景</td>
</tr>
<tr>
<td><strong>70B</strong></td>
<td>32核以上</td>
<td>128GB+</td>
<td>70GB+</td>
<td>多卡并行</td>
<td>科研机构高复杂度生成任务等场景</td>
</tr>
</tbody>
</table>
<p>GPU非必须，如果使用GPU性能会更好，支持列表参考<a href="https://github.com/ollama/ollama/blob/main/docs/gpu.md" target="_blank" rel="noopener nofollow">Ollama GPU支持列表</a></p>
<h3 id="ollama安装">Ollama安装</h3>
<p>首先，我们需要访问Ollama的官方网站并根据您的操作系统选择相应的版本进行下载和安装。（当前支持MacOS、Linux和Windows多个系统版本)</p>
<p>Ollama官网：<a href="https://ollama.com/download" target="_blank" rel="noopener nofollow">https://ollama.com/download</a></p>
<p><img src="https://img2024.cnblogs.com/blog/383528/202502/383528-20250208172642491-1838066248.png" alt="image" loading="lazy"></p>
<p>安装完成后，打开命令行工具，使用ollama下载deepseek模型，如下：</p>
<pre><code>➜  ~ ollama run deepseek-r1:8b
pulling manifest
pulling aabd4debf0c8...  18% ▕███████████████████████████                                                                                                                           ▏ 206 MB/1.1 GB  954 KB/s  15m54s

</code></pre>
<p>注：这个过程可能会比较耗时，具体取决于您的网络速度。ollama官网托管了多个版本的deepseek模型，我们这里为了方便，我们选择模型deepseek-r1:8b来进行演示。</p>
<h3 id="anythingllm安装">AnythingLLM安装</h3>
<p>下载并安装AnythingLLM，根据您的操作系统选择相应的版本。（当前支持MacOS、Linux和Windows多个系统版本)</p>
<p>AnythingLLM官网：<a href="https://anythingllm.com/desktop" target="_blank" rel="noopener nofollow">https://anythingllm.com/desktop</a></p>
<p><img src="https://img2024.cnblogs.com/blog/383528/202502/383528-20250208172701599-969491906.png" alt="image" loading="lazy"></p>
<p>安装完成后，打开AnythingLLM，会有一段欢迎提示如下：</p>
<p><img src="https://img2024.cnblogs.com/blog/383528/202502/383528-20250208172712014-701897369.png" alt="image" loading="lazy"></p>
<h2 id="数据导入">数据导入</h2>
<h3 id="文档准备">文档准备</h3>
<p>在开始之前，您需要准备一些文档数据，这些文档将作为知识库的基础。文档可以是各种格式，如txt、pdf、doc等，只要AnythingLLM支持即可。您可以选择上传一些与您感兴趣的主题相关的文档，以便模型能够从中学习到更多的知识。比如我们一些技术文档、论文、报告等，我们以技术报告“DeepSeek-V3 Technical Report”为例。<br>
打开AnythingLLM，按照提示创建一个新的工作区。接下来，点击工作区旁边的上传按钮，上传您希望包含在知识库中的文件。</p>
<p><img src="https://img2024.cnblogs.com/blog/383528/202502/383528-20250208172722047-260796226.png" alt="image" loading="lazy"></p>
<p>上传完成后，点击“Save and Embed”，以便模型能够处理这些文档内容。</p>
<h3 id="模型连接">模型连接</h3>
<p>打开AnythingLLM，点击工作区旁边的配置按钮，选择Ollama作为推理后端，并确保选择了deepseek模型和其他必要的参数。这样，您的本地知识库就准备好了。配置如下：</p>
<p><img src="https://img2024.cnblogs.com/blog/383528/202502/383528-20250208172742644-1515092020.png" alt="image" loading="lazy"></p>
<ul>
<li>点击配置按钮，并切换到 <code>Chat Settings</code>菜单项</li>
<li>在工作区 <code>Workspace LLM Provider</code>配置中选择Ollama</li>
<li>在工作区 <code>Workspace Chat model</code>配置中选择deepseek-r1:8b (注：只有使用ollama下载deepseek模型后，这里才会显示)</li>
<li>其他配置项可以根据需要进行调整，如果不确定，可以使用默认值</li>
</ul>
<p>点击保存 <code>Update workspace</code>，然后您就可以开始使用您的本地知识库了。</p>
<h1 id="测试与优化">测试与优化</h1>
<p>一旦完成安装和配置，您就可以通过AnythingLLM的工作区与模型进行交互了。尝试提出一些关于已上传文档的问题，看看模型是如何利用新学到的知识来回答的。</p>
<p><img src="https://img2024.cnblogs.com/blog/383528/202502/383528-20250208172756941-203889304.png" alt="image" loading="lazy"></p>
<p><img src="https://img2024.cnblogs.com/blog/383528/202502/383528-20250208172806446-448291853.png" alt="image" loading="lazy"></p>
<p>从上面截图上可以看到：</p>
<ul>
<li>AnythingLLM通过调用deepseek模型完成了知识问答</li>
<li>图片底部显示 <code>Hide Citations</code>表明，本次问答引用了我们之前上传的技术报告 <code>DeepSeek_V3.pdf</code>，虽然技术报告使用英文，但是deepseek由于支持多语言，在知识问答时能够自动进行翻译。</li>
</ul>
<p>注：如果您上传了多篇关于某个特定领域的论文或报告，那么询问模型有关该领域的细节时，能得到更加准确的回答。</p>
<h1 id="总结">总结</h1>
<p>通过使用Ollama、DeepSeek和AnythingLLM搭建本地知识库，我们不仅能够享受大模型带来的便利，还能够确保数据的安全性和隐私性。这种方法特别适合那些对数据保密有严格要求的企业和个人用户，同时也解决了在线DeepSeek不稳定的问题。</p>
<p>展望未来，随着技术的进步，本地部署的AI解决方案将会变得越来越普及，为用户提供更多的灵活性和控制权。</p>
<h1 id="参考资料">参考资料</h1>
<ul>
<li>[1] Ollama官网：<a href="https://ollama.com" target="_blank" rel="noopener nofollow">https://ollama.com</a></li>
<li>[2] AnythingLLM官网：<a href="https://anythingllm.com" target="_blank" rel="noopener nofollow">https://anythingllm.com</a></li>
<li>[3] DeepSeek官网：<a href="https://www.deepseek.com/" target="_blank" rel="noopener nofollow">https://www.deepseek.com/</a></li>
<li>[4] DeepSeek模型的Ollama地址：<a href="https://ollama.com/library/deepseek-r1" target="_blank" rel="noopener nofollow">https://ollama.com/library/deepseek-r1</a></li>
</ul>

</div>
<div id="MySignature" role="contentinfo">
    <h2> 转载声明 </h2>
<div>本文来自博客园，作者：<a href="https://www.cnblogs.com/informatics/" target="_blank">warm3snow</a></div>
<div>转载请注明原文链接：<a href="https://www.cnblogs.com/informatics/p/18704799" target="_blank">https://www.cnblogs.com/informatics/p/18704799</a> </div>
<div>本文版权归作者和博客园共有，欢迎转载，但未经作者同意必须在文章页面给出原文连接，否则保留追究法律责任的权利。 </div>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.10618695157638888" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-02-08 17:50">2025-02-08 17:26</span>&nbsp;
<a href="https://www.cnblogs.com/informatics">warm3snow</a>&nbsp;
阅读(<span id="post_view_count">100</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18704799" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18704799);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18704799', targetLink: 'https://www.cnblogs.com/informatics/p/18704799', title: '五分钟搭建属于你的AI助手：Ollama+DeepSeek+AnythingLLM深度整合教程' })">举报</a>
</div>
        