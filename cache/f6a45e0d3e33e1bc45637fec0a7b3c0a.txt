
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/huggingface/p/18634204" title="发布于 2024-12-26 21:13">
    <span role="heading" aria-level="2">自动评估基准 | 设计你的自动评估任务</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h1 id="设计你的自动评估任务">设计你的自动评估任务</h1>
<blockquote>
<p>这是 <strong>自动评估基准</strong> 系列文章的第二篇，敬请关注系列文章:</p>
<ul>
<li>基础概念</li>
<li>设计你的自动评估任务</li>
<li>一些评估测试集</li>
<li>技巧与提示</li>
</ul>
</blockquote>
<h2 id="选择数据集">选择数据集</h2>
<p>做评估时，你可以选择现有的数据集 (参考 <a href="https://github.com/huggingface/evaluation-guidebook/blob/main/contents/automated-benchmarks/some-evaluation-datasets.md" target="_blank" rel="noopener nofollow">一些评估数据集</a> 页面) 作为测试集，也可以设计自己的数据集。有一点非常重要，请注意：<strong>评估的结果与评估的数据集质量高度相关</strong> 。</p>
<h3 id="使用现有的数据集">使用现有的数据集</h3>
<p>这部分强烈建议仔细阅读！</p>
<h4 id="数据集需要注意的问题">数据集需要注意的问题</h4>
<p>样本是由谁创建的？<br>
在我看来，按照样本的标注员素质高低，数据集质量大致排名如下：专家构建数据集 &gt; 付费标注数据集 &gt; 众包数据集 &gt; MTurk 数据集。<br>
你可以在数据集的说明文档 (data card) 找到标注员的统计信息，可以帮助理解数据集语言多样性。</p>
<ul>
<li>
<p><strong>样本是否经过其他标注员或作者的审核？</strong><br>
你需要先弄明白：</p>
<ul>
<li>不同标注员标注结果是否一致？</li>
<li>完整数据集是否经过作者审核？<br>
标注员通常不是目标语言的母语使用者（例如 AWS Mechanical Turk），否则可能会出现拼写错误、语法错误或无意义的答案。</li>
</ul>
</li>
<li>
<p><strong>是否给标注员提供了明确的数据创建指导？</strong><br>
换句话说，数据集样本间的标注标准是否一致？</p>
</li>
</ul>
<h4 id="检查样本">检查样本</h4>
<p>随机抽取 50 个样本进行人工检查：</p>
<ul>
<li><em>检查质量</em>：
<ul>
<li>问题是否明确且不含歧义？</li>
<li>对应的回答是否正确？( <em>例如：TriviaQA 的每个问题通常包含多个标准答案，有时这些答案会相互冲突。</em> )</li>
<li>信息是否完整？( <em>例如: MMLU 有许多问题中缺少参考示意图。</em> )</li>
</ul>
</li>
<li><em>检查与任务相关性</em>：
<ul>
<li>样本问题是否是 LLM 特定评估任务的问题类型？</li>
<li>样本是否与测试用例相关？</li>
</ul>
</li>
</ul>
<p>数据集样本数量同样重要 (以确保自动评估基准结果在统计上显著，一般至少需要 100 个测试样本)。</p>
<h3 id="设计自己的数据集">设计自己的数据集</h3>
<p>有 3 种设计方法：</p>
<h4 id="整合数据">整合数据</h4>
<p>要使用自己的测试集评估模型执行特定任务的能力，可以从不同的现成数据源整理和聚合。实际上有许多评估测试集都是以这种方式构建的，例如 MATH 和 LSAT 就聚合了人工评估数据集。当然在整理数据时，请遵循上文的质量与任务相关性检查步骤。</p>
<h4 id="人工标注">人工标注</h4>
<p>关于 <code>人工标注</code> 的内容，本指南有一整个篇幅详细介绍，可以自行点击 <a href="https://github.com/huggingface/evaluation-guidebook/blob/main/contents/human-evaluation/using-human-annotators.md" target="_blank" rel="noopener nofollow">Using human annotators</a> 阅读。</p>
<h4 id="合成数据">合成数据</h4>
<ul>
<li>
<p><strong>使用 LLM 合成</strong><br>
这部分可以参考 HF 员工的 <a href="https://huggingface.co/blog/cosmopedia" target="_blank" rel="noopener nofollow">Cosmopedia</a> 博客！虽然此篇主要研究如何构建训练集，但想法和技术同样适用于构建测试集。<br>
合成的测试集仍需手动检查 (遵循上文步骤)。</p>
</li>
<li>
<p><strong>基于规则合成</strong><br>
如果任务允许，这个绝佳的方法几乎可以无限获取测试样本，并且避免数据污染。<br>
参考 <a href="https://arxiv.org/abs/2312.14890" target="_blank" rel="noopener nofollow">NPHardEval</a>，<a href="https://arxiv.org/abs/2309.17167" target="_blank" rel="noopener nofollow">DyVal</a>，<a href="https://arxiv.org/abs/2310.16049" target="_blank" rel="noopener nofollow">MuSR</a>, <a href="https://arxiv.org/abs/1502.05698" target="_blank" rel="noopener nofollow">BabiQA</a> 等。</p>
</li>
</ul>
<h2 id="选择推理方法">选择推理方法</h2>
<p>除了测试集，还需要选择合适的推理方法。</p>
<p>对于多项选择问答任务 (通常用于测试模型的知识储备或消除歧义的能力)，使用对数概率 (MCQA) 非常有效。</p>
<ul>
<li>优势：
<ul>
<li>可以保证所有模型都能获取正确答案。</li>
<li>能够提供模型 “置信度” 代理 (以及校准)。</li>
<li>评估速度快，尤其是单 token 预测任务时 (选择索引 A/B/C/D 或 Yes/No 等)。</li>
<li>允许获取小模型在任务表现上的信号。</li>
</ul>
</li>
<li>劣势：
<ul>
<li>可能高估小模型的表现。如果不做限制，会使得模型生成的内容超出可选范围。</li>
<li>估结果可能不具代表性。一些模型 <a href="https://arxiv.org/abs/2309.03882" target="_blank" rel="noopener nofollow">倾向于按多项选择的顺序生成特定选择</a>。</li>
</ul>
</li>
</ul>
<p>对于测试模型流畅性、推理或回答问题能力的任务，使用 QA 生成非常有效。</p>
<ul>
<li>优势：
<ul>
<li>与人类关心的点一致，即 LLM 生成文本是否流畅的能力。</li>
</ul>
</li>
<li>劣势：
<ul>
<li>可能存在评分困难 (见下面的 <code>度量标准</code> 部分)。</li>
<li>成本比对数似然评估稍高，尤其是需要采样的任务。</li>
</ul>
</li>
</ul>
<h2 id="选择-prompt">选择 prompt</h2>
<p>Prompt 设计关键问题：</p>
<ul>
<li>提供给模型的关于任务的信息量大小</li>
<li>如何向模型提供信息</li>
</ul>
<p>MCQA 或 QA 任务的通用 prompt 设计范式一般包含以下几个部分：</p>
<ul>
<li>任务 prompt (可选)：描述任务。</li>
<li>上下文：为问题提供额外的背景信息。
<ul>
<li><em>例如: 对于内容总结或信息提取任务，可以提供内容来源</em></li>
</ul>
</li>
<li>问题：prompt 的核心内容。</li>
<li>对于多项选择评估任务，可以增加选项。</li>
<li>连接词 (<code>问题</code>、<code>上下文</code>、<code>选项</code>等)。</li>
</ul>
<p>定义 prompt 时需要注意：</p>
<ul>
<li>在语义等价的 prompt 中，即使非常微小的变化也可能导致巨大差异的结果 (详见 <a href="https://github.com/huggingface/evaluation-guidebook/blob/main/contents/troubleshooting/troubleshooting-reproducibility.md" target="_blank" rel="noopener nofollow">Troubleshooting reproducibility</a> 的 <code>Different prompt</code> 部分)，并且 prompt 格式也可能对特定模型的输出造成影响。
<ul>
<li>如何缓解这一问题：
<ul>
<li>高成本方法：使用不同的 prompt 变体进行多次评估。</li>
<li>低成本方法：使用多种 prompt 格式分别分配给多个等效难度的测试样本进行单次评估。</li>
</ul>
</li>
</ul>
</li>
<li>在 prompt 中提供示例可以帮助模型输出遵循预期格式，示例可以通过连接词添加至 prompt。</li>
<li>注意模型可能倾向于对特定的 prompt 格式过拟合。
<ul>
<li><a href="https://arxiv.org/abs/2407.07890" target="_blank" rel="noopener nofollow">这篇论文</a> 对此有更详尽的探讨，文中展示了一些模型因在测试集 <strong>格式</strong> 上过拟合而导致的评估分数过高的情况。</li>
<li>我们特别观察到，在 Open LLM Leaderboard 2 上, Llama 3.2 和 Qwen 2.5 出于这个原因已经不再提供 few-shot 示例的 prompt 格式。</li>
</ul>
</li>
<li>对于一些测试任务的指标，你可能希望模型的输出限制在一个小范围。<br>
<em>可以跳转 <a href="https://github.com/huggingface/evaluation-guidebook/blob/main/contents/general-knowledge/model-inference-and-evaluation.md" target="_blank" rel="noopener nofollow">Model inference and evaluation</a> 页面的 <code>Constraining model outputs</code> 部分了解更多信息。</em></li>
</ul>
<h2 id="选择评估指标">选择评估指标</h2>
<p>如果你关注 <strong>对数概率</strong> 评估，那么你期望的度量指标会很简单：准确率 (选择最佳选项的频率)。如果在这个基础上你还想要进行标准化 (通过长度、字符、token 或 PMI)，那么度量指标就会变成困惑度 (perplexity)、召回率或 F1 分数。</p>
<p>对于 <strong>生成式</strong> 评估，你期望的度量指标范围会更广。<br>
为此你需要：</p>
<ol>
<li>确定生成结果的度量顺序，是直接拿生成结果比较，还是先使用某种方式进行标准化。
<ul>
<li>标准化如果设计不当，评估结果会有失偏颇 (参考这篇 <a href="https://huggingface.co/blog/open-llm-leaderboard-drop" target="_blank" rel="noopener nofollow">博客</a>)。但总的来说，它们都能在任务层面提供信号。</li>
<li>标准化对某些特定任务 (例如数学能力评估) 非常重要，因为你可能需要从格式化输出中提取有效的结果。</li>
<li>如果你想要通过添加机制 (如思维链) 来评估准确率，那么标准化同样重要，因为你需要将推理轨迹从实际结果中去除。</li>
</ul>
</li>
<li>确定生成结果与参考答案的比较方式。<br>
你可以采用任意的比较方法。评估匹配程度的有：精确匹配、前缀匹配等；评估摘要和翻译能力的有：ROUGE、BLEU、n-gram 等。更多评价指标可以点击 <a href="https://github.com/huggingface/lighteval/wiki/Metric-List" target="_blank" rel="noopener nofollow">这个页面</a> 查看，我会在后续更新关于在何时使用哪种指标的章节。</li>
</ol>
<p>总的来说，选择哪种评价指标取决于你的任务内容。对于某些领域 (如医疗、聊天机器人)，你可能不想要评估平均性能，而是需要评估 <strong>最差表现</strong> (如医疗知识输出质量、如果输出不实的后果等)。( <em>可以查看 <a href="https://ehudreiter.com/2024/07/10/challenges-in-evaluating-llms/" target="_blank" rel="noopener nofollow">这篇博客</a> 深入了解</em> )</p>
<h2 id="智能新任务功能性测试是什么">智能新任务：功能性测试是什么？</h2>
<p>对于代码领域，显然仅评估生成代码的语义是不够的，必须测试代码实际运行情况。所以需要专门设计一个功能性测试：对于给定 prompt 生成的代码段，测试并评估其是否能正确通过单元测试。</p>
<p>这种功能性测试方法极具前景，因为：</p>
<ul>
<li>使得生成测试用例更容易 (大部分情况下都可以基于规则生成测试用例)</li>
<li>减少过拟合</li>
<li>可以评估模型的特定主动能力</li>
</ul>
<p>不过很多新奇的想法需要一些创造性的工作才能实现！</p>
<p>IFEval 是一个不错的例子，它是用来测试模型指令遵循能力的评估基准，通过创建多个格式化指令 ( <em>例如：添加指定数量的特殊符号，仅将一句话字母大写，等等</em> ) 并严格测试生成结果的遵循与否。功能性测试的想法仍需更多的工作来扩展到其他的特征测试上！</p>
<hr>
<blockquote>
<p>英文原文: <a href="https://github.com/huggingface/evaluation-guidebook/blob/main/translations/zh/contents/automated-benchmarks/designing-your-automatic-evaluation.md" target="_blank" rel="noopener nofollow">https://github.com/huggingface/evaluation-guidebook/blob/main/translations/zh/contents/automated-benchmarks/designing-your-automatic-evaluation.md</a></p>
<p>原文作者: clefourrier</p>
<p>译者: SuSung-boy</p>
<p>审校: adeenayakup</p>
</blockquote>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.041240866537037034" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2024-12-26 21:14">2024-12-26 21:13</span>&nbsp;
<a href="https://www.cnblogs.com/huggingface">HuggingFace</a>&nbsp;
阅读(<span id="post_view_count">8</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18634204" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18634204);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18634204', targetLink: 'https://www.cnblogs.com/huggingface/p/18634204', title: '自动评估基准 | 设计你的自动评估任务' })">举报</a>
</div>
        