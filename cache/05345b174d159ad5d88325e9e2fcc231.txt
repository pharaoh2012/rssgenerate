
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/Big-Yellow/p/18895944" title="发布于 2025-05-25 21:24">
    <span role="heading" aria-level="2">CV中常用Backbone-3：Clip/SAM原理以及代码操作</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p>前面已经介绍了简单的视觉编码器，这里主要介绍多模态中使用比较多的两种backbone：1、Clip；2、SAM。对于这两个backbone简单介绍基本原理，主要是讨论使用这个backbone。<br>
1、<a href="https://www.big-yellow-j.top/posts/2025/04/30/ConvNeXt.html" target="_blank" rel="noopener nofollow">CV中常用Backbone-2：ConvNeXt模型详解</a><br>
2、<a href="https://www.big-yellow-j.top/posts/2025/01/18/CV-Backbone.html" target="_blank" rel="noopener nofollow">CV中常用Backbone(Resnet/Unet/Vit系列/多模态系列等)以及代码</a></p>
<h2 id="sam">SAM</h2>
<p>SAM已经出了两个版本分别是：SAM v1和SAM v2这里对这两种分别进行解释，并且着重了解一下他的数据集是怎么构建的（毕竟很多论文里面都会提到直接用SAM作为一种数据集生成工具）</p>
<h3 id="sam-v1">SAM v1<sup class="footnote-ref"><a href="#fn1" id="fnref1" rel="noopener nofollow">[1]</a></sup></h3>
<blockquote>
<p><a href="https://arxiv.org/pdf/2304.02643" target="_blank" rel="noopener nofollow">https://arxiv.org/pdf/2304.02643</a><br>
官方Blog：<a href="https://ai.meta.com/blog/segment-anything-foundation-model-image-segmentation/" target="_blank" rel="noopener nofollow">Introducing Segment Anything: Working toward the first foundation model for image segmentation</a><sup class="footnote-ref"><a href="#fn2" id="fnref2" rel="noopener nofollow">[2]</a></sup></p>
</blockquote>
<p><img src="https://i-blog.csdnimg.cn/img_convert/2c8fc8d3c02abaef0a62d384ded07bf6.png" alt="" loading="lazy"></p>
<p>结构上还是比较简单，首先在 <strong>Image Encoder</strong>：选择的是<a href="https://www.big-yellow-j.top/posts/2025/01/18/CV-Backbone.html#:~:text=768-,MAE%20%E4%B8%BB%E8%A6%81%E6%93%8D%E4%BD%9C%E6%B5%81%E7%A8%8B,-1%E3%80%81patch" target="_blank" rel="noopener nofollow">MAE</a>；<strong>Prompt Encoder</strong>：从上面结构图很容易知道就3类prompt：1、text用CLIP进行编码；2、points、bbox使用论文<sup class="footnote-ref"><a href="#fn3" id="fnref3" rel="noopener nofollow">[3]</a></sup>（<strong>主要是通过傅里叶特征映射方法来提高模型对于高频函数学习能力</strong>）中的方法来进行编码处理；3、mask这种内容就直接使用卷积进行编码而后将结果和编码后图像相加；</p>
<blockquote>
<p>对于points以及bbox<a href="https://github.com/tancik/fourier-feature-networks?tab=readme-ov-file" target="_blank" rel="noopener nofollow">编码</a>原理很简单就是用点或者坐标直接计算他们的傅里叶特征，比如说points的伪代码</p>
</blockquote>
<pre><code class="language-python">import numpy as np
# 假设输入点为2D，[x, y]
points = np.array([[0.5, 0.3], [0.2, 0.7]])  # 形状: (N, 2)
m = 256  # 映射维度
sigma = 10.0  # 频率控制参数

# 生成随机矩阵B
B = np.random.normal(0, sigma, size=(m, 2))  # 形状: (m, 2)
# 计算傅里叶特征
Bx = np.dot(points, B.T)  # 点积，形状: (N, m)
fourier_features = np.concatenate([np.cos(2 * np.pi * Bx), np.sin(2 * np.pi * Bx)], axis=1)  # 形状: (N, 2m)
</code></pre>
<p><strong>Mask decoder</strong>：掩码解码器可以有效的将图嵌入、提示嵌入和输出标记映射到掩码。本模型的解码器基于Transformer的解码器块修改，在解码器后添加了动态掩码预测头。解码器使用了提示自注意力和交叉注意力在提示到图嵌入（prompt-to-image embedding）和vice-versa两个方面进行了修改。完成这两个部分后，对图像进行上采样再使用MLP将输出标记映射到动态线性分类器上，最终得出每个图像位置的蒙板前景概率。</p>
<p><img src="https://i-blog.csdnimg.cn/img_convert/bf9684b56fa2c47a0c997653b63fabf0.png" alt="" loading="lazy"></p>
<p><strong>Resolving ambiguity</strong>：对于一个不确定的提示，模型会给出多个有效掩码，经过修改SAM可以由单个提示预测输出多个掩码（一般是3个--整体、部分、子部分）。训练时，仅掩码进行反向传播。为了对掩码进行排名，模型会预测每个掩码的置信分数（使用IOU度量），所谓的整体、部分、子部分，比如说：<br>
<img src="https://i-blog.csdnimg.cn/img_convert/655a5b4988b3610f86d51b0ce3889bc7.png" alt="" loading="lazy"></p>
<h3 id="sam-v2">SAM v2<sup class="footnote-ref"><a href="#fn4" id="fnref4" rel="noopener nofollow">[4]</a></sup></h3>
<blockquote>
<p><a href="https://arxiv.org/pdf/2408.00714" target="_blank" rel="noopener nofollow">https://arxiv.org/pdf/2408.00714</a></p>
</blockquote>
<p>SAM v2更像是SAM v1在视频邻域的泛化，整个模型结构如下所示：<br>
<img src="https://i-blog.csdnimg.cn/img_convert/2962d60e503a65a15eaf36ad7c7cf611.png" alt="" loading="lazy"></p>
<p>主要值得关注的是其中的 <strong>Memory Attention</strong>：将当前帧的特征与过去帧的特征和预测以及任何新的提示联系起来。通过堆叠了 L 个transformer模块，第一个模块将当前帧的图像编码作为输入。每个区块执行self-attention，然后cross-attention（提示/未提示）帧和对象的记忆，这些记忆存储在一个记忆库中，接着是一个 MLP。在self-attention和cross-attention中使用了vanilla注意力操作，从而受益于高效注意力内核的最新发展。<br>
<strong>memory encoder</strong>通过使用卷积模块对输出掩码进行下采样，并将其与图像编码器的无条件帧嵌入相加，生成记忆，然后使用轻量级卷积层来融合信息。<br>
<strong>memory bank</strong>通过维护<strong>最多N个最近帧的FIFO记忆队列来保留视频中目标对象的过去预测信息，并将提示信息存储在最多M个提示帧的FIFO队列中</strong>。例如，在VOS任务中，初始掩码是唯一的提示，内存库始终保留第一帧的记忆以及最多N个最近(非提示)帧的记忆。两组记忆都以空间特征图的形式存储。<br>
除空间存储器外，还根据每个帧的掩码解码器输出标记，将对象指针列表作为轻量级向量存储起来，用于存储要分割对象的高级语义信息。<br>
我们<strong>将时间位置信息嵌入到N个最近帧的memory中</strong>，允许模型表示短期物体运动，但不包含到提示帧的记忆中，因为提示帧的训练信号更稀疏，并且更难以推广到推理设置中，提示帧可能来自与训练期间看到的时间范围非常不同的时间范围。</p>
<h2 id="clip">Clip<sup class="footnote-ref"><a href="#fn5" id="fnref5" rel="noopener nofollow">[5]</a></sup></h2>
<p>Clip模型结构（论文里面提到的）也比较简单，其核心机制为：<strong>核心机制是通过对比学习和嵌入空间对齐，将图像和文本映射到一个共享的语义空间中</strong><br>
<img src="https://i-blog.csdnimg.cn/img_convert/03a9c3a5264e57a1bf41865ad1258a2a.png" alt="" loading="lazy"></p>
<p><strong>预训练过程</strong>：直接将文本和图像都进行编码，而后将编码后的内容通过计算他的相似度（比如：cosine similarities）来确保模型最后能够对齐文本和图像之间的特征。<br>
<strong>使用过程</strong>：对于给定的图像直接通过Clip的图像编码，而后将文本进行编码（文本编码中会有一个 label dataset通过从label dataset中抽取出标签和自己文本进行组合得到n条微博呢）再去计算最后的结果。</p>
<h2 id="代码操作">代码操作</h2>
<p>所有代码见：<a href="https://github.com/Big-Yellow-J/Big-Yellow-J.github.io/tree/master/code/Python/SAM-Clip/sam-clip.ipynb" target="_blank" rel="noopener nofollow">sam-clip.ipynb</a></p>
<h2 id="参考">参考</h2>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p><a href="https://arxiv.org/pdf/2304.02643" target="_blank" rel="noopener nofollow">https://arxiv.org/pdf/2304.02643</a> <a href="#fnref1" class="footnote-backref" rel="noopener nofollow">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p><a href="https://ai.meta.com/blog/segment-anything-foundation-model-image-segmentation/" target="_blank" rel="noopener nofollow">https://ai.meta.com/blog/segment-anything-foundation-model-image-segmentation/</a> <a href="#fnref2" class="footnote-backref" rel="noopener nofollow">↩︎</a></p>
</li>
<li id="fn3" class="footnote-item"><p><a href="https://arxiv.org/abs/2006.10739" target="_blank" rel="noopener nofollow">https://arxiv.org/abs/2006.10739</a> <a href="#fnref3" class="footnote-backref" rel="noopener nofollow">↩︎</a></p>
</li>
<li id="fn4" class="footnote-item"><p><a href="https://arxiv.org/pdf/2408.00714" target="_blank" rel="noopener nofollow">https://arxiv.org/pdf/2408.00714</a> <a href="#fnref4" class="footnote-backref" rel="noopener nofollow">↩︎</a></p>
</li>
<li id="fn5" class="footnote-item"><p><a href="https://arxiv.org/pdf/2103.00020" target="_blank" rel="noopener nofollow">https://arxiv.org/pdf/2103.00020</a> <a href="#fnref5" class="footnote-backref" rel="noopener nofollow">↩︎</a></p>
</li>
</ol>
</section>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.3571307646539352" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-05-25 21:39">2025-05-25 21:24</span>&nbsp;
<a href="https://www.cnblogs.com/Big-Yellow">Big-Yellow-J</a>&nbsp;
阅读(<span id="post_view_count">2</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18895944);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18895944', targetLink: 'https://www.cnblogs.com/Big-Yellow/p/18895944', title: 'CV中常用Backbone-3：Clip/SAM原理以及代码操作' })">举报</a>
</div>
        