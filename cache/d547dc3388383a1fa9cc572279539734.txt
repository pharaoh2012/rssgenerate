
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/zhoujishan/p/18905954" title="发布于 2025-06-01 11:12">
    <span role="heading" aria-level="2">Transformer详解</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h1 id="transformer架构">Transformer架构</h1>
<p>embedding 层的参数量 = 词汇表数量 * 嵌入维度</p>
<h2 id="为什么要使用transformer算法架构">为什么要使用transformer算法架构</h2>
<p>传统的RNN、CNN等算法架构存在的问题：</p>
<ul>
<li>扩展能力差</li>
<li>泛化能力弱</li>
</ul>
<h2 id="transformer的优势">Transformer的优势</h2>
<ul>
<li>自注意力机制：计算和使用序列中任意文本之间的依赖关系</li>
<li>位置编码：能够对序列中的token进行并行计算</li>
</ul>
<h2 id="transformer自注意力机制的计算">Transformer自注意力机制的计算</h2>
<p><img src="https://img2024.cnblogs.com/blog/2127117/202506/2127117-20250601111405856-2011113102.png" alt="" loading="lazy"></p>
<p>对于给定的序列x1、x2、x3，需要计算他们之间的对应关系，例如求x1和整个文本中x1、x2、x3之间的相关性。<br>
模型通过训练，已经提前得到一个(多注意力头的话会有多个)Wq、Wk、Wv矩阵，这样很容易求得各个输入token的q向量、k向量、v向量。</p>
<table>
<thead>
<tr>
<th></th>
<th>q向量</th>
<th>k向量</th>
<th>v向量</th>
</tr>
</thead>
<tbody>
<tr>
<td>x1</td>
<td>q1 = x1 * Wq</td>
<td>k1 = x1 * Wq</td>
<td>v1 = x1 * Wv</td>
</tr>
<tr>
<td>x2</td>
<td>q2 = x2* Wq</td>
<td>k2 = x2* Wk</td>
<td>v2 = x2* Wv</td>
</tr>
<tr>
<td>x3</td>
<td>q3 = x3 * Wq</td>
<td>k3 = x3 * Wk</td>
<td>v3 = x3 * Wv</td>
</tr>
</tbody>
</table>
<p>得到各个输入token x的q、k、v向量后，带入公式计算各个token直接的相关性。我们可以用一个二维表格进行表示。</p>
<table>
<thead>
<tr>
<th></th>
<th>x1</th>
<th>x2</th>
<th>x3</th>
</tr>
</thead>
<tbody>
<tr>
<td>x1</td>
<td>Attention(q1, k1, v1)</td>
<td>Attention(q2, k1, v1)</td>
<td>Attention(q3, k1, v1)</td>
</tr>
<tr>
<td>x2</td>
<td>Attention(q1, k2, v2)</td>
<td>Attention(q2 k2, v2)</td>
<td>Attention(q3, k2, v2)</td>
</tr>
<tr>
<td>x3</td>
<td>Attention(q1, k3, v3)</td>
<td>Attention(q2, k3, v3)</td>
<td>Attention(q3, k3, v3)</td>
</tr>
</tbody>
</table>
<p>得到的各个token间的关联性得分为，如下的结果，表示x1、x2、x3分别只和自己相近，和其它token均没有相关性。</p>
<table>
<thead>
<tr>
<th></th>
<th>x1</th>
<th>x2</th>
<th>x3</th>
</tr>
</thead>
<tbody>
<tr>
<td>x1</td>
<td>0.90</td>
<td>0.03</td>
<td>0.08</td>
</tr>
<tr>
<td>x2</td>
<td>0.02</td>
<td>0.96</td>
<td>0.01</td>
</tr>
<tr>
<td>x3</td>
<td>0.08</td>
<td>0.01</td>
<td>0.91</td>
</tr>
</tbody>
</table>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.010520601015046297" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-06-01 11:14">2025-06-01 11:12</span>&nbsp;
<a href="https://www.cnblogs.com/zhoujishan">jeasonzhou</a>&nbsp;
阅读(<span id="post_view_count">0</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18905954);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18905954', targetLink: 'https://www.cnblogs.com/zhoujishan/p/18905954', title: 'Transformer详解' })">举报</a>
</div>
        