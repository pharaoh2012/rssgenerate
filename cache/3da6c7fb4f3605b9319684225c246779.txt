
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/Ctrl-cCtrl-v/p/18906192" title="发布于 2025-06-01 17:51">
    <span role="heading" aria-level="2">RWKV-7 架构理解</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body blogpost-body-html">
<p>阅读之前你可以前往 <a href="https://rwkv.cn/docs/RWKV-Wiki/Introduction" target="_blank" rel="noopener nofollow">RWKV wiki</a> 了解一些关于 RWKV 的基本知识，不过他们的 wiki 似乎没有对模型架构的详细介绍，于是便有了这篇文章。</p>
<h1>RWKV-7 的核心：动态状态演化机制</h1>
<p>RWKV-V7 的动态状态演化机制可以通俗理解为 <strong>“在线学习上下文关系的动态记忆更新” </strong>。它的核心思想是：<strong>通过实时计算和更新一个内部状态（state）来动态捕捉上下文中 key 和 value 的关联关系，并利用这个状态处理当前输入的 query（在 RWKV 中是 r）以生成输出 </strong>。</p>
<h2 data-spm-anchor-id="a2ty_o01.29997173.0.i76.4252c921puxtk5">1. <strong data-spm-anchor-id="a2ty_o01.29997173.0.i75.4252c921puxtk5">状态（state）的本质</strong></h2>
<p data-spm-anchor-id="a2ty_o01.29997173.0.i78.4252c921puxtk5">在 RWKV 中，state 是一个三维张量（<code class="codespan cursor-pointer z-[9] relative">B, H, N, N</code>），其中：</p>
<ul>
<li><code class="codespan cursor-pointer z-[9] relative">B</code>：批量大小（batch size）</li>
<li><code class="codespan cursor-pointer z-[9] relative">H</code>：注意力头数量（head count）</li>
<li data-spm-anchor-id="a2ty_o01.29997173.0.i79.4252c921puxtk5"><code class="codespan cursor-pointer z-[9] relative">N</code>：每个头的维度（head size）</li>
</ul>
<p data-spm-anchor-id="a2ty_o01.29997173.0.i80.4252c921puxtk5">state 的作用是 <strong>维护一个动态的“知识库” </strong>，记录历史输入的 key（k）和 value（v）之间的关系。它类似于传统 RNN 的隐藏状态，但更复杂，因为它显式建模了 key-value 的交互。</p>
<h2 data-spm-anchor-id="a2ty_o01.29997173.0.i81.4252c921puxtk5">2. <strong>状态更新公式与代码实现</strong></h2>
<p>官方公式：</p>
<p data-spm-anchor-id="a2ty_o01.29997173.0.i80.4252c921puxtk5"><code><span class="katex-display"><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="mord"><span class="mord mathnormal">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist"><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span class="vlist-s">​</span></span></span></span></span><span class="mrel">=</span></span><span class="base"><span class="mord"><span class="mord mathnormal">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist"><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span>1</span></span></span><span class="vlist-s">​</span></span></span></span></span><span class="mbin">⋅</span></span><span class="base"><span class="mord text"><span class="mord">diag</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist"><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span class="vlist-s">​</span></span></span></span></span><span class="mclose">)</span><span class="mbin">+</span></span><span class="base"><span class="minner"><span class="mopen delimcenter"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord mathnormal">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist"><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span>1</span></span></span><span class="vlist-s">​</span></span></span></span></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist"><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span><span class="vlist-s">​</span></span></span></span></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist"><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span class="vlist-s">​</span></span></span></span></span><span class="mbin">+</span><span class="mord"><span class="mord mathnormal">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist"><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span><span class="vlist-s">​</span></span></span></span></span><span class="mord"><span class="mord mathnormal">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist"><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span class="vlist-s">​</span></span></span></span></span><span class="mclose delimcenter"><span class="delimsizing size1">)</span></span></span></span></span></span></span></code></p>
<p>在代码中（<code class="codespan cursor-pointer z-[9] relative">RWKV7_OP</code> 函数），这个公式被拆解为：</p>
<div>
<div class="my-2 code-cntainer code-cntainer-mobile" dir="ltr">
<div class="language-python  -mt-8 rounded-t-xl rounded-b-xl overflow-hidden  code-content">
<div id="code-textarea-23f1d958-8003-450a-99d2-d68dd676dccf-17" class="code-textarea h-full w-full ">
<div class="cm-editor ͼ1 ͼ3 ͼ4 ͼo">
<div class="cm-scroller" tabindex="-1"><code>state = state * w + state @ a @ b + v @ k</code></div>
</div>
</div>
</div>
<div id="plt-canvas-23f1d958-8003-450a-99d2-d68dd676dccf-17" class="scrollbar-hidden max-w-full overflow-x-auto bg-[#202123] text-white"></div>
</div>
</div>
<p>该公式对应 RWKV-V7 的 <strong>动态状态演化机制 </strong>，其核心思想是通过 <strong>在线学习 </strong>维护一个低秩状态矩阵 <code class="codespan cursor-pointer z-[9] relative">S_t</code>，以捕捉上下文中的 key-value 关系。公式分为三部分：</p>
<ol start="1">
<li>
<p><strong>权重衰减项（遗忘旧信息）</strong></p>
<div>
<div class="my-2 code-cntainer code-cntainer-mobile" dir="ltr">
<div class="language-python  -mt-8 rounded-t-xl rounded-b-xl overflow-hidden  code-content">
<div id="code-textarea-83f18611-42d8-4df1-9bf0-c4d907a3b0c7-9-0-1" class="code-textarea h-full w-full ">
<div class="cm-editor ͼ1 ͼ3 ͼ4 ͼo">
<div class="cm-scroller" tabindex="-1">
<div class="cm-gutters" aria-hidden="true">&nbsp;</div>
<div class="cm-content" role="textbox" contenteditable="false" spellcheck="false" translate="no" aria-multiline="true" aria-placeholder="Enter your code here..." data-language="python" aria-autocomplete="list">
<div class="cm-activeLine cm-line"><span class="ͼq">state</span> <span class="ͼv">*</span> <span class="ͼq">w</span></div>
</div>
</div>
</div>
</div>
</div>
<div id="plt-canvas-83f18611-42d8-4df1-9bf0-c4d907a3b0c7-9-0-1" class="scrollbar-hidden max-w-full overflow-x-auto bg-[#202123] text-white"></div>
</div>
</div>
<ul>
<li><strong>作用 </strong>：控制历史信息的遗忘速率。</li>
<li><strong>数学形式 </strong>：<code class="codespan cursor-pointer z-[9] relative">S_t = S_{t-1} ⋅ diag(w_t)</code>，其中 <code class="codespan cursor-pointer z-[9] relative">w_t</code> 是负值（通过 <code class="codespan cursor-pointer z-[9] relative">exp(-exp(...))</code> 生成）。</li>
<li><strong>思想 </strong>：类似 RNN 的遗忘门，<code class="codespan cursor-pointer z-[9] relative">w</code> 越小（更负），遗忘越多，确保模型专注于当前上下文。</li>
</ul>
</li>
<li>
<p><strong>学习率调整项（动态修正知识）</strong></p>
<div>
<div class="my-2 code-cntainer code-cntainer-mobile" dir="ltr">
<div class="language-python  -mt-8 rounded-t-xl rounded-b-xl overflow-hidden  code-content">
<div id="code-textarea-83f18611-42d8-4df1-9bf0-c4d907a3b0c7-9-1-1" class="code-textarea h-full w-full ">
<div class="cm-editor ͼ1 ͼ3 ͼ4 ͼo">
<div class="cm-scroller" tabindex="-1">
<div class="cm-gutters" aria-hidden="true">&nbsp;</div>
<div class="cm-content" role="textbox" contenteditable="false" spellcheck="false" translate="no" aria-multiline="true" aria-placeholder="Enter your code here..." data-language="python" aria-autocomplete="list">
<div class="cm-activeLine cm-line"><span class="ͼq">state</span> <span class="ͼv">@</span> <span class="ͼq">aa</span> <span class="ͼv">@</span> <span class="ͼq">bb</span></div>
</div>
</div>
</div>
</div>
</div>
<div id="plt-canvas-83f18611-42d8-4df1-9bf0-c4d907a3b0c7-9-1-1" class="scrollbar-hidden max-w-full overflow-x-auto bg-[#202123] text-white"></div>
</div>
</div>
<ul>
<li><strong>作用 </strong>：根据当前输入调整已有知识的权重。</li>
<li><strong>数学形式 </strong>：<code class="codespan cursor-pointer z-[9] relative">S_t = S_{t-1} ⋅ a_t^⊤ b_t</code>，其中 <code class="codespan cursor-pointer z-[9] relative">a</code> 和 <code class="codespan cursor-pointer z-[9] relative">b</code> 是动态学习率参数（<code class="codespan cursor-pointer z-[9] relative">a = -k</code>, <code class="codespan cursor-pointer z-[9] relative">b = k ⋅ η</code>）。</li>
<li><strong>思想 </strong>：模拟梯度下降中的学习率乘以梯度方向，使状态更新更适应当前输入的特征分布。</li>
</ul>
</li>
<li>
<p><strong>新信息注入项（更新知识库）</strong></p>
<div>
<div class="my-2 code-cntainer code-cntainer-mobile" dir="ltr">
<div class="language-python  -mt-8 rounded-t-xl rounded-b-xl overflow-hidden  code-content">
<div id="code-textarea-83f18611-42d8-4df1-9bf0-c4d907a3b0c7-9-2-1" class="code-textarea h-full w-full ">
<div class="cm-editor ͼ1 ͼ3 ͼ4 ͼo">
<div class="cm-scroller" tabindex="-1">
<div class="cm-gutters" aria-hidden="true">&nbsp;</div>
<div class="cm-content" role="textbox" contenteditable="false" spellcheck="false" translate="no" aria-multiline="true" aria-placeholder="Enter your code here..." data-language="python" aria-autocomplete="list">
<div class="cm-activeLine cm-line"><span class="ͼq">vv</span> <span class="ͼv">@</span> <span class="ͼq">kk</span></div>
</div>
</div>
</div>
</div>
</div>
<div id="plt-canvas-83f18611-42d8-4df1-9bf0-c4d907a3b0c7-9-2-1" class="scrollbar-hidden max-w-full overflow-x-auto bg-[#202123] text-white"></div>
</div>
</div>
<ul>
<li><strong>作用 </strong>：将当前 token 的 key-value 对（<code class="codespan cursor-pointer z-[9] relative">k_t</code>, <code class="codespan cursor-pointer z-[9] relative">v_t</code>）外积加入状态。</li>
<li><strong>数学形式 </strong>：<code class="codespan cursor-pointer z-[9] relative">S_t += v_t^⊤ ⋅ k_t</code>。</li>
<li data-spm-anchor-id="a2ty_o01.29997173.0.i40.42525171ZOPka6"><strong>思想 </strong>：直接注入新信息，确保模型能实时捕捉上下文中的新关联（如实体关系、语义依赖）</li>
</ul>
</li>
</ol>
<p>&nbsp;</p>
<h1 data-spm-anchor-id="a2ty_o01.29997173.0.i80.4252c921puxtk5">GPT 与 RNN&nbsp;模式</h1>
<p>在进一步了解之前需要知道的是，RWKV 分为两种模式，一种是 GPT 模式，一种是 RNN 模式，区别在于：</p>
<h4 data-spm-anchor-id="a2ty_o01.29997173.0.i59.51b7c921YMPkKP"><strong data-spm-anchor-id="a2ty_o01.29997173.0.i58.51b7c921YMPkKP">GPT 模式 </strong>（<a href="https://github.com/RWKV/RWKV-LM/blob/main/RWKV-v7/rwkv_v7_demo.py" target="_blank" rel="noopener nofollow"><code class="codespan cursor-pointer z-[9] relative">rwkv_v7_demo.py</code></a>）</h4>
<ul>
<li data-spm-anchor-id="a2ty_o01.29997173.0.i57.51b7c921YMPkKP"><strong>计算方式 </strong>：采用 <strong>Transformer 的 GPT 模式 </strong>，一次性处理整个输入序列（类似标准 Transformer 的前向传播）。</li>
<li><strong>状态管理 </strong>：无需显式维护隐藏状态（state），直接通过注意力机制处理上下文。</li>
<li><strong>适用场景 </strong>：适合 <strong>预填充（prefill） </strong>阶段（即处理初始提示词），但自回归生成时效率较低。</li>
<li><strong>性能特点 </strong>：
<ul>
<li>预填充阶段可以并行计算所有 token 的表示。</li>
<li>自回归生成时需重新计算所有 token 的表示（无状态缓存），导致速度较慢。</li>
</ul>
</li>
</ul>
<h4 data-spm-anchor-id="a2ty_o01.29997173.0.i60.51b7c921YMPkKP"><strong>RNN 模式 </strong>（<a href="https://github.com/RWKV/RWKV-LM/blob/main/RWKV-v7/rwkv_v7_demo_rnn.py" target="_blank" rel="noopener nofollow"><code class="codespan cursor-pointer z-[9] relative">rwkv_v7_demo_rnn.py</code></a>）</h4>
<ul>
<li><strong>计算方式 </strong>：采用 <strong>RNN 模式 </strong>，逐个 token 处理，显式维护隐藏状态（state）。</li>
<li><strong>状态管理 </strong>：通过 <code class="codespan cursor-pointer z-[9] relative">state</code> 变量保存每个时间步的中间状态（如 <code class="codespan cursor-pointer z-[9] relative">att_kv</code>、<code class="codespan cursor-pointer z-[9] relative">ffn_x_prev</code> 等），避免重复计算。</li>
<li><strong>适用场景 </strong>：适合 <strong>自回归生成 </strong>（逐 token 生成），效率更高。</li>
<li><strong>性能特点 </strong>：
<ul>
<li>预填充阶段效率低（需逐 token 处理）。</li>
<li data-spm-anchor-id="a2ty_o01.29997173.0.i61.51b7c921YMPkKP">生成阶段只需计算当前 token 的表示，速度显著优于 GPT 模式。</li>
</ul>
</li>
</ul>
<p>此外，官方还提供了结合两种模式的 fast 模式: <a href="https://github.com/RWKV/RWKV-LM/blob/main/RWKV-v7/rwkv_v7_demo_fast.py" target="_blank" rel="noopener nofollow">rwkv_v7_demo_fast.py</a></p>
<h1>GPT 模式</h1>
<h2>Time mix(<span style="font-size: 14px"><code>RWKV_Tmix_x070</code>)</span></h2>
<p><span style="font-size: 14px">Time mix 是整个 RWKV-7 的核心，也是最复杂的部分。</span></p>
<p><span style="font-size: 14px"><code class="codespan cursor-pointer z-[9] relative">RWKV_Tmix_x070</code> 是 RWKV 模型中实现 Time Mixing 的核心模块，用于处理序列数据中的时序依赖关系。它是 RWKV-V7 架构中负责模拟注意力机制和动态状态更新的组件，类似于 Transformer 中的自注意力模块，但采用了更高效的线性复杂度设计。</span></p>
<pre class="language-python highlighter-hljs"><code>@MyFunction
def forward(self, x, v_first):
    # 获取输入张量的维度：批量大小 B、序列长度 T、特征维度 C
    B, T, C = x.size()
    
    # 获取注意力头数量 H 和每个头的维度 N（固定为 64）
    H = self.n_head
    N = self.head_size

    # 使用 time_shift 操作获取前一个时间步的信息，并与当前输入做差值
    xx = self.time_shift(x) - x  # 得到时间差分项，用于生成不同方向的输入

    # 根据时间差分项和可学习参数，生成不同方向的输入
    xr = x + xx * self.x_r   # receptance 输入方向
    xw = x + xx * self.x_w   # decay weight 输入方向
    xk = x + xx * self.x_k   # key 输入方向
    xv = x + xx * self.x_v   # value 输入方向
    xa = x + xx * self.x_a   # learning rate 输入方向
    xg = x + xx * self.x_g   # gate 输入方向

    # 通过线性层生成 r（receptance），类似于传统 attention 中的 query
    r = self.receptance(xr)

    # 构造 w（权重衰减项）：控制 state 的遗忘速率
    # 使用 tanh 和 softplus 确保 w 值在 (-inf, -0.5) 范围内
    w = -F.softplus(-(self.w0 + torch.tanh(xw @ self.w1) @ self.w2)) - 0.5

    # 通过线性层生成 key 向量
    k = self.key(xk)

    # 通过线性层生成 value 向量
    v = self.value(xv)

    # 如果是第一层，则保存当前 v 作为 v_first（初始 value）
    if self.layer_id == 0:
        v_first = v
    else:
        # 否则使用门控机制对 v 进行残差连接，保留一部分初始信息
        v = v + (v_first - v) * torch.sigmoid(self.v0 + (xv @ self.v1) @ self.v2)

    # 生成动态学习率 a，控制 state 更新的学习率（类似在线梯度下降中的学习率）
    a = torch.sigmoid(self.a0 + (xa @ self.a1) @ self.a2)

    # 生成门控参数 g，控制最终输出的激活强度
    g = torch.sigmoid(xg @ self.g1) @ self.g2

    # 对 key 进行缩放和 L2 归一化，增强数值稳定性
    kk = k * self.k_k
    kk = F.normalize(kk.view(B, T, H, -1), dim=-1, p=2.0).view(B, T, C)

    # 调整 key 的尺度，结合学习率 a 和可学习参数 self.k_a
    k = k * (1 + (a - 1) * self.k_a)

    # 调用 RWKV7_OP 函数进行状态更新和 attention 输出计算
    x = RWKV7_OP(r, w, k, v, -kk, kk * a)

    # 使用 GroupNorm 对输出进行归一化
    x = self.ln_x(x.view(B * T, C)).view(B, T, C)

    # 添加一个额外的残差连接，融合 r, k, v 的乘积项
    x = x + ((r.view(B, T, H, -1) *
              k.view(B, T, H, -1) *
              self.r_k).sum(dim=-1, keepdim=True) *
             v.view(B, T, H, -1)).view(B, T, C)

    # 最终输出经过门控 g 控制，并通过线性层输出
    x = self.output(x * g)

    # 返回输出和 v_first，供下一层使用
    return x, v_first</code></pre>
<p data-spm-anchor-id="a2ty_o01.29997173.0.i93.4252c921puxtk5">模块基于以下两个核心机制：</p>
<h3>一. <strong>时间差分与方向调整</strong></h3>
<p>首先，Time Shift 通过 <code class="codespan cursor-pointer z-[9] relative">time_shift(x) - x</code> 得到当前 token 与前一 token 的差异，然后结合可学习参数生成多个“方向”的输入（如 <code class="codespan cursor-pointer z-[9] relative">xr</code>, <code class="codespan cursor-pointer z-[9] relative">xw</code>, <code class="codespan cursor-pointer z-[9] relative">xk</code>, <code class="codespan cursor-pointer z-[9] relative">xv</code>, <code class="codespan cursor-pointer z-[9] relative">xa</code>, <code class="codespan cursor-pointer z-[9] relative">xg</code>），分别用于构建不同的注意力相关参数。</p>
<p data-spm-anchor-id="a2ty_o01.29997173.0.i99.4252c921puxtk5">具体来说，<strong>Time Shift 的作用是将输入张量在时间维度上向前移动一个单位，</strong>这样在处理当前时间步 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist"><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span class="vlist-s">​</span></span></span></span></span></span></span></span> 时，模型可以获取到到上一个时间步 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist"><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span>1</span></span></span><span class="vlist-s">​</span></span></span></span></span></span></span></span> 的信息。</p>
<p data-spm-anchor-id="a2ty_o01.29997173.0.i99.4252c921puxtk5">举个例子：</p>
<pre class="language-python highlighter-hljs"><code># 假设原始输入是：
x = [
 [x_0_0, x_0_1, ..., x_0_C],   # 时间步 t=0
 [x_1_0, x_1_1, ..., x_1_C],   # 时间步 t=1
 ...
 [x_T_0, x_T_1, ..., x_T_C]    # 时间步 t=T
]

# Time Shift 后得到的是：
x_shifted = [
 [0, 0, ..., 0],               # 第一个位置用 0 填充（表示没有前一步）
 [x_0_0, x_0_1, ..., x_0_C],   # 第二个位置是 t=0 的值（相当于延迟了一个时间步）
 ...
 [x_{T-1}_0, ..., x_{T-1}_C]   # 最后一个位置是 t=T-1 的值
]</code></pre>
<p>&nbsp;</p>
<p>接着利用 Time Shift 的结果 <code>xx</code> 我们就可以计算不同的注意力相关参数。</p>
<pre class="language-python highlighter-hljs"><code>xr = x + xx * self.x_r
xw = x + xx * self.x_w
xk = x + xx * self.x_k
xv = x + xx * self.x_v
xa = x + xx * self.x_a
xg = x + xx * self.x_g</code></pre>
<p>这些新变量 <code class="codespan cursor-pointer z-[9] relative">xr, xw, xk, xv, xa, xg</code> 分别用于生成注意力机制中的不同角色：</p>
<div class="qwen-markdown-table-wrap qwen-markdown-table-wrap-pc group relative w-full">
<div class="scrollbar-hidden relative max-w-full overflow-x-auto whitespace-nowrap rounded-lg">
<table class="qwen-markdown-table w-full max-w-full table-auto rounded-xl text-left text-sm text-gray-500 dark:text-gray-400">
<thead class="qwen-markdown-table-thead border-none bg-gray-50 text-xs uppercase text-gray-700 dark:bg-gray-850 dark:text-gray-400">
<tr class="qwen-markdown-table-thead-tr">
<th class="qwen-markdown-table-thead-tr-th cursor-pointer select-none border border-gray-50 dark:border-gray-850" scope="col">
<div class="qwen-markdown-table-thead-tr-th-col flex items-center gap-1.5">变量</div>
</th>
<th class="qwen-markdown-table-thead-tr-th cursor-pointer select-none border border-gray-50 dark:border-gray-850" scope="col">
<div class="qwen-markdown-table-thead-tr-th-col flex items-center gap-1.5">对应角色</div>
</th>
<th class="qwen-markdown-table-thead-tr-th cursor-pointer select-none border border-gray-50 dark:border-gray-850" scope="col">
<div class="qwen-markdown-table-thead-tr-th-col flex items-center gap-1.5">作用</div>
</th>
</tr>
</thead>
<tbody class="qwen-markdown-table-tbody">
<tr class="qwen-markdown-table-tbody-tr bg-white text-xs dark:border-gray-850 dark:bg-gray-900">
<td class="qwen-markdown-table-tbody-tr-td w-max border border-gray-50 text-gray-900 dark:border-gray-850 dark:text-white">
<div class="qwen-markdown-table-tbody-tr-td-col flex items-center gap-[2px]"><code class="codespan cursor-pointer z-[9] relative">xr</code></div>
</td>
<td class="qwen-markdown-table-tbody-tr-td w-max border border-gray-50 text-gray-900 dark:border-gray-850 dark:text-white">
<div class="qwen-markdown-table-tbody-tr-td-col flex items-center gap-[2px]">receptance (r)</div>
</td>
<td class="qwen-markdown-table-tbody-tr-td w-max border border-gray-50 text-gray-900 dark:border-gray-850 dark:text-white">
<div class="qwen-markdown-table-tbody-tr-td-col flex items-center gap-[2px]">类似 query，在状态更新中起调节作用</div>
</td>
</tr>
<tr class="qwen-markdown-table-tbody-tr bg-white text-xs dark:border-gray-850 dark:bg-gray-900">
<td class="qwen-markdown-table-tbody-tr-td w-max border border-gray-50 text-gray-900 dark:border-gray-850 dark:text-white">
<div class="qwen-markdown-table-tbody-tr-td-col flex items-center gap-[2px]"><code class="codespan cursor-pointer z-[9] relative">xw</code></div>
</td>
<td class="qwen-markdown-table-tbody-tr-td w-max border border-gray-50 text-gray-900 dark:border-gray-850 dark:text-white">
<div class="qwen-markdown-table-tbody-tr-td-col flex items-center gap-[2px]">decayweight (w)</div>
</td>
<td class="qwen-markdown-table-tbody-tr-td w-max border border-gray-50 text-gray-900 dark:border-gray-850 dark:text-white">
<div class="qwen-markdown-table-tbody-tr-td-col flex items-center gap-[2px]">控制遗忘速率</div>
</td>
</tr>
<tr class="qwen-markdown-table-tbody-tr bg-white text-xs dark:border-gray-850 dark:bg-gray-900">
<td class="qwen-markdown-table-tbody-tr-td w-max border border-gray-50 text-gray-900 dark:border-gray-850 dark:text-white">
<div class="qwen-markdown-table-tbody-tr-td-col flex items-center gap-[2px]"><code class="codespan cursor-pointer z-[9] relative">xk</code></div>
</td>
<td class="qwen-markdown-table-tbody-tr-td w-max border border-gray-50 text-gray-900 dark:border-gray-850 dark:text-white">
<div class="qwen-markdown-table-tbody-tr-td-col flex items-center gap-[2px]">key (k)</div>
</td>
<td class="qwen-markdown-table-tbody-tr-td w-max border border-gray-50 text-gray-900 dark:border-gray-850 dark:text-white">
<div class="qwen-markdown-table-tbody-tr-td-col flex items-center gap-[2px]">用于构建上下文关联</div>
</td>
</tr>
<tr class="qwen-markdown-table-tbody-tr bg-white text-xs dark:border-gray-850 dark:bg-gray-900">
<td class="qwen-markdown-table-tbody-tr-td w-max border border-gray-50 text-gray-900 dark:border-gray-850 dark:text-white">
<div class="qwen-markdown-table-tbody-tr-td-col flex items-center gap-[2px]"><code class="codespan cursor-pointer z-[9] relative">xv</code></div>
</td>
<td class="qwen-markdown-table-tbody-tr-td w-max border border-gray-50 text-gray-900 dark:border-gray-850 dark:text-white">
<div class="qwen-markdown-table-tbody-tr-td-col flex items-center gap-[2px]">value (v)</div>
</td>
<td class="qwen-markdown-table-tbody-tr-td w-max border border-gray-50 text-gray-900 dark:border-gray-850 dark:text-white">
<div class="qwen-markdown-table-tbody-tr-td-col flex items-center gap-[2px]">上下文信息载体</div>
</td>
</tr>
<tr class="qwen-markdown-table-tbody-tr bg-white text-xs dark:border-gray-850 dark:bg-gray-900">
<td class="qwen-markdown-table-tbody-tr-td w-max border border-gray-50 text-gray-900 dark:border-gray-850 dark:text-white">
<div class="qwen-markdown-table-tbody-tr-td-col flex items-center gap-[2px]"><code class="codespan cursor-pointer z-[9] relative">xa</code></div>
</td>
<td class="qwen-markdown-table-tbody-tr-td w-max border border-gray-50 text-gray-900 dark:border-gray-850 dark:text-white">
<div class="qwen-markdown-table-tbody-tr-td-col flex items-center gap-[2px]">learning rate (a)</div>
</td>
<td class="qwen-markdown-table-tbody-tr-td w-max border border-gray-50 text-gray-900 dark:border-gray-850 dark:text-white">
<div class="qwen-markdown-table-tbody-tr-td-col flex items-center gap-[2px]">动态调整学习率</div>
</td>
</tr>
<tr class="qwen-markdown-table-tbody-tr bg-white text-xs dark:border-gray-850 dark:bg-gray-900">
<td class="qwen-markdown-table-tbody-tr-td w-max border border-gray-50 text-gray-900 dark:border-gray-850 dark:text-white">
<div class="qwen-markdown-table-tbody-tr-td-col flex items-center gap-[2px]"><code class="codespan cursor-pointer z-[9] relative">xg</code></div>
</td>
<td class="qwen-markdown-table-tbody-tr-td w-max border border-gray-50 text-gray-900 dark:border-gray-850 dark:text-white">
<div class="qwen-markdown-table-tbody-tr-td-col flex items-center gap-[2px]">gate (g)</div>
</td>
<td class="qwen-markdown-table-tbody-tr-td w-max border border-gray-50 text-gray-900 dark:border-gray-850 dark:text-white">
<div class="qwen-markdown-table-tbody-tr-td-col flex items-center gap-[2px]" data-spm-anchor-id="a2ty_o01.29997173.0.i113.4252c921puxtk5">非线性激活门控</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<h3 data-spm-anchor-id="a2ty_o01.29997173.0.i8.42525171ZOPka6"><strong data-spm-anchor-id="a2ty_o01.29997173.0.i10.42525171ZOPka6">二.线性变换生成核心参数</strong></h3>
<pre class="language-python highlighter-hljs"><code>r = self.receptance(xr)
w = -F.softplus(-(self.w0 + torch.tanh(xw @ self.w1) @ self.w2)) - 0.5 # soft-clamp to (-inf, -0.5)
k = self.key(xk)
v = self.value(xv)</code></pre>
<ul>
<li><strong><code class="codespan cursor-pointer z-[9] relative">receptance(xr)</code> </strong>: 生成 <code class="codespan cursor-pointer z-[9] relative">r</code>（类似传统注意力中的 query，但用于控制当前状态如何与新的输入进行交互，决定哪些信息会被“接收”并用于输出计算）。</li>
<li><strong><code class="codespan cursor-pointer z-[9] relative">w</code> </strong>: 权重衰减参数（控制遗忘速率），类似于 RNN 中的遗忘门。
<ul>
<li><strong>公式 </strong>：<code class="codespan cursor-pointer z-[9] relative">w = -softplus(-(...)) - 0.5</code>，确保 <code class="codespan cursor-pointer z-[9] relative">w</code> 为负值。</li>
<li><strong>作用 </strong>：模拟指数衰减，遗忘旧信息。</li>
</ul>
</li>
<li><strong><code class="codespan cursor-pointer z-[9] relative">key(xk)</code> </strong>: 生成 <code class="codespan cursor-pointer z-[9] relative">k</code>（key 向量，决定新信息如何与当前 state 进行交互）。</li>
<li><strong><code class="codespan cursor-pointer z-[9] relative">value(xv)</code> </strong>: 生成 <code class="codespan cursor-pointer z-[9] relative">v</code>（value 向量，表示输入的“值”，包含实际的内容信息）。</li>
</ul>
<h3>三. <strong data-spm-anchor-id="a2ty_o01.29997173.0.i51.42525171ZOPka6">残差连接机制</strong></h3>
<pre class="language-python highlighter-hljs"><code>if self.layer_id == 0:
    v_first = v # store the v of the first layer
else:
    v = v + (v_first - v) * torch.sigmoid(self.v0 + (xv @ self.v1) @ self.v2) # add value residual</code></pre>
<blockquote>
<h4 data-spm-anchor-id="a2ty_o01.29997173.0.i68.42525171ZOPka6">一、什么是残差连接？</h4>
<p><strong>✅ 简单定义：</strong></p>
<p><strong>残差连接（Residual Connection） </strong>是指将某一层的输入直接加到该层的输出上。<br>数学表达为：</p>
<p data-spm-anchor-id="a2ty_o01.29997173.0.i55.42525171ZOPka6"><code><span class="katex-display"><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="mord text"><span class="mord">output</span></span><span class="mrel">=</span></span><span class="base"><span class="mord text"><span class="mord">input</span></span><span class="mbin">+</span></span><span class="base"><span class="mord text"><span class="mord">F(input)</span></span></span></span></span></span></code></p>
<p>其中 <code class="codespan cursor-pointer z-[9] relative">F(input)</code> 是网络中某个函数或模块对输入的变换。</p>
<hr>
<p><strong>🎯 二、为什么需要残差连接？</strong></p>
<p>残差连接最早出现在 <strong>ResNet </strong>中，用于解决深度神经网络中的两个关键问题：</p>
<p>1. <strong>梯度消失/爆炸</strong></p>
<ul>
<li>在深层网络中，反向传播时梯度可能会指数级衰减或爆炸。</li>
<li>残差连接可以缓解这个问题，使得梯度更容易流动。</li>
</ul>
<p>2. <strong>信息丢失 / 语义漂移</strong></p>
<ul>
<li>随着层数加深，原始输入的信息可能被逐渐稀释。</li>
<li>残差连接可以让模型在处理过程中始终保留原始输入的信息。</li>
</ul>
<p>3. <strong>训练稳定性提升</strong></p>
<p><span style="font-size: 14px">实验表明，加入残差连接后，深层网络更容易优化和收敛。</span></p>
</blockquote>
<p data-spm-anchor-id="a2ty_o01.29997173.0.i55.42525171ZOPka6">在长序列建模中，随着序列变长，中间 token 可能会逐渐丢失最初的上下文信息，通过 <code class="codespan cursor-pointer z-[9] relative">v_first</code> 引入初始 token 的 value，可以让后续层仍然保有最初的信息。<code class="codespan cursor-pointer z-[9] relative">(v_first - v)</code> 是一个残差项，使得当前值 <code class="codespan cursor-pointer z-[9] relative">v</code> 可以根据初始值进行调整，这种设计有助于缓解梯度消失问题，提升训练稳定性。</p>
<p data-spm-anchor-id="a2ty_o01.29997173.0.i55.42525171ZOPka6">主要公式可以拆解为三个主要部分：</p>
<div>
<div class="my-2 code-cntainer code-cntainer-mobile" dir="ltr">
<div class="language-python  -mt-8 rounded-t-xl rounded-b-xl overflow-hidden  code-content">
<div id="code-textarea-e5575214-b477-45a4-ba9b-828e86834099-10" class="code-textarea h-full w-full ">
<div class="cm-editor ͼ1 ͼ3 ͼ4 ͼo">
<div class="cm-scroller" tabindex="-1">
<div class="cm-content" role="textbox" contenteditable="false" spellcheck="false" translate="no" aria-multiline="true" aria-placeholder="Enter your code here..." data-language="python" aria-autocomplete="list">
<div class="cm-activeLine cm-line"><span class="ͼq">v</span> <span class="ͼv">=</span> <span class="ͼq">v</span> <span class="ͼv">+</span></div>
<div class="cm-line">(<span class="ͼq">v_first</span> <span class="ͼv">-</span> <span class="ͼq">v</span>) <span class="ͼv">*</span></div>
<div class="cm-line"><span class="ͼq">torch</span><span class="ͼv">.</span><span class="ͼq">sigmoid</span>(<span class="ͼq">self</span><span class="ͼv">.</span><span class="ͼq">v0</span> <span class="ͼv">+</span> (<span class="ͼq">xv</span> <span class="ͼv">@</span> <span class="ͼq">self</span><span class="ͼv">.</span><span class="ͼq">v1</span>) <span class="ͼv">@</span> <span class="ͼq">self</span><span class="ͼv">.</span><span class="ͼq">v2</span>)</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<h4>1. <code class="codespan cursor-pointer z-[9] relative">v</code>: 当前时间步的 value 向量</h4>
<ul>
<li>是通过 <code class="codespan cursor-pointer z-[9] relative">self.value(xv)</code> 得到的当前 token 的 value 表示。</li>
<li>它表示模型对当前 token 的理解或编码。</li>
</ul>
<h4 data-spm-anchor-id="a2ty_o01.29997173.0.i52.42525171ZOPka6">2. <code class="codespan cursor-pointer z-[9] relative">v_first</code>: 第一个时间步的 value</h4>
<ul>
<li>来自第一个层的第一个 token 的 value（在 <code class="codespan cursor-pointer z-[9] relative">layer_id == 0</code> 时被赋值）。</li>
<li>目的是在整个序列处理过程中保留初始信息（类似 RNN 中的“初始状态”或 Transformer 中的 <code class="codespan cursor-pointer z-[9] relative">[CLS]</code> token）。</li>
</ul>
<h4>3. <code class="codespan cursor-pointer z-[9] relative">(v_first - v)</code>: 初始值与当前值之间的差异</h4>
<ul>
<li>这是一个残差项，表示当前 token 与初始 token 的语义差距。</li>
<li>如果当前 token 与初始 token 差异较大，则这个值会更大，可能意味着需要更多的初始信息来修正当前值。</li>
</ul>
<h4>4. <code class="codespan cursor-pointer z-[9] relative">torch.sigmoid(...)</code>：门控函数</h4>
<ul>
<li>将线性变换的结果映射到 <code class="codespan cursor-pointer z-[9] relative">[0, 1]</code> 区间。</li>
<li>控制 <code class="codespan cursor-pointer z-[9] relative">(v_first - v)</code> 对当前 <code class="codespan cursor-pointer z-[9] relative">v</code> 的影响程度。</li>
<li>类似于 LSTM 中的遗忘门或 GRU 中的更新门。</li>
</ul>
<h4>5. <code class="codespan cursor-pointer z-[9] relative">self.v0 + (xv @ self.v1) @ self.v2</code>：可学习的门控参数</h4>
<ul>
<li><code class="codespan cursor-pointer z-[9] relative">xv</code>: 经过 time_shift 处理后的输入，用于生成 value 的原始输入。</li>
<li><code class="codespan cursor-pointer z-[9] relative">self.v1</code>, <code class="codespan cursor-pointer z-[9] relative">self.v2</code>: 两个低秩矩阵，构成 LoRA 结构（Low-Rank Adaptation），用于减少参数量。</li>
<li>整个表达式是一个门控信号，根据当前输入动态决定是否引入初始信息。</li>
</ul>
<h3><strong data-spm-anchor-id="a2ty_o01.29997173.0.i70.42525171ZOPka6">四. 动态学习率参数 <code class="codespan cursor-pointer z-[9] relative">a</code> 和门控参数 <code class="codespan cursor-pointer z-[9] relative">g</code></strong></h3>
<pre class="language-python highlighter-hljs"><code>a = torch.sigmoid(self.a0 + (xa @ self.a1) @ self.a2) # a is "in-context learning rate"
g = torch.sigmoid(xg @ self.g1) @ self.g2</code></pre>
<p><strong><code class="codespan cursor-pointer z-[9] relative">a</code> 是动态学习率参数 </strong>，用于控制状态（state）更新的强度，使模型能够根据上下文自适应地调整记忆更新的幅度；</p>
<p><strong><code class="codespan cursor-pointer z-[9] relative">g</code> 是门控参数 </strong>，用于调节最终输出的激活程度，通过可学习的权重决定哪些信息应该被强调或抑制，从而增强模型的表达能力和非线性建模能力。两者共同提升了模型对长序列的建模效率和准确性。</p>
<h3 data-spm-anchor-id="a2ty_o01.29997173.0.i103.42525171ZOPka6"><strong>五. 调用 RWKV7_OP 状态更新函数</strong></h3>
<pre class="language-python highlighter-hljs"><code>x = RWKV7_OP(r, w, k, v, -kk, kk*a)</code></pre>
<h3>六.&nbsp;<strong>组合输出</strong></h3>
<pre class="language-python highlighter-hljs"><code>x = self.ln_x(x.view(B * T, C)).view(B, T, C)
x = x + ((r.view(B,T,H,-1)*k.view(B,T,H,-1)*self.r_k).sum(dim=-1, keepdim=True) * v.view(B,T,H,-1)).view(B,T,C)
x = self.output(x * g)</code></pre>
<ul>
<li><strong><code class="codespan cursor-pointer z-[9] relative">self.ln_x</code> </strong>: 对 <code class="codespan cursor-pointer z-[9] relative">x</code> 进行分组归一化（GroupNorm），提升训练稳定性。</li>
<li><strong>残差连接 </strong>：将 <code class="codespan cursor-pointer z-[9] relative">r</code>, <code class="codespan cursor-pointer z-[9] relative">k</code>, <code class="codespan cursor-pointer z-[9] relative">v</code> 的乘积加入输出。
<ul>
<li><strong>公式 </strong>：<code class="codespan cursor-pointer z-[9] relative">x += (r * k * r_k) · v</code>，增强非线性表达。</li>
</ul>
</li>
<li data-spm-anchor-id="a2ty_o01.29997173.0.i108.42525171ZOPka6"><strong>门控输出 </strong>：<code class="codespan cursor-pointer z-[9] relative">x = self.output(x * g)</code>，通过门控参数 <code class="codespan cursor-pointer z-[9] relative">g</code> 调制输出。</li>
</ul>
<h2><strong data-spm-anchor-id="a2ty_o01.29997173.0.i113.3bdbc921a7pOTf">通道混合 ChannelMix （<code>RWKV_CMix_x070</code>）</strong></h2>
<pre class="language-python highlighter-hljs"><code>class RWKV_CMix_x070(MyModule):
    def __init__(self, args, layer_id):
        super().__init__()
        self.args = args
        self.layer_id = layer_id
        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))  # 时间维度的位移操作
        with torch.no_grad():
            self.x_k = nn.Parameter(torch.empty(1, 1, args.n_embd))  # 可学习的权重参数
        self.key = nn.Linear(args.n_embd, args.dim_ffn, bias=False)  # 第一阶段线性变换
        self.value = nn.Linear(args.dim_ffn, args.n_embd, bias=False)  # 第二阶段线性变换

    @MyFunction
    def forward(self, x):
        xx = self.time_shift(x) - x  # 计算时间差分
        k = x + xx * self.x_k  # 调整输入特征
        k = torch.relu(self.key(k)) ** 2  # 非线性激活
        return self.value(k)  # 输出变换后的特征</code></pre>
<h3 data-spm-anchor-id="a2ty_o01.29997173.0.i119.3bdbc921a7pOTf"><strong>关键组件解析</strong></h3>
<h4><strong>(1) 时间位移（Time Shift）</strong></h4>
<ul>
<li>使用 <code class="codespan cursor-pointer z-[9] relative">nn.ZeroPad2d((0, 0, 1, -1))</code> 对输入张量进行时间维度的位移（前移一位），生成 <code class="codespan cursor-pointer z-[9] relative">time_shift(x)</code>。</li>
<li>计算差值 <code class="codespan cursor-pointer z-[9] relative">xx = time_shift(x) - x</code>，捕捉序列中相邻位置的局部变化特征。</li>
</ul>
<h4><strong>(2) 输入调整（Input Gating）</strong></h4>
<ul>
<li>引入可学习参数 <code class="codespan cursor-pointer z-[9] relative">self.x_k</code>，通过 <code class="codespan cursor-pointer z-[9] relative">k = x + xx * self.x_k</code> 调整输入特征，类似门控机制，控制历史信息的保留比例。</li>
</ul>
<h4><strong>(3) 非线性变换</strong></h4>
<ul>
<li><strong>第一阶段 </strong>：通过 <code class="codespan cursor-pointer z-[9] relative">key</code> 线性层将输入从 <code class="codespan cursor-pointer z-[9] relative">n_embd</code> 维度映射到更高维度 <code class="codespan cursor-pointer z-[9] relative">dim_ffn</code>（通常为 <code class="codespan cursor-pointer z-[9] relative">4 * n_embd</code>）。</li>
<li><strong>激活函数 </strong>：使用 <code class="codespan cursor-pointer z-[9] relative">ReLU</code> 激活后平方（<code class="codespan cursor-pointer z-[9] relative">torch.relu(...) ** 2</code>），增强非线性表达能力。</li>
<li data-spm-anchor-id="a2ty_o01.29997173.0.i116.3bdbc921a7pOTf"><strong>第二阶段 </strong>：通过 <code class="codespan cursor-pointer z-[9] relative">value</code> 线性层将特征映射回 <code class="codespan cursor-pointer z-[9] relative">n_embd</code> 维度，形成最终输出。</li>
</ul>
<h3><strong>与传统 FFN 的对比</strong></h3>
<div class="qwen-markdown-table-wrap qwen-markdown-table-wrap-pc group relative w-full">
<div class="scrollbar-hidden relative max-w-full overflow-x-auto whitespace-nowrap rounded-lg">
<table class="qwen-markdown-table w-full max-w-full table-auto rounded-xl text-left text-sm text-gray-500 dark:text-gray-400">
<thead class="qwen-markdown-table-thead border-none bg-gray-50 text-xs uppercase text-gray-700 dark:bg-gray-850 dark:text-gray-400">
<tr class="qwen-markdown-table-thead-tr">
<th class="qwen-markdown-table-thead-tr-th cursor-pointer select-none border border-gray-50 dark:border-gray-850" scope="col">
<div class="qwen-markdown-table-thead-tr-th-col flex items-center gap-1.5">特性</div>
</th>
<th class="qwen-markdown-table-thead-tr-th cursor-pointer select-none border border-gray-50 dark:border-gray-850" scope="col">
<div class="qwen-markdown-table-thead-tr-th-col flex items-center gap-1.5" data-spm-anchor-id="a2ty_o01.29997173.0.i125.3bdbc921lFEqpY">RWKV_CMix_x070</div>
</th>
<th class="qwen-markdown-table-thead-tr-th cursor-pointer select-none border border-gray-50 dark:border-gray-850" scope="col">
<div class="qwen-markdown-table-thead-tr-th-col flex items-center gap-1.5">传统 Transformer FFN</div>
</th>
</tr>
</thead>
<tbody class="qwen-markdown-table-tbody">
<tr class="qwen-markdown-table-tbody-tr bg-white text-xs dark:border-gray-850 dark:bg-gray-900">
<td class="qwen-markdown-table-tbody-tr-td w-max border border-gray-50 text-gray-900 dark:border-gray-850 dark:text-white">
<div class="qwen-markdown-table-tbody-tr-td-col flex items-center gap-[2px]" data-spm-anchor-id="a2ty_o01.29997173.0.i129.3bdbc921lFEqpY"><strong>激活函数</strong></div>
</td>
<td class="qwen-markdown-table-tbody-tr-td w-max border border-gray-50 text-gray-900 dark:border-gray-850 dark:text-white">
<div class="qwen-markdown-table-tbody-tr-td-col flex items-center gap-[2px]" data-spm-anchor-id="a2ty_o01.29997173.0.i127.3bdbc921lFEqpY"><code class="codespan cursor-pointer z-[9] relative">ReLU(x)^2</code></div>
</td>
<td class="qwen-markdown-table-tbody-tr-td w-max border border-gray-50 text-gray-900 dark:border-gray-850 dark:text-white">
<div class="qwen-markdown-table-tbody-tr-td-col flex items-center gap-[2px]"><code class="codespan cursor-pointer z-[9] relative">GELU</code>或<code class="codespan cursor-pointer z-[9] relative">Swish</code></div>
</td>
</tr>
<tr class="qwen-markdown-table-tbody-tr bg-white text-xs dark:border-gray-850 dark:bg-gray-900">
<td class="qwen-markdown-table-tbody-tr-td w-max border border-gray-50 text-gray-900 dark:border-gray-850 dark:text-white" data-spm-anchor-id="a2ty_o01.29997173.0.i130.3bdbc921lFEqpY">
<div class="qwen-markdown-table-tbody-tr-td-col flex items-center gap-[2px]"><strong>线性层</strong></div>
</td>
<td class="qwen-markdown-table-tbody-tr-td w-max border border-gray-50 text-gray-900 dark:border-gray-850 dark:text-white">
<div class="qwen-markdown-table-tbody-tr-td-col flex items-center gap-[2px]">两层（<code class="codespan cursor-pointer z-[9] relative">key</code>+<code class="codespan cursor-pointer z-[9] relative">value</code>）</div>
</td>
<td class="qwen-markdown-table-tbody-tr-td w-max border border-gray-50 text-gray-900 dark:border-gray-850 dark:text-white">
<div class="qwen-markdown-table-tbody-tr-td-col flex items-center gap-[2px]">两层（<code class="codespan cursor-pointer z-[9] relative" data-spm-anchor-id="a2ty_o01.29997173.0.i126.3bdbc921lFEqpY">linear + activation</code>）</div>
</td>
</tr>
<tr class="qwen-markdown-table-tbody-tr bg-white text-xs dark:border-gray-850 dark:bg-gray-900">
<td class="qwen-markdown-table-tbody-tr-td w-max border border-gray-50 text-gray-900 dark:border-gray-850 dark:text-white">
<div class="qwen-markdown-table-tbody-tr-td-col flex items-center gap-[2px]"><strong>时间依赖性</strong></div>
</td>
<td class="qwen-markdown-table-tbody-tr-td w-max border border-gray-50 text-gray-900 dark:border-gray-850 dark:text-white">
<div class="qwen-markdown-table-tbody-tr-td-col flex items-center gap-[2px]">显式引入时间差分（<code class="codespan cursor-pointer z-[9] relative">time_shift</code>）</div>
</td>
<td class="qwen-markdown-table-tbody-tr-td w-max border border-gray-50 text-gray-900 dark:border-gray-850 dark:text-white">
<div class="qwen-markdown-table-tbody-tr-td-col flex items-center gap-[2px]">无</div>
</td>
</tr>
<tr class="qwen-markdown-table-tbody-tr bg-white text-xs dark:border-gray-850 dark:bg-gray-900">
<td class="qwen-markdown-table-tbody-tr-td w-max border border-gray-50 text-gray-900 dark:border-gray-850 dark:text-white">
<div class="qwen-markdown-table-tbody-tr-td-col flex items-center gap-[2px]"><strong>参数量</strong></div>
</td>
<td class="qwen-markdown-table-tbody-tr-td w-max border border-gray-50 text-gray-900 dark:border-gray-850 dark:text-white">
<div class="qwen-markdown-table-tbody-tr-td-col flex items-center gap-[2px]" data-spm-anchor-id="a2ty_o01.29997173.0.i132.3bdbc921lFEqpY">较低（依赖<code class="codespan cursor-pointer z-[9] relative">dim_ffn</code>）</div>
</td>
<td class="qwen-markdown-table-tbody-tr-td w-max border border-gray-50 text-gray-900 dark:border-gray-850 dark:text-white">
<div class="qwen-markdown-table-tbody-tr-td-col flex items-center gap-[2px]" data-spm-anchor-id="a2ty_o01.29997173.0.i134.3bdbc921lFEqpY">较高（通常为</div>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.3472513788761574" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-06-01 17:59">2025-06-01 17:51</span>&nbsp;
<a href="https://www.cnblogs.com/Ctrl-cCtrl-v">TimTu</a>&nbsp;
阅读(<span id="post_view_count">11</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18906192);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18906192', targetLink: 'https://www.cnblogs.com/Ctrl-cCtrl-v/p/18906192', title: 'RWKV-7 架构理解' })">举报</a>
</div>
        