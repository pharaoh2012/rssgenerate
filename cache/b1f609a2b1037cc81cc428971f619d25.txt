
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/mjunz/p/18705472" title="发布于 2025-02-08 21:58">
    <span role="heading" aria-level="2">RocketMQ实战—7.生产集群部署和生产参数</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body blogpost-body-html">
<p data-track="1" data-pm-slice="0 0 []"><strong>大纲</strong></p>
<p data-track="2"><strong>1.RocketMQ生产集群部署和生产参数分析</strong></p>
<p data-track="3"><strong>2.RocketMQ生产集群10wTPS压测</strong></p>
<p data-track="4"><strong>3.RocketMQ生产级故障案例</strong></p>
<p data-track="5">&nbsp;</p>
<p data-track="6"><strong>1.RocketMQ生产集群部署和生产参数分析</strong></p>
<p data-track="7"><strong>(1)服务器数量</strong></p>
<p data-track="8">4C8G阿⾥云⾼配服务器共四台，公⽹IP假设如下：</p>
<pre class="highlighter-hljs"><code>139.224.217.92，106.15.250.248，47.102.152.14，139.224.212.58</code></pre>
<p data-track="10"><strong>(2)集群规划</strong></p>
<p data-track="11">采取2台NameServer，2组MasterSlave Broker的部署结构。</p>
<div class="pgc-img"><img src="https://p3-sign.toutiaoimg.com/tos-cn-i-6w9my0ksvp/88ddada8ba7f4bddab2285f737341950~tplv-obj.image?lk3s=ef143cfe&amp;traceid=2025020821561250CAC7733ED2DCEA2691&amp;x-expires=2147483647&amp;x-signature=u%2BWchW%2ByUNqBBoefFQfAmtaHtbw%3D" data-ic="false" data-width="772" data-height="738" data-ic-uri=""></div>
<p data-track="12"><strong>(3)环境搭建</strong></p>
<p data-track="13"><strong>一.下载安装包</strong></p>
<pre class="highlighter-hljs"><code>https://www.apache.org/dyn/closer.cgi?path=rocketmq/4.9.2/rocketmq-all-4.9.2-bin-release.zip</code></pre>
<p data-track="15">或者直接在服务器：</p>
<pre class="highlighter-hljs"><code>$ wget https://archive.apache.org/dist/rocketmq/4.9.2/rocketmq-all-4.9.2-bin-release.zip
$ wget https://mirror.bit.edu.cn/apache/rocketmq/4.9.2/rocketmq-all-4.9.2-bin-release.zip</code></pre>
<p data-track="17">以上两个链接都可，后⾯的链接是在中国的镜像服务，速度会快⼀些。</p>
<p data-track="18">&nbsp;</p>
<p data-track="19"><strong>二.解压安装包</strong></p>
<p data-track="20">这⾥下载的包解压路径是：</p>
<pre class="highlighter-hljs"><code>$ ll
usr/local/rocketmq/rocketmq-4.9.2</code></pre>
<p data-track="22">解压后的包结构如下：</p>
<pre class="highlighter-hljs"><code>$ ls
benchmark  bin  conf  lib  LICENCE  NOTICE  README.md</code></pre>
<p data-track="24">接下来要选择⼀套集群模式来部署，官⽅提供配置⽂件内容中，集群模式有以下⼏种：2m-2s-async(2主2从异步)、2m-2s-sync(2主2从同步)、2m-noslave(2主)。这里采取官⽅推荐的第⼀种模式，2m-2s-async(2主2从异步)的模式。</p>
<p data-track="25">&nbsp;</p>
<p data-track="26"><strong>三.前置⼯作：修改/etc/hosts⽂件，修改IP映射</strong></p>
<pre class="highlighter-hljs"><code>$ vi /etc/hosts
139.224.217.92 rocketmq-nameserver1 
139.224.217.92 rocketmq-master1-slave 
106.15.250.248 rocketmq-nameserver2 
106.15.250.248 rocketmq-master2-slave 
47.102.152.14 rocketmq-master1 
139.224.212.58 rocketmq</code></pre>
<p data-track="28"><strong>四.修改Broker-Master配置和Broker-slave配置</strong></p>
<p data-track="29">步骤⼀：进⼊Broker-Master的conf/2m-2s-async这个配置⽂件⽬录，执⾏vim broker-a.properties命令编辑配置⽂件，把以下内容粘贴进配置⽂件中：</p>
<pre class="highlighter-hljs"><code># 集群名称，如果公司有多组MQ集群，建议起不同名称 
brokerClusterName=rocketmq-cluster 
# Broker名称 
brokerName=broker-a 
# 0表示的是当前是Master节点，⼤于0，则表示是Slave节点 
brokerId=0 
# 删除⽂件时间点，默认凌晨是4点 
deleteWhen=04 
# ⽂件保留时间，默认保留48⼩时，我们这⾥设置为168⼩时也就是⼀周时间 
fileReservedTime=168 
# ASYNC_MASTER 异步复制Master 
# SYNC_MASTER 同步双写Master 
brokerRole=ASYNC_MASTER 
# 刷盘⽅式，ASYNC_FLUSH异步刷盘 
# SYNC_FLUSH 同步刷盘
flushDiskType=ASYNC_FLUSH 
# Broker 对外服务的监听端⼝ 
listenPort=10911 
# 是否允许 Broker ⾃动创建Topic，⼀般线下可以开启，线上最好关闭 
autoCreateTopicEnable=true 
# 是否允许 Broker ⾃动创建订阅组，⼀般线下可以开启，线上最好关闭 
autoCreateSubscriptionGroup=true 
# nameServer地址;分号分割 
namesrvAddr=rocketmq-nameserver1:9876;rocketmq-nameserver2:9876 
# 存储路径 
storePathRootDir=/usr/local/rocketmq/rocketmq-4.9.2/store/broker-a 
# commitLog 存储路径 
storePathCommitLog=/usr/local/rocketmq/rocketmq-4.9.2/store/broker-a/commitlog 
# 消费队列存储路径存储路径 
storePathConsumeQueue=/usr/local/rocketmq/rocketmq-4.9.2/store/broker-a/consumequeue 
# 消息索引存储路径 
storePathIndex=/usr/local/rocketmq/rocketmq-4.9.2/store/broker-a/index 
# checkpoint 检查点⽂件存储路径 
storeCheckpoint=/usr/local/rocketmq/rocketmq-4.9.2/store/checkpoint 
# abort ⽂件存储路径 
abortFile=/usr/local/rocketmq/rocketmq-4.9.2/store/abort</code></pre>
<p data-track="31">步骤二：进⼊Broker-Slave的conf/2m-2s-async这个配置⽂件⽬录，执⾏vim broker-a.properties命令编辑配置⽂件，把以下内容粘贴进配置⽂件中：</p>
<pre class="highlighter-hljs"><code># 集群名称，如果公司有多组MQ集群，建议起不同名称
brokerClusterName=rocketmq-cluster
# Broker名称
brokerName=broker-a-s
# 0表示的是当前是Master节点，⼤于0，则表示是Slave节点
brokerId=1
# 删除⽂件时间点，默认凌晨是4点
deleteWhen=04
# ⽂件保留时间，默认保留48⼩时，我们这⾥设置为168⼩时也就是⼀周时间
fileReservedTime=168
# ASYNC_MASTER 异步复制Master
# SYNC_MASTER 同步双写Master
brokerRole=SLAVE
# 刷盘⽅式，ASYNC_FLUSH异步刷盘
# SYNC_FLUSH 同步刷盘
flushDiskType=ASYNC_FLUSH
# Broker 对外服务的监听端⼝
listenPort=10911
# 是否允许 Broker ⾃动创建Topic，⼀般线下可以开启，线上最好关闭
autoCreateTopicEnable=true
# 是否允许 Broker ⾃动创建订阅组，⼀般线下可以开启，线上最好关闭
autoCreateSubscriptionGroup=true
# nameServer地址;分号分割
namesrvAddr=rocketmq-nameserver1:9876;rocketmq-nameserver2:9876
# 这个参数代表当前Broker监听的IP，如果不指定的话，会默认选⼀个IP，但是在阿⾥云服务器上，⼀般是内外⽹两个⽹卡，两个IP，此时就可能会出错，造成⽆法通讯
brokerIP1=47.102.152.14
# 存在Broker主从时，在Broker主节点上配置了brokerIP2的话，Broker从节点会连接主节点配置的brokerIP2来同步，所以从节点可以配置这个IP做主从同步brokerIP2 
# 存储路径
storePathRootDir=/usr/local/rocketmq/rocketmq-4.9.2/store/broker-a-s
# commitLog 存储路径
storePathCommitLog=/usr/local/rocketmq/rocketmq-4.9.2/store/broker-a-s/commitlog
# 消费队列存储路径存储路径
storePathConsumeQueue=/usr/local/rocketmq/rocketmq-4.9.2/store/broker-a-s/consumequeue
# 消息索引存储路径
storePathIndex=/usr/local/rocketmq/rocketmq-4.9.2/store/broker-a-s/index
#checkpoint 检查点⽂件存储路径
storeCheckpoint=/usr/local/rocketmq/rocketmq-4.9.2/store/checkpoint
#abort ⽂件存储路径
abortFile=/usr/local/rocketmq/rocketmq-4.9.2/store/abort</code></pre>
<p data-track="33">注意：上⾯是⼀对Master-Slave的配置⽂件。由于⼀共有四台机器，所以需要配置两组；第⼆组配置⽂件中的配置保持不变，只需要把：</p>
<pre class="highlighter-hljs"><code>brokerName=broker-a 改为 brokerName=broker-b
brokerName=broker-a-s 改为 brokerName=broker-b-s</code></pre>
<p data-track="35"><strong>五.创建⽂件存储路径</strong></p>
<p data-track="36">在47.102.152.14机器上执⾏：</p>
<pre class="highlighter-hljs"><code>$ mkdir -p /usr/local/rocketmq/rocketmq-4.9.2/store/broker-a
$ mkdir -p /usr/local/rocketmq/rocketmq-4.9.2/store/broker-a/consumequeue
$ mkdir -p /usr/local/rocketmq/rocketmq-4.9.2/store/broker-a/commitlog
$ mkdir -p /usr/local/rocketmq/rocketmq-4.9.2/store/broker-a/index</code></pre>
<p data-track="38">在139.224.212.58机器上执⾏：</p>
<pre class="highlighter-hljs"><code>$ mkdir -p /usr/local/rocketmq/rocketmq-4.9.2/store/broker-b
$ mkdir -p /usr/local/rocketmq/rocketmq-4.9.2/store/broker-b/consumequeue
$ mkdir -p /usr/local/rocketmq/rocketmq-4.9.2/store/broker-b/commitlog
$ mkdir -p /usr/local/rocketmq/rocketmq-4.9.2/store/broker-b/index</code></pre>
<p data-track="40">在139.224.217.92机器上执⾏：</p>
<pre class="highlighter-hljs"><code>$ mkdir -p /usr/local/rocketmq/rocketmq-4.9.2/store/broker-a-s
$ mkdir -p /usr/local/rocketmq/rocketmq-4.9.2/store/broker-a-s/consumequeue
$ mkdir -p /usr/local/rocketmq/rocketmq-4.9.2/store/broker-a-s/commitlog
$ mkdir -p /usr/local/rocketmq/rocketmq-4.9.2store/broker-a-s/index</code></pre>
<p data-track="42">在106.15.250.248机器上执⾏：</p>
<pre class="highlighter-hljs"><code>$ mkdir -p /usr/local/rocketmq/rocketmq-4.9.2/store/broker-b-s
$ mkdir -p /usr/local/rocketmq/rocketmq-4.9.2/store/broker-b-s
$ mkdir -p /consumequeue /usr/local/rocketmq/rocketmq-4.9.2store/broker-b-s/commitlog
$ mkdir -p /usr/local/rocketmq/rocketmq-4.9.2store/broker-b-s/index</code></pre>
<p data-track="44"><strong>六.启动NameServer</strong></p>
<p data-track="45">步骤一：编辑NameServer的配置⽂件</p>
<p data-track="46">JDK堆内存默认给的是4G，年轻代2G，我们机器配置有限，就调⼩⼀倍，设置成2g。</p>
<pre class="highlighter-hljs"><code>$ vi /usr/local/rocketmq/rocketmq-4.9.2/bin/runserver.sh
JAVA_OPT="${JAVA_OPT} -server -Xms4g -Xmx4g -Xmn2g -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=320m" 
JAVA_OPT="${JAVA_OPT} -XX:+UseConcMarkSweepGC -XX:+UseCMSCompactAtFullCollection -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled -XX:SoftRefLRUPolicyMSPerMB=0 -XX:+CMSClassUnloadingEnabled -XX:SurvivorRatio=8 -XX:-UseParNewGC" 
JAVA_OPT="${JAVA_OPT} -verbose:gc -Xloggc:${GC_LOG_DIR}/rmq_srv_gc_%p_%t.log -XX:+PrintGCDetails -XX:+PrintGCDateStamps"
JAVA_OPT="${JAVA_OPT} -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=30m"
else 
JAVA_OPT="${JAVA_OPT} -server -Xms4g -Xmx4g -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=320m" 
JAVA_OPT="${JAVA_OPT} -XX:+UseG1GC -XX:G1HeapRegionSize=16m -XX:G1ReservePercent=25 -XX:InitiatingHeapOccupancyPercent=30 -XX:SoftRefLRUPolicyMSPerMB=0" 
JAVA_OPT="${JAVA_OPT} -Xlog:gc*:file=${GC_LOG_DIR}/rmq_srv_gc_%p_%t.log:time,tags:filecount=5,filesize=30M" </code></pre>
<p data-track="48">步骤二：修改完成后，后台启动两台NameServer</p>
<pre class="highlighter-hljs"><code>$ nohup sh /usr/local/rocketmq/rocketmq-4.9.2/bin/mqnamesrv &gt;/usr/local/rocketmq/rocketmq-4.9.2/logs/mqnamesrv.log 2&gt;&amp;1 &amp;</code></pre>
<p data-track="50">启动完成后，输⼊jps，出现如下内容，即说明启动成功：</p>
<pre class="highlighter-hljs"><code>$ jps
14416 NamesrvStartup
14435 Jps</code></pre>
<p data-track="52"><strong>七.启动四台Broker</strong></p>
<p data-track="53">步骤一：编辑Broker的配置文件</p>
<pre class="highlighter-hljs"><code>JAVA_OPT="${JAVA_OPT} -server -Xms8g -Xmx8g"
JAVA_OPT="${JAVA_OPT} -XX:+UseG1GC -XX:G1HeapRegionSize=16m -XX:G1ReservePercent=25 -XX:InitiatingHeapOccupancyPercent=30 -XX:SoftRefLRUPolicyMSPerMB=0"
JAVA_OPT="${JAVA_OPT} -verbose:gc -Xloggc:${GC_LOG_DIR}/rmq_broker_gc_%p_%t.log -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCApplicationStoppedTime -XX:+PrintAdaptiveSizePolicy"
JAVA_OPT="${JAVA_OPT} -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=30m"
JAVA_OPT="${JAVA_OPT} -XX:-OmitStackTraceInFastThrow"
JAVA_OPT="${JAVA_OPT} -XX:+AlwaysPreTouch"
JAVA_OPT="${JAVA_OPT} -XX:MaxDirectMemorySize=15g"
JAVA_OPT="${JAVA_OPT} -XX:-UseLargePages -XX:-UseBiasedLocking"
JAVA_OPT="${JAVA_OPT} -Djava.ext.dirs=${JAVA_HOME}/jre/lib/ext:${BASE_DIR}/lib:${JAVA_HOME}/lib/ext"</code></pre>
<p data-track="55">步骤二：按照上⾯步骤中的Broker节点规划，分别启动四台Broker。</p>
<p data-track="56">注意在指定配置⽂件的时候，要修改机器上配置好的那个⽂件，分别如下：</p>
<pre class="highlighter-hljs"><code>47.102.152.14
$ nohup sh /usr/local/rocketmq/rocketmq-4.9.2/bin/mqbroker -c /usr/local/rocketmq/rocketmq-4.9.2/conf/2m-2s-async/broker-a.properties &gt; /usr/local/rocketmq/rocketmq-4.9.2/logs/broker-a.log 2&gt;&amp;1 &amp;

139.224.212.58 
$ nohup sh /usr/local/rocketmq/rocketmq-4.9.2/bin/mqbroker -c /usr/local/rocketmq/rocketmq-4.9.2/conf/2m-2s-async/broker-b.properties &gt; /usr/local/rocketmq/rocketmq-4.9.2/logs/broker-b.log 2&gt;&amp;1 &amp;

139.224.217.92 
$ nohup sh /usr/local/rocketmq/rocketmq-4.9.2/bin/mqbroker -c /usr/local/rocketmq/rocketmq-4.9.2/conf/2m-2s-async/broker-a-s.properties &gt; /usr/local/rocketmq/rocketmq-4.9.2/logs/broker-a-s.log 2&gt;&amp;1 &amp;

106.15.250.248 
$ nohup sh /usr/local/rocketmq/rocketmq-4.9.2/bin/mqbroker -c /usr/local/rocketmq/rocketmq-4.9.2/conf/2m-2s-async/broker-b-s.properties &gt; /usr/local/rocketmq/rocketmq-4.9.2/logs/broker-b-s.log 2&gt;&amp;1 &amp;</code></pre>
<p data-track="58">输⼊jps，出现如下内容，即说明启动成功：</p>
<pre class="highlighter-hljs"><code>$ jps
14258 Jps
14196 BrokerStartup</code></pre>
<p data-track="60"><strong>八.启动RocketMQ控制台，查看集群信息</strong></p>
<p data-track="61">这个rocketmq-all-4.9.2-bin-release.zip包在本地解压后，⾥⾯有⼀个console项⽬，文件名为rocketmq-console。打开之后把NameServer配置成我们⾃⼰的NameServer地址即可启动console查看mq集群的情况。使用IDEA打开console项⽬，会⾃动加载依赖环境，加载完成后修改配置⽂件application.properties里的nameserver地址。接着对console项目进行打包，打包命令是：</p>
<pre class="highlighter-hljs"><code>$ mvn clean package -Dmaven.test.skip=true</code></pre>
<p data-track="63">打包完成后，在target⽬录下，找到rocketmq-console-ng-1.0.0.jar放到服务器上即可运⾏。这⾥是将这个console项⽬放到了集群环境的⼀台服务器中：47.102.152.14。在服务区中启动console项⽬的命令是：</p>
<pre class="highlighter-hljs"><code>$ nohup java -jar rocketmq-console-ng-1.0.0.jar &gt;/usr/local/rocketmq/rocketmq-4.9.2/logs/mq-console.log 2&gt;&amp;1 &amp;</code></pre>
<p data-track="65">启动后访问如下链接，即可查看集群情况。</p>
<pre class="highlighter-hljs"><code>http://47.102.152.14:8080/#/cluster</code></pre>
<p data-track="67"><strong>(4)集群参数调优</strong></p>
<p data-track="68"><strong>一.JVM参数调优</strong></p>
<p data-track="69">-server -Xms4g -Xmx4g -Xmn2g</p>
<p data-track="70">Broker堆内存与新⽣代⼤⼩，因为Broker是很吃内存的，所以尽可能给多⼀点内存。这里因为服务器是4C8G，所以设置堆内存4G，新⽣代2G，并且不允许堆内存⾃动扩展。虽然RocketMQ使⽤的是G1垃圾回收器，但直接给⼀个固定⼤内存，与给⼀个范围相比，可让G1⾃动调整的效率更⾼。</p>
<p data-track="71">&nbsp;</p>
<p data-track="72">-XX:+AlwaysPreTouch</p>
<p data-track="73">RocketMQ是使⽤⻚缓存的，这个参数可以在RocketMQ开启时，预先把每⼀⻚都分配好。但坏处就是，会导致RocketMQ启动速度变慢。如果不关心启动速度，可以开启这个参数，让RocektMQ在运⾏初期提升性能，虽然运⾏⼀段时间后性能没有差异。</p>
<p data-track="74">&nbsp;</p>
<p data-track="75">-XX:-UseBiasedLocking</p>
<p data-track="76">官⽅推荐关闭偏向锁以减少GC停顿时间。</p>
<p data-track="77">&nbsp;</p>
<p data-track="78">-XX:+UseG1GC</p>
<p data-track="79">&nbsp;</p>
<p data-track="80">-XX:G1HeapRegionSize=16m</p>
<p data-track="81">&nbsp;</p>
<p data-track="82">-XX:G1ReservePercent=25</p>
<p data-track="83">通过-XX:G1ReservePercent可以指定G1为分配担保预留的空间⽐例，默认10%。也就是⽼年代会预留10%的空间来给新⽣代的对象晋升。这个值设置为25，表示会预留⽐较多的空间来给新⽣代的对象晋升，这个参数设定也是官⽅推荐的参数。</p>
<p data-track="84">&nbsp;</p>
<p data-track="85">-XX:InitiatingHeapOccupancyPercent=30</p>
<p data-track="86">通过-XX:InitiatingHeapOccupancyPercent指定触发全局并发标记的⽼年代使⽤占⽐。默认值45%，表示的是⽼年代占堆的⽐例超过45%。如果Mixed GC周期结束后⽼年代使⽤率还是超过45%，那么会再次触发全局并发标记过程。这样就会导致频繁的⽼年代GC，影响应⽤吞吐量，这个参数调整成30性能会更好。</p>
<p data-track="87">&nbsp;</p>
<p data-track="88">以上这些参数都是经过RocketMQ官⽅测试后的参数设置，在Broker启动⽂件⾥⾯这些参数也都是设置好的，所以基本上不需要我们⾃⼰来调整。</p>
<p data-track="89">&nbsp;</p>
<p data-track="90">-XX:MaxGCPauseMillis = 20</p>
<p data-track="91">这个参数的含义是，每次GC时期望GC停顿的时间是多久，20代表20ms。这个参数对于RocketMQ运⾏还是很关键的，因为RocektMQ需要很⾼的性能，很低的延迟。所以很多⼈会有⼀个误区，认为这个参数需要调整的⽐较⼩，这样可以提⾼RocketMQ的响应速度。真实的情况是，如果这个-XX:MaxGCPauseMillis = 20参数设置过⼩，G1垃圾回收器会因为这个⽐较⼩的停顿时间，⽽给新⽣代⼀个⾮常⼩的空间，这就导致频繁的触发YGC。⽽RocketMQ⼜是⼀个QPS⾮常⾼的系统，那么就很可能会造成对象晋升⾄⽼年代过快，造成频繁Full GC。所以MaxGCPauseMillis值尽可能不要设置太⼩，⼀般来说可以不设置保持默认即可。</p>
<p data-track="92">&nbsp;</p>
<p data-track="93"><strong>二.OS操作系统内核参数调优</strong></p>
<p data-track="94">调优参数列表：</p>
<pre class="highlighter-hljs"><code>$ export PATH=$PATH:/sbin
$ sudo sysctl -w vm.extra_free_kbytes=512000
$ sudo sysctl -w vm.min_free_kbytes=512000
$ sudo sysctl -w vm.overcommit_memory=1
$ sudo sysctl -w vm.drop_caches=1
$ sudo sysctl -w vm.zone_reclaim_mode=0
$ sudo sysctl -w vm.max_map_count=65536
$ sudo sysctl -w vm.dirty_background_ratio=50
$ sudo sysctl -w vm.dirty_ratio=50
$ sudo sysctl -w vm.dirty_writeback_centisecs=36000
$ sudo sysctl -w vm.page-cluster=3
$ sudo sysctl -w vm.swappiness=1
$ echo 'ulimit -n 655350' &gt;&gt; /etc/profile
$ echo '* hard nofile 655350' &gt;&gt; /etc/security/limits.conf
$ echo '* hard memlock unlimited' &gt;&gt; /etc/security/limits.conf
$ echo '* soft memlock unlimited' &gt;&gt; /etc/security/limits.conf</code></pre>
<p data-track="96"><strong>三.调优前的单Producer压测</strong></p>
<p data-track="97">步骤一：首先看调优前的单Producer压测效果(即⽆消费者)。压测机的配置是4C8G两台，两台机器使⽤RocketMQ⾃带的benckmark压测组件进⾏压测。进⼊rocketmq-4.9.2/benchmark⽬录下，执⾏如下命令：</p>
<pre class="highlighter-hljs"><code>$ cd /usr/local/rocketmq/rocketmq-4.9.2/benchmark
$ sh producer.sh -n 139.224.217.92:9876;106.15.250.248:9876</code></pre>
<p data-track="99">可以看到这样的结果：第⼀台机器，发送的TPS是1.5-1.6w左右，最⼤响应时间456ms，平均响应时间4ms左右。第⼆台机器，发送的tps是2.4w左右，最⼤响应时间485ms，平均响应时间2.7ms左右。⽽我们此时观察RocketMQ客户端数据，两台Master节点的TPS总计在4w+。总体来说，单Producer的QPS在4w+的时候还是有很好的响应能⼒的。</p>
<p data-track="100">&nbsp;</p>
<p data-track="101">步骤二：接下来再继续加⼀点机器，把本地的Producer也启动起来。也就是打开源码包，找到example这个项⽬。找到org.apache.rocketmq.example.benchmark.Producer，然后把它启动起来，但要注意修改⼀下Producer的NameServer地址：</p>
<pre class="highlighter-hljs"><code>final DefaultMQProducer producer = new DefaultMQProducer("benchmark_producer", rpcHook, msgTraceEnable, null);
producer.setInstanceName(Long.toString(System.currentTimeMillis()));

if (commandLine.hasOption('n')) {
    String ns = commandLine.getOptionValue('n');
    producer.setNamesrvAddr(ns);
}
producer.serNamesrvAddr("139.224.217.92:9876;106.15.250.248:9876");

producer.setCompressMsgBodyOverHowmuch(Integer.MAX_VALUE);

producer.start();</code></pre>
<p data-track="103">可以看到，我们本地也能模拟出2k+的TPS。而此时两台压测机明显TPS也是下降了⼀些的，所以基本上这个集群抗住4w+TPS是差不多的，⽽且性能也还不错。此外注意看此时的NameServer系统指标： 可以看到CPU其实还没有到达极限，内存使⽤⽐较⾼了，⼤概在80%左右。⽽两台Broker-master机器的CPU，内存使⽤都⽐较⾼，但是还没有达到极限。</p>
<p data-track="104">&nbsp;</p>
<p data-track="105"><strong>四.调优前的单Consumer压测</strong></p>
<p data-track="106">接着看看调优前的单consumer压测效果(即⽆⽣产者)，进⼊rocketmq-4.9.2/benchmark⽬录下执⾏如下命令：</p>
<pre class="highlighter-hljs"><code>$ cd /usr/local/rocketmq/rocketmq-4.9.2/benchmark
$ sh consumer.sh -t BenchmarkTest -n 139.224.217.92:9876;106.15.250.248:9876 -g test2</code></pre>
<p data-track="108">可以看到，消费者消费的速度明显要⽐写⼊速度要快很多，两台都能达到4w左右。注意：其中RT这个时间⾮常久，原因是前面测试了单独的Producer，而Consumer⼀直没有开启。直到开启测试Consumer时已经过了很久了，这就导致这个时间计算的结果是从数据到MQ后就开始计算，所以值⽐较⼤。两台NameServer的压⼒看起来⾮常⼩，CPU和内存都基本没⽤。</p>
<p data-track="109">&nbsp;</p>
<p data-track="110"><strong>五.调优后单Producer的压测效果</strong></p>
<p data-track="111">从TPS上看，整体的TPS略有提升，但是提升并不明显，总体TPS提升了⼤概1-3k左右。系统指标：两台NameServer(包含Broker-Slave节点)的CPU和内存使⽤，也略有上升。两台Broker-Master节点的CPU，内存使⽤率也略有上升。</p>
<p data-track="112">&nbsp;</p>
<p data-track="113"><strong>六.调优后单Consumer的压测效果</strong></p>
<p data-track="114">很奇怪的现像发⽣了：这个总体的消费TPS只有总计不到两万了。其实在观测的过程中，最⾼达到过总计4w，但是依然和最开始测试的过程差距⽐较⼤，这是什么原因呢？从CPU、内存这两个关键指标来看，没什么问题，因为总体负载都不是很⾼。但是注意，磁盘IO⾮常⾼，说明现在影响到Consumer效率的是磁盘IO负载过于⾼了。</p>
<p data-track="115">&nbsp;</p>
<p data-track="116">之所以产⽣磁盘IO负载过高这个现象，是因为调整参数时，⽂件句柄调整的过⼤655350，导致IO压⼒飙升，反⽽对性能产⽣了较⼤影响。另外就是，⽂件落⼊磁盘，消费者消费数据时从磁盘读取数据到内存。最终恢复Linux内核参数65535后，压测结果回归正常。所以调整参数时，要进行前后压测对比，不能直接按照文档复制。</p>
<p data-track="117">&nbsp;</p>
<p data-track="118">把参数还原后，发送消息的TPS仍然是4w左右。可以说前⾯优化之后的1-3k从数量级来说，没有什么太⼤的性能提升。所以没有必要为了这⼀点写⼊效率，去调优参数，并且对消费者端造成巨⼤影响。</p>
<p data-track="119">&nbsp;</p>
<p data-track="120"><strong>2.RocketMQ生产集群10wTPS压测</strong></p>
<p data-track="121">两台4核8G的Master机器，通过压测可以扛下4w写TPS。单台4核8G的Master可以扛下2w写TPS，但CPU负载已经很高了。单台4核8G的Master可以扛下4W读TPS。所以如果要扛10wTPS，可以用5台4核8G的Master机器。</p>
<p data-track="122">&nbsp;</p>
<p data-track="123"><strong>3.RocketMQ生产级故障案例</strong></p>
<p data-track="124"><strong>(1)RocketMQ的VIP端⼝故障</strong></p>
<p data-track="125"><strong>(2)completbleFuture不规范使⽤导致消费速率低</strong></p>
<p data-track="126"><strong>(3)Producer发送消息失败问题</strong></p>
<p data-track="127">&nbsp;</p>
<p data-track="128"><strong>(1)RocketMQ的VIP端⼝故障</strong></p>
<p data-track="129">在打开Console控制台，打开Cluster时，看到如下所示的报错：</p>
<pre class="highlighter-hljs"><code>org.apache.rocketmq.remoting.exception.RemotingConnectException:
connect to &lt;139.224.212.58:10909&gt; failed</code></pre>
<p data-track="131">⽽Cluster⻚⾯也没有什么TPS数据什么的，这是什么原因？查看这个节点的Broker⽇志发现是⼀个远程链接问题：</p>
<pre class="highlighter-hljs"><code>2022-01-15 04:04:20 ERROR BrokerControllerScheduledThread1 - SyncTopicConfig Exception, 47.102.152.14:10911 
org.apache.rocketmq.remoting.exception.RemotingConnectException: connect to 47.102.152.14:10909 failed 
    at org.apache.rocketmq.remoting.netty.NettyRemotingClient.invokeSync(NettyRemotingClient.java:3 94) ~ [rocketmq-remoting-4.9.2.jar:4.9.2] 
    at org.apache.rocketmq.broker.out.BrokerOuterAPI.getAllTopicConfig(BrokerOuterAPI.java:330) ~ [rocketmq-broker-4.9.2.jar:4.9.2] at org.apache.rocketmq.broker.slave.SlaveSynchronize.syncTopicConfig(SlaveSynchronize.java:60) [rocketmq-broker-4.9.2.jar:4.9.2] 
    at org.apache.rocketmq.broker.slave.SlaveSynchronize.syncAll(SlaveSynchronize.java:49) [rocketmq-broker-4.9.2.jar:4.9.2] 
    at org.apache.rocketmq.broker.BrokerController$12.run(BrokerController.java:1144) [rocketmq-broker-4.9.2.jar:4.9.2] 
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_202] 
    at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [na:1.8.0_202] 
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(Scheduled ThreadPoolExecutor.java:180) [na:1.8.0_202] 
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadP oolExecutor.java:294) [na:1.8.0_202] 
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_202] 
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_202] 
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_202] 2022-01-15 04:04:20 INFO BrokerControllerScheduledThread1 - Update slave consumer offset from master, 47.102.152.14:10911 2022-01-15 04:04:20 INFO BrokerControllerScheduledThread1 - Update slave delay offset from master, 47.102.152.14:10911</code></pre>
<p data-track="133">很奇怪，看了异常信息，发现是主从同步的过程没有成功，原因是同步Topic配置信息失败：</p>
<pre class="highlighter-hljs"><code>SyncTopicConfig Exception, 47.102.152.14:10911
org.apache.rocketmq.remoting.exception.RemotingConnectException: connect to 47.102.152.14:10909 failed</code></pre>
<p data-track="135">⽐较奇怪的是，有这么⼀句话"47.102.152.14:10909 failed"，就是这么⼀句话容易让⼈疑惑。因为当前Broker是47.102.152.14节点上Broker的Slave节点，所以就先去查了⼀下47.102.152.14的配置如下：</p>
<pre class="highlighter-hljs"><code># 集群名称，如果公司有多组MQ集群，建议起不同名称
brokerClusterName=rocketmq-cluster
# Broker名称
brokerName=broker-a
# 0表示的是当前是Master节点，⼤于0，则表示是Slave节点
brokerId=0
# 删除⽂件时间点，默认凌晨是4点
deleteWhen=04
# ⽂件保留时间，默认保留48⼩时，我们这⾥设置为168⼩时也就是⼀周时间
fileReservedTime=168
# ASYNC_MASTER 异步复制Master
# SYNC_MASTER 同步双写Master
brokerRole=ASYNC_MASTER
# 刷盘⽅式，ASYNC_FLUSH异步刷盘
# SYNC_FLUSH 同步刷盘
flushDiskType=ASYNC_FLUSH
# Broker 对外服务的监听端⼝
listenPort=10911
# 是否允许 Broker ⾃动创建Topic，⼀般线下可以开启，线上最好关闭
autoCreateTopicEnable=true
# 是否允许 Broker ⾃动创建订阅组，⼀般线下可以开启，线上最好关闭
autoCreateSubscriptionGroup=true
# nameServer地址;分号分割
namesrvAddr=rocketmq-nameserver1:9876;rocketmq-nameserver2:9876
# 这个参数代表当前Broker监听的IP，如果不指定的话，会默认选⼀个IP，但是在阿⾥云服务器上，⼀般是内外⽹两个⽹卡，两个IP，此时就可能会出错，造成⽆法通讯
brokerIP1=47.102.152.14
# 存在Broker主从时，在Broker主节点上配置了brokerIP2的话，Broker从节点会连接主节点配置的brokerIP2来同步，所以从节点可以配置这个IP做主从同步brokerIP2 
# 存储路径
storePathRootDir=/usr/local/rocketmq/rocketmq-4.9.2/store/broker-a
# commitLog 存储路径
storePathCommitLog=/usr/local/rocketmq/rocketmq-4.9.2/store/broker-a/commitlog
# 消费队列存储路径存储路径
storePathConsumeQueue=/usr/local/rocketmq/rocketmq-4.9.2/store/broker-a/consumequeue
# 消息索引存储路径
storePathIndex=/usr/local/rocketmq/rocketmq-4.9.2/store/broker-a/index
#checkpoint 检查点⽂件存储路径
storeCheckpoint=/usr/local/rocketmq/rocketmq-4.9.2/store/checkpoint
#abort ⽂件存储路径
abortFile=/usr/local/rocketmq/rocketmq-4.9.2/store/abort</code></pre>
<p data-track="137">这个端⼝确实是10911，那为什么会出现10909这个端⼝呢？再去查Slave的端⼝，发现也是配置的是10911这个端⼝。所以不存在配置错误的问题，那此时就只能确定为，是RocketMQ本身的什么机制造成的。通过在官⽹、社区的搜索，最终发现，这个问题是存在类似情况的。</p>
<pre class="highlighter-hljs"><code>https://github.com/maihaoche/rocketmq-spring-boot-starter/issues/6</code></pre>
<p data-track="139">这个issue和这里情况还不完全⼀样，因为这个issue说的是Producer Consumer和服务端Broker的链接，⽽这里是Broker和Slave之间的连接。</p>
<p data-track="140">&nbsp;</p>
<p data-track="141">不过本质上应该类似，因为VIP端⼝默认就是Broker监听端⼝减去2，所以这个问题基本已经清晰了，是因为主从同步过程中，端⼝-2之后⽆法通讯造成的。所以接下来，要么配置⼀下不启⽤VIP端⼝，要么开启10909这个端⼝。基于专⻔接⼝做专⻔的事情这⼀考虑，还是设置开启10909这个端⼝。</p>
<p data-track="142">&nbsp;</p>
<p data-track="143"><strong>(2)completbleFuture不规范使⽤导致消费速率低</strong></p>
<p data-track="144">前⾯介绍过，消费者由于需要调⽤第三⽅平台接口等待响应结果，造成消费速率较低，所以使用了completbleFuture，但其实completbleFuture的使⽤也是有很⼤讲究的。</p>
<p data-track="145">&nbsp;</p>
<p data-track="146">在使⽤了completbleFuture之后，本来以为消费的速率会⽐正常消费要快很多，因为避免了等待响应的这种情况，⽽实际的结果却并不是如此。</p>
<p data-track="147">&nbsp;</p>
<p data-track="148">我们来准备⼀下参数，PostMan发起⼀个活动通知消息推送：</p>
<pre class="highlighter-hljs"><code>postman.setGlobalVariable("startTime",Date.parse(new Date("2022/01/13 15:07:26")));
postman.setGlobalVariable("endTime",Date.parse(new Date("2022/01/15 15:07:26")));
{
    "name": "促销活动",
    "informType": 2,
    "startTime": {{ startTime }},
    "endTime": {{ endTime }},
    "remark": "促销活动说明",
    "status": 1,
    "type": 1,
    "rule": {
        "key": "满减",
        "value": {
            "满减": "200,30"
        }
    },
    "createUser": 0 
}</code></pre>
<p data-track="150">调⽤接⼝：</p>
<pre class="highlighter-hljs"><code>http://localhost:8080/demo/promotion/send</code></pre>
<p data-track="152">调⽤成功后，会持续不断的发送消息到RocketMQ。然后pushService中，会不断的去消费消息，紧接着发送消息出去。我们可以在Consumer消费者⾥⾯通过不同的⽅式来调⽤不同的消费者。</p>
<p data-track="153">&nbsp;</p>
<p data-track="154">通过两者消费速率的对⽐发现，速率有⾮常⼤的差异，最⾼能差5倍以上，以下是实验数据：</p>
<p data-track="155">&nbsp;</p>
<p data-track="156">数据一：正常的消费速率，⼀直持续稳定在600-800之间，最⾼甚⾄能达到1k的消费速率。</p>
<p data-track="157">&nbsp;</p>
<p data-track="158">数据二：completableFuture，最开始启动的时候能达到⽐较⾼的600速率，运⾏⼀定时间后，就降到200速率了。</p>
<p data-track="159">&nbsp;</p>
<p data-track="160">很明显在运⾏⼀段时间后，completableFuture这种⽅式的消费速率下降了，并且消费效率明显⽐普通的消费回调⽅式要慢许多，那这究竟是怎么回事呢？</p>
<p data-track="161">&nbsp;</p>
<p data-track="162">Consumer端消费很慢，基本上主要的原因就是Consumer端线程太忙或者不够，这个时候就要回到completableFuture本身了，我们在代码中使⽤的是completableFuture.Async()⽅法：</p>
<pre class="highlighter-hljs"><code>List&lt;CompletableFuture&lt;AltResult&gt;&gt; futureList = msgList.stream()
    .map(e -&gt; CompletableFuture.supplyAsync(() -&gt; handleMessageExt(e)))
    .collect(Collectors.toList());</code></pre>
<p data-track="164">在CompletableFuture方法内部：</p>
<pre class="highlighter-hljs"><code>public class CompletableFuture&lt;T&gt; implements Future&lt;T&gt;, CompletionStage&lt;T&gt; {
    ...
    private static final Executor asyncPool = useCommonPool ? ForkJoinPool.commonPool() : new ThreadPerTaskExecutor();
    public static &lt;U&gt; CompletableFuture&lt;U&gt; supplyAsync(Supplier&lt;U&gt; supplier) {
        return asyncSupplyStage(asyncPool, supplier);
    }
    ...
    static final class ThreadPerTaskExecutor implements Executor {
        public void execute(Runnable r) { new Thread(r).start(); }
    }
}</code></pre>
<p data-track="166">而ForkJoinPool.commonPool()如下：</p>
<pre class="highlighter-hljs"><code>private static ForkJoinPool makeCommonPool() {
    int parallelism = -1;
    ForkJoinWorkerThreadFactory factory = null;
    UncaughtExceptionHandler handler = null;
    try {  // ignore exceptions in accessing/parsing properties
        String pp = System.getProperty("java.util.concurrent.ForkJoinPool.common.parallelism");
        String fp = System.getProperty("java.util.concurrent.ForkJoinPool.common.threadFactory");
        String hp = System.getProperty("java.util.concurrent.ForkJoinPool.common.exceptionHandler");
        if (pp != null)
            parallelism = Integer.parseInt(pp);
        if (fp != null)
            factory = ((ForkJoinWorkerThreadFactory)ClassLoader.getSystemClassLoader().loadClass(fp).newInstance());
        if (hp != null)
            handler = ((UncaughtExceptionHandler)ClassLoader.getSystemClassLoader().loadClass(hp).newInstance());
    } catch (Exception ignore) {
        
    }
    if (factory == null) {
        if (System.getSecurityManager() == null)
            factory = defaultForkJoinWorkerThreadFactory;
        else // use security-managed default
            factory = new InnocuousForkJoinWorkerThreadFactory();
    }
    if (parallelism &lt; 0 &amp;&amp; (parallelism = Runtime.getRuntime().availableProcessors() - 1) &lt;= 0)
        parallelism = 1;
    if (parallelism &gt; MAX_CAP)
        parallelism = MAX_CAP;
    return new ForkJoinPool(parallelism, factory, handler, LIFO_QUEUE, "ForkJoinPool.commonPool-worker-");
}</code></pre>
<p data-track="168">如果使⽤supplyAsync，那么就会有⼀个默认的线程池，⽽线程池⾥⾯的线程数⼤⼩是根据CPU核数 - 1来决定的。例如是4核那么线程数就是3，如果⾃定义⼀个公⽤线程池，所有消费者都使⽤这个线程池，假如开启了80个线程，但是真正处理消费后的核⼼⽅法逻辑的，其实只有3个线程。其他的77个任务相当于都在队列⾥⾯等待ForkJoinPool⾥的线程完成，所以导致整个消费慢了。</p>
<p data-track="169">&nbsp;</p>
<p data-track="170">如果其中⼀个ForkJoin线程慢了会拖累整个消费，最终可能影响到当前服务⾥订阅的所有Topic，影响整个MQ。这种场景⼀般不太会遇到，都是实战中开发时才会出现千奇百怪的问题。就这么个问题，如果不是对completableFuture.supplyAsync()源码极其了解，⼀般也想不到会出现这样的问题。所以，其实我们本意是想提升效率，最终却导致整体消费效率降低了。</p>
<p data-track="171">&nbsp;</p>
<p data-track="172"><strong>(3)Producer发送消息失败问题</strong></p>
<p data-track="173"><strong>一.问题描述</strong></p>
<p data-track="174">在修改batch推送消息，并使⽤线程池来并发推送消息⾄MQ后(在未改造线程池和batch之前没有出现下述异常)，针对百万⽤户活动消息推送⾄MQ的压测中，Producer出现了⾮常频繁的RemotingTooMuchRequestException。</p>
<p data-track="175">&nbsp;</p>
<p data-track="176">⾸先看报错⽇志的第⼀段：</p>
<pre class="highlighter-hljs"><code>org.apache.rocketmq.remoting.exception.RemotingTooMuchRequestException: sendDefaultImpl call timeout
    at org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl.sendDefaultImpl(DefaultMQProducerImpl.java:683) ~ [rocketmq-client-4.9.2.jar:4.9.2]
    at org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl.send(DefaultMQProducerImpl.java:1391) ~ [rocketmq-client-4.9.2.jar:4.9.2]
    at org.apache.rocketmq.client.producer.DefaultMQProducer.send(DefaultMQProducer.java:916) ~ [rocketmq-client-4.9.2.jar:4.9.2]
    at com.demo.eshop.promotion.mq.producer.DefaultProducer.sendMessages(DefaultProducer.java:155) [classes/:na]
    at com.demo.eshop.promotion.mq.producer.DefaultProducer.sendMessages(DefaultProducer.java:107) [classes/:na]
    at com.demo.eshop.promotion.mq.consumer.listener.PlatFormPromotionUserBucketListener.lambd a$consumeMessage$0(PlatFormPromotionUserBucketListener.java:116) [classes/:na]
    at com.demo.eshop.promotion.mq.consumer.listener.PlatFormPromotionUserBucketListener$$Lam bda$1092/806913152.run(Unknown Source) [classes/:na] 
    at com.demo.eshop.common.concurrent.SafeThreadPool.lambda$execute$0(SafeThreadPool.java: 42) [classes/:na] 
    at com.demo.eshop.common.concurrent.SafeThreadPool$$Lambda$1091/1786949609.run(Unkno wn Source) [classes/:na] 
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_40] 
    at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_40] 
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_40] 
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_40] 
    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_40] INFO 22512 --- 
        [lientSelector_1] RocketmqRemoting : closeChannel: close the connection to remote address[47.102.152.14:10911] result: true INFO 22512 --- 
        [lientSelector_1] RocketmqRemoting : closeChannel: close the connection to remote address[47.102.152.14:10911] result: true</code></pre>
<p data-track="178">接着看报错⽇志的第⼆段：</p>
<pre class="highlighter-hljs"><code>org.apache.rocketmq.client.exception.MQClientException: Send [3] times, still failed, cost [5037]ms, Topic: platform_promotion_send_topic, BrokersSent: [broker-b, broker-a, broker-b] See http://rocketmq.apache.org/docs/faq/ for further details. 
    at org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl.sendDefaultImpl(DefaultMQProducerImpl.java:681) ~[rocketmq-client-4.9.2.jar:4.9.2] 
    at org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl.send(DefaultMQProducerImpl.j ava:1391) ~ [rocketmq-client-4.9.2.jar:4.9.2] 
    at org.apache.rocketmq.client.producer.DefaultMQProducer.send(DefaultMQProducer.java:916) ~ [rocketmq-client-4.9.2.jar:4.9.2] 
    at com.demo.eshop.promotion.mq.producer.DefaultProducer.sendMessages(DefaultProducer.java: 155) [classes/:na]
    at com.demo.eshop.promotion.mq.producer.DefaultProducer.sendMessages(DefaultProducer.java: 107) [classes/:na] 
    at com.demo.eshop.promotion.mq.consumer.listener.PlatFormPromotionUserBucketListener.lambd a$consumeMessage$0(PlatFormPromotionUserBucketListener.java:116) [classes/:na] 
    at com.demo.eshop.promotion.mq.consumer.listener.PlatFormPromotionUserBucketListener$$Lam bda$1092/806913152.run(Unknown Source) [classes/:na] 
    at com.demo.eshop.common.concurrent.SafeThreadPool.lambda$execute$0(SafeThreadPool.java: 42) [classes/:na] 
    at com.demo.eshop.common.concurrent.SafeThreadPool$$Lambda$1091/1786949609.run(Unkno wn Source) [classes/:na] 
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_40] 
    at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_40] 
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_40] 
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_40] 
    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_40] Caused by: org.apache.rocketmq.remoting.exception.RemotingTimeoutException: wait response on the channel &lt;139.224.212.58:10911&gt; timeout, 3084(ms) 
    at org.apache.rocketmq.remoting.netty.NettyRemotingAbstract.invokeSyncImpl(NettyRemotingAbstr act.java:438) ~[rocketmq-remoting-4.9.2.jar:4.9.2] 
    at org.apache.rocketmq.remoting.netty.NettyRemotingClient.invokeSync(NettyRemotingClient.java:3 77) ~ [rocketmq-remoting-4.9.2.jar:4.9.2] 
    at org.apache.rocketmq.client.impl.MQClientAPIImpl.sendMessageSync(MQClientAPIImpl.java:505) ~ [rocketmq-client-4.9.2.jar:4.9.2] 
    at org.apache.rocketmq.client.impl.MQClientAPIImpl.sendMessage(MQClientAPIImpl.java:489) ~ [rocketmq-client-4.9.2.jar:4.9.2]
    at org.apache.rocketmq.client.impl.MQClientAPIImpl.sendMessage(MQClientAPIImpl.java:433) ~ [rocketmq-client-4.9.2.jar:4.9.2] 
    at org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl.sendKernelImpl(DefaultMQPro ducerImpl.java:870) ~ [rocketmq-client-4.9.2.jar:4.9.2] 
    at org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl.sendDefaultImpl(DefaultMQPr oducerImpl.java:606) ~ [rocketmq-client-4.9.2.jar:4.9.2] 
    ... 13 common frames omitted 2022-01-23 15:37:49.092 ERROR 22512 --- [sgThreadPool-27] c.r.e.p.mq.producer.DefaultProducer :发送MQ消息失败: type:平台优惠活动消息</code></pre>
<p data-track="180"><strong>二.问题原因分析</strong></p>
<p data-track="181">那么这个问题出现的原因是什么呢？先进⼊源码看看情况：在rocketmq-client-4.9.2.jar这个依赖中，找到DefaultMQProducerImpl这个类。在DefaultMQProducerImpl这个类中第683⾏，发现有异常信息抛出。</p>
<pre class="highlighter-hljs"><code>//在rocketmq-client-4.9.2.jar的org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl类中683行：
String info = String.format("Send [%d] times, still failed, cost [%d]ms, Topic: %s, BrokersSent: %s",
    times,
    System.currentTimeMillis() - beginTimestampFirst,
    msg.getTopic(),
    Arrays.toString(brokersSent));

info += FAQUrl.suggestTodo(FAQUrl.SEND_MSG_FAILED);

MQClientException mqClientException = new MQClientException(info, exception);
if (callTimeout) {
    throw new RemotingTooMuchRequestException("sendDefaultImpl call timeout");
}</code></pre>
<p data-track="183">这⾥有⼀个callTimeout参数，判断最后是否抛出了异常。那么这个参数是在哪⾥定义、更新的？</p>
<pre class="highlighter-hljs"><code>boolean callTimeout = false;
MessageQueue mq = null;
Exception exception = null;
SendResult sendResult = null;
int timesTotal = communicationMode == CommunicationMode.SYNC ? 1 + this.defaultMQProducer.getRetryTimesWhenSendFailed() : 1;
int times = 0;
String[] brokersSent = new String[timesTotal];
for (; times &lt; timesTotal; times++) {
    String lastBrokerName = null == mq ? null : mq.getBrokerName();
    MessageQueue mqSelected = this.selectOneMessageQueue(topicPublishInfo, lastBrokerName);
    if (mqSelected != null) {
        mq = mqSelected;
        brokersSent[times] = mq.getBrokerName();
        try {
            beginTimestampPrev = System.currentTimeMillis();
            if (times &gt; 0) {
                //Reset topic with namespace during resend.
                msg.setTopic(this.defaultMQProducer.withNamespace(msg.getTopic()));
            }
            long costTime = beginTimestampPrev - beginTimestampFirst;
            if (timeout &lt; costTime) {
                callTimeout = true;
                break;
            }</code></pre>
<p data-track="185">beginTimestampFirst，beginTimestampPrev⽐较好理解，就是sendmessage开始执⾏的时间，以及尝试发送的时间。或者说，尝试和MQ建⽴链接到得到⼀个反馈结果之后的时间。两个时间相减，如果⼤于了timeout这个时间，就说明超时了。</p>
<p data-track="186">&nbsp;</p>
<p data-track="187">那么这timeout是什么？我们在producer.send()发送的时候，实际上是调⽤了下⾯这个⽅法的：</p>
<pre class="highlighter-hljs"><code>public class DefaultMQProducer extends ClientConfig implements MQProducer {
    ...
    @Override
    public SendResult send(Collection&lt;Message&gt; msgs, long timeout) throws MQClientException, RemotingException, MQBrokerException, InterruptedException {
        return this.defaultMQProducerImpl.send(batch(msgs), timeout);
    }
    ...
}</code></pre>
<p data-track="189">⽽往下⼀层，实际上是调⽤了defaultMQProducerImpl.send()发出的消息。可以看到有⼀个参数，叫做timeout，defaultMQProducerImpl.send()消息的调⽤代码位置如下：</p>
<pre class="highlighter-hljs"><code>public class DefaultMQProducerImpl implements MQProducerInner {
    ...
    public SendResult send(Message msg, long timeout) throws MQClientException, RemotingException, MQBrokerException, InterruptedException {
        return this.sendDefaultImpl(msg, CommunicationMode.SYNC, null, timeout);
    }
    ...
}</code></pre>
<p data-track="191">也就是实际执⾏的send操作，是defaultMQProducerImpl的内部方法"private SendResult sendDefaultImpl"发出的。⽽初始化这么⼀个SendResult对象，会传⼊我们调⽤send的时候的timeout参数。</p>
<pre class="highlighter-hljs"><code>public class DefaultMQProducerImpl implements MQProducerInner {
    ...
    private SendResult sendDefaultImpl(
        Message msg,
        final CommunicationMode communicationMode,
        final SendCallback sendCallback,
        final long timeout
    ) throws MQClientException, RemotingException, MQBrokerException, InterruptedException {
        ...
        ...
    }
    ...
}</code></pre>
<p data-track="193">如果我们调⽤send()的时候，没有传⼊这个timeout参数，那么这个参数有⼀个默认值，是3000ms。没有传⼊超时参数的代码如下：</p>
<pre class="highlighter-hljs"><code>public class DefaultMQProducer extends ClientConfig implements MQProducer {
    ...
    @Override
    public SendResult send(
        Collection&lt;Message&gt; msgs) throws MQClientException, RemotingException, MQBrokerException, InterruptedException {
        return this.defaultMQProducerImpl.send(batch(msgs));
    }
    ...
}</code></pre>
<p data-track="195">点进去send()⽅法，发现它会get⼀个默认的Timeout时间：</p>
<pre class="highlighter-hljs"><code>public class DefaultMQProducerImpl implements MQProducerInner {
    ...
    public SendResult send(Message msg) throws MQClientException, RemotingException, MQBrokerException, InterruptedException {
        return send(msg, this.defaultMQProducer.getSendMsgTimeout());
    }
    ...
}</code></pre>
<p data-track="197">进⼊这个获取timeOut的⽅法：</p>
<pre class="highlighter-hljs"><code>public class DefaultMQProducer extends ClientConfig implements MQProducer {
    ...
    //Timeout for sending messages.
    private int sendMsgTimeout = 3000;
    public int getSendMsgTimeout() {
        return sendMsgTimeout;
    }
    ...
}</code></pre>
<p data-track="199">看到这⾥可知：发送消息的时间如果超过了3000ms还没有发送成功， 就会导致发送失败。</p>
<p data-track="200">&nbsp;</p>
<p data-track="201"><strong>三.发送失败的几种原因</strong></p>
<p data-track="202">Broker侧的原因：</p>
<pre class="highlighter-hljs"><code>原因1：并发太⾼导致发送给阻塞。
原因2：消息积压。
原因3：内存不够⽤，导致⼤量消息写磁盘效率极速下降。
原因4：频繁GC，导致send()操作等待过久。
原因5：发送的消息的⽹络带宽超过了最⼤⽹络带宽。
原因6：Chanel过于繁忙导致等待时间⽐较⻓，消息过⼤导致写⼊RocketMQ时间过⻓，其他消息等待时间过⻓。 </code></pre>
<p data-track="204">Client侧的原因：</p>
<pre class="highlighter-hljs"><code>原因1：设置的timeout时间不合理导致消息还没有发送成功久直接就失败了。
原因2：设置的batch数量过⼤，导致⼀次batch中的数据太多，发送时间过⻓导致超时。 
原因3：设置的发送线程池线程个数太多，导致RocketMQ连接压⼒过⼤。</code></pre>
<p data-track="206">经过排查发现：Broker的⽇志⼀切正常、没有什么报错信息。内存、CPU、GC情况也⽐较正常，其top信息除了内存其他的都⽐较正常。从列出的GC信息也可看到，从启动到现在，RocketMQ总计11次YGC，0次FGC。从⽹络带宽信息⾥⾯也可以看到，在发送⼤量消息前和发送⼤量消息后，⽹络使⽤量虽然很⼤(最⾼达到了20MB+)，但还远未达到100MB的程度，因为⽹卡是100MB，所以还⾮常富裕。所以从RocketMQ服务本身来看，没有什么问题。</p>
<p data-track="207">&nbsp;</p>
<p data-track="208">到⽬前为⽌，这个问题的排查，还是没有头绪，因为从RocketMQ本身来说，服务⾮常健康，那么问题出在哪⾥呢？</p>
<p data-track="209">&nbsp;</p>
<p data-track="210">我们回过头看⽇志，⽇志中有这么⼀句话：Send [3] times, still failed, cost [5037]ms。也就是说，花费了5037ms发送消息，尝试了3次最终还是发送失败，那是不是说，是超时时间的问题？超时时间调得⼤⼀点，会不会就解决了？</p>
<pre class="highlighter-hljs"><code>org.apache.rocketmq.client.exception.MQClientException: Send [3] times, still failed, cost [5037]ms</code></pre>
<p data-track="212">于是在代码中加⼊了发送消息，添加⼀个超时时间，设置为10000ms：</p>
<pre class="highlighter-hljs"><code>sharedSendMsgThreadPool.execute(() -&gt; {
    defaultProducer.sendMessages(RocketMqConstant.PLATFORM_PROMOTION_SEND_TOPIC, sendBatch, "平台优惠活动消息", 10000);
});</code></pre>
<p data-track="214">重新运⾏代码，执⾏推送全员优惠活动，惊喜的发现不报错了！但是这种改法有个问题，就是如果对于时效性要求特别⾼的场景很显然是不合适的。因为超时时间10000ms，意味着最慢要10s才能发送成功。加上消费者端识别到消息，拉取消息，最终处理，还是会耗时⽐较久的。那么还有没有其他的⽅式？</p>
<p data-track="215">&nbsp;</p>
<p data-track="216">根据上面列举的可能原因，除了Broker本身外，还有⼀些其他的可能原因，就是batchSize过⼤，或者是Chanel接⼝太忙。根据代码，batchSize显然是不⼤的，我们特意控制在100左右。那是不是有可能是我们线程池 + batch推送，导致端⼝以及⽹络Chanel压⼒过⼤导致超时？如果能再开⼀个端⼝，来建⽴Chanel是不是就能够减轻单端⼝的压⼒？</p>
<p data-track="217">&nbsp;</p>
<p data-track="218">注意，这是在我们服务器带宽、CPU、IO压⼒还⽐较⼩，就出现这个问题。所以接下来就简单了，直接设置⼀个参数，让Producer发送消息的时候能和Broker的其他端⼝建⽴Chanel。实际上还是和前⾯的那个案例相关，这个参数就是VipChannel参数。</p>
<p data-track="219">&nbsp;</p>
<p data-track="220">在Producer端可以这么设置：</p>
<pre class="highlighter-hljs"><code>@Component
public class DefaultProducer {
    private final TransactionMQProducer producer;

    @Autowired
    public DefaultProducer(RocketMQProperties rocketMQProperties) {
        producer = new TransactionMQProducer(RocketMqConstant.PUSH_DEFAULT_PRODUCER_GROUP);
        producer.setCompressMsgBodyOverHowmuch(Integer.Max_VALUE);
        producer.setNamesrvAddr(rocketMQProperties.getNameServer());
        producer.setVipChannelEnabled(true);
        start();
    }
    ...
}</code></pre>
<p data-track="222">开启之后，把超时时间去掉，发现程序运⾏完全OK，没有任何报错。</p>
<p>&nbsp;</p>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.008665930494212963" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-02-08 21:59">2025-02-08 21:58</span>&nbsp;
<a href="https://www.cnblogs.com/mjunz">东阳马生架构</a>&nbsp;
阅读(<span id="post_view_count">0</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18705472" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18705472);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18705472', targetLink: 'https://www.cnblogs.com/mjunz/p/18705472', title: 'RocketMQ实战—7.生产集群部署和生产参数' })">举报</a>
</div>
        