
		<h1 class="postTitle">
			<a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/zackstang/p/18855501" title="发布于 2025-05-05 21:56">
    <span role="heading" aria-level="2">Wan2.1 t2v模型Lora Fine-Tune</span>
    

</a>

		</h1>
		<div class="clear"></div>
		<div class="postBody">
			<div id="cnblogs_post_body" class="blogpost-body blogpost-body-html">
<h1>Wan2.1 t2v模型Lora Fine-Tune</h1>
<h2>1. Wan2.1模型</h2>
<p>Wan2.1是由阿里巴巴开源的AI视频生成大模型，具备强大的视觉生成能力，支持文本到视频（T2V）和图像到视频（I2V）任务。该模型基于前沿的Diffusion Transformer架构，结合了因果3D变分自编码器（VAE）和优化的训练策略，能够高效处理时空信息，生成高质量、连贯性强的视频。</p>
<p>该模型基于主流的视频Diffusion（扩散模型）和Transformer架构。扩散模型通过逐步去除噪声来生成数据，而Transformer架构则基于自注意力机制（Attention）捕捉长时程依赖关系，从而生成时空一致的高质量视频。在权威评测集VBench中，Wan2.1的14B参数专业版本以总分86.22%的成绩大幅超越了国内外其他模型（如Sora、Luma、Pika等），稳居榜首位置<sup>[1]</sup>。该模型能够生成复杂运动、还原真实物理规律、提升影视质感，并优化指令遵循。</p>
<p>本文不会介绍Wan 2.1 模型的原理，而是主要介绍如何对Wan 2.1 模型进行lora fine-tune，生成需要的视频风格，使用的工具为Diffusion-Pipe<sup>[2]</sup>。</p>
<h2>2. Diffusion Pipe</h2>
<p>Diffusion-Pipe 是一个用于扩散模型（diffusion models）的管道并行训练脚本，旨在通过分布式计算和高效的内存管理，训练那些超出单个 GPU 内存限制的大型模型。</p>
<p>使用这个项目进行训练可以简化很多流程，目前它可以提供的功能包括：</p>
<ol>
<li>管道并行训练：通过将模型分割到多个 GPU 上进行训练，Diffusion-Pipe 能够处理比单个 GPU 内存更大的模型。</li>
<li>支持多种模型：目前支持 SDXL、Flux、LTX-Video、HunyuanVideo（t2v）、Cosmos、Lumina Image 2.0、Wan2.1（t2v 和 i2v）以及 Chroma 等多种模型。</li>
<li>高效的多进程预缓存：通过多进程和多 GPU 预缓存潜在变量和文本嵌入，减少训练时的内存需求，加速训练过程。</li>
<li>Tensorboard 日志记录：记录训练过程中的关键指标，便于实时监控训练进度。</li>
<li>评估集指标计算：在保留的评估集上计算指标，帮助衡量模型的泛化能力。</li>
<li>训练状态检查点：支持训练状态的自动保存与恢复，确保训练过程的连续性。</li>
<li>易于扩展：通过实现一个简单的子类即可添加对新模型的支持，简化了集成流程。</li>
</ol>
<h2>3. 环境准备</h2>
<p>在这次lora fine-tune中，我们使用的环境为：</p>
<ol>
<li>计算资源：AWS g5.4xlarge实例（1块Nvidia A10G）</li>
<li>操作系统：ubuntu 22.04</li>
<li>模型：wan2.1-t2v-1.3b</li>
</ol>
<h3>3.1. 配置训练环境</h3>
<p>设置diffusion-pipe训练环境：</p>
<table>
<tbody>
<tr>
<td>
<p># 创建conda环境</p>
<p>conda create -n diffusion-pipe python=3.12</p>
<p>conda activate diffusion-pipe</p>
<p># 安装pytorch和cuda-nvcc</p>
<p>pip install torch==2.4.1 torchvision==0.19.1 --index-url https://download.pytorch.org/whl/cu121</p>
<p>conda install nvidia::cuda-nvcc</p>
<p># 下载diffusion-pipe</p>
<p>git clone <a href="https://github.com/tdrussell/diffusion-pipe" rel="noopener nofollow">https://github.com/tdrussell/diffusion-pipe</a></p>
<p>cd diffusion-pipe</p>
<p>pip install -r requirements.txt</p>
<p># 最后下载wan 2.1 模型到models/目录下：</p>
<p>huggingface-cli download Wan-AI/Wan2.1-T2V-1.3B-Diffusers --local-dir ./wan2.1-t2v-1.3b</p>
</td>
</tr>
</tbody>
</table>
<h3>3.2. 准备训练数据</h3>
<p>在准备数据集时，需要准备符合下面要求的数据：</p>
<ol>
<li>图片：至少10-15张图片（7-8张一般也可以，但效果相对会差一些），常规图片格式均可以（例如jpg, jpeg, png, webp等）。也可以使用mp4格式的短视频（2-3秒），但是会消耗较多的VRAM</li>
<li>Text Prompt：每张图片必须有一个对应的 .txt 文件，里面包含对应图片的描述说明</li>
<li>触发词：一个唯一的触发关键词，确保模型能映射到新学习到的风格或人物</li>
</ol>
<p>在准备Text Prompt时，注意以下格式：</p>
<ol>
<li>要有描述性，但要简洁（避免提示语过长或过短）</li>
<li>包含有关背景、服装、动作等方面的细节</li>
<li>在所有的“.txt”文件中始终使用相同的触发词</li>
</ol>
<p>示例数据：</p>
<p>A portrait of a GF Carty Chan with short brown hair, wearing a white T-shirt, set against a clear blue sky.</p>
<p><img src="https://img2024.cnblogs.com/blog/1287132/202504/1287132-20250430153007954-453055100.jpg"></p>
<p>在数据集准备完成后，将其上传到 /diffusion-pipe/data/input 文件夹中。</p>
<p>&nbsp;</p>
<h2>4. 训练</h2>
<p>首先编辑examples/dataset.toml文件，指定path为数据路径，例如：</p>
<p>path = '/home/ubuntu/diffusion-pipe/data/input'</p>
<p>以及设置：</p>
<p>num_repeats = 10</p>
<p>然后编辑examples/wan_14b_min_vram.toml文件，指定输出路径以及部分训练参数，例如epoch：</p>
<table>
<tbody>
<tr>
<td>
<p># change this</p>
<p>output_dir = '/home/ubuntu/diffusion-pipe/output'</p>
<p># and this</p>
<p>dataset = 'examples/dataset.toml'</p>
<p>epochs = 400</p>
<p>save_every_n_epochs = 10</p>
<p># 以及修改模型路径</p>
<p>ckpt_path = '/home/ubuntu/diffusion-pipe/models/wan2.1-t2v-1.3b'</p>
</td>
</tr>
</tbody>
</table>
<p>设置好这些配置后，开启一个新的tmux session进行训练：</p>
<table>
<tbody>
<tr>
<td>
<p>tmux new -s training</p>
<p>conda activate diffusion-pipe</p>
<p>cd diffusion-pipe</p>
<p>NCCL_P2P_DISABLE="1" NCCL_IB_DISABLE="1" deepspeed --num_gpus=1 train.py --deepspeed --config examples/wan_14b_min_vram.toml</p>
</td>
</tr>
</tbody>
</table>
<h2><span style="font-size: 1.5em">5. 训练结果</span></h2>
<p>在训练到390个epoch后，手动停止训练：</p>
<p>[Rank 0] step=31199, skipped=0, lr=[2e-05], mom=[[0.9, 0.99]]</p>
<p>steps: 31199 loss: 0.0023 iter time (s): 1.691 samples/sec: 0.591</p>
<p>[Rank 0] step=31200, skipped=0, lr=[2e-05], mom=[[0.9, 0.99]]</p>
<p>steps: 31200 loss: 0.0052 iter time (s): 1.694 samples/sec: 0.590</p>
<p>Saving model to directory epoch390</p>
<p>然后使用ComfyUI里构建一个Wan2.1的text2video的工作流查看效果，使用提示词：</p>
<p>GF Carty Chan, short dark hair, wearing a white T-shirt, Hair blowing in the wind。</p>
<p>&nbsp;</p>
<p>未使用lora时生成的视频效果：</p>
<p><img src="https://img2024.cnblogs.com/blog/1287132/202504/1287132-20250430153454795-802629385.gif" alt="" loading="lazy"></p>
<p>&nbsp;</p>
<p>使用lora后生成的视频效果：</p>
<p><img src="https://img2024.cnblogs.com/blog/1287132/202504/1287132-20250430153517508-1910487011.gif" alt="" loading="lazy"></p>
<p><img src="https://img2024.cnblogs.com/blog/1287132/202504/1287132-20250430153528066-731882550.gif" alt="" loading="lazy"></p>
<p>&nbsp;</p>
<p>可以看到生成的视频与训练数据里的人物基本保持了一致。</p>
<p>&nbsp;</p>
<h2>5. 总结</h2>
<p>通过 Diffusion-Pipe 对 Wan2.1 模型进行 LoRA 微调，可以在有限资源下有效调整视频生成风格，尤其适合特定人物或艺术风格的定制化视频生成任务。整个流程具备良好的可复现性和扩展性，适合进一步探索个性化AI视频创作场景。</p>
<p>下一篇文章我们会继续介绍如何使用图片和视频对 Wan2.1 模型进行 image-to-video 的 LoRA 微调。</p>
<p>&nbsp;</p>
<h2>References</h2>
<p>[1] 边缘云玩转通义万相Wan2.1-T2V推理业务最佳实践: <a href="https://help.aliyun.com/zh/ens/use-cases/wan2-1-t2v-1-3b-best-practice-of-reasoning-business" rel="noopener nofollow">https://help.aliyun.com/zh/ens/use-cases/wan2-1-t2v-1-3b-best-practice-of-reasoning-business</a></p>
<p>[2] Diffusion-Pine Github: <a href="https://github.com/tdrussell/diffusion-pipe" rel="noopener nofollow">https://github.com/tdrussell/diffusion-pipe</a></p>
</div>
<div class="clear"></div>

		</div>
		<div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.5517873867222223" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-05-05 21:57">2025-05-05 21:56</span>&nbsp;
<a href="https://www.cnblogs.com/zackstang">ZacksTang</a>&nbsp;
阅读(<span id="post_view_count">33</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18855501);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18855501', targetLink: 'https://www.cnblogs.com/zackstang/p/18855501', title: 'Wan2.1 t2v模型Lora Fine-Tune' })">举报</a>
</div>
	