
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/MrVolleyball/p/19052292" title="发布于 2025-08-22 11:10">
    <span role="heading" aria-level="2">彩笔运维勇闯机器学习--最小二乘法的数学推导</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h2 id="前言">前言</h2>
<p>今天我们来讨论一下回归算法当中的数学实现。本人数学也是渣，大学时期概率论一直挂到清考才勉强通过，+_+ !!，如今勇闯机器学习，硬着头皮重新学习了微积分和线代，也是为了记录自己最近的状态，避免过段时间忘记了。描述的时候有不周全的地方，请各位大佬们多担待了</p>
<p>本节将会运用一些数学知识来解释一下相关的回归算法的合理性，虽有些枯燥，但知其然也知其所以然，多了解一些总是好的</p>
<h2 id="最小二乘法">最小二乘法</h2>
<p>最小二乘法的核心思想是找到一组参数，使得模型预测值与实际观测值之间的误差平方和最小。最小二乘法是回归模型中非常常用的计算回归系数的方法：</p>
<p></p><div class="math display">\[\text{f} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\]</div><p></p><p>其中<span class="math inline">\(y_i\)</span>是真实值，<span class="math inline">\(\hat{y}_i\)</span>是预测值</p>
<h4 id="推导过程">推导过程</h4>
<p>先用最简单的一元线性回归，一元线性回归的数学模型为：</p>
<p></p><div class="math display">\[\hat{y_i}=β_0+β_1x_i
\]</div><p></p><p>带入公式：</p>
<p></p><div class="math display">\[\text{f} = \sum_{i=1}^{n} (y_i - (β_0+β_1x_i))^2 = \sum_{i=1}^{n} (y_i - β_0 - β_1x_i)^2
\]</div><p></p><p>由于要讨论的是<span class="math inline">\(β_0\)</span>和<span class="math inline">\(β_1\)</span>，这是一个多变量函数，为了研究单独变量，可以分别对其求偏导</p>
<p></p><div class="math display">\[\frac{\partial f}{\partial β_0} = (\sum_{i=1}^{n} (y_i - β_0 - β_1x_i)^2)'
\]</div><p></p><p>首先，有限个数的求和之后的导数=有限个数导数之后求和，把<span class="math inline">\((y_i - β_0 - β_1x_i)^2\)</span>看成一个整体</p>
<p></p><div class="math display">\[\frac{\partial f}{\partial β_0} = \sum_{i=1}^{n} ((y_i - β_0 - β_1x_i)^2)'
\]</div><p></p><p>这是复合函数求导，那就来个剥洋葱法则，先对平方求导，再对加法求导</p>
<p></p><div class="math display">\[\frac{\partial f}{\partial β_0} = \sum_{i=1}^{n} 2(y_i - β_0 - β_1x_i)⋅(y_i - β_0 - β_1x_i)'
\]</div><p></p><p>由于是对<span class="math inline">\(β_0\)</span>求导，其余可认为是常数，求导为0</p>
<p></p><div class="math display">\[\frac{\partial f}{\partial β_0} = \sum_{i=1}^{n} 2(y_i - β_0 - β_1x_i) ⋅ -1 =-2\sum_{i=1}^{n} β_0(y_i - β_0 - β_1x_i)
\]</div><p></p><p>导数是函数切线的斜率，要找到函数的最小值，就是其导数为0的地方</p>
<p></p><div class="math display">\[\frac{\partial f}{\partial β_0}=-2\sum_{i=1}^{n} (y_i - β_0 - β_1x_i)=0
\]</div><p></p><p>整理一下：</p>
<p></p><div class="math display">\[\sum_{i=1}^{n} (y_i - β_0 - β_1x_i)=\sum_{i=1}^{n}y_i - \sum_{i=1}^{n}β_0 - \sum_{i=1}^{n}β_1x_i=0
\]</div><p></p><p>方程1： $$\sum_{i=1}^{n}y_i = nβ_0 + β_1⋅\sum_{i=1}^{n}x_i$$</p>
<p>同理对<span class="math inline">\(β_1\)</span>求偏导</p>
<p></p><div class="math display">\[\frac{\partial f}{\partial β_1} = \sum_{i=1}^{n} 2(y_i - β_0 - β_1x_i) ⋅ -x_i =0
\]</div><p></p><p>整理一下：</p>
<p></p><div class="math display">\[\sum_{i=1}^{n} 2(y_i - β_0 - β_1x_i) ⋅ -x_i=-2(\sum_{i=1}^{n} x_iy_i-\sum_{i=1}^{n}β_0x_i-\sum_{i=1}^{n}β_1x_i^2)=0
\]</div><p></p><p>方程2：</p>
<p></p><div class="math display">\[\sum_{i=1}^{n} x_iy_i = β_0⋅\sum_{i=1}^{n}x_i+β_1⋅\sum_{i=1}^{n}x_i^2
\]</div><p></p><p>我们将样本数据<span class="math inline">\((x_i, y_i)\)</span>求平均值，就是样本均值</p>
<p></p><div class="math display">\[\bar{x}=\frac{1}{n}\sum_{i=1}^{n}x_i
\]</div><p></p><p></p><div class="math display">\[\bar{y}=\frac{1}{n}\sum_{i=1}^{n}y_i
\]</div><p></p><p>带入方程1：</p>
<p></p><div class="math display">\[\bar{y} = β_0 + β_1\bar{x}
\]</div><p></p><p>将<span class="math inline">\(β_0\)</span>带入方程2计算<span class="math inline">\(β_1\)</span>：</p>
<p></p><div class="math display">\[\sum_{i=1}^{n} x_iy_i = (\bar{y} - β_1\bar{x})⋅\sum_{i=1}^{n}x_i+β_1⋅\sum_{i=1}^{n}x_i^2 = n\bar{x}\bar{y}-nβ_1\bar{x}^2+β_1⋅\sum_{i=1}^{n}x_i^2
\]</div><p></p><p></p><div class="math display">\[\sum_{i=1}^{n} x_iy_i - n\bar{x}\bar{y} = β_1(-n\bar{x}^2+\sum_{i=1}^{n}x_i^2)
\]</div><p></p><p></p><div class="math display">\[β_1=\frac{\sum_{i=1}^{n} x_iy_i - n\bar{x}\bar{y}}{\sum_{i=1}^{n}x_i^2-n\bar{x}^2}
\]</div><p></p><p>经过漫长的推导：</p>
<p></p><div class="math display">\[β_1=\frac{\sum_{i=1}^{n} x_iy_i - n\bar{x}\bar{y}}{\sum_{i=1}^{n}x_i^2-n\bar{x}^2}
\]</div><p></p><p></p><div class="math display">\[β_0 = \bar{y} - β_1\bar{x}
\]</div><p></p><h4 id="小结">小结</h4>
<p>通过最小二乘法，一步一步计算出截距与回归系数的公式，这其中用到的数学知识主要有：多元函数求偏导、导数的计算</p>
<h2 id="多元回归下的最小二乘法">多元回归下的最小二乘法</h2>
<h4 id="推导过程-1">推导过程</h4>
<p>多元线性回归的数学模型：</p>
<p></p><div class="math display">\[y = β_0 + β_1x_1 + β_2x_2 + \dots + β_nx_n
\]</div><p></p><p>相比于一元回归的最小二乘法，多元回归可谓有一点复杂，因为特征数量的增加，带来的样本与特征的快速上升</p>
<p>比如有3个样本，2个特征，记为：<span class="math inline">\(y = β_0 + β_1x_1 + β_2x_2\)</span></p>
<p></p><div class="math display">\[x^{(1)} = [1,2]
\]</div><p></p><p></p><div class="math display">\[x^{(2)} = [3,4]
\]</div><p></p><p></p><div class="math display">\[x^{(3)} = [5,6]
\]</div><p></p><p>用矩阵表达：</p>
<p></p><div class="math display">\[X=\begin{bmatrix}
1 &amp; 2 \\
3 &amp; 4 \\
5 &amp; 6
\end{bmatrix}
\]</div><p></p><p>假设有m个特征，n个样本</p>
<p></p><div class="math display">\[\hat{y}_i = β_0 + β_1x_1^{(1)} + β_2x_2^{(1)} + \dots + β_nx_n^{(1)}
\]</div><p></p><p></p><div class="math display">\[\hat{y}_i = β_0 + β_1x_1^{(2)} + β_2x_2^{(2)} + \dots + β_nx_n^{(2)}
\]</div><p></p><p></p><div class="math display">\[...
\]</div><p></p><p></p><div class="math display">\[\hat{y}_i = β_0 + β_1x_1^{(m)} + β_2x_2^{(m)} + \dots + β_nx_n^{(m)}
\]</div><p></p><p></p><div class="math display">\[X=\begin{bmatrix}
1 &amp; x_1^{(1)} &amp; x_2^{(1)} &amp; \dots &amp; x_n^{(1)} \\
1 &amp; x_1^{(2)} &amp; x_2^{(2)} &amp; \dots &amp; x_n^{(2)} \\
... \\
1 &amp; x_1^{(m)} &amp; x_2^{(m)} &amp; \dots &amp; x_n^{(m)} \\
\end{bmatrix}
\]</div><p></p><p></p><div class="math display">\[β=\begin{bmatrix}
β_0 \\
β_1 \\
... \\
β_n \\
\end{bmatrix}
\]</div><p></p><p>所以通过矩阵的点积，可以将公式改写为，在m个特征，n个样本下：</p>
<p></p><div class="math display">\[\hat{y}_i=Xβ
\]</div><p></p><p>带入最小二乘法公式：</p>
<p></p><div class="math display">\[\text{f} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \sum_{i=1}^{n} (y_i - Xβ)^2 = \| {y_i} - Xβ \|_2 = (y_i - Xβ)^T(y_i - Xβ)
\]</div><p></p><p>展开矩阵：</p>
<p></p><div class="math display">\[(y_i - Xβ)^T(y_i - Xβ) = y_i^Ty_i-y_i^TXβ-X^Tβ^Ty_i+X^Tβ^TXβ
\]</div><p></p><p>由于 <span class="math inline">\(y_i^TXβ\)</span> 的转置矩阵就是 <span class="math inline">\(X^Tβ^Ty_i\)</span> ：</p>
<p></p><div class="math display">\[= y_i^Ty_i-2X^Tβ^Ty_i+X^Tβ^TXβ
\]</div><p></p><p>为了找到β最小值，先求导然后令导数为0</p>
<p></p><div class="math display">\[\frac{\partial f}{\partial β} = (y_i^Ty_i-2X^Tβ^Ty_i+X^Tβ^TXβ)' = -2X^Ty_i+2X^TXβ = 0
\]</div><p></p><p><code>=&gt;</code></p>
<p></p><div class="math display">\[X^Ty_i=X^TXβ
\]</div><p></p><p>两边同时乘以<span class="math inline">\(X^TX\)</span>逆矩阵，换句话说，<span class="math inline">\(X^TX\)</span>是可逆矩阵：</p>
<p></p><div class="math display">\[β=(X^TX)^{-1}X^Ty_i
\]</div><p></p><h4 id="小结-1">小结</h4>
<p>这其中用到的数学知识主要有：导数、矩阵等方面的知识</p>
<p>用MathJax语法写公式真的太费劲了！还不如在纸上手写</p>
<h2 id="联系我">联系我</h2>
<ul>
<li>联系我，做深入的交流</li>
</ul>
<p><img alt="" width="500" height="200" loading="lazy" data-src="https://img2024.cnblogs.com/blog/1416773/202411/1416773-20241121135740959-1907948957.png#" class="lazyload"></p>
<hr>
<p>至此，本文结束<br>
在下才疏学浅，有撒汤漏水的，请各位不吝赐教...</p>

</div>
<div id="MySignature" role="contentinfo">
    <p>本文来自博客园，作者：<a href="https://www.cnblogs.com/MrVolleyball/" target="_blank">it排球君</a>，转载请注明原文链接：<a href="https://www.cnblogs.com/MrVolleyball/p/19052292" target="_blank">https://www.cnblogs.com/MrVolleyball/p/19052292</a></p>
<div>本文版权归作者和博客园共有，欢迎转载，但未经作者同意必须在文章页面给出原文连接，否则保留追究法律责任的权利。 </div>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0" data-date-updated="2025-08-22 11:10">2025-08-22 11:10</span>&nbsp;
<a href="https://www.cnblogs.com/MrVolleyball">it排球君</a>&nbsp;
阅读(<span id="post_view_count">39</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(19052292);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '19052292', targetLink: 'https://www.cnblogs.com/MrVolleyball/p/19052292', title: '彩笔运维勇闯机器学习--最小二乘法的数学推导' })">举报</a>
</div>
        