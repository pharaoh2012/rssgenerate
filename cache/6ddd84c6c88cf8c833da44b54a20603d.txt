
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/huggingface/p/18796531" title="发布于 2025-03-27 17:40">
    <span role="heading" aria-level="2">常见的 AI 模型格式</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p><img src="https://img-s2.andfun.cn/devrel/posts/2025/03/c39a030242264.png" alt="" loading="lazy"></p>
<blockquote>
<p><strong>来源</strong>：<a href="https://blog.ngxson.com/common-ai-model-formats" target="_blank" rel="noopener nofollow">博客链接</a></p>
</blockquote>
<p>过去两年，开源 AI 社区一直在热烈讨论新 AI 模型的开发。每天都有越来越多的模型在 <a href="https://huggingface.co" target="_blank" rel="noopener nofollow">Hugging Face</a> 上发布，并被用于实际应用中。然而，开发者在使用这些模型时面临的一个挑战是<strong>模型格式的多样性</strong>。</p>
<p>在本文中，我们将探讨当下常见的 AI 模型格式，包括：</p>
<ul>
<li><strong>GGUF</strong></li>
<li><strong>PyTorch</strong></li>
<li><strong>Safetensors</strong></li>
<li><strong>ONNX</strong></li>
</ul>
<p>我们将分析每种格式的<strong>优缺点</strong>，并提供<strong>使用建议</strong>，帮助你选择最适合的格式。</p>
<h2 id="gguf">GGUF</h2>
<p>GGUF 最初是为 <a href="https://github.com/ggml-org/llama.cpp" target="_blank" rel="noopener nofollow">llama.cpp</a> 项目开发的。GGUF 是一种二进制格式，旨在实现快速的模型加载和保存，并易于阅读。模型通常使用 PyTorch 或其他框架开发，然后转换为 GGUF 格式以与 GGML 一起使用。</p>
<p>随着时间的推移，GGUF 已成为开源社区中共享 AI 模型最流行的格式之一。它得到了许多知名推理运行时的支持，包括 <a href="https://github.com/ggml-org/llama.cpp" target="_blank" rel="noopener nofollow">llama.cpp</a>、<a href="https://ollama.com/" target="_blank" rel="noopener nofollow">ollama</a> 和 <a href="https://github.com/vllm-project/vllm" target="_blank" rel="noopener nofollow">vLLM</a>。</p>
<p>目前，GGUF 主要用于语言模型。虽然也可以将其用于其他类型的模型，例如通过 <a href="https://github.com/leejet/stable-diffusion.cpp" target="_blank" rel="noopener nofollow">stable-diffusion.cpp</a> 实现的扩散模型，但这并不像在语言模型中的应用那样普遍。</p>
<p>GGUF 文件包含以下部分：</p>
<ul>
<li>一个以键值对组织的元数据部分。该部分包含有关模型的信息，例如其架构、版本和超参数。</li>
<li>一个张量元数据部分。该部分包括模型中张量的详细信息，例如它们的形状、数据类型和名称。</li>
<li>最后，一个包含张量数据本身的部分。</li>
</ul>
<p><img src="https://img-s2.andfun.cn/devrel/posts/2025/03/f48905b65c9ef.png" alt="Diagram by @mishig25 (GGUF v3)" loading="lazy"></p>
<p>GGUF 格式和 GGML 库还提供了灵活的<strong>量化方案</strong>，能够在保持良好精度的同时实现高效的模型存储。一些最常见的量化方案包括：</p>
<ul>
<li><code>Q4_K_M</code>：大多数张量被量化为 4 位，部分张量被量化为 6 位。这是最常用的量化方案。</li>
<li><code>IQ4_XS</code>：几乎所有张量都被量化为 4 位，但借助<strong>重要性矩阵</strong>。该矩阵用于校准每个张量的量化，可能在保持存储效率的同时提高精度。</li>
<li><code>IQ2_M</code>：类似于 <code>IQ4_XS</code>，但使用 2 位量化。这是最激进的量化方案，但在某些模型上仍能实现良好的精度。它适用于内存非常有限的硬件。</li>
<li><code>Q8_0</code>：所有张量都被量化为 8 位。这是最不激进的量化方案，提供几乎与原始模型相同的精度。</li>
</ul>
<p><img src="https://img-s2.andfun.cn/devrel/posts/2025/03/968f7233b6621.png" alt="" loading="lazy"></p>
<p>GGUF 格式的 Llama-3.1 8B 模型示例，链接<a href="https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/tree/main" target="_blank" rel="noopener nofollow">在此</a></p>
<p>让我们回顾一下 GGUF 的优缺点：</p>
<ul>
<li><strong>优点</strong>：
<ul>
<li>简单：单文件格式易于共享和分发。</li>
<li>快速：通过与 <code>mmap()</code> 的兼容性实现模型的快速加载和保存。</li>
<li>高效：提供灵活的量化方案。</li>
<li>便携：作为一种二进制格式，无需特定库即可轻松读取。</li>
</ul>
</li>
<li><strong>缺点</strong>：
<ul>
<li>大多数模型需要从其他格式（如 PyTorch、Safetensors）转换为 GGUF。</li>
<li>并非所有模型都可转换。部分模型不受 llama.cpp 支持。</li>
<li>模型保存为 GGUF 格式后，修改或微调并不容易。</li>
</ul>
</li>
</ul>
<p>GGUF 主要用于生产环境中的<strong>模型服务</strong>，其中快速加载时间至关重要。它也用于开源社区内的<strong>模型共享</strong>，因为其格式简单，便于分发。</p>
<p><strong>有用资源：</strong></p>
<ul>
<li><a href="https://github.com/ggml-org/llama.cpp" target="_blank" rel="noopener nofollow">llama.cpp</a> 项目，提供了将 HF 模型转换为 GGUF 的脚本。</li>
<li><a href="https://huggingface.co/spaces/ggml-org/gguf-my-repo" target="_blank" rel="noopener nofollow">gguf-my-repo</a> 空间允许在不下载到本地的情况下将模型转换为 GGUF 格式。</li>
<li><a href="https://ollama.com/" target="_blank" rel="noopener nofollow">ollama</a> 和 <a href="https://huggingface.co/docs/hub/en/ollama" target="_blank" rel="noopener nofollow">HF-ollama 集成</a> 支持通过 <code>ollama run</code> 命令运行 HF Hub 上的任何 GGUF 模型。</li>
</ul>
<h2 id="pytorch-ptpth">PyTorch (.pt/.pth)</h2>
<p>.pt/.pth 扩展名代表 PyTorch 的默认序列化格式，存储包含学习参数（权重、偏置）、优化器状态和训练元数据的模型状态字典。</p>
<p>PyTorch 模型可以保存为两种格式：</p>
<ul>
<li><strong>.pt</strong>：此格式保存整个模型，包括其架构和学习参数。</li>
<li><strong>.pth</strong>：此格式仅保存模型的状态字典，其中包括模型的学习参数和一些元数据。</li>
</ul>
<p>PyTorch 格式基于 Python 的 <a href="https://docs.python.org/3/library/pickle.html" target="_blank" rel="noopener nofollow">pickle</a> 模块，该模块用于序列化 Python 对象。为了理解 <code>pickle</code> 的工作原理，让我们看以下示例：</p>
<pre><code class="language-python">import pickle
model_state_dict = { "layer1": "hello", "layer2": "world" }
pickle.dump(model_state_dict, open("model.pkl", "wb"))
</code></pre>
<p>The <code>pickle.dump()</code> 函数将 <code>model_state_dict</code> 字典序列化并保存到名为 <code>model.pkl</code>. 的文件中。输出文件现在包含字典的二进制表示:</p>
<p><img src="https://img-s2.andfun.cn/devrel/posts/2025/03/e455c0e57afdf.jpg" alt="model.pkl hex view" loading="lazy"><br>
model.pkl hex view</p>
<p>要将序列化的字典加载回 Python，我们可以使用 <code>pickle.load()</code> 函数:</p>
<pre><code class="language-python">import pickle
model_state_dict = pickle.load(open("model.pkl", "rb"))
print(model_state_dict)
# Output: {'layer1': 'hello', 'layer2': 'world'}
</code></pre>
<p>如你所见，<code>pickle</code> 模块提供了一种简单的方法来序列化 Python 对象。然而，它也有一些局限性：</p>
<ul>
<li><strong>安全性</strong>：任何东西都可以被 pickle，<strong>包括恶意代码</strong>。如果序列化数据未经过适当验证，这可能会导致安全漏洞。例如，Snyk 的这篇文章解释了 <a href="https://snyk.io/articles/python-pickle-poisoning-and-backdooring-pth-files/" target="_blank" rel="noopener nofollow">pickle 文件如何被植入后门</a>。</li>
<li><strong>效率</strong>：它不支持延迟加载或部分数据加载。这可能导致在处理大型模型时<strong>加载速度慢</strong>和<strong>内存使用率高</strong>。</li>
<li><strong>可移植性</strong>：它是特定于 Python 的，这使得与其他语言共享模型变得具有挑战性。</li>
</ul>
<p>如果你仅在 Python 和 PyTorch 环境中工作，PyTorch 格式可能是一个合适的选择。然而，近年来，AI 社区一直在转向更高效和安全的序列化格式，例如 GGUF 和 Safetensors。</p>
<p><strong>有用资源：</strong></p>
<ul>
<li><a href="https://pytorch.org/docs/stable/generated/torch.save.html" target="_blank" rel="noopener nofollow">PyTorch 文档</a> 关于保存和加载模型。</li>
<li><a href="https://github.com/pytorch/executorch" target="_blank" rel="noopener nofollow">executorch</a> 项目，提供了一种将 PyTorch 模型转换为 <code>.pte</code> 的方法，这些模型可在移动和边缘设备上运行。</li>
</ul>
<h2 id="safetensors">Safetensors</h2>
<p>由 Hugging Face 开发的 <a href="https://huggingface.co/docs/safetensors/en/index" target="_blank" rel="noopener nofollow">safetensors</a> 解决了传统 Python 序列化方法（如 PyTorch 使用的 <code>pickle</code>）中存在的安全性和效率问题。该格式使用受限的反序列化过程来防止代码执行漏洞。</p>
<p>一个 safetensors 文件包含：</p>
<ul>
<li>
<p>以 JSON 格式保存的元数据部分。该部分包含模型中所有张量的信息，例如它们的形状、数据类型和名称。它还可以选择性地包含自定义元数据。</p>
</li>
<li>
<p>张量数据部分。</p>
<p><img src="https://img-s2.andfun.cn/devrel/posts/2025/03/e84152827fe7e.jpg" alt="Safetensors 格式结构图" loading="lazy"><br>
Safetensors 格式结构图</p>
</li>
<li>
<p><strong>优点</strong>：</p>
<ul>
<li>安全：Safetensors 采用受限的反序列化过程来防止代码执行漏洞。</li>
<li>快速：它支持延迟加载和部分数据加载，从而可以加快加载速度并降低内存使用率。这与 GGUF 类似，你可以使用 <code>mmap()</code> 映射文件。</li>
<li>高效：支持量化张量。</li>
<li>可移植：它设计为跨编程语言可移植，使得与其他语言共享模型变得容易。</li>
</ul>
</li>
<li>
<p><strong>缺点</strong>：</p>
<ul>
<li>量化方案不如 GGUF 灵活。这主要是由于 PyTorch 提供的量化支持有限。</li>
<li>需要 JSON 解析器来读取元数据部分。这在处理像 C++ 这样的低级语言时可能会出现问题，因为这些语言没有内置的 JSON 支持。</li>
</ul>
</li>
</ul>
<p>注意：虽然在理论上元数据可以保存在文件中，但在实践中，模型元数据通常存储在一个单独的 JSON 文件中。这既可能是优点也可能是缺点，具体取决于使用场景。</p>
<p>safetensors 格式是 Hugging Face 的 <a href="https://huggingface.co/docs/transformers/index" target="_blank" rel="noopener nofollow">transformers</a> 库使用的默认序列化格式。它在开源社区中广泛用于<strong>共享、训练、微调和部署 AI 模型</strong>。Hugging Face 上发布的新模型都以 safetensors 格式存储，包括 Llama、Gemma、Phi、Stable-Diffusion、Flux 等许多模型。</p>
<p><strong>有用资源：</strong></p>
<ul>
<li><a href="https://huggingface.co/docs/transformers/quicktour" target="_blank" rel="noopener nofollow">transformers</a> 库关于保存和加载模型的文档。</li>
<li><a href="https://huggingface.co/docs/transformers/en/quantization/bitsandbytes" target="_blank" rel="noopener nofollow">bitsandbytes 指南</a> 关于如何量化模型并将其保存为 safetensors 格式。</li>
<li><a href="https://huggingface.co/mlx-community" target="_blank" rel="noopener nofollow">mlx-community</a> 组织在 HF 上提供与 MLX 框架（Apple 芯片）兼容的模型。</li>
</ul>
<h2 id="onnx">ONNX</h2>
<p>开放神经网络交换（Open Neural Network Exchange，ONNX）格式提供了一种与供应商无关的机器学习模型表示方法。它是 <a href="https://onnx.ai/" target="_blank" rel="noopener nofollow">ONNX 生态系统</a> 的一部分，该生态系统包括用于不同框架（如 PyTorch、TensorFlow 和 MXNet）之间互操作的工具和库。</p>
<p>ONNX 模型以 <code>.onnx</code> 扩展名的单个文件保存。与 GGUF 或 Safetensors 不同，ONNX 不仅包含模型的张量和元数据，还包含模型的<strong>计算图</strong>。</p>
<p>在模型文件中包含计算图使得在处理模型时具有更大的灵活性。例如，当发布新模型时，你可以轻松地将其转换为 ONNX 格式，而无需担心模型的架构或推理代码，因为计算图已经保存在文件中。</p>
<p><img src="https://img-s2.andfun.cn/devrel/posts/2025/03/081249b1deacd.jpg" alt="ONNX 格式的计算图示例，由 Netron 生成" loading="lazy"></p>
<p>ONNX 格式的计算图示例，由 <a href="https://netron.app/" target="_blank" rel="noopener nofollow">Netron</a> 生成</p>
<ul>
<li><strong>优点</strong>：
<ul>
<li>灵活性：在模型文件中包含计算图使得在不同框架之间转换模型时更加灵活。</li>
<li>可移植性：得益于 ONNX 生态系统，ONNX 格式可以轻松部署在各种平台和设备上，包括移动设备和边缘设备。</li>
</ul>
</li>
<li><strong>缺点</strong>：
<ul>
<li>对量化张量的支持有限。ONNX 本身不支持量化张量，而是将它们分解为整数张量和比例因子张量。这可能导致在处理量化模型时质量下降。</li>
<li>复杂架构可能需要为不支持的层使用操作符回退或自定义实现。这可能会在将模型转换为 ONNX 格式时导致性能损失。</li>
</ul>
</li>
</ul>
<p>总体而言，如果你正在处理移动设备或浏览器内推理，ONNX 是一个不错的选择。</p>
<p><strong>有用资源：</strong></p>
<ul>
<li><a href="https://huggingface.co/onnx-community" target="_blank" rel="noopener nofollow">onnx-community</a> 组织在 HF 上提供 ONNX 格式的模型以及转换指南。</li>
<li><a href="https://github.com/huggingface/transformers.js" target="_blank" rel="noopener nofollow">transformer.js</a> 项目，允许在浏览器中使用 WebGPU 或 WebAssembly 运行 ONNX 模型。</li>
<li><a href="https://onnxruntime.ai/" target="_blank" rel="noopener nofollow">onnxruntime</a> 项目，提供在各种平台和硬件上的高性能推理引擎。</li>
<li><a href="https://netron.app/" target="_blank" rel="noopener nofollow">netron</a> 项目，允许在浏览器中可视化 ONNX 模型。</li>
</ul>
<h2 id="硬件支持">硬件支持</h2>
<p>在选择模型格式时，重要的是要考虑模型将部署在哪种硬件上。下表显示了每种格式的硬件支持建议：</p>
<table>
<thead>
  <tr>
    <th>硬件</th>
    <th>GGUF</th>
    <th>PyTorch</th>
    <th>Safetensors</th>
    <th>ONNX</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>CPU</td>
    <td>✅ (最佳)</td>
    <td>🟡</td>
    <td>🟡</td>
    <td>✅</td>
  </tr>
  <tr>
    <td>GPU</td>
    <td>✅</td>
    <td>✅</td>
    <td>✅</td>
    <td>✅</td>
  </tr>
  <tr>
    <td>移动设备部署</td>
    <td>✅</td>
    <td>🟡 (通过 executorch)</td>
    <td>❌</td>
    <td>✅</td>
  </tr>
  <tr>
    <td>Apple 芯片</td>
    <td>✅</td>
    <td>🟡</td>
    <td>✅ (通过 MLX 框架)</td>
    <td>✅</td>
  </tr>
</tbody>
</table>
<p>说明：</p>
<ul>
<li>✅: 完全支持</li>
<li>🟡: 部分支持或性能较低</li>
<li>❌: 不支持</li>
</ul>
<h2 id="结论">结论</h2>
<p>在本文中，我们探讨了当今使用的一些常见 AI 模型格式，包括 GGUF、PyTorch、Safetensors 和 ONNX。每种格式都有其自身的优缺点，因此根据具体的用例和硬件需求选择合适的格式至关重要。</p>
<h2 id="脚注">脚注</h2>
<p>mmap：内存映射文件是一种操作系统功能，允许将文件映射到内存中。这对于在不将整个文件加载到内存中的情况下读写大文件非常有益。</p>
<p>延迟加载（lazy-loading）：延迟加载是一种技术，它将数据的加载推迟到实际需要时。这有助于在处理大型模型时减少内存使用并提高性能。</p>
<p>计算图（computation graph）：在机器学习的上下文中，计算图是一种流程图，展示了数据如何通过模型流动以及每一步如何执行不同的计算（例如加法、乘法或激活函数的应用）。</p>
<hr>
<blockquote>
<p>英文原文: <a href="https://blog.ngxson.com/common-ai-model-formats" target="_blank" rel="noopener nofollow">https://blog.ngxson.com/common-ai-model-formats</a></p>
<p>原文作者: Xuan Son Nguyen</p>
<p>译者: Adeena</p>
</blockquote>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.7220360905011574" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-03-27 17:40">2025-03-27 17:40</span>&nbsp;
<a href="https://www.cnblogs.com/huggingface">HuggingFace</a>&nbsp;
阅读(<span id="post_view_count">136</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18796531" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18796531);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18796531', targetLink: 'https://www.cnblogs.com/huggingface/p/18796531', title: '常见的 AI 模型格式' })">举报</a>
</div>
        