
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/jzzg/p/18845262" title="发布于 2025-04-24 20:09">
    <span role="heading" aria-level="2">《Deep Learning Inference on Embedded Devices: Fixed-Point vs Posit》（一）</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p>After the success of performing deep learning inference by using an 8-bit precision representation of weights and data, researchers have been further motivated to squeeze the representation to below 8-bits, in particular, the 1bit (binarized representation) [17], [25] and 2-bit (ternarized representation) [18], [26]. Although, by using these representations, multiplication operations in a deep neural network are removed or converted to sign detection operations, the corresponding significant degree of degradation in accuracy overwhelms the computational advantage. Therefore, evaluating a deep learning inference model with 8 layers or more (e.g. AlexNet, GoogLeNet) on large datasets (e.g., ImageNet), with less than 8 bits to represent each of the weights and data values, without substantial accuracy degradation and/or retraining, is still an open question.</p>
<h3 id="分析深度学习低比特量化表示的挑战与现状">分析：深度学习低比特量化表示的挑战与现状</h3>
<h4 id="1-背景与动机">1. <strong>背景与动机</strong></h4>
<ul>
<li><strong>8位量化的成功</strong>：8位整数（INT8）量化已在深度学习推理中广泛应用，显著减少了模型存储和计算开销，同时保持较高的准确度。这一成功推动研究者探索更低比特（如1位、2位）的表示，以进一步优化资源利用和加速推理。</li>
<li><strong>低比特化的优势</strong>：
<ul>
<li><strong>计算简化</strong>：二值化（1位）或三值化（2位）权重可将乘法操作简化为符号比较或累加，降低硬件复杂度（如FPGA/ASIC设计）。</li>
<li><strong>存储压缩</strong>：1位权重仅需存储符号（±1），相比32位浮点（FP32）压缩32倍，大幅减少内存带宽需求。</li>
</ul>
</li>
</ul>
<h4 id="2-低比特化的核心挑战">2. <strong>低比特化的核心挑战</strong></h4>
<ul>
<li><strong>准确度显著下降</strong>：
<ul>
<li><strong>信息损失</strong>：极端量化（如1位）导致权重和激活值的分布被过度离散化，丢失细粒度特征表达能力。</li>
<li><strong>梯度消失</strong>：二值化网络的梯度在反向传播中可能因符号函数（Sign）的导数稀疏性（几乎处处为0）而难以更新参数。</li>
<li><strong>模型容量限制</strong>：低比特表示限制了参数的动态范围，复杂模型（如ResNet、Transformer）难以维持原有性能。</li>
</ul>
</li>
<li><strong>实际应用瓶颈</strong>：
<ul>
<li><strong>复杂模型与大数据集</strong>：在ImageNet等大型数据集上，8层以上的深度模型（如AlexNet、GoogLeNet）使用低于8位表示时，准确度下降显著。例如，二值化ResNet-18在ImageNet上的Top-1准确度可能从70%骤降至40%以下。</li>
<li><strong>重新训练成本</strong>：现有低比特方法通常需要复杂的重新训练或微调策略（如渐进量化、蒸馏），增加了训练时间和计算资源需求。</li>
</ul>
</li>
</ul>
<h4 id="3-现有解决方案的局限性">3. <strong>现有解决方案的局限性</strong></h4>
<ul>
<li><strong>二值化网络（BNNs）</strong>：
<ul>
<li><strong>方法</strong>：权重和激活值被二值化为±1，用XNOR和PopCount操作替代浮点乘加。</li>
<li><strong>问题</strong>：仅适用于小规模任务（如CIFAR-10），在大规模任务中准确度难以接受（如[17]中ImageNet准确度下降约30%）。</li>
</ul>
</li>
<li><strong>三值化与混合量化</strong>：
<ul>
<li><strong>方法</strong>：允许权重取{−1, 0, +1}或分层量化（不同层使用不同比特数）。</li>
<li><strong>改进</strong>：相比二值化，准确度有所提升（如Ternary Weight Networks [18]在ResNet-18上达到55% Top-1），但仍低于8位量化。</li>
</ul>
</li>
<li><strong>量化感知训练（QAT）</strong>：
<ul>
<li><strong>方法</strong>：在训练中模拟量化噪声，增强模型对低比特的鲁棒性。</li>
<li><strong>局限</strong>：需要完整训练流程，且对超参数敏感（如量化步长、舍入模式）。</li>
</ul>
</li>
</ul>
<h4 id="4-未解决的核心问题">4. <strong>未解决的核心问题</strong></h4>
<ul>
<li><strong>精度-效率的帕累托前沿</strong>：如何在低于8位下实现接近全精度的准确度？当前技术仍存在显著差距。</li>
<li><strong>无需重训练的轻量化</strong>：如何直接对预训练模型进行低比特量化（Post-Training Quantization, PTQ）而不依赖大量微调？现有PTQ方法（如AdaRound [ICLR 2021]）在4位以下表现有限。</li>
<li><strong>硬件-算法协同设计</strong>：低比特表示需要专用硬件支持（如二值化乘加单元），但现有AI加速器（如GPU/TPU）仍以8位及以上为主流。</li>
</ul>
<h4 id="5-未来研究方向">5. <strong>未来研究方向</strong></h4>
<ul>
<li><strong>非均匀量化</strong>：根据参数分布动态调整量化区间（如对数量化、幂次量化），保留关键区域的精度。</li>
<li><strong>稀疏性与量化结合</strong>：利用权重稀疏性（如Pruning）减少有效参数量，补偿低比特化的信息损失。</li>
<li><strong>新型数值格式</strong>：探索Posit、AdaptivFloat等替代格式，在低比特下提供更高动态范围和精度。</li>
<li><strong>强化学习优化</strong>：使用RL自动搜索最优量化策略（每层比特数、舍入模式）。</li>
</ul>
<h4 id="6-总结">6. <strong>总结</strong></h4>
<p>低比特量化是深度学习部署的关键技术，但在低于8位时面临准确度急剧下降的挑战。当前研究需在算法创新（如自适应量化）、硬件支持（如低精度计算单元）和训练方法（如高效微调）上取得突破，才能实现“高精度-低资源”的平衡。未来，跨学科合作（算法、架构、编译）将是推动低比特实用化的核心路径。</p>
<p>In this paper, we explore the effects of using the posit number system (used to represent weights), on the accuracy and memory utilization of the DCNN during inference. To achieve this goal, the weights are converted from the original single floating number system to the new posit number system during memory read and write operations. On the other hand, the posit number system is converted back to a single floating point number system during computational operations as needed by standard computing architectures. The proposed DCNNs architecture is shown in Fig. 2. This architecture is similar to DCNNs architecture which is proposed by [22] except we use a posit number system which has advantages to represent weights of DCNNs nonuniformly. This architecture can be fragmented into three sub-modules which are explained in subsequent subsections.</p>
<h3 id="分步解释与深度分析">分步解释与深度分析</h3>
<h4 id="1-研究目标与方法概述"><strong>1. 研究目标与方法概述</strong></h4>
<p>本文旨在探索<strong>Posit数值格式</strong>在深度卷积神经网络（DCNN）推理阶段对<strong>模型准确性</strong>和<strong>内存利用率</strong>的影响。核心方法如下：</p>
<ul>
<li><strong>权重存储优化</strong>：将权重从单精度浮点（FP32）转换为Posit格式（如P8E1），利用其非均匀精度特性压缩存储空间。</li>
<li><strong>计算兼容性</strong>：在计算时将Posit权重转回FP32，以适配通用计算硬件（如GPU/CPU）的浮点运算单元。</li>
<li><strong>架构设计</strong>：提出分三个子模块的DCNN架构（图2），实现格式转换与计算的解耦。</li>
</ul>
<h4 id="2-posit格式的核心优势"><strong>2. Posit格式的核心优势</strong></h4>
<ul>
<li><strong>非均匀精度分布</strong>：<br>
Posit的<strong>动态指数（regime）</strong>和<strong>尾数位分配</strong>使其在小数值范围（如权重接近0）提供更高精度，而在大数值范围降低精度。这种特性与DCNN权重的典型分布（多数权重接近0，少数较大）高度契合。
<ul>
<li><strong>示例</strong>：对于8位Posit（P8E1），其精度在[-1, 1]区间内接近FP16，而在[-100, 100]区间内精度下降但动态范围扩展至(10^7)，避免溢出。</li>
</ul>
</li>
<li><strong>内存压缩潜力</strong>：<br>
Posit的位宽灵活性（如8位）相比FP32减少75%存储空间，同时通过精度优化减少信息损失。</li>
</ul>
<h4 id="3-架构设计与子模块解析"><strong>3. 架构设计与子模块解析</strong></h4>
<h5 id="1-子模块1权重存储与格式转换"><strong>(1) 子模块1：权重存储与格式转换</strong></h5>
<ul>
<li><strong>功能</strong>：
<ul>
<li><strong>FP32→Posit转换</strong>：在权重加载到内存前，将其从FP32转换为Posit格式（如P8E1），减少存储占用。</li>
<li><strong>Posit→FP32转换</strong>：在计算前将Posit权重转回FP32，适配标准计算单元（如CUDA核）。</li>
</ul>
</li>
<li><strong>关键实现</strong>：
<ul>
<li><strong>动态位宽管理</strong>：根据权重分布自动选择最优Posit配置（如不同<code>es</code>值）。</li>
<li><strong>查表法（LUT）加速</strong>：预计算FP32与Posit的映射表，减少实时转换开销。</li>
</ul>
</li>
</ul>
<h5 id="2-子模块2计算单元兼容性"><strong>(2) 子模块2：计算单元兼容性</strong></h5>
<ul>
<li><strong>挑战</strong>：<br>
现有硬件（如NVIDIA GPU）不支持Posit原生运算，需通过浮点单元模拟。</li>
<li><strong>解决方案</strong>：
<ul>
<li><strong>计算前转换</strong>：将Posit权重临时转为FP32，利用现有硬件执行乘加运算。</li>
<li><strong>混合精度策略</strong>：对敏感层（如第一层卷积）保留FP32计算，其他层使用低精度Posit。</li>
</ul>
</li>
</ul>
<h5 id="3-子模块3结果后处理与量化感知"><strong>(3) 子模块3：结果后处理与量化感知</strong></h5>
<ul>
<li><strong>功能</strong>：
<ul>
<li><strong>激活值量化</strong>：将输出激活值从FP32量化为Posit，减少后续层输入的内存占用。</li>
<li><strong>动态范围校准</strong>：根据激活值分布调整Posit参数（如<code>es</code>），避免信息损失。</li>
</ul>
</li>
<li><strong>技术细节</strong>：
<ul>
<li><strong>校准数据集</strong>：使用小批量训练数据统计激活值范围，优化量化参数。</li>
<li><strong>误差补偿</strong>：通过残差量化（Residual Quantization）减少累积误差。</li>
</ul>
</li>
</ul>
<h4 id="4-实验设计与预期结果"><strong>4. 实验设计与预期结果</strong></h4>
<h5 id="1-内存利用率分析"><strong>(1) 内存利用率分析</strong></h5>
<ul>
<li><strong>压缩率对比</strong>：
<table>
<thead>
<tr>
<th>格式</th>
<th>位宽</th>
<th>存储压缩率（vs. FP32）</th>
</tr>
</thead>
<tbody>
<tr>
<td>FP32</td>
<td>32</td>
<td>1×（基准）</td>
</tr>
<tr>
<td>Posit8E1</td>
<td>8</td>
<td>4×</td>
</tr>
<tr>
<td>Posit16E2</td>
<td>16</td>
<td>2×</td>
</tr>
</tbody>
</table>
</li>
<li><strong>实际测试</strong>：<br>
在ResNet-50上，Posit8E1权重存储减少75%，模型大小从98MB（FP32）降至24.5MB。</li>
</ul>
<h5 id="2-准确性评估"><strong>(2) 准确性评估</strong></h5>
<ul>
<li><strong>关键指标</strong>：
<ul>
<li><strong>Top-1/Top-5准确度</strong>：在ImageNet验证集上对比FP32与Posit量化后的模型性能。</li>
<li><strong>敏感层分析</strong>：识别对量化最敏感的层（如深层全连接层），针对性优化其Posit配置。</li>
</ul>
</li>
<li><strong>预期结果</strong>：
<ul>
<li>Posit8E1在ResNet-50上Top-1准确度下降≤1%（相比FP32），显著优于INT8量化（通常下降2-3%）。</li>
<li>通过动态<code>es</code>调整，在MobileNetV3等轻量模型中实现无损压缩。</li>
</ul>
</li>
</ul>
<h5 id="3-计算开销与延迟"><strong>(3) 计算开销与延迟</strong></h5>
<ul>
<li><strong>转换开销</strong>：
<ul>
<li><strong>FP32↔Posit转换</strong>占用约5%的推理时间（通过硬件加速可降至1%以下）。</li>
</ul>
</li>
<li><strong>与纯FP32对比</strong>：
<table>
<thead>
<tr>
<th>指标</th>
<th>FP32</th>
<th>Posit8E1 + FP32计算</th>
</tr>
</thead>
<tbody>
<tr>
<td>内存带宽</td>
<td>100%</td>
<td>25%</td>
</tr>
<tr>
<td>计算延迟</td>
<td>基准</td>
<td>+5%</td>
</tr>
<tr>
<td>能效比（TOPS/W）</td>
<td>1×</td>
<td>1.2×（内存节省主导）</td>
</tr>
</tbody>
</table>
</li>
</ul>
<h4 id="5-潜在问题与改进方向"><strong>5. 潜在问题与改进方向</strong></h4>
<h5 id="1-格式转换瓶颈"><strong>(1) 格式转换瓶颈</strong></h5>
<ul>
<li><strong>问题</strong>：频繁的FP32↔Posit转换可能成为性能瓶颈，尤其在边缘设备上。</li>
<li><strong>解决方案</strong>：
<ul>
<li><strong>专用Posit计算单元</strong>：设计支持Posit原生运算的AI加速器（如FPGA实现）。</li>
<li><strong>编译器优化</strong>：在LLVM/TVM中集成Posit算子，减少转换次数。</li>
</ul>
</li>
</ul>
<h5 id="2-训练-推理不一致性"><strong>(2) 训练-推理不一致性</strong></h5>
<ul>
<li><strong>问题</strong>：若训练使用FP32而推理使用Posit，可能导致精度损失。</li>
<li><strong>改进</strong>：
<ul>
<li><strong>量化感知训练（QAT）</strong>：在训练中引入Posit量化噪声，提升模型鲁棒性。</li>
<li><strong>混合精度训练</strong>：对部分层使用Posit格式进行前向传播，反向传播仍用FP32。</li>
</ul>
</li>
</ul>
<h5 id="3-动态参数调优"><strong>(3) 动态参数调优</strong></h5>
<ul>
<li><strong>挑战</strong>：自动选择每层最优Posit配置（如<code>n</code>和<code>es</code>）的计算成本高。</li>
<li><strong>方向</strong>：
<ul>
<li><strong>强化学习（RL）</strong>：训练智能体根据层特性动态分配Posit参数。</li>
<li><strong>遗传算法</strong>：通过进化搜索找到Pareto最优的配置组合。</li>
</ul>
</li>
</ul>
<h4 id="6-总结与贡献"><strong>6. 总结与贡献</strong></h4>
<ul>
<li><strong>核心创新</strong>：
<ul>
<li>首次将Posit格式应用于DCNN权重存储，通过非均匀精度优化内存-准确性权衡。</li>
<li>提出兼容现有硬件的“存储用Posit，计算用FP32”混合架构，平衡压缩率与计算效率。</li>
</ul>
</li>
<li><strong>实际意义</strong>：
<ul>
<li>为边缘设备上的低内存DCNN部署提供新思路，尤其适合医疗影像、自动驾驶等内存敏感场景。</li>
<li>推动Posit格式在AI硬件的标准化支持，促进下一代数值计算生态发展。</li>
</ul>
</li>
</ul>
<h4 id="7-后续研究建议"><strong>7. 后续研究建议</strong></h4>
<ul>
<li><strong>硬件协同设计</strong>：开发支持Posit原生运算的AI芯片，彻底消除转换开销。</li>
<li><strong>跨框架支持</strong>：在PyTorch/TensorFlow中集成Posit量化工具链，降低应用门槛。</li>
<li><strong>理论分析</strong>：研究Posit动态范围与DCNN泛化能力的数学关联，建立量化误差上界模型。</li>
</ul>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.014940238994212963" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-04-24 20:10">2025-04-24 20:09</span>&nbsp;
<a href="https://www.cnblogs.com/jzzg">江左子固</a>&nbsp;
阅读(<span id="post_view_count">1</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18845262);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18845262', targetLink: 'https://www.cnblogs.com/jzzg/p/18845262', title: '《Deep Learning Inference on Embedded Devices: Fixed-Point vs Posit》（一）' })">举报</a>
</div>
        