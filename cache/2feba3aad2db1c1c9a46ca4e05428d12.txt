
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/SkyXZ/p/18927874" title="å‘å¸ƒäº 2025-06-14 02:48">
    <span role="heading" aria-level="2">æ‰‹æŠŠæ‰‹æ•™ä½ å®ç°PyTorchç‰ˆViTï¼šå›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­çš„Transformerå®æˆ˜</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<blockquote>
<p>ä½œè€…ï¼šSkyXZ</p>
<p>CSDNï¼š<a href="https://blog.csdn.net/xiongqi123123?spm=1000.2115.3001.5343" target="_blank" rel="noopener nofollow">SkyXZï½-CSDNåšå®¢</a></p>
<p>åšå®¢å›­ï¼š<a href="https://www.cnblogs.com/SkyXZ" target="_blank">SkyXZ - åšå®¢å›­</a></p>
</blockquote>
<ul>
<li>ViTè®ºæ–‡Arxivåœ°å€ï¼š<a href="https://arxiv.org/pdf/2010.11929" target="_blank" rel="noopener nofollow">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;æœ€è¿‘å…·èº«æ™ºèƒ½è¶³å¤Ÿç«çƒ­ï¼ŒVLMã€VLAã€VLNå±‚å‡ºä¸ç©·å‘å±•è¿…é€Ÿï¼Œè€ŒTransformerä½œä¸ºè¿™äº›æ¶æ„æœ€é‡è¦çš„åº•åº§ä¹‹ä¸€ï¼Œå¾—ç›Šäºå…¶å¼ºå¤§çš„å»ºæ¨¡èƒ½åŠ›ã€è‰¯å¥½çš„å¯æ‰©å±•æ€§ä¸ç»Ÿä¸€çš„ç»“æ„è®¾è®¡ï¼ŒTransformer å·²ç»æˆä¸ºæ„å»ºå¤šæ¨¡æ€æ™ºèƒ½ç³»ç»Ÿçš„äº‹å®æ ‡å‡†ã€‚ä»æœ€åˆçš„ BERTã€GPT åœ¨ NLP ä¸­çš„æˆåŠŸï¼Œåˆ° ViTã€CLIPã€RT-1 ç­‰æ¨¡å‹åœ¨è§†è§‰å’Œæ§åˆ¶é¢†åŸŸçš„å»¶ä¼¸ï¼ŒTransformer æ„ç­‘èµ·äº†ç»Ÿä¸€è¯­è¨€ã€è§†è§‰ä¹ƒè‡³åŠ¨ä½œç©ºé—´çš„æ¡¥æ¢ã€‚</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>æ—¢ç„¶ Transformer æˆä¸ºäº†å…·èº«æ™ºèƒ½çš„åŸºç¡€è®¾æ–½ï¼Œé‚£ä½œä¸ºä¸€åæƒ³èµ°è¿›æœºå™¨äººã€èµ°è¿›æœªæ¥çš„å·¥ç¨‹å¸ˆï¼Œæˆ‘å½“ç„¶ä¹Ÿè¦å­¦ä¼šå®ƒã€‚</strong>äºæ˜¯æˆ‘å†³å®šä»æœ€ç»å…¸ã€æœ€åŸºç¡€çš„ Vision Transformerï¼ˆViTï¼‰å…¥æ‰‹ï¼Œä¸€æ­¥æ­¥ä»åŸç†å‡ºå‘ï¼Œäº²æ‰‹ç”¨ PyTorch å¤ç°ï¼Œå¹¶æ•´ç†ä¸‹è¿™ä¸€è·¯çš„å­¦ä¹ è¿‡ç¨‹ä¸æ€è€ƒï¼Œä½œä¸ºè¿™ç¯‡åšå®¢çš„åˆ†äº«å†…å®¹ã€‚å¦‚æœä½ ä¹Ÿå¯¹ Transformer åœ¨è§†è§‰é¢†åŸŸçš„åº”ç”¨æ„Ÿå…´è¶£ï¼Œæˆ–è€…æ­£åœ¨å…¥é—¨å…·èº«æ™ºèƒ½ç›¸å…³æ–¹å‘ï¼Œå¸Œæœ›è¿™ç¯‡æ–‡ç« èƒ½å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼</p>
<p>PSï¼šğŸ’» é¡¹ç›®å®Œæ•´ä»£ç å·²ä¸Šä¼ è‡³Githubï¼š<a href="https://github.com/xiongqi123123/ViT_PyTorch.git" target="_blank" rel="noopener nofollow">ViT_PyTorch</a>ï¼Œå¦‚æœä½ åœ¨é˜…è¯»ä¸­æœ‰ä»»ä½•é—®é¢˜ã€å»ºè®®æˆ–é”™è¯¯æŒ‡å‡ºï¼Œä¹Ÿæ¬¢è¿åœ¨è¯„è®ºåŒºä¸æˆ‘è®¨è®ºï¼Œæˆ‘ä»¬å…±åŒè¿›æ­¥ï¼</p>
<h2 id="ä¸€vitä»è®ºæ–‡å‡ºå‘ç†è§£æ¶æ„è®¾è®¡">ä¸€ã€ViTï¼šä»è®ºæ–‡å‡ºå‘ç†è§£æ¶æ„è®¾è®¡</h2>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;åœ¨æ­£å¼åŠ¨æ‰‹å¤ç°ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆä»æºå¤´å‡ºå‘ï¼Œæ¥è¯»ä¸€è¯» Vision Transformer çš„åŸå§‹è®ºæ–‡ï¼šã€Š<strong>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</strong>ã€‹[<a href="https://arxiv.org/abs/2010.11929" target="_blank" rel="noopener nofollow">arXiv:2010.11929</a>]ã€‚è¿™æ˜¯ç”± Google Research äº 2020 å¹´æå‡ºçš„ä¸€ç¯‡å…·æœ‰é‡Œç¨‹ç¢‘æ„ä¹‰çš„è®ºæ–‡ï¼Œå®ƒé¦–æ¬¡å±•ç¤ºäº† <strong>çº¯ Transformer æ¶æ„åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šå¯ä»¥ä¸ä¾èµ–ä»»ä½•å·ç§¯æ¨¡å—ï¼Œä¾ç„¶å–å¾—ä¼˜ç§€æ€§èƒ½</strong>ã€‚</p>
<p><img src="https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250613213218339-753981665.png" alt="image-20250613213212649" loading="lazy"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Transformer åŸæœ¬æ˜¯ä¸ºäº†è§£å†³è¯­è¨€æ–‡å­—å¤„ç†ä»»åŠ¡è€Œæå‡ºçš„æ¨¡å‹ï¼Œå…¶è®¾è®¡åˆè¡·æ˜¯ç”¨äºå»ºæ¨¡åºåˆ—æ•°æ®ä¸­çš„é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚åœ¨ NLP é¢†åŸŸä¸­ï¼ŒTransformer èƒ½å¤Ÿé€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶çµæ´»åœ°æ•æ‰å•è¯ä¹‹é—´çš„å…¨å±€å…³ç³»ï¼Œæå¤§æå‡äº†è¯­è¨€ç†è§£ä¸ç”Ÿæˆçš„èƒ½åŠ›ã€‚è€Œè°·æ­Œçš„ç ”ç©¶å›¢é˜Ÿæå‡ºäº†éå¸¸å¤§èƒ†ä¹Ÿéå¸¸ä¼˜é›…çš„ä¸€ä¸ªæ€æƒ³ï¼šå¦‚æœæˆ‘ä»¬èƒ½æŠŠå›¾åƒåˆ‡å‰²æˆå°å—ï¼ˆPatchï¼‰ï¼Œå†æŠŠæ¯ä¸ª Patch å½“ä½œä¸€ä¸ªâ€œè¯â€ï¼Œæ˜¯å¦ä¹Ÿèƒ½å°†å›¾åƒè½¬åŒ–ä¸ºåºåˆ—ï¼Œä»è€Œè®© Transformer ä¹Ÿèƒ½å¤„ç†è§†è§‰ä¿¡æ¯ï¼Ÿè€Œå…¶æå‡ºçš„ViT å°±æ˜¯è¿™æ ·åšçš„ï¼šå®ƒå°†ä¸€å¼ å›¾åƒåˆ’åˆ†ä¸ºå›ºå®šå¤§å°çš„ Patchï¼ˆå¦‚ 16Ã—16ï¼‰ï¼Œå°†æ¯ä¸ª Patch å±•å¹³æˆå‘é‡ï¼Œå†é€šè¿‡ä¸€ä¸ªçº¿æ€§æŠ•å½±å±‚å°†å…¶æ˜ å°„åˆ°ç»Ÿä¸€çš„ç»´åº¦ç©ºé—´ï¼Œæœ€ç»ˆå½¢æˆä¸€ä¸ª token åºåˆ—ã€‚éšåï¼ŒViT åœ¨è¿™ä¸ª token åºåˆ—å‰åŠ ä¸Šä¸€ä¸ªå¯å­¦ä¹ çš„ <code>[CLS]</code> tokenï¼Œå¹¶å åŠ ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰ï¼Œä»¥ä¿ç•™å›¾åƒä¸­çš„ç©ºé—´ä½ç½®ä¿¡æ¯ã€‚æ•´ä¸ªåºåˆ—å°±åƒä¸€æ®µæ–‡æœ¬ï¼Œé€å…¥å¤šå±‚æ ‡å‡†çš„ Transformer ç¼–ç å™¨ç»“æ„è¿›è¡Œå¤„ç†ï¼Œæœ€åé€šè¿‡ <code>CLS</code> token çš„è¾“å‡ºï¼Œå®Œæˆæ•´å¼ å›¾åƒçš„åˆ†ç±»ä»»åŠ¡ã€‚è¿™ç§æ–¹æ³•ä¸ä¾èµ–ä»»ä½•å·ç§¯æ“ä½œï¼Œå®Œå…¨åŸºäºåºåˆ—å»ºæ¨¡ï¼Œå±•ç°äº† Transformer åœ¨å›¾åƒå»ºæ¨¡ä¸Šçš„å·¨å¤§æ½œåŠ›ã€‚</p>
<p><img src="https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250613215655365-764269196.png" alt="image-20250613215651876" loading="lazy"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ViTçš„æ¶æ„å¦‚ä¸Šå›¾ï¼Œä¸å¯»å¸¸çš„åˆ†ç±»ç½‘ç»œç±»ä¼¼ï¼Œæ•´ä¸ªVision Transformerå¯ä»¥åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼Œä¸€éƒ¨åˆ†æ˜¯ç‰¹å¾æå–éƒ¨åˆ†ï¼Œå¦ä¸€éƒ¨åˆ†æ˜¯åˆ†ç±»éƒ¨åˆ†ï¼Œ<strong>ç‰¹å¾æå–éƒ¨åˆ†</strong>æ˜¯å…¶æœ€æ ¸å¿ƒçš„ç»„æˆï¼Œå®ƒåŒ…æ‹¬äº†Patch Embeddingã€Positional Encodingä»¥åŠTransformer Encoderï¼Œ<strong>åˆ†ç±»éƒ¨åˆ†</strong> åˆ™æ˜¯ç´§æ¥åœ¨ç‰¹å¾æå–ä¹‹åï¼Œé€šè¿‡ä¸€ä¸ªå¯å­¦ä¹ çš„ <code>[CLS]</code> token æ¥ä»£è¡¨æ•´å¼ å›¾åƒçš„å…¨å±€è¯­ä¹‰ã€‚è¿™ä¸ª token ä¼šéšç€å…¶ä»– token ä¸€èµ·å‚ä¸ Transformer ç¼–ç è¿‡ç¨‹ï¼Œæœ€ç»ˆè¢«é€å…¥ä¸€ä¸ªç®€å•çš„ <strong>MLP åˆ†ç±»å¤´</strong> è¿›è¡Œç±»åˆ«é¢„æµ‹ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬æŒ‰ç…§å¦‚ä¸‹çš„åˆ’åˆ†æ¥é€ä¸ªè®²è§£ViTç½‘ç»œæ¶æ„</p>
<ul>
<li><strong>å›¾åƒåˆ†å—ä¸çº¿æ€§åµŒå…¥æ¨¡å—ï¼ˆPatch Embeddingï¼‰</strong></li>
</ul>
<p><img src="https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250614121658329-2037866168.png" alt="image-20250614121653494" loading="lazy"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ViT çš„ç¬¬ä¸€æ­¥æ“ä½œï¼Œå°±æ˜¯å°†è¾“å…¥å›¾åƒè½¬åŒ–ä¸ºä¸€ç³»åˆ—çš„ <strong>è§†è§‰ token</strong>ï¼Œè¿™ä¸ªè¿‡ç¨‹è¢«ç§°ä¸º <strong>Patch Embedding</strong>ï¼ŒPatch æŒ‡çš„å°±æ˜¯åˆ†å‰²åçš„ä¸€å°å—å›¾åƒåŒºåŸŸï¼Œå®ƒçš„æ ¸å¿ƒæ€æƒ³éå¸¸ç›´æ¥ï¼š</p>
<blockquote>
<p>å°†ä¸€å¼ äºŒç»´å›¾åƒæŒ‰ç…§å›ºå®šå¤§å°ï¼ˆå¦‚ 16Ã—16ï¼‰åˆ’åˆ†æˆè‹¥å¹²ä¸ªå°å—ï¼ˆPatchï¼‰ï¼Œç„¶åå°†æ¯ä¸ª Patch å±•å¹³æˆä¸€ä¸ªå‘é‡ï¼Œå†é€šè¿‡ä¸€ä¸ªçº¿æ€§å±‚å°†å…¶æ˜ å°„åˆ°æŒ‡å®šçš„ç»´åº¦ç©ºé—´ï¼ˆä¾‹å¦‚ 768ç»´ï¼‰ï¼Œä»è€Œå¾—åˆ°ä¸€ç»„è¾“å…¥ tokenï¼Œä¾› Transformer ä½¿ç”¨ã€‚</p>
</blockquote>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;è¿™ä¸ªå¤„ç†æ–¹å¼æœ¬è´¨ä¸Šå°±æ˜¯åœ¨æ¨¡æ‹Ÿ NLP ä¸­â€œå°†æ¯ä¸ªå•è¯ç¼–ç ä¸ºå‘é‡â€çš„è¿‡ç¨‹â€”â€”åªä¸è¿‡è¿™é‡Œçš„â€œå•è¯â€æ˜¯å›¾åƒå— patchï¼Œè€Œä¸æ˜¯æ–‡å­—ï¼Œæˆ‘ä»¬å‡è®¾å‡è®¾è¾“å…¥å›¾åƒå¤§å°ä¸º <code>224Ã—224Ã—3</code>ï¼ŒPatch å¤§å°ä¸º <code>16Ã—16</code>ï¼Œåˆ™ä¸€å¼ å›¾åƒå°†è¢«åˆ’åˆ†ä¸º$ (224/16)^2=14Ã—14=196$ ä¸ª patchï¼Œè€Œæ¯ä¸ª Patch å°†è¢«å±•å¹³æˆä¸€ä¸ª <span class="math inline">\(16 Ã— 16 Ã— 3 = 768\)</span> ç»´çš„å‘é‡ï¼Œå°†å…¶å±•å¹³æˆå‘é‡åï¼Œå†é€šè¿‡ä¸€ä¸ª <code>Linear</code> å±‚æ˜ å°„åˆ°æ¨¡å‹çš„ embedding ç©ºé—´ï¼ˆæ‰‹åŠ¨è®¾ç½®ï¼ŒViT-Baseä¸º 768 ç»´ï¼ŒViT-Largeä¸º1024ï¼ŒViT-Hugeä¸º1280ï¼Œé€šå¸¸ä½¿ç”¨768ï¼‰ï¼Œæœ€ç»ˆæˆ‘ä»¬å°±èƒ½å¾—åˆ°ä¸€ä¸ªå½¢çŠ¶ä¸ºï¼š<code>[batch_size, 196, embed_dim]</code>çš„patch token åºåˆ—ï¼Œè€Œæˆ‘ä»¬è¯¥å¦‚ä½•å¯¹å›¾åƒè¿›è¡Œåˆ†å‰²å®ç°Patch Embeddingå‘¢ï¼Ÿè¿™æ—¶å€™æˆ‘ä»¬ä¾¿å¯ä»¥æƒ³åˆ°æˆ‘ä»¬çš„å·ç§¯ï¼Œç”±äºå·ç§¯ä½¿ç”¨çš„æ˜¯æ»‘åŠ¨çª—å£çš„æ€æƒ³ï¼Œå› æ­¤æˆ‘ä»¬åªéœ€è¦å°†å·ç§¯æ ¸ä»¥åŠæ­¥é•¿è®¾ç½®æˆä¸Patch-Sizeç›¸ç­‰ä¾¿å¯ï¼Œè¿™æ—¶ä¸¤ä¸ªå›¾ç‰‡åŒºåŸŸçš„ç‰¹å¾æå–è¿‡ç¨‹å°±ä¸ä¼šæœ‰é‡å ï¼Œå½“æˆ‘ä»¬è¾“å…¥çš„å›¾ç‰‡æ˜¯<code>[224, 224, 3]</code>çš„æ—¶å€™ï¼Œæˆ‘ä»¬å¯ä»¥è·å¾—ä¸€ä¸ª<code>[14, 14, 768]</code>çš„ç‰¹å¾å±‚ã€‚</p>
<p><img src="https://i-blog.csdnimg.cn/blog_migrate/2c1957df057acb9c81aa653920479cb5.gif#pic_center" alt="å·ç§¯" loading="lazy"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;è€Œè·å¾—äº†ç‰¹å¾ä¿¡æ¯ä¹‹åæˆ‘ä»¬éœ€è¦å°†å¾—åˆ°çš„ç‰¹å¾ä¿¡æ¯ç»„åˆæˆåºåˆ—ï¼Œç»„åˆçš„æ–¹å¼å¾ˆç®€å•ï¼Œæˆ‘ä»¬åªéœ€è¦å¯¹è¿™ä¸ªç‰¹å¾å›¾è¿›è¡Œå±•å¹³ï¼ˆFlattenï¼‰å¹¶è½¬ç½®ä¸ºæ ‡å‡†åºåˆ—æ ¼å¼ï¼Œä¾¿å¯ä»¥å¾—åˆ°æœ€ç»ˆçš„ Patch Token åºåˆ—ï¼Œç”¨äºè¾“å…¥ Transformerï¼Œæˆ‘ä»¬åœ¨ä¸Šé¢å¯¹å›¾åƒè¿›è¡Œåˆ†å‰²åå¾—åˆ°äº†ä¸€ä¸ª<code>[14, 14, 768]</code>çš„ç‰¹å¾å±‚ï¼Œæˆ‘ä»¬å°†è¿™ä¸ªç‰¹å¾å›¾çš„<strong>é«˜å®½ç»´åº¦è¿›è¡Œå¹³é“º</strong>åå³å¯å¾—åˆ°<strong>ä¸€ä¸ª<code>[196, 768]</code>çš„ç‰¹å¾å±‚</strong>ï¼Œè‡³æ­¤Patch Embeddingä¾¿å®Œæˆå•¦ï¼</p>
<ul>
<li><strong>åˆ†ç±»æ ‡è®°ä¸ä½ç½®ç¼–ç æ¨¡å—ï¼ˆcls_token + Position Embeddingï¼‰</strong></li>
</ul>
<p><img src="https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250614121720877-524693168.png" alt="image-20250614121715327" loading="lazy"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;åœ¨å®Œæˆ Patch Embedding å¾—åˆ°å½¢å¦‚ <code>[batch_size, 196, 768]</code> çš„ Patch Token åºåˆ—åï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬è¦åšä¸¤ä»¶å…³é”®çš„äº‹æƒ…ï¼š</p>
<ol>
<li>
<p>æ·»åŠ  <code>[CLS] Token</code> â€”â€” å›¾åƒçš„â€œå…¨å±€æ‘˜è¦â€å…¥å£</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Transformer æœ€åˆåœ¨å¤„ç†æ–‡æœ¬ä»»åŠ¡æ—¶ï¼Œä¼šåœ¨åºåˆ—çš„æœ€å‰é¢æ·»åŠ ä¸€ä¸ªç‰¹æ®Šçš„ <code>[CLS]</code> Tokenï¼Œç”¨äºèšåˆæ•´ä¸ªå¥å­çš„è¯­ä¹‰ä¿¡æ¯ã€‚åŒç†ï¼Œåœ¨ ViT ä¸­ä¹Ÿå¼•å…¥äº† <code>[CLS] Token</code>ï¼Œå®ƒå¹¶ä¸ä»£è¡¨æŸä¸ªå…·ä½“çš„ Patchï¼Œè€Œæ˜¯ä½œä¸ºä¸€ä¸ªå…¨å±€çš„ä»£è¡¨ Tokenï¼Œåœ¨ Transformer ä¸­â€œå‚ä¸â€æ¯ä¸€å±‚çš„ä¿¡æ¯äº¤äº’ï¼Œæœ€ç»ˆç”¨äºæå–æ•´ä¸ªå›¾åƒçš„å…¨å±€ç‰¹å¾ã€‚å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œç¼–å·ä¸º <code>0*</code> çš„é‚£ä¸ªä½ç½®å³è¡¨ç¤º <code>[CLS] Token</code>ï¼Œå…¶åˆå§‹å€¼æ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•°å‘é‡ï¼Œç»´åº¦ä¸ Patch Token ç›¸åŒï¼ˆä¾‹å¦‚ 768ï¼‰ï¼Œç»è¿‡ Transformer ç¼–ç åï¼ŒViT ä¼š<strong>ä½¿ç”¨è¿™ä¸ª <code>[CLS] Token</code> çš„è¾“å‡ºå‘é‡ä½œä¸ºå›¾åƒçš„åˆ†ç±»ç»“æœè¾“å…¥</strong>åˆ° MLP Head ä¸­ï¼Œå®Œæˆæœ€ç»ˆåˆ†ç±»ã€‚</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;æ·»åŠ äº† <code>[CLS] Token</code> ä¹‹åï¼ŒåŸæœ¬çš„ <code>196</code> ä¸ª Patch Token åºåˆ—å°±å˜æˆäº† <code>197</code> ä¸ª Tokenï¼Œå½¢çŠ¶å˜ä¸ºäº†å½¢å¦‚ï¼š<code>[batch_size, 196 + 1, 768]</code></p>
</li>
<li>
<p>æ·»åŠ ä½ç½®ç¼–ç ï¼ˆPositional Embeddingï¼‰â€”â€” å¸®åŠ©æ¨¡å‹ç†è§£â€œå›¾åƒä¸­çš„ä½ç½®â€</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ç”±äº Transformer æ˜¯å®Œå…¨åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶æ„å»ºçš„ï¼Œå®ƒå¹¶ä¸å…·å¤‡å·ç§¯ç½‘ç»œä¸­å¤©ç„¶çš„<strong>ä½ç½®ä¿¡æ¯å»ºæ¨¡èƒ½åŠ›</strong>ã€‚æ‰€ä»¥æˆ‘ä»¬è¿˜éœ€è¦ç»™æ¯ä¸ª Token æ·»åŠ ä¸€ä¸ª<strong>ä½ç½®ç¼–ç </strong>ï¼Œç”¨äºå‘Šè¯‰æ¨¡å‹è¿™ä¸ª Token æ¥è‡ªäºå›¾åƒçš„å“ªä¸€å—åŒºåŸŸã€‚ViT é‡‡ç”¨çš„æ˜¯ä¸€ç§ <strong>å¯å­¦ä¹ çš„ç»å¯¹ä½ç½®ç¼–ç </strong>ï¼Œä¹Ÿå°±æ˜¯ä¸ºæ¯ä¸€ä¸ª Token çš„ä½ç½®ï¼ˆåŒ…æ‹¬ <code>[CLS]</code> Tokenï¼‰éƒ½åˆå§‹åŒ–ä¸€ä¸ªå¯å­¦ä¹ çš„ä½ç½®å‘é‡ï¼Œå¹¶ä¸åŸå§‹ Token ç›¸åŠ ï¼Œè¿™æ ·ï¼Œæ¨¡å‹å°±èƒ½åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­è‡ªå·±æŒæ¡ç©ºé—´é¡ºåºå’Œè¯­ä¹‰ä¹‹é—´çš„å…³ç³»ã€‚</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ä½ç½®ç¼–ç çš„å½¢çŠ¶ä¸è¾“å…¥åºåˆ—ä¸€è‡´ï¼Œä¹Ÿæ˜¯<code>[1, 196 + 1, 768]</code>ï¼Œä¸”ä½ç½®ç¼–ç çš„åŠ å…¥æ–¹å¼éå¸¸ç®€å•å³ï¼š<code>tokens = tokens + pos_embed  # [B, 197, 768]</code>,ç»è¿‡è¿™ä¸¤ä¸ªæ­¥éª¤ä¹‹åï¼ŒViT çš„è¾“å…¥æ‰çœŸæ­£å‡†å¤‡å¥½ï¼Œå¯ä»¥é€å…¥ Transformer ç¼–ç å™¨ä¸­è¿›è¡Œå¤šå±‚ç‰¹å¾äº¤äº’ä¸å»ºæ¨¡ï¼Œè‡³æ­¤cls_token + Position Embeddingä¾¿å®Œæˆå•¦ï¼</p>
</li>
</ol>
<ul>
<li><strong>æ ‡å‡† Transformer ç¼–ç å™¨ï¼ˆMulti-head Attention + LayerNorm + MLP + æ®‹å·®è¿æ¥ï¼‰</strong></li>
</ul>
<p><img src="https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250613220458003-7279660.png" alt="image-20250613220454690" loading="lazy"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;å½“æˆ‘ä»¬å¾—åˆ°äº†å¸¦æœ‰ <code>[CLS] Token</code> å’Œä½ç½®ç¼–ç çš„å®Œæ•´ Patch åºåˆ—ï¼ˆå½¢çŠ¶ä¸º <code>[B, 197, 768]</code>ï¼‰ä¹‹åï¼ŒViT ä¼šå°†å…¶é€å…¥ä¸€ç³»åˆ—æ ‡å‡†çš„ <strong>Transformer Encoder Block</strong> ä¸­è¿›è¡Œæ·±åº¦å»ºæ¨¡ã€‚æ¯ä¸€ä¸ª Block çš„è®¾è®¡ä¸åŸå§‹çš„ NLP Transformer ä¸­çš„ Encoder ä¿æŒä¸€è‡´ï¼Œç»“æ„éå¸¸ç»å…¸ï¼Œç”±ä¸¤ä¸ªå­æ¨¡å—ç»„æˆï¼š</p>
<ol>
<li>
<p><strong>LayerNorm + å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆMulti-Head Self Attentionï¼‰</strong></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;åœ¨è¿™ä¸ªå­æ¨¡å—ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå¯¹è¾“å…¥è¿›è¡Œ LayerNorm å½’ä¸€åŒ–ï¼Œå†é€å…¥ Multi-Head Self-Attention æ¨¡å—ï¼Œè¿™é‡Œçš„è‡ªæ³¨æ„åŠ›çš„ä½œç”¨æ˜¯å»ºç«‹æ‰€æœ‰ Token ä¹‹é—´çš„<strong>å…¨å±€å…³ç³»</strong>ï¼Œä½¿æ¯ä¸ª Token éƒ½èƒ½è·å–å…¶ä»–åŒºåŸŸçš„ä¿¡æ¯ï¼Œè¿™ä¹Ÿæ˜¯Transformerçš„çµé­‚éƒ¨åˆ†ï¼Œå…¶å…·ä½“å®ç°åˆ™æ˜¯è®©æ¯ä¸ª Token é€šè¿‡æŸ¥è¯¢ï¼ˆQueryï¼‰ä¸æ‰€æœ‰å…¶ä»– Token çš„é”®ï¼ˆKeyï¼‰è¿›è¡ŒåŒ¹é…ï¼Œè®¡ç®—å…¶å¯¹å…¶ä»–ä½ç½®çš„å…³æ³¨æƒé‡ï¼Œä»è€Œæå–å¯¹å½“å‰ä»»åŠ¡æœ€æœ‰ç”¨çš„ä¿¡æ¯ã€‚ç”¨å…¬å¼å’Œå›¾ç‰‡è¡¨ç¤ºå¦‚ä¸‹ï¼š</p>
</li>
</ol>
<p></p><div class="math display">\[\text{Attention}(Q, K, V) = \text{Softmax}\left( \frac{QK^\top}{\sqrt{d_k}} \right) V
\]</div><p></p><p><img src="https://i-blog.csdnimg.cn/blog_migrate/b0dfd9f9109a979f94a1f8aa4e6663e3.gif#pic_center" alt="kqv" loading="lazy"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;å¯¹äºåˆå­¦è€…æ¥è¯´ï¼Œè¿™éƒ¨åˆ†å†…å®¹çœ‹èµ·æ¥å¯èƒ½ä¼šæ¯”è¾ƒæŠ½è±¡ï¼Œä½†æ˜¯æˆ‘ä»¬å¦‚æœå°†å®ƒæ‹†è§£ä¸€æ­¥ä¸€æ­¥æ¥çœ‹ï¼Œå…¶å®éå¸¸ç›´è§‚ã€‚åœ¨å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œæ¯ä¸€ä¸ªè¾“å…¥çš„ <strong>Token</strong>ï¼ˆå›¾åƒçš„ Patchï¼‰éƒ½ä¼šè¢«åˆ†åˆ«æ˜ å°„å‡ºä¸‰ä¸ªå‘é‡ï¼Œåˆ†åˆ«æ˜¯ï¼š<strong>Queryï¼ˆæŸ¥è¯¢å‘é‡ï¼‰ã€Keyï¼ˆé”®å‘é‡ï¼‰ã€Valueï¼ˆå€¼å‘é‡ï¼‰</strong>ï¼Œç”¨ä¸€ä¸ªç®€å•æ˜äº†çš„æ¯”å–»æ¥ç†è§£ï¼šå‡è®¾ä½ åœ¨å‚åŠ ä¸€æ¬¡ä¼šè®®ï¼ˆæ³¨æ„åŠ›æœºåˆ¶ï¼‰ï¼Œä½ æ˜¯ Queryï¼Œè€Œä¼šè®®å®¤é‡Œæ¯ä¸€ä¸ªä¸ä¼šè€…ï¼ˆåŒ…æ‹¬ä½ è‡ªå·±ï¼‰éƒ½æ˜¯ä¸€ä¸ª Keyï¼ŒåŒæ—¶ä»–ä»¬æ‰‹é‡Œéƒ½æ‹¿ç€ä¸€ä»½èµ„æ–™ï¼ˆValueï¼‰ï¼Œä½ ä¼šæ ¹æ®è‡ªå·±å’Œå…¶ä»–äºº Key çš„â€œç›¸ä¼¼ç¨‹åº¦â€å†³å®šä½ è¦å¤šå¤§ç¨‹åº¦å‚è€ƒä»–ä»¬çš„èµ„æ–™ï¼ˆValueï¼‰â€”â€”è¿™å°±æ˜¯æ³¨æ„åŠ›æƒé‡çš„è®¡ç®—ã€‚è®¾å½“å‰è¾“å…¥åºåˆ—ä¸ºçŸ©é˜µ <span class="math inline">\(X \in \mathbb{R}^{n \times d}\)</span>ï¼Œå…¶ä¸­ <span class="math inline">\(n\)</span> æ˜¯åºåˆ—é•¿åº¦ï¼ˆä¾‹å¦‚ ViT ä¸­æ˜¯ 197 ä¸ª Tokenï¼‰ï¼Œ<span class="math inline">\(d\)</span> æ˜¯æ¯ä¸ª Token çš„ç»´åº¦ï¼ˆä¾‹å¦‚ 768ï¼‰ã€‚æˆ‘ä»¬ç”¨ä¸‰ç»„å¯å­¦ä¹ çš„å‚æ•°çŸ©é˜µå°†å…¶å˜æ¢ä¸ºï¼š</p>
<p></p><div class="math display">\[[
Q = XW^Q,\quad K = XW^K,\quad V = XW^V
]
\]</div><p></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ç„¶åè®¡ç®—æ³¨æ„åŠ›å¾—åˆ†ï¼ˆScoreï¼‰ï¼š</p>
<p></p><div class="math display">\[[
\text{Score} = \frac{QK^\top}{\sqrt{d_k}}
]
\]</div><p></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;æ¥ç€ä½¿ç”¨ Softmax å¯¹å¾—åˆ†è¿›è¡Œå½’ä¸€åŒ–ï¼Œå¾—åˆ°æ³¨æ„åŠ›æƒé‡ <span class="math inline">\(\alpha\)</span>ï¼š</p>
<p></p><div class="math display">\[[
\alpha = \text{Softmax}\left( \frac{QK^\top}{\sqrt{d_k}} \right)
]
\]</div><p></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;æœ€ååŠ æƒç»„åˆæ‰€æœ‰ Value å‘é‡ï¼Œå¾—åˆ°æ–°çš„è¾“å‡ºè¡¨ç¤ºï¼š</p>
<p></p><div class="math display">\[[
\text{Attention}(Q, K, V) = \alpha V
]
\]</div><p></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>è¿™å¥—è®¡ç®—æµç¨‹ç”¨äººè¯è¯´å°±æ˜¯ï¼Œ</strong>æˆ‘ä»¬è¿™å¥—æ³¨æ„åŠ›ç³»ç»Ÿå‡è®¾æœ‰ä¸‰ä¸ªè¾“å…¥åˆ†åˆ«æ˜¯<code>input-1</code>ã€<code>input-2</code>ã€<code>input-3</code>ä»¥åŠä¸‰ä¸ªå¯¹åº”çš„è¾“å‡º<code>output-1</code>ã€<code>output-2</code>ã€<code>output-3</code>ï¼Œæ¯ä¸ªè¾“å…¥éƒ½æœ‰ä»–ä»¬è‡ªå·±çš„QKVå‘é‡</p>
<p><img src="https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250614010037294-166599101.png" alt="image-20250614010033394" loading="lazy"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;å¦‚æœæˆ‘ä»¬è¦æ±‚<code>output-1</code>ï¼Œé‚£æˆ‘ä»¬é¦–å…ˆå…ˆå°†<code>input-1</code>çš„QæŸ¥è¯¢å‘é‡ä¸ä¸‰ä¸ªè¾“å…¥çš„Ké”®å‘é‡åˆ†åˆ«ç›¸ä¹˜å¾—åˆ°å¯¹åº”çš„åˆ†æ•°ï¼Œè¿™ä¸ªåˆ†æ•°ä»£è¡¨çš„ä¾¿æ˜¯ <code>input-1</code> å¯¹å…¶å®ƒä¸‰ä¸ªè¾“å…¥çš„â€œæ³¨æ„åŠ›ç¨‹åº¦â€ï¼›æ¥ä¸‹æ¥æˆ‘ä»¬å°†è¿™ä¸‰ä¸ªåˆ†æ•°åˆ†åˆ«æ±‚ä¸€æ¬¡<code>softmax</code>ä½¿å®ƒä»¬å˜æˆ 0 åˆ° 1 ä¹‹é—´çš„æ¦‚ç‡å€¼ï¼Œå¹¶ä¸”åŠ èµ·æ¥ä¸º 1ï¼Œè¿™ä¸ªè¿‡ç¨‹å¯ä»¥ç†è§£ä¸ºï¼š<strong>åˆ†é…å…³æ³¨åº¦</strong>ï¼Œå‘Šè¯‰æˆ‘ä»¬è¯¥â€œå…³æ³¨è°ã€å…³æ³¨å¤šå°‘â€ï¼›æœ€åï¼Œç”¨åˆšæ‰å¾—åˆ°çš„è¿™ä¸‰ä¸ªæ³¨æ„åŠ›æƒé‡ï¼Œå»åˆ†åˆ«åŠ æƒå¯¹åº”çš„ <strong>å€¼å‘é‡ V</strong>ï¼Œå†æŠŠå®ƒä»¬åŠ åœ¨ä¸€èµ·ï¼Œå¾—åˆ°çš„å°±æ˜¯æœ€ç»ˆçš„ <code>output-1</code></p>
<p><img src="https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250614025205817-1784798237.png" alt="image-20250614010355459" loading="lazy"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ä¹Ÿå°±æ˜¯è¯´ï¼šè¾“å‡º = æ‰€æœ‰å…³æ³¨å¯¹è±¡çš„â€œå€¼â€ Ã— â€œå…³æ³¨å®ƒçš„ç¨‹åº¦â€çš„åŠ æƒå’Œã€‚æ¯ä¸ª Query ä¼šæ ¹æ®ä¸æ‰€æœ‰ Key çš„ç›¸ä¼¼ç¨‹åº¦ï¼Œå¯¹å¯¹åº”çš„ Value è¿›è¡ŒåŠ æƒæ±‚å’Œï¼›è¿™æ ·çš„è¯æ‰€æœ‰ Token ä¹‹é—´éƒ½èƒ½è¿›è¡Œä¿¡æ¯äº¤æ¢ï¼Œä»è€Œæ•æ‰ <strong>å…¨å±€ä¸Šä¸‹æ–‡ä¾èµ–</strong>ï¼›å› æ­¤æœ€åçš„è¾“å‡ºçš„è™½ç„¶ä»ç„¶æ˜¯ä¸€ä¸ªä¸åŸå§‹ Token æ•°é‡ç›¸åŒçš„æ–°åºåˆ—ï¼Œä½†æ¯ä¸ª Token çš„è¡¨ç¤ºå·²ç»èåˆäº†å…¨å±€ä¿¡æ¯ã€‚</p>
<ol start="2">
<li>
<p><strong>LayerNorm + MLP å‰é¦ˆç¥ç»ç½‘ç»œ</strong></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;åœ¨æ¯ä¸ª Transformer Block ä¸­ï¼Œé™¤äº†æ³¨æ„åŠ›æœºåˆ¶ä¹‹å¤–ï¼Œè¿˜æœ‰ä¸€ä¸ªéå¸¸é‡è¦çš„éƒ¨åˆ†ï¼Œé‚£å°±æ˜¯ <strong>å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeed Forward Network, FFNï¼‰</strong>ï¼Œä¹Ÿå¸¸è¢«ç§°ä¸º <strong>MLP å­æ¨¡å—</strong>ã€‚è¿™ä¸ªå­æ¨¡å—çš„ç»“æ„å…¶å®éå¸¸ç®€å•ï¼Œå°±æ˜¯ä¸¤ä¸ªå…¨è¿æ¥å±‚ï¼ˆLinearï¼‰ï¼Œä¸­é—´å†åŠ ä¸€ä¸ªéçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ GELUï¼‰ï¼š</p>
<p></p><div class="math display">\[FFN(x)=Linear 
2
â€‹
 (GELU(Linear 
1
â€‹
 (x)))
\]</div><p></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;è¿™é‡Œçš„ Linear å±‚ä¹Ÿå°±æ˜¯æˆ‘ä»¬ç†Ÿæ‚‰çš„å…¨è¿æ¥å±‚ï¼Œç»´åº¦çš„å˜åŒ–ä¸€èˆ¬æ˜¯è¿™æ ·çš„ï¼šé¦–å…ˆç¬¬ä¸€ä¸ª Linear å±‚ä¼šæŠŠè¾“å…¥çš„ç»´åº¦ä» <code>d_model</code>ï¼ˆæ¯”å¦‚ 768ï¼‰æå‡åˆ°ä¸€ä¸ªæ›´é«˜çš„ç»´åº¦ï¼ˆæ¯”å¦‚ 3072ï¼‰ï¼Œæ¥ç€é€šè¿‡ GELU æ¿€æ´»å‡½æ•°å¼•å…¥éçº¿æ€§ï¼Œæœ€åå†ç”¨ä¸€ä¸ª Linear å±‚å°†ç»´åº¦é™å›åŸæ¥çš„ <code>d_model</code>è¿™ä¸ª FFN çš„ç»“æ„å¯ä»¥ç†è§£ä¸ºå¯¹æ¯ä¸ª Token ç‹¬ç«‹åœ°è¿›è¡Œæ›´æ·±å±‚æ¬¡çš„ç‰¹å¾å˜æ¢ã€‚ä¸åŒäºå¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶é‚£ç§è·¨ Token çš„ä¿¡æ¯äº¤äº’ï¼Œå‰é¦ˆç¥ç»ç½‘ç»œçš„å¤„ç†æ˜¯<strong>é€ Token çš„ç‚¹å¯¹ç‚¹éçº¿æ€§å˜æ¢</strong>ï¼Œä¸»è¦ç”¨äºå¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚è€Œæ®‹å·®è¿æ¥çš„å¼•å…¥å¯ä»¥åœ¨æ¯ä¸ª Transformer Block å†…å½¢æˆä¸€ç§<strong>çŸ­è·¯è·¯å¾„ï¼ˆShortcut Pathï¼‰</strong>ï¼Œå®ƒèƒ½æœ‰æ•ˆç¼“è§£æ·±å±‚ç½‘ç»œä¸­çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼šä¸å…¶ç›´æ¥å­¦ä¹ ä¸€ä¸ªæ˜ å°„å‡½æ•° <span class="math inline">\(F(x)\)</span>ï¼Œä¸å¦‚è®©ç½‘ç»œå­¦ä¹  <span class="math inline">\(F(x) = H(x) - x\)</span>ï¼Œå³è®©æ¨¡å‹å…³æ³¨â€œè¾“å…¥ä¸è¾“å‡ºçš„å·®å€¼â€ï¼Œè¿™æ ·åè€Œæ›´å®¹æ˜“ä¼˜åŒ–ã€‚å› æ­¤å®Œæ•´çš„è®¡ç®—æµç¨‹å¦‚ä¸‹ï¼š</p>
<p></p><div class="math display">\[y=x+FFN(LayerNorm(x))
\]</div><p></p></li>
</ol>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;è‡³æ­¤ï¼ŒViT çš„æ ¸å¿ƒç»“æ„ä¹Ÿå°±å®Œæ•´æ‹¼è£…å®Œæˆäº†ã€‚ä» Patch Embedding åˆ° <code>[CLS] Token</code> ä¸ä½ç½®ç¼–ç ï¼Œå†åˆ°æ·±åº¦çš„å¤šå±‚ Transformer ç¼–ç å™¨ï¼ŒViT å®Œæ•´åœ°å°†è¯­è¨€æ¨¡å‹çš„ç»“æ„ç§»æ¤åˆ°äº†è§†è§‰é¢†åŸŸï¼Œå¹¶å–å¾—äº†çªç ´æ€§çš„è¡¨ç°ã€‚Transformer Block æ˜¯ ViT çš„â€œå»ºæ¨¡å¤§è„‘â€ï¼Œä¹Ÿæ˜¯å…¶é€šç”¨æ€§ä¸å¼ºå¤§æ€§èƒ½çš„æ ¹åŸºã€‚</p>
<ul>
<li><strong>åˆ†ç±»å¤´ï¼ˆClassification Headï¼‰</strong></li>
</ul>
<p><img src="https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250613220512171-1855280189.png" alt="image-20250613220508587" loading="lazy"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ç»è¿‡å¤šä¸ª Transformer Block çš„æ·±åº¦ç‰¹å¾æå–ä¹‹åï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ªæ–°çš„åºåˆ—è¡¨ç¤ºï¼Œå…¶å½¢çŠ¶ä¸º <code>[B, 197, 768]</code>ï¼ˆå‡è®¾æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ ViT-Base æ¨¡å‹ï¼‰ï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªä½ç½®çš„ Token ä»ç„¶æ˜¯æˆ‘ä»¬åœ¨æœ€å¼€å§‹åŠ å…¥çš„ <code>[CLS] Token</code>ã€‚è¿™ä¸ª <code>[CLS] Token</code> å¯ä»¥çœ‹ä½œæ˜¯æ•´ä¸ªå›¾åƒçš„å…¨å±€è¯­ä¹‰è¡¨ç¤ºï¼Œå› ä¸ºåœ¨å¤šè½®æ³¨æ„åŠ›äº¤äº’ä¸­ï¼Œå®ƒå·²ç»â€œèåˆâ€äº†æ‰€æœ‰ Patch çš„ä¿¡æ¯ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åªéœ€è¦ä»åºåˆ—ä¸­å–å‡ºè¿™ä¸€ä½ç½®çš„å‘é‡ï¼ˆå³ç¬¬ä¸€ä¸ª Tokenï¼‰ï¼Œç„¶åé€å…¥ä¸€ä¸ªå…¨è¿æ¥å±‚ï¼ˆLinearï¼‰å°±å¯ä»¥å®Œæˆåˆ†ç±»ä»»åŠ¡äº†ã€‚</p>
<h2 id="äºŒå®æˆ˜å¤ç°pytorchç‰ˆvitç½‘ç»œæ¶æ„">äºŒã€å®æˆ˜å¤ç°PyTorchç‰ˆViTç½‘ç»œæ¶æ„</h2>
<h3 id="ä¸€æ¨¡å—1patchembeddingç±»">ï¼ˆä¸€ï¼‰æ¨¡å—1ï¼šPatchEmbeddingç±»</h3>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>PatchEmbedding</code> æ˜¯ ViT ä¸­æœ€å…³é”®çš„ä¸€æ­¥ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨å·ç§¯æ“ä½œå°†è¾“å…¥å›¾åƒåˆ’åˆ†ä¸ºè‹¥å¹²ä¸é‡å çš„å°å—ï¼ˆPatchï¼‰ï¼Œæ¯ä¸ª Patch è¢«ç¼–ç ä¸ºä¸€ä¸ªå‘é‡ï¼Œæˆ‘ä»¬é‡‡ç”¨ç­‰æ­¥é•¿å·ç§¯çš„æ–¹å¼å®ç°åˆ’åˆ†ï¼Œå¹¶åœ¨å±•å¹³åå°†å…¶é€å…¥ Transformer æ¨¡å—è¿›è¡Œåç»­å¤„ç†ã€‚</p>
<pre><code class="language-python">class VisionPatchEmbedding(nn.Module):
    def __init__(self, image_size, patch_size, in_channels, embed_dim, flatter=True):
        super().__init__()
        self.proj = nn.Conv2d(in_channels, embed_dim, patch_size, patch_size)
        self.norm = nn.LayerNorm(embed_dim)
        self.flatter = flatter

    def forward(self, x):
        x = self.proj(x)
        if self.flatter:
            x = x.flatten(2).transpose(1, 2)  # [B, C, H, W] -&gt; [B, N, C]
        x = self.norm(x)
        return x
</code></pre>
<h3 id="äºŒæ¨¡å—2positionembedding">ï¼ˆäºŒï¼‰æ¨¡å—2ï¼šPositionEmbedding</h3>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ç”±äºViT ä¸åƒCNNï¼Œå…¶æ²¡æœ‰å·ç§¯æ„Ÿå—é‡ï¼Œå› æ­¤éœ€è¦åŠ å…¥ä½ç½®ç¼–ç ï¼ˆ<code>pos_embed</code>ï¼‰æ¥ä¿ç•™ä½ç½®ä¿¡æ¯æ¯ä¸€ä¸ªPatchçš„ä½ç½®ä¿¡æ¯</p>
<pre><code class="language-python">self.cls_token = nn.Parameter(torch.zeros(1, 1, num_features))
self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, num_features))
</code></pre>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ç”±ViTViTåœ¨<strong>é¢„è®­ç»ƒæ—¶</strong>é€šå¸¸ä½¿ç”¨å›ºå®šåˆ†è¾¨ç‡ï¼ˆå¦‚ <code>224x224</code>ï¼‰ï¼Œå›¾åƒè¢«åˆ†å‰²ä¸º <code>14x14</code> ä¸ª Patchï¼ˆ<code>patch_size=16</code>ï¼‰ï¼Œä½ç½®ç¼–ç  <code>pos_embed</code> çš„å½¢çŠ¶ä¸º <code>[1, 197, 768]</code>ï¼ˆ<code>197 = 1(cls_token) + 14x14</code>ï¼‰ï¼Œä½†æ˜¯åŒæ ·ç”±äºViTæ²¡æœ‰æ„Ÿå—é‡ä¸€è¯´ï¼Œå› æ­¤<strong>å®é™…åº”ç”¨</strong>å½“è¾“å…¥åˆ†è¾¨ç‡ä¸åŒï¼ˆå¦‚ <code>256x256</code>ï¼‰æ—¶ï¼ŒPatch æ•°é‡å˜ä¸º <code>16x16 = 256</code>ï¼ˆ<code>+1 cls_token = 257</code>ï¼‰ï¼ŒåŸæ¥çš„ä½ç½®ç¼–ç ï¼ˆ<code>197</code>ï¼‰ä¾¿æ— æ³•ç›´æ¥ä½¿ç”¨ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦é€šè¿‡åŒä¸‰æ¬¡æ’å€¼ï¼ˆ<code>bicubic</code>ï¼‰å°† <code>14x14</code> çš„ä½ç½®ç¼–ç è°ƒæ•´åˆ°æ–°åˆ†è¾¨ç‡å¯¹åº”çš„ç½‘æ ¼å°ºå¯¸ï¼Œå…·ä½“å®ç°å¦‚ä¸‹ï¼š</p>
<pre><code class="language-python">img_token_pos_embed = F.interpolate(
    img_token_pos_embed, size=self.features_shape, mode='bicubic', align_corners=False
)
pos_embed = torch.cat((cls_token_pos_embed, img_token_pos_embed), dim=1)
x = self.pos_drop(x + pos_embed)
</code></pre>
<h3 id="ä¸‰æ¨¡å—3multi-head-attentionä¸mlp">ï¼ˆä¸‰ï¼‰æ¨¡å—3ï¼šMulti-head Attentionä¸MLP</h3>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;æ¥ç€æˆ‘ä»¬å®ç°Transformerä¸­æœ€å…³é”®çš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªç±»<code>SelfAttention</code> æ¥å®ç°äº†å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨è¿™ä¸ªæ¨¡å—é‡Œæˆ‘ä»¬å…ˆç”¨ä¸€ä¸ªçº¿æ€§å±‚åŒæ—¶ç”ŸæˆæŸ¥è¯¢ï¼ˆQï¼‰ã€é”®ï¼ˆKï¼‰ã€å€¼ï¼ˆVï¼‰ï¼Œå¹¶æŒ‰å¤´æ•°æ‹†åˆ†ç»´åº¦ï¼Œç„¶åè®¡ç®— Q å’Œ K çš„ç‚¹ç§¯å¹¶ç¼©æ”¾ï¼Œé€šè¿‡ <code>softmax</code>å¾—åˆ°æ³¨æ„åŠ›æƒé‡ï¼Œåˆ©ç”¨æƒé‡åŠ æƒå€¼ï¼ˆVï¼‰ï¼Œæœ€åå°†å¤šå¤´ç»“æœæ‹¼æ¥åé€šè¿‡çº¿æ€§å˜æ¢å’Œ <code>dropout</code>ï¼Œä½¿è¾“å‡ºå…·æœ‰ä¸è¾“å…¥ç»´åº¦ç›¸åŒçš„ç‰¹å¾ï¼Œå®Œæˆä¿¡æ¯çš„åŠ¨æ€èåˆä¸è¡¨è¾¾å¢å¼ºã€‚</p>
<pre><code class="language-python">class SelfAttention(nn.Module):
    def __init__(self, dim, num_heads, qkv_bias=False, attn_drop_rate=0.0, proj_drop_rate=0.0):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        self.scale = self.head_dim ** -0.5

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop_rate)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop_rate)

    def forward(self, x):
        B, N, C = x.shape
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2,0,3,1,4)
        q, k, v = qkv[0], qkv[1], qkv[2]

        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale
        attn = attn .softmax(dim=-1)
        attn = self.attn_drop(attn)

        x = torch.matmul(attn, v).transpose(1,2).reshape(B,N,C)
        x = self.proj(x)
        x = self.proj_drop(x)

        return x
</code></pre>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;åœ¨è¿™ä¸ª <code>MLP</code> æ¨¡å—ä¸­ï¼Œæˆ‘è®¾è®¡äº†ä¸€ä¸ªä¸¤å±‚çš„å…¨è¿æ¥ç½‘ç»œï¼Œé¦–å…ˆé€šè¿‡ <code>fc1</code> å°†è¾“å…¥ç‰¹å¾æ˜ å°„åˆ°éšè—ç»´åº¦ï¼Œç„¶åç»è¿‡æ¿€æ´»å‡½æ•°éçº¿æ€§å˜æ¢ï¼Œæ¥ç€ç”¨ dropout åšæ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæ¥ç€å†é€šè¿‡ <code>fc2</code> æ˜ å°„åˆ°è¾“å‡ºç»´åº¦ï¼Œæœ€åå†ç”¨ä¸€æ¬¡ dropoutï¼Œè¿™ä¸ªè¿‡ç¨‹ç”¨æ¥å¸®åŠ©æ¨¡å‹æ•æ‰æ›´ä¸°å¯Œçš„éçº¿æ€§ç‰¹å¾ï¼Œæå‡è¡¨è¾¾èƒ½åŠ›ã€‚</p>
<pre><code class="language-python">class MLP(nn.Module):
    def __init__(self, in_features, hidden_features, out_features, act_layer, drop_rate):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        drop_probs = (drop_rate, drop_rate)

        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.drop1 = nn.Dropout(drop_probs[0])
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop2 = nn.Dropout(drop_probs[1])

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop1(x)
        x = self.fc2(x)
        x = self.drop2(x)
        return x
</code></pre>
<h3 id="å››æ¨¡å—4encoderå±‚å †å ">ï¼ˆå››ï¼‰æ¨¡å—4ï¼šEncoderå±‚å †å </h3>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ç”±äºPyTorchä¸­æ²¡æœ‰ç°æˆçš„<code>DropPath</code>å‡½æ•°å¯ä»¥ä½¿ç”¨ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦è‡ªå·±å®ç°è¿™ä¸€ç”¨æ³•ï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨DropPathæ¥éšæœºä¸¢å¼ƒå®Œæ•´è·¯å¾„å®ç°æ·±åº¦ç½‘ç»œçš„æ­£åˆ™åŒ–ï¼Œå¹¶åœ¨è®­ç»ƒæ—¶ä»¥æ¦‚ç‡drop_pathè·³è¿‡å½“å‰æ¨¡å—å¹¶ç¼©æ”¾å‰©ä½™è·¯å¾„ä»¥ä¿æŒæœŸæœ›å€¼ï¼›åœ¨Blockç±»ä¸­æˆ‘åˆ™å°è£…äº†å®Œæ•´çš„Transformerå±‚ç»“æ„ï¼ŒåŒ…å«LayerNormå½’ä¸€åŒ–ã€å¤šå¤´æ³¨æ„åŠ›ã€MLPå‰é¦ˆç½‘ç»œå’Œæ®‹å·®è¿æ¥ï¼Œå…¶ä¸­æ³¨æ„åŠ›éƒ¨åˆ†ä½¿ç”¨æˆ‘è‡ªå®šä¹‰çš„SelfAttentionæ¨¡å—ï¼ŒMLPé‡‡ç”¨å…ˆæ‰©å±•åå‹ç¼©çš„ç»“æ„è®¾è®¡ï¼Œä¸¤è€…éƒ½é›†æˆäº†DropPathæœºåˆ¶</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;äºæ˜¯ä¸€ä¸ªå®Œæ•´çš„Transformer Blockè®¡ç®—æµç¨‹å¦‚ä¸‹ï¼š</p>
<div class="mermaid">graph TD
    A[è¾“å…¥ x]
    A --&gt; B[LayerNorm]
    B --&gt; C[Multi-Head Self-Attention]
    C --&gt; D["Residual Add: x + Attention"]
    D --&gt; E[LayerNorm]
    E --&gt; F[FeedForward MLP]
    F --&gt; G["Residual Add: D + MLP"]
    G --&gt; H[è¾“å‡º y]
</div><pre><code class="language-python">class DropPath(nn.Module):
    def __init__(self, drop_prob=None):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob

    def drop_path(self, x, drop_prob, training):
        if drop_prob == 0. or not training:
            return x
        keep_prob       = 1 - drop_prob
        shape           = (x.shape[0],) + (1,) * (x.ndim - 1)
        random_tensor   = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
        random_tensor.floor_() 
        output          = x.div(keep_prob) * random_tensor
        return output

    def forward(self, x):
        return self.drop_path(x, self.drop_prob, self.training)
    
class Block(nn.Module):
    def __init__(self, dim, num_heads, mlp_radio, qkv_bias, drop, attn_drop, drop_path, act_layer, norm_layer):
        super().__init__()
        self.norm_1 = norm_layer(dim)
        self.attn = SelfAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop_rate=attn_drop, proj_drop_rate=drop)
        self.norm_2 = norm_layer(dim)
        self.mlp = MLP(in_features=dim, hidden_features=int(dim * mlp_radio), out_features=None, act_layer=act_layer, drop_rate=drop_path)
        self.drop_path = DropPath(drop_path) if drop_path &gt; 0.0 else nn.Identity() # ä¸¢å¼ƒè·¯å¾„

    def forward(self, x):
        x = x + self.drop_path(self.attn(self.norm_1(x)))
        x = x + self.drop_path(self.mlp(self.norm_2(x)))
        return x
</code></pre>
<h3 id="äº”æ¨¡å—5vitæ•´ä½“æ¨¡å‹ç±»">ï¼ˆäº”ï¼‰æ¨¡å—5ï¼šViTæ•´ä½“æ¨¡å‹ç±»</h3>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;æœ€åæˆ‘ä»¬æ¥å®ç°æˆ‘ä»¬å®Œæ•´çš„ViTâ€”VisonTransformerï¼Œåœ¨è¿™ä¸ª <code>VisonTransformer</code> ç±»ä¸­ï¼Œæˆ‘å°†å‰é¢ä»‹ç»çš„å„ä¸ªæ¨¡å—æ•´åˆåœ¨ä¸€èµ·ï¼Œå®ç°äº†å®Œæ•´çš„ViTç½‘ç»œã€‚é¦–å…ˆï¼Œæˆ‘ç”¨å·ç§¯å°†è¾“å…¥å›¾åƒåˆ‡åˆ†æˆå›ºå®šå¤§å°çš„Patchï¼Œå¹¶æ˜ å°„åˆ°ç‰¹å¾ç©ºé—´ï¼›æ¥ç€é€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„åˆ†ç±»tokenå’Œä½ç½®ç¼–ç ï¼Œä¸ºæ¨¡å‹æä¾›ä½ç½®ä¿¡æ¯å¼¥è¡¥å·ç§¯â€œæ„Ÿå—é‡â€ç¼ºå¤±çš„é—®é¢˜ã€‚ä¹‹åï¼Œæˆ‘å †å å¤šä¸ªTransformerç¼–ç å™¨Blockï¼Œæ¯ä¸ªBlockåŒ…å«å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶å’ŒMLPæ¨¡å—ï¼Œé€šè¿‡æ®‹å·®è¿æ¥å’Œå½’ä¸€åŒ–ä¿è¯ä¿¡æ¯çš„æœ‰æ•ˆä¼ é€’ä¸ç‰¹å¾æŠ½è±¡ã€‚æœ€åï¼Œæˆ‘å–åˆ†ç±»tokençš„è¾“å‡ºï¼Œé€šè¿‡çº¿æ€§å±‚æ˜ å°„åˆ°ç›®æ ‡ç±»åˆ«ï¼Œå®ç°å›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚</p>
<pre><code class="language-python">class VisonTransformer(nn.Module):
    def __init__(self, input_shape, patch_size, in_channels, num_classes, num_features, depth,
                 num_heads, mlp_ratio, qkv_bias, drop_rate, attn_drop_rate, drop_path_rate,
                 norm_layer, act_layer):
        super().__init__()
        self.input_shape = input_shape # è¾“å…¥çš„ç»´åº¦
        self.patch_size = patch_size # Patch çš„å¤§å°
        self.in_channels = in_channels # è¾“å…¥çš„ç»´åº¦
        self.num_classes = num_classes # è¾“å‡ºç±»åˆ«æ•°
        self.num_features = num_features # ç‰¹å¾ç»´åº¦
        self.depth = depth # Transformerç¼–ç å™¨å±‚æ•°
        self.num_heads = num_heads # Transformeræ³¨æ„åŠ›å¤´æ•°
        self.mlp_ratio = mlp_ratio # MLP æ¯”ä¾‹ MLP:å¤šå±‚æ„ŸçŸ¥æœº,ç´§éš Self-Attention ä¹‹åï¼Œç”¨äºéçº¿æ€§å˜æ¢ï¼šå¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼›ç‰¹å¾æ˜ å°„ï¼šå°† Self-Attention æå–çš„ç‰¹å¾è¿›ä¸€æ­¥è½¬æ¢ã€‚ 
        self.qkv_bias = qkv_bias # æ˜¯å¦ä½¿ç”¨åç½®
        self.drop_rate = drop_rate # ä¸¢å¼ƒç‡
        self.attn_drop_rate = attn_drop_rate # æ³¨æ„åŠ›ä¸¢å¼ƒç‡
        self.drop_path_rate = drop_path_rate # ä¸¢å¼ƒè·¯å¾„ç‡
        self.norm_layer = norm_layer # å½’ä¸€åŒ–å±‚
        self.act_layer = act_layer # æ¿€æ´»å‡½æ•°å±‚

        self.features_shape = [input_shape[1] // patch_size, input_shape[2] // patch_size]  # [14, 14]
        self.num_patches = self.features_shape[0] * self.features_shape[1]
        self.patch_embed = VisionPatchEmbedding(input_shape, patch_size, in_channels, num_features) # å°†è¾“å…¥å›¾ç‰‡åˆ†å‰²æˆpatchï¼Œå¹¶è¿›è¡Œçº¿æ€§æ˜ å°„

        # ViT ä¸æ˜¯ CNNï¼Œæ²¡æœ‰"æ„Ÿå—é‡"ï¼Œæ‰€ä»¥å¼•å…¥äº†ä½ç½®ç¼–ç ï¼Œæ¥ä¸ºæ¯ä¸ª patch åŠ ä¸Šä½ç½®ä¿¡æ¯ï¼›
        self.pretrained_features_shape = [224 // patch_size, 224 // patch_size] # é¢„è®­ç»ƒçš„ç‰¹å¾å›¾å°ºå¯¸

        self.cls_token = nn.Parameter(torch.zeros(1, 1, num_features)) # åˆ†ç±» token 196, 768 -&gt; 197, 768
        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, num_features)) # ä½ç½®ç¼–ç  197, 768 -&gt; 197, 768

        self.pos_drop = nn.Dropout(drop_rate) # ä¸¢å¼ƒç‡
        self.norm = norm_layer(self.num_features) # å½’ä¸€åŒ–

        self.dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)] # ä¸¢å¼ƒè·¯å¾„ç‡
        self.blocks = nn.Sequential(
            *[
                Block(
                    dim = num_features,
                    num_heads = num_heads,
                    mlp_radio = mlp_ratio,
                    qkv_bias = qkv_bias,
                    drop = drop_rate,
                    attn_drop = attn_drop_rate,
                    drop_path = self.dpr[i],
                    norm_layer = norm_layer,
                    act_layer = act_layer 
                )for i in range(depth)
            ]
        )
        self.head = nn.Linear(num_features, num_classes) if num_classes &gt; 0 else nn.Identity()

    def forward_features(self,x):
        x = self.patch_embed(x)
        cls_token = self.cls_token.expand(x.shape[0], -1, -1) # å°†åˆ†ç±» token æ‰©å±•åˆ°ä¸è¾“å…¥ç‰¹å¾å›¾ç›¸åŒçš„å½¢çŠ¶
        x = torch.cat((cls_token, x), dim=1) # å°†åˆ†ç±» token ä¸è¾“å…¥ç‰¹å¾å›¾æ‹¼æ¥

        cls_token_pos_embed = self.pos_embed[:, 0:1, :] # åˆ†ç±» token çš„ä½ç½®ç¼–ç 
        img_token_pos_embed = self.pos_embed[:, 1:, :]  # [1, num_patches, num_features]
        # å˜æˆ[1, H, W, C]
        img_token_pos_embed = img_token_pos_embed.view(1, self.features_shape[0], self.features_shape[1], -1).permute(0, 3, 1, 2)  # [1, C, H, W]
        # æ’å€¼
        img_token_pos_embed = F.interpolate(
            img_token_pos_embed,
            size=self.features_shape,  # [H, W]
            mode='bicubic',
            align_corners=False
        )
        # å˜å›[1, num_patches, C]
        img_token_pos_embed = img_token_pos_embed.permute(0, 2, 3, 1).reshape(1, -1, img_token_pos_embed.shape[1])

        pos_embed = torch.cat((cls_token_pos_embed, img_token_pos_embed), dim=1) # å°†åˆ†ç±» token çš„ä½ç½®ç¼–ç ä¸å›¾åƒ token çš„ä½ç½®ç¼–ç æ‹¼æ¥
        
        x = self.pos_drop(x + pos_embed) # å°†ä½ç½®ç¼–ç ä¸è¾“å…¥ç‰¹å¾å›¾ç›¸åŠ 

        x = self.blocks(x)
        x = self.norm(x)

        return x[:, 0] # è¿”å›åˆ†ç±» token çš„ç‰¹å¾
    
    def forward(self, x):
        x = self.forward_features(x)
        x = self.head(x)
        return x
</code></pre>
<p><font color="red"><strong>è‡³æ­¤æˆ‘ä»¬å®Œæ•´çš„ViTä¾¿æ­å»ºå®Œæˆäº†ï¼</strong></font></p>
<h3 id="å…­å®ç°æ•°æ®åŠ è½½ä»£ç æ•°æ®åŠ è½½lossä¼˜åŒ–å™¨">ï¼ˆå…­ï¼‰å®ç°æ•°æ®åŠ è½½ä»£ç ï¼ˆæ•°æ®åŠ è½½ã€lossã€ä¼˜åŒ–å™¨ï¼‰</h3>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;æ•°æ®é›†åŠ è½½éƒ¨åˆ†æ¯”è¾ƒç®€ç­”ï¼Œä¸è¿‡å¤šèµ˜è¿°ï¼Œæˆ‘çš„æ•°æ®é›†ç»“æ„åŠå…·ä½“ä»£ç å¦‚ä¸‹ï¼š</p>
<p><img src="https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250614015615748-1709429269.png" alt="image-20250614015610919" loading="lazy"></p>
<pre><code class="language-python">import os 
from torch.utils.data import Dataset, DataLoader 
from PIL import Image 
import torchvision.transforms as transforms 

class ViTDataset(Dataset):
    def __init__(self, root, split, transform=None, target_transform=None, img_size=224):
        super().__init__()
        self.split = split 
        self.img_size = img_size  # å›¾åƒå¤§å°
        self.transform = transform if transform is not None else transforms.ToTensor()
        self.target_transform = target_transform  # æ ‡ç­¾å˜æ¢
        # æ„å»ºæ•°æ®é›†æ ¹ç›®å½•
        self.data_dir = os.path.join(root, split)  # è®­ç»ƒé›†æˆ–æµ‹è¯•é›†ç›®å½•
        # è·å–æ‰€æœ‰ç±»åˆ«
        self.classes = sorted(os.listdir(self.data_dir))
        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}
        # æ”¶é›†æ‰€æœ‰å›¾åƒæ–‡ä»¶è·¯å¾„å’Œå¯¹åº”çš„æ ‡ç­¾
        self.images = []
        self.labels = []
        for class_name in self.classes:
            class_dir = os.path.join(self.data_dir, class_name)
            if not os.path.isdir(class_dir):
                continue
            for img_name in os.listdir(class_dir):
                if img_name.endswith(('.jpg', '.jpeg', '.png')):
                    img_path = os.path.join(class_dir, img_name)
                    self.images.append(img_path)
                    self.labels.append(self.class_to_idx[class_name])
        print(f"åŠ è½½äº† {len(self.images)} å¼ å›¾åƒç”¨äº{split}é›†ï¼Œå…±{len(self.classes)}ä¸ªç±»åˆ«")

    def __len__(self):
        return len(self.images)
    
    def __getitem__(self, index):
        # è·å–å›¾åƒè·¯å¾„å’Œæ ‡ç­¾
        img_path = self.images[index]
        label = self.labels[index]
        # åŠ è½½å›¾åƒ
        image = Image.open(img_path).convert('RGB')
        # è°ƒæ•´å›¾åƒå¤§å°
        image = image.resize((self.img_size, self.img_size), Image.Resampling.BILINEAR)
        # åº”ç”¨å˜æ¢
        image = self.transform(image)
        if self.target_transform is not None:
            label = self.target_transform(label)
            
        return image, label
    

def ViTDataLoad(root, batch_size, num_workers, img_size):
    # åˆ›å»ºè®­ç»ƒæ•°æ®é›†
    train_dataset = ViTDataset(
        root=root,
        split='train',  # ä½¿ç”¨è®­ç»ƒé›†åˆ’åˆ†
        img_size=img_size
    )
    
    # åˆ›å»ºéªŒè¯æ•°æ®é›†
    val_dataset = ViTDataset(
        root=root,
        split='val',  # ä½¿ç”¨éªŒè¯é›†åˆ’åˆ†
        img_size=img_size
    )
    
    # åˆ›å»ºè®­ç»ƒæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,  # éšæœºæ‰“ä¹±æ•°æ®
        num_workers=num_workers,  # å¤šçº¿ç¨‹åŠ è½½
        pin_memory=True,  # æ•°æ®é¢„åŠ è½½åˆ°å›ºå®šå†…å­˜ï¼ŒåŠ é€ŸGPUä¼ è¾“
        drop_last=True  # ä¸¢å¼ƒæœ€åä¸è¶³ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®
    )
    
    # åˆ›å»ºéªŒè¯æ•°æ®åŠ è½½å™¨
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,  # ä¸æ‰“ä¹±æ•°æ®
        num_workers=num_workers,
        pin_memory=True
    )
    
    return train_loader, val_loader
</code></pre>
<h3 id="ä¸ƒå®ç°è®­ç»ƒä»£ç ">ï¼ˆä¸ƒï¼‰å®ç°è®­ç»ƒä»£ç </h3>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;æ¥ä¸‹æ¥æˆ‘ä»¬æ¥å®Œæˆæˆ‘ä»¬çš„è®­ç»ƒä»£ç ï¼Œæˆ‘é€šè¿‡ä¸Šä¸€èŠ‚å®šä¹‰çš„<code>ViTDataLoad</code>æ¥åŠ è½½è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ï¼Œè®­ç»ƒä¸­æˆ‘é‡‡ç”¨å¸¸ç”¨çš„äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼ˆ<code>CrossEntropyLoss</code>ï¼‰æ¥è¡¡é‡åˆ†ç±»æ•ˆæœï¼Œä¼˜åŒ–å™¨ä½¿ç”¨äº†æ›´é€‚åˆTransformerçš„<code>AdamW</code>ï¼Œå…·ä½“å®ç°å¦‚ä¸‹ï¼Œä¸è¿‡å¤šèµ˜è¿°ï¼š</p>
<pre><code class="language-python">from model.transformer_net import VisonTransformer
from dataset_load import ViTDataLoad
import torch 
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
import matplotlib.pyplot as plt

def train(
    root="/home/xq/Working/dockertrain_test/input/timmdataset/african-wildlife", # æ•°æ®é›†æ ¹ç›®å½•
    img_size=224,
    patch_size=16,
    in_channels=3,
    num_features=768,
    depth=12,
    num_heads=12,
    mlp_ratio=4.0,
    qkv_bias=True,
    drop_rate=0.1,
    attn_drop_rate=0.1,
    drop_path_rate=0.1,
    epochs=50,
    batch_size=4,
    num_workers=4,
    lr=1e-4,
    device=None
):
    device = device or ("cuda" if torch.cuda.is_available() else "cpu")
    # æ•°æ®åŠ è½½
    train_loader, val_loader = ViTDataLoad(root, batch_size, num_workers, img_size)
    num_classes = len(train_loader.dataset.classes)
    input_shape = (in_channels, img_size, img_size)

    # æ¨¡å‹
    model = VisonTransformer(
        input_shape=input_shape,
        patch_size=patch_size,
        in_channels=in_channels,
        num_classes=num_classes,
        num_features=num_features,
        depth=depth,
        num_heads=num_heads,
        mlp_ratio=mlp_ratio,
        qkv_bias=qkv_bias,
        drop_rate=drop_rate,
        attn_drop_rate=attn_drop_rate,
        drop_path_rate=drop_path_rate,
        norm_layer=nn.LayerNorm,
        act_layer=nn.GELU
    ).to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=lr)
    best_acc = 0
    train_loss_list, val_loss_list = [], []
    train_acc_list, val_acc_list = [], []

    for epoch in range(epochs):
        model.train()
        total_loss, correct, total = 0, 0, 0
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}")
        for images, labels in pbar:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            total_loss += loss.item() * images.size(0)
            _, preds = outputs.max(1)
            correct += preds.eq(labels).sum().item()
            total += labels.size(0)
        train_loss = total_loss / total
        train_acc = correct / total
        train_loss_list.append(train_loss)
        train_acc_list.append(train_acc)

        # éªŒè¯
        model.eval()
        val_loss, val_correct, val_total = 0, 0, 0
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)
                val_loss += loss.item() * images.size(0)
                _, preds = outputs.max(1)
                val_correct += preds.eq(labels).sum().item()
                val_total += labels.size(0)
        val_loss = val_loss / val_total
        val_acc = val_correct / val_total
        val_loss_list.append(val_loss)
        val_acc_list.append(val_acc)

        print(f"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}")
        # ä¿å­˜æœ€ä¼˜æ¨¡å‹
        if val_acc &gt; best_acc:
            best_acc = val_acc
            torch.save(model.state_dict(), "best_vit.pth")

    # å¯è§†åŒ–losså’Œacc
    plt.figure()
    plt.plot(train_loss_list, label="Train Loss")
    plt.plot(val_loss_list, label="Val Loss")
    plt.legend()
    plt.title("Loss Curve")
    plt.savefig("loss_curve.png")
    plt.figure()
    plt.plot(train_acc_list, label="Train Acc")
    plt.plot(val_acc_list, label="Val Acc")
    plt.legend()
    plt.title("Accuracy Curve")
    plt.savefig("acc_curve.png")
    print("è®­ç»ƒå®Œæˆï¼Œæœ€ä¼˜éªŒè¯å‡†ç¡®ç‡ï¼š", best_acc)

if __name__ == "__main__":
    train()
</code></pre>
<h3 id="å…«å®ç°éªŒè¯ä»£ç ">ï¼ˆå…«ï¼‰å®ç°éªŒè¯ä»£ç </h3>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;éªŒè¯ä»£ç ä¹Ÿæ¯”è¾ƒç®€å•ï¼Œæœ‰PyTorchåŠæ·±åº¦å­¦ä¹ åŸºç¡€çš„åŒå­¦å¯ä»¥å¾ˆå¿«å®ç°ï¼Œæ•…è¿™é‡Œä¹Ÿä¸å†èµ˜è¿°ï¼š</p>
<pre><code class="language-python">import torch
from model.transformer_net import VisonTransformer
import torchvision.transforms as transforms
from PIL import Image
import sys
import os

img_size = 224
patch_size = 16
in_channels = 3
num_features = 768
depth = 12
num_heads = 12
mlp_ratio = 4.0
qkv_bias = True
drop_rate = 0.1
attn_drop_rate = 0.1
drop_path_rate = 0.1

classes = ['cat', 'dog']  
num_classes = len(classes)
input_shape = (in_channels, img_size, img_size)

def load_model(device):
    model = VisonTransformer(
        input_shape=input_shape,
        patch_size=patch_size,
        in_channels=in_channels,
        num_classes=num_classes,
        num_features=num_features,
        depth=depth,
        num_heads=num_heads,
        mlp_ratio=mlp_ratio,
        qkv_bias=qkv_bias,
        drop_rate=drop_rate,
        attn_drop_rate=attn_drop_rate,
        drop_path_rate=drop_path_rate,
        norm_layer=torch.nn.LayerNorm,
        act_layer=torch.nn.GELU
    ).to(device)
    model.load_state_dict(torch.load("best_vit.pth", map_location=device))
    model.eval()
    return model

def predict(img_path, model, device):
    transform = transforms.Compose([
        transforms.Resize((img_size, img_size)),
        transforms.ToTensor(),
    ])
    img = Image.open(img_path).convert('RGB')
    img = transform(img).unsqueeze(0).to(device)
    with torch.no_grad():
        output = model(img)
        pred = output.argmax(dim=1).item()
    return classes[pred]

if __name__ == "__main__":
    img_path = sys.argv[1]
    if not os.path.exists(img_path):
        print(f"å›¾ç‰‡ä¸å­˜åœ¨: {img_path}") 
        sys.exit(1)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = load_model(device)
    pred_class = predict(img_path, model, device)
    print(f"å›¾ç‰‡ {img_path} çš„é¢„æµ‹ç±»åˆ«ä¸º: {pred_class}") 
</code></pre>
<h2 id="ä¸‰vitåœ¨è‡ªå®šä¹‰æ•°æ®é›†åŠcifar-10è¿›è¡Œè®­ç»ƒä¸æµ‹è¯•">ä¸‰ã€ViTï¼šåœ¨è‡ªå®šä¹‰æ•°æ®é›†åŠCIFAR-10è¿›è¡Œè®­ç»ƒä¸æµ‹è¯•</h2>
<h3 id="ä¸€è‡ªå®šä¹‰æ•°æ®é›†">ï¼ˆä¸€ï¼‰è‡ªå®šä¹‰æ•°æ®é›†</h3>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;æˆ‘ä»¬é¦–å…ˆåœ¨æˆ‘ä»¬è‡ªå®šä¹‰æ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œè¯·ä¿è¯æ•°æ®é›†æ ¼å¼ä¸ºï¼š</p>
<p><img src="https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250614015615748-1709429269.png" alt="image-20250614015610919" loading="lazy"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;æ¥ç€åœ¨è®­ç»ƒä»£ç ä¸­ä¿®æ”¹æ•°æ®é›†è·¯å¾„å¹¶è¿è¡Œä¸‹è¿°å‘½ä»¤å³å¯å¼€å§‹è®­ç»ƒï¼š</p>
<p><img src="https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250614020707019-134992667.png" alt="image-20250614020702703" loading="lazy"></p>
<pre><code class="language-bash">python3 train.py
</code></pre>
<p><img src="https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250614021703517-1895368011.png" alt="image-20250614021641186" loading="lazy"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;è®­ç»ƒå®Œæˆåæ–‡ä»¶å†…ä¼šæœ‰ä¸€ä¸ªbest_vit.pthä»¥åŠä¸¤ä¸ªè®­ç»ƒçš„AccåŠLosså›¾ç”¨äºåˆ†æ</p>
<img src="https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250614021736970-201144847.png" alt="image-20250614021732228" style="zoom: 50%">
<img src="https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250614021746883-2043602797.png" alt="image-20250614021741707" style="zoom: 50%">
<p>æ¥ä¸‹æ¥æˆ‘ä»¬è¿è¡Œ<code>python3 predict.py [img_path]</code>å³å¯æ‰§è¡Œæ¨ç†å•¦ï¼</p>
<p><img src="https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250614022159568-687739081.png" alt="image-20250614022155334" loading="lazy"></p>
<h3 id="äºŒcifar-10æ•°æ®é›†">ï¼ˆäºŒï¼‰CIFAR-10æ•°æ®é›†</h3>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;åœ¨å®Œæˆä¸Šè¿°è‡ªå®šä¹‰æ•°æ®é›†ä¹‹åæˆ‘ä»¬ä¾¿å¯ä»¥ç»§ç»­å°è¯•CIFAR-10å•¦ï¼æˆ‘ä»¬é¦–å…ˆæ¥ä¸‹è½½æˆ‘ä»¬çš„CIFAR-10æ•°æ®é›†ï¼ŒCIFAR-10æ•°æ®é›†å·²ç»é›†æˆè¿›äº†Torchï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ä½¿ç”¨PyTorchæ¥å£ç›´æ¥ä¸‹è½½ï¼Œå…·ä½“ä¸‹è½½æ–¹å¼å¦‚ä¸‹ï¼Œä¸è¿‡å¤šèµ˜è¿°ï¼š</p>
<pre><code class="language-bash"># CIFAR-10å…¨é‡
import torchvision.datasets as datasets
train_dataset = datasets.CIFAR10(root='./data', train=True, download=True)
test_dataset = datasets.CIFAR10(root='./data', train=False, download=True)
</code></pre>
<p><img src="https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250614020608805-1211984873.png" alt="image-20250614020604397" loading="lazy"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ç”±äºæˆ‘ä»¬ä¸‹è½½ä¸‹æ¥çš„CIFAR-10æ•°æ®é›†æ ¼å¼å¦‚ä¸‹ï¼ˆdata_batch_1 ~ data_batch_5ï¼šè®­ç»ƒæ•°æ®ï¼ˆæ¯ä¸ª10,000å¼ å›¾åƒï¼‰ï¼Œtest_batchï¼šæµ‹è¯•æ•°æ®ï¼ˆ10,000å¼ å›¾åƒï¼‰ï¼Œbatches.metaï¼šå…ƒæ•°æ®æ–‡ä»¶ï¼ˆåŒ…å«ç±»åˆ«åç§°ç­‰ä¿¡æ¯ï¼‰ï¼‰ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦å¯¹æˆ‘ä»¬çš„<code>dataset_load.py</code>ä»£ç è¿›è¡Œä¿®æ”¹ä»¥é€‚é…æˆ‘ä»¬çš„CIFAR-10æ•°æ®é›†</p>
<p><img src="https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250614021352130-1121553942.png" alt="image-20250614021347738" loading="lazy"></p>
<pre><code class="language-python">import os
import pickle
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import torchvision.transforms as transforms

class CIFAR10Dataset(Dataset):
    def __init__(self, root, train=True, transform=None, target_transform=None):
        super().__init__()
        self.root = root
        self.train = train
        self.transform = transform
        self.target_transform = target_transform
        
        # CIFAR-10ç±»åˆ«åç§°
        self.classes = [
            'airplane', 'automobile', 'bird', 'cat', 'deer',
            'dog', 'frog', 'horse', 'ship', 'truck'
        ]
        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}
        
        self.data = []
        self.targets = []
        
        if self.train:
            for i in range(1, 6):
                batch_file = os.path.join(root, f'data_batch_{i}')
                with open(batch_file, 'rb') as f:
                    batch_data = pickle.load(f, encoding='bytes')
                    self.data.append(batch_data[b'data'])
                    self.targets.extend(batch_data[b'labels'])
            self.data = np.vstack(self.data)
        else:
            test_file = os.path.join(root, 'test_batch')
            with open(test_file, 'rb') as f:
                test_data = pickle.load(f, encoding='bytes')
                self.data = test_data[b'data']
                self.targets = test_data[b'labels']
        
        # å°†æ•°æ®reshapeä¸ºå›¾åƒæ ¼å¼ (N, 32, 32, 3)
        self.data = self.data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)
        
        print(f"åŠ è½½äº† {len(self.data)} å¼ CIFAR-10å›¾åƒç”¨äº{'è®­ç»ƒ' if train else 'æµ‹è¯•'}ï¼Œå…±{len(self.classes)}ä¸ªç±»åˆ«")

    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, index):
        img = self.data[index]
        target = self.targets[index]
        img = Image.fromarray(img)
        if self.transform is not None:
            img = self.transform(img)
        
        if self.target_transform is not None:
            target = self.target_transform(target)
            
        return img, target

def CIFAR10DataLoad(root, batch_size, num_workers=4, img_size=224):
    train_transform = transforms.Compose([
        transforms.Resize((img_size, img_size)),  
        transforms.RandomHorizontalFlip(p=0.5), 
        transforms.RandomRotation(10), 
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) 
    ])
    
    test_transform = transforms.Compose([
        transforms.Resize((img_size, img_size)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = CIFAR10Dataset(
        root=root,
        train=True,
        transform=train_transform
    )
    
    test_dataset = CIFAR10Dataset(
        root=root,
        train=False,
        transform=test_transform
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=True
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )
    
    return train_loader, test_loader


if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®åŠ è½½å™¨
    root = "/home/xq/Temp/cifar-10-batches-py"
    train_loader, test_loader = CIFAR10DataLoad(root, batch_size=32)
    # æµ‹è¯•ä¸€ä¸ªbatch
    for images, labels in train_loader:
        print(f"å›¾åƒbatchå½¢çŠ¶: {images.shape}")
        print(f"æ ‡ç­¾batchå½¢çŠ¶: {labels.shape}")
        print(f"æ ‡ç­¾èŒƒå›´: {labels.min()} - {labels.max()}")
        break 
</code></pre>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;æ¥ç€ä¿®æ”¹æˆ‘ä»¬è®­ç»ƒä»£ç ï¼Œä¸»è¦ä¿®æ”¹æ•°æ®é›†è·¯å¾„ã€æ•°æ®åŠ è½½å™¨çš„è°ƒç”¨å³å¯ï¼š</p>
<p><img src="https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250614022644488-1443084450.png" alt="image-20250614022640209" loading="lazy"></p>
<p><img src="https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250614022710649-285381462.png" alt="image-20250614022706545" loading="lazy"></p>
<pre><code class="language-bash">python3 train.py
</code></pre>
<p><img src="https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250614022758350-1870577983.png" alt="image-20250614022753939" loading="lazy"></p>
<p><img src="https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250614022821932-833062892.png" alt="image-20250614022817463" loading="lazy"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>ç”±äºCIFAR-10æ•°æ®é›†æ¯”è¾ƒå¤§ï¼Œè®­ç»ƒé€Ÿåº¦è¾ƒæ…¢ï¼Œæˆ‘ä»¬è€å¿ƒç­‰å¾…å³å¯</strong>ï¼Œè®­ç»ƒå®Œæˆåè¿è¡Œå¦‚ä¸‹å‘½ä»¤å³å¯æ‰§è¡Œæ¨ç†ï¼š</p>
<pre><code class="language-bash"># å•ä¸ªé¢„æµ‹
python3 predict_cifar10.py &lt;å›¾ç‰‡è·¯å¾„&gt;
# Top-Ké¢„æµ‹
python3 predict_cifar10.py &lt;å›¾ç‰‡è·¯å¾„&gt; --top-k 3
</code></pre>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.39652777777777776" data-date-updated="2025-06-14 12:19">2025-06-14 02:48</span>&nbsp;
<a href="https://www.cnblogs.com/SkyXZ">SkyXZ</a>&nbsp;
é˜…è¯»(<span id="post_view_count">64</span>)&nbsp;
è¯„è®º(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18927874);return false;">æ”¶è—</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18927874', targetLink: 'https://www.cnblogs.com/SkyXZ/p/18927874', title: 'æ‰‹æŠŠæ‰‹æ•™ä½ å®ç°PyTorchç‰ˆViTï¼šå›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­çš„Transformerå®æˆ˜' })">ä¸¾æŠ¥</a>
</div>
        