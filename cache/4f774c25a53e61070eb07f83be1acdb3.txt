
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/itzgr/p/18817166" title="发布于 2025-04-09 19:02">
    <span role="heading" aria-level="2">附043.KubeEdge边缘云部署实施方案</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p></p><div class="toc"><div class="toc-container-header">目录</div><ul><li><a href="#kubeedge介绍" rel="noopener nofollow">KubeEdge介绍</a><ul><li><a href="#kubeedge概述" rel="noopener nofollow">KubeEdge概述</a></li><li><a href="#kubeedge优势" rel="noopener nofollow">KubeEdge优势</a></li><li><a href="#kubeedge架构" rel="noopener nofollow">KubeEdge架构</a></li></ul></li><li><a href="#kubeedge部署" rel="noopener nofollow">KubeEdge部署</a><ul><li><a href="#部署依赖" rel="noopener nofollow">部署依赖</a></li><li><a href="#部署规划" rel="noopener nofollow">部署规划</a></li><li><a href="#主机名配置" rel="noopener nofollow">主机名配置</a></li><li><a href="#变量准备" rel="noopener nofollow">变量准备</a></li><li><a href="#互信配置" rel="noopener nofollow">互信配置</a></li><li><a href="#环境预配置" rel="noopener nofollow">环境预配置</a></li><li><a href="#安装keadm" rel="noopener nofollow">安装keadm</a></li><li><a href="#设置标签" rel="noopener nofollow">设置标签</a></li><li><a href="#安装cni" rel="noopener nofollow">安装CNI</a></li><li><a href="#设置云端" rel="noopener nofollow">设置云端</a></li><li><a href="#设置调度" rel="noopener nofollow">设置调度</a></li><li><a href="#设置边缘端" rel="noopener nofollow">设置边缘端</a></li><li><a href="#边缘端优化" rel="noopener nofollow">边缘端优化</a></li><li><a href="#确认验证" rel="noopener nofollow">确认验证</a></li></ul></li><li><a href="#kubeedge测试验证" rel="noopener nofollow">KubeEdge测试验证</a><ul><li><a href="#调度测试" rel="noopener nofollow">调度测试</a></li></ul></li></ul></div><p></p>
<h2 id="kubeedge介绍">KubeEdge介绍</h2>
<h3 id="kubeedge概述">KubeEdge概述</h3>
<p>KubeEdge 是一个开源系统，将原生的容器化的业务流程和设备管理功能扩展到边缘节点。KubeEdge 是基于 Kubernetes 构建的，并为云，边缘之间的网络通信，应用程序部署以及元数据同步提供核心基础架构支持。同时 KubeEdge 还支持 MQTT，并允许开发人员编写自定义逻辑并在 Edge 上启用一定资源的设备进行通信。</p>
<p>KubeEdge 由云端和边缘端组成。</p>
<h3 id="kubeedge优势">KubeEdge优势</h3>
<p>KubeEdge 的优势主要包括：</p>
<ul>
<li>边缘计算</li>
</ul>
<p>借助在 Edge 上运行业务应用，可以在产生数据的边端存储和处理大量数据。这样可以减少边缘和云之间的网络带宽需求和消耗，提高响应速度，降低成本并保护客户的数据隐私。</p>
<ul>
<li>简化开发</li>
</ul>
<p>开发人员可以编写基于 HTTP 或 MQTT 的常规应用程序，对其进行容器化，然后在 Edge 或 Cloud 中的任何一个更合适的位置运行应用程序。</p>
<ul>
<li>Kubernetes 原生支持</li>
</ul>
<p>借助 KubeEdge，用户可以像在传统的 Kubernetes 集群一样，在 Edge 节点上编排应用程序，管理设备并监视应用程序和设备状态。</p>
<ul>
<li>丰富的应用</li>
</ul>
<p>可以轻松地将现有的复杂机器学习，图像识别，事件处理等其他高级应用程序部署到 Edge。</p>
<ul>
<li>
<p>云边可靠协作<br>
在不稳定的云边网络上，可以保证消息传递的可靠性，不会丢失。</p>
</li>
<li>
<p>边缘自治<br>
当云边之间的网络不稳定或者边缘端离线或重启时，确保边缘节点可以自主运行，同时确保边缘端的应用正常运行。</p>
</li>
<li>
<p>边缘设备管理<br>
通过 Kubernetes 的原生API，并由CRD来管理边缘设备。</p>
</li>
<li>
<p>极致轻量的边缘代理<br>
在资源有限的边缘端上运行的非常轻量级的边缘代理(EdgeCore)。</p>
</li>
</ul>
<h3 id="kubeedge架构">KubeEdge架构</h3>
<p>KubeEdge 由以下组件组成：</p>
<p>云上（CloudCore）部分：<br>
CloudHub: Web 套接字服务器，负责在云端缓存信息、监视变更，并向 EdgeHub 端发送消息。<br>
EdgeController: kubernetes 的扩展控制器，用于管理边缘节点和 pod 的元数据，以便可以将数据定位到对应的边缘节点。<br>
DeviceController: 一个扩展的 kubernetes 控制器，用于管理边缘 IoT 设备，以便设备元数据/状态数据可以在 edge 和 cloud 之间同步。</p>
<p>边缘（EdgeCore）部分：<br>
EdgeHub: Web 套接字客户端，负责与 Cloud Service 进行交互以进行边缘计算（例如 KubeEdge 体系结构中的 Edge Controller）。这包括将云侧资源更新同步到边缘，并将边缘侧主机和设备状态变更报告给云。<br>
Edged: 在边缘节点上运行并管理容器化应用程序的代理。<br>
EventBus: 一个与 MQTT 服务器（mosquitto）进行交互的 MQTT 客户端，为其他组件提供发布和订阅功能。<br>
ServiceBus: 用于与 HTTP 服务器 （REST） 交互的 HTTP 客户端，为云组件提供 HTTP 客户端功能，以访问在边缘运行的 HTTP 服务器。<br>
DeviceTwin: 负责存储设备状态并将设备状态同步到云端。它还为应用程序提供查询接口。<br>
MetaManager: Edged 端和 Edgehub 端之间的消息处理器。它还负责将元数据存储到轻量级数据库（SQLite）或从轻量级数据库（SQLite）检索元数据。</p>
<p>KubeEdge架构图如下：<br>
<img src="https://release-1-20.docs.kubeedge.io/zh/assets/images/kubeedge_arch-a0fa6324bc543a933d766e45d5f00f77.png" alt="001" loading="lazy"></p>
<p><font color="red">提示：更多介绍参考官方文档：<a href="https://release-1-20.docs.kubeedge.io/zh/docs/" target="_blank" rel="noopener nofollow">为什么选择KubeEdge</a><br>
</font></p>
<h2 id="kubeedge部署">KubeEdge部署</h2>
<h3 id="部署依赖">部署依赖</h3>
<p>KubeEdge基于原生Kubernetes基础上构建，将Kubernetes能力从云端延伸到边缘，并使其适配边缘计算的要求，比如网络不稳定性以及资源受限场景等，因此在部署KubeEdge前，请先准备好一个Kubernetes集群。有关 Kubernetes 的部署可参考： <a href="https://blog.csdn.net/tagaochen1276/article/details/146888946" target="_blank" rel="noopener nofollow">附042.Kubernetes_v1.32.3生成环境高可用部署</a></p>
<p><font color="red">提示：Kubernetes 和 KubeEdge 版本匹配和兼容可查看： <a href="https://github.com/kubeedge/kubeedge?tab=readme-ov-file#kubernetes-compatibility/" target="_blank" rel="noopener nofollow">Kubernetes compatibility</a> 。<br>
</font></p>
<p>对于容器运行时，可支持如下：</p>
<ul>
<li>docker</li>
<li>containerd</li>
<li>cri-o</li>
<li>virtlet</li>
</ul>
<h3 id="部署规划">部署规划</h3>
<p>本方案基于 Keadm 部署工具实现完整生产环境可用的 KubeEdge 边缘云部署。<br>
其主要信息如下：</p>
<ul>
<li>版本：KubeEdge v1.20.0 版本；</li>
<li>Keadm：采用 Keadm 部署 KubeEdge ；</li>
<li>OS：Ubuntu Server 24.04 LTS；</li>
<li>containerd：容器运行时。</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: center">节点主机名</th>
<th style="text-align: center">IP</th>
<th style="text-align: center">类型</th>
<th style="text-align: center">备注</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center">master01</td>
<td style="text-align: center">172.24.8.181</td>
<td style="text-align: center">Kubernetes master节点</td>
<td style="text-align: center">KubeEdge Cloud节点</td>
</tr>
<tr>
<td style="text-align: center">master02</td>
<td style="text-align: center">172.24.8.182</td>
<td style="text-align: center">Kubernetes master节点</td>
<td style="text-align: center">KubeEdge Cloud节点</td>
</tr>
<tr>
<td style="text-align: center">master03</td>
<td style="text-align: center">172.24.8.183</td>
<td style="text-align: center">Kubernetes master节点</td>
<td style="text-align: center">KubeEdge Cloud节点</td>
</tr>
<tr>
<td style="text-align: center">worker01</td>
<td style="text-align: center">172.24.8.184</td>
<td style="text-align: center">Kubernetes worker节点</td>
<td style="text-align: center"></td>
</tr>
<tr>
<td style="text-align: center">worker02</td>
<td style="text-align: center">172.24.8.185</td>
<td style="text-align: center">Kubernetes worker节点</td>
<td style="text-align: center"></td>
</tr>
<tr>
<td style="text-align: center">worker03</td>
<td style="text-align: center">172.24.8.186</td>
<td style="text-align: center">Kubernetes worker节点</td>
<td style="text-align: center"></td>
</tr>
<tr>
<td style="text-align: center">worker03</td>
<td style="text-align: center">172.24.8.186</td>
<td style="text-align: center">Kubernetes worker节点</td>
<td style="text-align: center"></td>
</tr>
<tr>
<td style="text-align: center">edgenode01</td>
<td style="text-align: center">172.24.8.187</td>
<td style="text-align: center">KubeEdge worker节点</td>
<td style="text-align: center"></td>
</tr>
<tr>
<td style="text-align: center">edgenode02</td>
<td style="text-align: center">172.24.8.188</td>
<td style="text-align: center">KubeEdge worker节点</td>
<td style="text-align: center"></td>
</tr>
</tbody>
</table>
<p><font color="red">提示：Kubernetes 集群部署不在本方案讨论内，但本方案基于快速部署目的，会使用 Kubernetes 的 master01 充当部署节点，从而快速批量完成对edgenode节点的相关部署。<br>
</font></p>
<h3 id="主机名配置">主机名配置</h3>
<p>需要对所有节点主机名进行相应配置。</p>
<pre><code class="language-shell">root@localhost:~# hostnamectl set-hostname edgenode01	    #其他节点依次修改
</code></pre>
<p><font color="red">提示：如上需要在所有节点修改对应的主机名。   </font></p>
<p>生产环境通常建议在内网部署dns服务器，使用dns服务器进行解析，本指南采用本地hosts文件名进行解析。<br>
如下hosts文件修改仅需在master01执行，后续使用批量分发至其他所有节点。</p>
<pre><code class="language-shell">root@master01:~# cat &gt;&gt; /etc/hosts &lt;&lt; EOF
172.24.8.187 edgenode01
172.24.8.188 edgenode02
EOF

root@master01:~# cat /etc/hosts
#……
172.24.8.181 master01
172.24.8.182 master02
172.24.8.183 master03
172.24.8.184 worker01
172.24.8.185 worker02
172.24.8.186 worker03
172.24.8.187 edgenode01
172.24.8.188 edgenode02
</code></pre>
<p><font color="red">提示：如上仅需在master01节点上操作。   </font></p>
<h3 id="变量准备">变量准备</h3>
<p>为实现自动化部署，自动化分发相关文件，提前定义相关主机名、IP组、变量等。</p>
<pre><code class="language-shell">root@master01:~# wget http://down.linuxsb.com/mydeploy/kubeedge/v1.20.0/kubeedgenodeenv.sh

root@master01:~# vi kubeedgenodeenv.sh            #确认相关主机名和IP
#!/bin/bash
#***************************************************************#
# ScriptName: kubeedgenodeenv.sh
# Author: xhy
# Create Date: 2025-04-01 16:12
# Modify Author: xhy
# Modify Date: 2025-04-01 16:15
# Version: v1
#***************************************************************#

# Kubernetes 集群 MASTER 机器 IP 数组
export K8S_MASTER_IPS=(172.24.8.181 172.24.8.182 172.24.8.183)

# Kubernetes 集群 MASTER IP 对应的主机名数组
export K8S_MASTER_NAMES=(master01 master02 master03)

# Kubernetes 集群 NODE 机器 IP 数组
export K8S_NODE_IPS=(172.24.8.184 172.24.8.185 172.24.8.186)

# Kubernetes 集群 NODE IP 对应的主机名数组
export K8S_NODE_NAMES=(worker01 worker02 worker03)

# Kubernetes 集群 所有机器 IP 数组
export K8S_ALL_IPS=(172.24.8.181 172.24.8.182 172.24.8.183 172.24.8.184 172.24.8.185 172.24.8.186)

# Kubernetes 集群 所有IP 对应的主机名数组
export K8S_ALL_NAMES=(master01 master02 master03 worker01 worker02 worker03)

# Kubeedge 集群 所有机器 IP 数组
export EDGE_ALL_IPS=(172.24.8.187 172.24.8.188)

# Kubeedge 集群 所有IP 对应的主机名数组
export EDGE_ALL_NAMES=(edgenode01 edgenode02)
</code></pre>
<p><font color="red">提示：如上仅需在master01节点上操作。   </font></p>
<h3 id="互信配置">互信配置</h3>
<p>为了方便远程分发文件和执行命令，本方案配置master01节点到其它节点的 ssh信任关系，即免秘钥管理所有其他节点。</p>
<pre><code class="language-shell">root@master01:~# source kubeedgenodeenv.sh                                #载入变量
    
root@master01:~# for edge_ip in ${EDGE_ALL_IPS[@]}
  do
    echo -e "\n\n\033[33m[INFO] &gt;&gt;&gt; ${edge_ip}...\033[0m"
    ssh-copy-id -i ~/.ssh/id_rsa.pub root@${edge_ip}
  done
  
root@master01:~# for edge_name in ${EDGE_ALL_NAMES[@]}
  do
    echo -e "\n\n\033[33m[INFO] &gt;&gt;&gt; ${edge_name}...\033[0m"
    ssh-copy-id -i ~/.ssh/id_rsa.pub root@${edge_name}
  done

root@master01:~# for all_ip in ${K8S_ALL_IPS[@]}
  do
    echo -e "\n\n\033[33m[INFO] &gt;&gt;&gt; ${all_ip}...\033[0m"
    sleep 2
    scp -rp /etc/hosts root@${all_ip}:/etc/hosts
  done
</code></pre>
<p><font color="red">提示：如上仅需在master01节点上操作。   </font></p>
<h3 id="环境预配置">环境预配置</h3>
<p>需要在每个边缘节点上安装一个容器运行时，以使边缘引擎 EdgeCore 能够成功安装，并且边缘 Pod 可以运行在边缘节点上。<br>
Keadm 用于部署 KubeEdge ，在正式使用 Keadm 部署 KubeEdge 之前需要对操作系统环境进行准备，即环境预配置。<br>
环境的预配置本方案使用脚本自动完成。<br>
使用如下脚本对基础环境进行初始化，主要功能包括：</p>
<ul>
<li>安装 containerd ，边缘引擎 EdgeCore 依赖的容器组件</li>
<li>关闭 SELinux 及防火墙</li>
<li>优化相关内核参数，针对生产环境的基础系统调优配置</li>
<li>关闭 swap</li>
<li>设置相关模块，主要为转发模块</li>
<li>配置相关基础软件，系统部分基础依赖包</li>
<li>创建 container 所使用的独立目录</li>
<li>配置 crictl 和运行时的连接，便于后期使用 crictl 命令</li>
</ul>
<p><font color="red">提示：后续ctr命令下载镜像的时候，若需要使用containerd的加速，必须带上--hosts-dir，ctr 当前环境所管理的镜像都属于 k8s.io ，因此创建一个别名。 </font></p>
<pre><code class="language-shell">root@master01:~# wget http://down.linuxsb.com/mydeploy/k8s/v1.32.3/uk8spreconfig.sh

root@master01:~# vim uk8spreconfig.sh
#!/bin/bash
#***************************************************************#
# ScriptName: uk8spreconfig.sh
# Author: xhy
# Create Date: 2025-03-29 12:30
# Modify Author: xhy
# Modify Date: 2025-04-01 16:43
# Version: v1
#***************************************************************#

CONTAINERD_VERSION=1.7.26-1

# Initialize the machine. This needs to be executed on every machine.
apt update &amp;&amp; apt upgrade -y &amp;&amp; apt autoremove -y

# Install package
sudo apt -y install conntrack ntpdate ntp ipvsadm ipset jq iptables sysstat wget

# Add Docker's official GPG key
sudo apt -y install ca-certificates curl gnupg
sudo install -m 0755 -d /etc/apt/keyrings
sudo curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
sudo chmod a+r /etc/apt/keyrings/docker.gpg

# Add the repository to Apt sources:
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://mirrors.aliyun.com/docker-ce/linux/ubuntu \
  "$(. /etc/os-release &amp;&amp; echo "$VERSION_CODENAME")" stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null
sudo apt update

# Install and config containerd
apt-cache madison containerd
sudo apt -y install containerd.io=${CONTAINERD_VERSION}
sleep 3s

mkdir -p /etc/containerd/certs.d/docker.io
mkdir -p /data/containerd

cat &gt; /etc/containerd/config.toml &lt;&lt;EOF
disabled_plugins = ["io.containerd.internal.v1.restart"]
root = "/data/containerd"
version = 2

[plugins]

  [plugins."io.containerd.grpc.v1.cri"]
#    sandbox_image = "registry.k8s.io/pause:3.10"
    sandbox_image = "registry.aliyuncs.com/google_containers/pause:3.10"

    [plugins."io.containerd.grpc.v1.cri".containerd]

      [plugins."io.containerd.grpc.v1.cri".containerd.runtimes]

        [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
          runtime_type = "io.containerd.runc.v2"

          [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
            SystemdCgroup = true

    [plugins."io.containerd.grpc.v1.cri".registry]
      config_path = "/etc/containerd/certs.d"

  [plugins."io.containerd.runtime.v1.linux"]
    shim_debug = true
EOF

cat &gt; /etc/containerd/certs.d/docker.io/hosts.toml &lt;&lt;EOF
server = "https://registry-1.docker.io"

[host."https://docker.1ms.run"]
  capabilities = ["pull", "resolve", "push"]

[host."https://dbzucv6w.mirror.aliyuncs.com"]
  capabilities = ["pull", "resolve", "push"]

[host."https://docker.xuanyuan.me"]
  capabilities = ["pull", "resolve", "push"]
EOF

# config crictl &amp; containerd
cat &gt; /etc/crictl.yaml &lt;&lt;EOF
runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
timeout: 10
debug: false
EOF

echo 'alias ctrpull="ctr -n k8s.io images pull --hosts-dir /etc/containerd/certs.d"' &gt;&gt; /etc/profile.d/custom_bash.sh

systemctl restart containerd
systemctl enable containerd --now

# Turn off and disable the firewalld.
systemctl disable ufw --now || true

# Modify related kernel parameters &amp; Disable the swap.
cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt; EOF
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.tcp_tw_recycle = 0
vm.swappiness = 0
vm.overcommit_memory = 1
vm.panic_on_oom = 0
net.ipv6.conf.all.disable_ipv6 = 1
EOF
sysctl -p /etc/sysctl.d/k8s.conf &gt;&amp;/dev/null
swapoff -a
sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
modprobe br_netfilter
modprobe overlay

sysctl --system

# Add ipvs modules
cat &gt; /etc/modules-load.d/ipvs.conf &lt;&lt;EOF
ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
nf_conntrack
br_netfilter
overlay
EOF

systemctl restart systemd-modules-load.service
</code></pre>
<p><font color="red">提示：containerd 镜像加速配置更多可参考：</font></p><font color="red">
</font><p><font color="red"><a href="https://www.cnblogs.com/fsdstudy/p/18355827" target="_blank">containerd配置镜像加速器</a> <a href="https://github.com/containerd/containerd/blob/main/docs/hosts.md" target="_blank" rel="noopener nofollow">containerd官方加速配置</a><br>
</font></p>
<p><font color="red">提示：如上仅需在master01节点上操作，建议初始化完后进行重启。   </font></p>
<pre><code class="language-shell">root@master01:~# source kubeedgenodeenv.sh
root@master01:~# chmod +x *.sh

root@master01:~# for edge_ip in ${EDGE_ALL_IPS[@]}
  do
    echo -e "\n\n\033[33m[INFO] &gt;&gt;&gt; ${edge_ip}...\033[0m"
    sleep 2
    scp -rp /etc/hosts root@${edge_ip}:/etc/hosts
    scp -rp uk8spreconfig.sh root@${edge_ip}:/root/
    ssh root@${edge_ip} "bash /root/uk8spreconfig.sh"
  done
</code></pre>
<p><font color="red">提示：如上仅需在master01节点上操作。   </font></p>
<h3 id="安装keadm">安装keadm</h3>
<p>云端（CloudCore）部分需要设置云端节点，即 KubeEdge 主节点，边缘（EdgeCore）部分需要设置边缘端节点，即 KubeEdge 工作节点。<br>
因此建议所有节点安装 keadm 工具。</p>
<pre><code class="language-shell">root@master01:~# mkdir keadm
root@master01:~# cd keadm/
root@master01:~/keadm# KEADMVERSION=v1.20.0
root@master01:~/keadm# wget https://github.com/kubeedge/kubeedge/releases/download/${KEADMVERSION}/keadm-${KEADMVERSION}-linux-amd64.tar.gz
root@master01:~/keadm# tar -zxvf keadm-${KEADMVERSION}-linux-amd64.tar.gz
root@master01:~/keadm# cp keadm-${KEADMVERSION}-linux-amd64/keadm/keadm /usr/local/bin/keadm
root@master01:~/keadm# keadm version
version: version.Info{Major:"1", Minor:"20", GitVersion:"v1.20.0", GitCommit:"bae61505d4919c404665f70347ad0646aaa98958", GitTreeState:"clean", BuildDate:"2025-01-21T12:47:49Z", GoVersion:"go1.22.9", Compiler:"gc", Platform:"linux/amd64"}
</code></pre>
<p>分发keadm工具。</p>
<pre><code class="language-shell">root@master01:~/keadm# source /root/kubeedgenodeenv.sh

root@master01:~/keadm# for edge_ip in ${EDGE_ALL_IPS[@]}
  do
    echo -e "\n\n\033[33m[INFO] &gt;&gt;&gt; ${edge_ip}...\033[0m"
    sleep 2
    scp -rp /usr/local/bin/keadm root@${edge_ip}:/usr/local/bin/keadm
  done
</code></pre>
<h3 id="设置标签">设置标签</h3>
<p>本实验直接复用 Kubernetes master 的高可用，采用 VIP 对外暴露，因此规划将 CloudCore 部署在 master 节点。<br>
keadm 底层基于 Kubernetes helm chart 进行部署，因此也支持通过 helm values 来创建自定义参数来控制部署情况。</p>
<pre><code class="language-shell">root@master01:~/keadm# kubectl label nodes master0{1,2,3} cloudcore=enabled
</code></pre>
<h3 id="安装cni">安装CNI</h3>
<p>默认边缘节点安装containerd并不会安装cni，需要手动额外安装。<br>
同时需要配置calico网络以及手动分发相关calico证书，本实验直接从当前已存在的master01节点快速分发。</p>
<pre><code class="language-shell">root@master01:~/keadm# source /root/kubeedgenodeenv.sh

root@master01:~/keadm# for edge_ip in ${EDGE_ALL_IPS[@]}
  do
    echo -e "\n\n\033[33m[INFO] &gt;&gt;&gt; ${edge_ip}...\033[0m"
    sleep 2
    scp -rp /opt/cni/bin/* root@${edge_ip}:/opt/cni/bin/
    scp -rp /etc/cni/net.d/* root@${edge_ip}:/etc/cni/net.d/
  done
</code></pre>
<p><font color="red">提示：也可参考如下手动安装cni：</font></p><font color="red">
<pre><code class="language-shell">root@master01:~/keadm# wget https://github.com/containernetworking/plugins/releases/download/v1.6.2/cni-plugins-linux-amd64-v1.6.2.tgz
root@master01:~/keadm# tar zxvf cni-plugins-linux-amd64-v1.6.2.tgz -C /opt/cni/bin
</code></pre>
</font> 
<h3 id="设置云端">设置云端</h3>
<p>使用 keadm init 设置 KubeEdge 主节点，即设置 CloudCore 端。<br>
默认情况下边缘节点需要访问 cloudcore 中 10000 ，10002 端口。</p>
<ul>
<li>自定义部署<br>
创建自定义相关配置，然后进行部署。</li>
</ul>
<pre><code class="language-shell">root@master01:~/keadm# vim myvalues.yaml
cloudCore:
  replicaCount: 3
  nodeSelector:
    cloudcore: enabled
  tolerations:
    - key: node-role.kubernetes.io/control-plane
      effect: NoSchedule

  modules:
    cloudHub:
      advertiseAddress:
        - "172.24.8.180"

    dynamicController:
      enable: true 

iptablesManager:
  nodeSelector:
    cloudcore: enabled
  tolerations:
    - key: node-role.kubernetes.io/control-plane
      effect: NoSchedule

mosquitto:
  nodeSelector:
    cloudcore: enabled
  tolerations:
    - key: node-role.kubernetes.io/control-plane
      effect: NoSchedule

root@master01:~/keadm# keadm init \
  --profile=myvalues.yaml \
  --kubeedge-version=v1.20.0 \
  --kube-config=/root/.kube/config

</code></pre>
<p><font color="red">提示：keadm init 将安装并运行 cloudcore，生成证书并安装 CRD，同时提供了--set key=value 命令行参数，通过它可以设置特定的配置。也支持 helm 方式配置参数，官方默认的 helm chart 参数参考：<a href="https://github.com/kubeedge/kubeedge/blob/master/manifests/charts/cloudcore/values.yaml" target="_blank" rel="noopener nofollow">cloudcore-values.yaml</a><br>
</font></p>
<ul>
<li>查看结果<br>
查看云端部署结果。</li>
</ul>
<pre><code class="language-shell">root@master01:~/keadm# kubectl -n kubeedge get all -owide
</code></pre>
<p><img src="https://tp.linuxsb.com/study/kubernetes/f043/002.png" alt="002" loading="lazy"></p>
<h3 id="设置调度">设置调度</h3>
<p>在边缘节点添加至集群之前，建议设置调度策略，避免系统自带的 DaemonSet 调度到边缘节点。</p>
<p>主要需要设置的相关Pod有 calico-node ，kube-proxy 。</p>
<pre><code class="language-shell">root@master01:~/keadm# kubectl get daemonset -n kube-system | grep -v NAME | awk '{print $1}' | xargs -n 1 kubectl patch daemonset -n kube-system --type='json' -p='[{"op": "replace","path": "/spec/template/spec/affinity","value":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchExpressions":[{"key":"node-role.kubernetes.io/edge","operator":"DoesNotExist"}]}]}}}}]'

root@master01:~/keadm# kubectl -n kube-system rollout restart daemonset kube-proxy calico-node

root@master01:~/keadm# kubectl -n kube-system get daemonset kube-proxy -o yaml | grep -A5 affinity
root@master01:~/keadm# kubectl -n kube-system get daemonset calico-node -o yaml | grep -A5 affinity
</code></pre>
<p>本实验持久存储 Longhorn 也存在 DaemonSet ，且 Longhorn 是基于 helm 部署，需要做如下调整：</p>
<pre><code class="language-shell">root@master01:~/longhorn# vim myvalues.yaml
longhornManager:
  nodeSelector:
    longhorn-storage: enabled

  extraVolumeMounts:
    - name: timeconfig
      mountPath: /etc/localtime
      readOnly: true
  extraVolumes:
    - name: timeconfig
      hostPath:
        path: /etc/localtime

  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: node-role.kubernetes.io/edge
            operator: DoesNotExist

longhornDriver:
  nodeSelector:
    longhorn-storage: enabled

  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: node-role.kubernetes.io/edge
            operator: DoesNotExist

csi:
  plugin:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: node-role.kubernetes.io/edge
              operator: DoesNotExist
  attacher:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: node-role.kubernetes.io/edge
              operator: DoesNotExist
  provisioner:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: node-role.kubernetes.io/edge
              operator: DoesNotExist
  resizer:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: node-role.kubernetes.io/edge
              operator: DoesNotExist

longhornUI:
  replicas: 3
  nodeSelector:
    longhorn-ui: enabled
  tolerations:
    - key: node-role.kubernetes.io/control-plane
      effect: NoSchedule

  extraVolumeMounts:
    - name: timeconfig
      mountPath: /etc/localtime
      readOnly: true
  extraVolumes:
    - name: timeconfig
      hostPath:
        path: /etc/localtime

  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: node-role.kubernetes.io/edge
            operator: DoesNotExist
ingress:
  enabled: true
  ingressClassName: "nginx"
  host: "longhorn.linuxsb.com"
  path: /
  pathType: Prefix
  annotations:
    nginx.ingress.kubernetes.io/auth-type: basic
    nginx.ingress.kubernetes.io/auth-secret: longhorn-basic-auth
    nginx.ingress.kubernetes.io/auth-realm: 'Authentication Required'
    nginx.ingress.kubernetes.io/proxy-body-size: 50m
    nginx.ingress.kubernetes.io/ssl-redirect: "false"

root@master01:~/longhorn# helm upgrade --install longhorn longhorn/longhorn --create-namespace --namespace longhorn-system -f myvalues.yaml
root@master01:~/longhorn# kubectl -n longhorn-system rollout restart  daemonsets.apps engine-image-ei-db6c2b6f longhorn-csi-plugin longhorn-manager
</code></pre>
<h3 id="设置边缘端">设置边缘端</h3>
<p>使用 keadm joini 设置 KubeEdge 边缘节点，即设置 EdgeCore 端。<br>
默认情况下边缘节点需要访问 cloudcore 中 10000 ，10002 端口。</p>
<p>提示：</p>
<ol>
<li>--cloudcore-ipport 是必填参数。</li>
<li>加上 --token 会自动为边缘节点生成证书。</li>
<li>需要保证云和边缘端使用的 KubeEdge 版本相同。</li>
</ol>
<ul>
<li>获取加入的token<br>
在云端获取token，然后在边缘端加入。</li>
</ul>
<pre><code class="language-shell">root@master01:~/keadm# keadm gettoken
2b299ac235ea1ae400a02dfd2442bafa57f5e7b1a0239a6551e7d5c8e6366ecd.eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE3NDQwNDAzNjV9.v8fv9295l4OaqwISKpIDkRrA8FT-0y45VJRmSwgSJaY
</code></pre>
<ul>
<li>边缘节点join</li>
</ul>
<pre><code class="language-shell">root@edgenode01:~# keadm join --cloudcore-ipport=172.24.8.180:10000 \
 --token=2b299ac235ea1ae400a02dfd2442bafa57f5e7b1a0239a6551e7d5c8e6366ecd.eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE3NDQwNDAzNjV9.v8fv9295l4OaqwISKpIDkRrA8FT-0y45VJRmSwgSJaY \
 --kubeedge-version=v1.20.0 --cgroupdriver=systemd --edgenode-name=edgenode01

root@edgenode02:~# keadm join --cloudcore-ipport=172.24.8.180:10000 \
 --token=2b299ac235ea1ae400a02dfd2442bafa57f5e7b1a0239a6551e7d5c8e6366ecd.eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE3NDQwNDAzNjV9.v8fv9295l4OaqwISKpIDkRrA8FT-0y45VJRmSwgSJaY \
 --kubeedge-version=v1.20.0 --cgroupdriver=systemd --edgenode-name=edgenode02
</code></pre>
<h3 id="边缘端优化">边缘端优化</h3>
<p>edgeStream用于边缘节点与云端的通信，建议开启。</p>
<pre><code class="language-shell">root@edgenode01:~# vim /etc/kubeedge/config/edgecore.yaml
apiVersion: edgecore.config.kubeedge.io/v1alpha2
#……
modules:
#……
  edgeStream:
    enable: true
root@edgenode01:~# systemctl restart edgecore.service

root@edgenode02:~# vim /etc/kubeedge/config/edgecore.yaml
apiVersion: edgecore.config.kubeedge.io/v1alpha2
#……
modules:
#……
  edgeStream:
    enable: true
    
root@edgenode02:~# systemctl restart edgecore.service
</code></pre>
<h3 id="确认验证">确认验证</h3>
<p>查看当前部署情况。</p>
<pre><code class="language-shell">root@master01:~/keadm# kubectl get nodes -o wide
NAME         STATUS   ROLES           AGE   VERSION                    INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
edgenode01   Ready    agent,edge      62m   v1.30.7-kubeedge-v1.20.0   172.24.8.187   &lt;none&gt;        Ubuntu 24.04.2 LTS   6.8.0-57-generic   containerd://1.7.27
edgenode02   Ready    agent,edge      62m   v1.30.7-kubeedge-v1.20.0   172.24.8.188   &lt;none&gt;        Ubuntu 24.04.2 LTS   6.8.0-57-generic   containerd://1.7.27
master01     Ready    control-plane   8d    v1.32.3                    172.24.8.181   &lt;none&gt;        Ubuntu 24.04.2 LTS   6.8.0-56-generic   containerd://1.7.26
master02     Ready    control-plane   8d    v1.32.3                    172.24.8.182   &lt;none&gt;        Ubuntu 24.04.2 LTS   6.8.0-56-generic   containerd://1.7.26
master03     Ready    control-plane   8d    v1.32.3                    172.24.8.183   &lt;none&gt;        Ubuntu 24.04.2 LTS   6.8.0-56-generic   containerd://1.7.26
worker01     Ready    &lt;none&gt;          8d    v1.32.3                    172.24.8.184   &lt;none&gt;        Ubuntu 24.04.2 LTS   6.8.0-56-generic   containerd://1.7.26
worker02     Ready    &lt;none&gt;          8d    v1.32.3                    172.24.8.185   &lt;none&gt;        Ubuntu 24.04.2 LTS   6.8.0-56-generic   containerd://1.7.26
worker03     Ready    &lt;none&gt;          8d    v1.32.3                    172.24.8.186   &lt;none&gt;        Ubuntu 24.04.2 LTS   6.8.0-56-generic   containerd://1.7.26
</code></pre>
<p>查看当前KubeEdge的Pod。</p>
<pre><code class="language-shell">root@master01:~/keadm# kubectl get pods -o wide -n kubeedge
NAME                           READY   STATUS    RESTARTS   AGE   IP             NODE         NOMINATED NODE   READINESS GATES
cloud-iptables-manager-klqkb   1/1     Running   0          92m   172.24.8.182   master02     &lt;none&gt;           &lt;none&gt;
cloud-iptables-manager-v4np5   1/1     Running   0          92m   172.24.8.181   master01     &lt;none&gt;           &lt;none&gt;
cloud-iptables-manager-vknlw   1/1     Running   0          92m   172.24.8.183   master03     &lt;none&gt;           &lt;none&gt;
cloudcore-5cc47bd5c7-bfk9f     1/1     Running   0          92m   172.24.8.181   master01     &lt;none&gt;           &lt;none&gt;
cloudcore-5cc47bd5c7-x822x     1/1     Running   0          92m   172.24.8.182   master02     &lt;none&gt;           &lt;none&gt;
cloudcore-5cc47bd5c7-zh9z2     1/1     Running   0          92m   172.24.8.183   master03     &lt;none&gt;           &lt;none&gt;
edge-eclipse-mosquitto-hfl88   1/1     Running   0          64m   172.24.8.188   edgenode02   &lt;none&gt;           &lt;none&gt;
edge-eclipse-mosquitto-j8ft7   1/1     Running   0          64m   172.24.8.187   edgenode01   &lt;none&gt;           &lt;none&gt;
</code></pre>
<h2 id="kubeedge测试验证">KubeEdge测试验证</h2>
<h3 id="调度测试">调度测试</h3>
<p>KubeEdge部署好之后，可以使用定向调度测试应用运行。</p>
<pre><code class="language-shell">root@master01:~/keadm# cat &gt; edgetest.yaml &lt;&lt;EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-edgename
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      nodeName: edgenode01
      hostNetwork: true
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
EOF

root@master01:~/keadm# kubectl apply -f edgetest.yaml

root@master01:~/keadm# kubectl get pods -o wide
NAME                              READY   STATUS    RESTARTS   AGE     IP             NODE         NOMINATED NODE   READINESS GATES
nginx-edgename-75c465f8b9-hcxj7   1/1     Running   0          3m38s   172.24.8.187   edgenode01   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>访问： <a href="http://172.24.8.187" target="_blank" rel="noopener nofollow">http://172.24.8.187</a> 验证部署情况。</p>
<p><img src="https://tp.linuxsb.com/study/kubernetes/f043/003.png" alt="003" loading="lazy"></p>

</div>
<div id="MySignature" role="contentinfo">
    <div style="background: #f7acbc; color: #0; font-size: small">
<p>
作者：<a href="http://www.linuxsb.com/">木二</a>
</p>
<p>
出处：<a href="http://www.cnblogs.com/itzgr/">http://www.cnblogs.com/itzgr/</a>
</p>
<p>
关于作者：云计算、虚拟化，Linux，多多交流！
</p>
<p>
本文版权归作者所有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出<a href="#" style="background: #b6ff00; color: #0; font-size: medium">原文链接</a>!如有其他问题，可邮件（xhy@itzgr.com）咨询。
</p>
</div>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.22117417500810185" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-04-09 19:02">2025-04-09 19:02</span>&nbsp;
<a href="https://www.cnblogs.com/itzgr">木二</a>&nbsp;
阅读(<span id="post_view_count">3</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18817166" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18817166);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18817166', targetLink: 'https://www.cnblogs.com/itzgr/p/18817166', title: '附043.KubeEdge边缘云部署实施方案' })">举报</a>
</div>
        