
		<h1 class="postTitle">
			<a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/charlieroro/p/18709638" title="发布于 2025-02-14 19:00">
    <span role="heading" aria-level="2">使用Ollama和AnythingLLM搭建本地AI</span>
    

</a>

		</h1>
		<div class="clear"></div>
		<div class="postBody">
			<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h2 id="搭建本地博客ai">搭建本地博客AI</h2>
<p></p><div class="toc"><div class="toc-container-header">目录</div><ul><li><a href="#搭建本地博客ai" rel="noopener nofollow">搭建本地博客AI</a><ul><li><a href="#环境" rel="noopener nofollow">环境</a></li><li><a href="#下载ollama" rel="noopener nofollow">下载ollama</a></li><li><a href="#选择模型" rel="noopener nofollow">选择模型</a><ul><li><a href="#选择嵌入embedder模型" rel="noopener nofollow">选择嵌入(embedder)模型</a><ul><li><a href="#查看性能测试" rel="noopener nofollow">查看性能测试</a></li><li><a href="#估算内存" rel="noopener nofollow">估算内存</a></li><li><a href="#选择模型-1" rel="noopener nofollow">选择模型</a></li><li><a href="#量化类型介绍" rel="noopener nofollow">量化类型介绍</a><ul><li><a href="#q5_0-vs--q5_k" rel="noopener nofollow"><strong>Q5_0 vs  Q5_K</strong></a></li><li><a href="#q5_k-变体q5_k_sq5_k_mq5_k_l" rel="noopener nofollow">Q5_K 变体(Q5_K_S、Q5_K_M、Q5_K_L)</a></li></ul></li></ul></li><li><a href="#选择llm模型" rel="noopener nofollow">选择LLM模型</a></li></ul></li><li><a href="#下载模型" rel="noopener nofollow">下载模型</a><ul><li><a href="#下载llm" rel="noopener nofollow">下载LLM</a></li><li><a href="#下载embedder" rel="noopener nofollow">下载Embedder</a></li></ul></li><li><a href="#下载anythingllm" rel="noopener nofollow">下载AnythingLLM</a><ul><li><a href="#配置向量数据库" rel="noopener nofollow">配置向量数据库</a></li></ul></li><li><a href="#测试使用" rel="noopener nofollow">测试使用</a></li><li><a href="#tips" rel="noopener nofollow">Tips</a><ul><li><a href="#reset向量数据库" rel="noopener nofollow">Reset向量数据库</a></li><li><a href="#llm没有使用自己的文档" rel="noopener nofollow">LLM没有使用自己的文档</a></li><li><a href="#ai回答逻辑混乱" rel="noopener nofollow">AI回答逻辑混乱</a></li><li><a href="#ollama" rel="noopener nofollow">Ollama</a><ul><li><a href="#日志中出现告警或错误" rel="noopener nofollow">日志中出现告警或错误</a></li><li><a href="#命令行" rel="noopener nofollow">命令行</a></li><li><a href="#debug" rel="noopener nofollow">Debug</a></li><li><a href="#ollama-faq" rel="noopener nofollow">Ollama FAQ</a></li></ul></li></ul></li><li><a href="#参考" rel="noopener nofollow">参考</a></li><li><a href="#todo" rel="noopener nofollow">TODO</a></li></ul></li></ul></div><p></p>
<h3 id="环境">环境</h3>
<ul>
<li><strong>Env</strong>：MacBook Pro M2</li>
<li><strong>Total memory</strong>：16GB</li>
</ul>
<h3 id="下载ollama">下载ollama</h3>
<ul>
<li><a href="https://ollama.com/" target="_blank" rel="noopener nofollow">下载</a>并按照提示安装启动ollama。</li>
<li>在浏览器中访问<code>http://localhost:11434</code>，若返回"Ollama is running"，则表示启动成功。</li>
</ul>
<h3 id="选择模型">选择模型</h3>
<h4 id="选择嵌入embedder模型">选择嵌入(embedder)模型</h4>
<h5 id="查看性能测试">查看性能测试</h5>
<p><a href="https://huggingface.co/spaces/mteb/leaderboard" target="_blank" rel="noopener nofollow">mteb</a>展示了文本嵌入模型的性能测试结果。由于博客以中英文为主，因此在过滤语言时应该包含<code>cmn</code>、<code>zho</code>和<code>eng</code>(语言代码<a href="https://github.com/embeddings-benchmark/mteb/blob/main/docs/mmteb/readme.md#contribution-point-guideline" target="_blank" rel="noopener nofollow">使用</a>的是<a href="https://en.wikipedia.org/wiki/ISO_639-3" target="_blank" rel="noopener nofollow">ISO 639-3</a>标准)</p>
<img src="https://img2024.cnblogs.com/blog/1334952/202502/1334952-20250213150343270-1402600397.png" alt="image" style="zoom: 33%">
<p>运行结果如下：</p>
<img src="https://img2024.cnblogs.com/blog/1334952/202502/1334952-20250213150742640-107085415.png" alt="image" style="zoom: 35%">
<h5 id="估算内存">估算内存</h5>
<p>在选择模型(不仅限嵌入模型)时，需要考虑模型占用的性能，通常参数越多，占用的内存也就越大。模型占用的内存<a href="https://www.substratus.ai/blog/calculating-gpu-memory-for-llm" target="_blank" rel="noopener nofollow">估算公式</a>如下：</p>
<p></p><div class="math display">\[M=\frac{(P * 4 B)}{(32 / Q)} * 1.2
\]</div><p></p><table>
<thead>
<tr>
<th>Symbol</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>M</td>
<td>用千兆字节 (GB) 表示的 GPU 内存</td>
</tr>
<tr>
<td>P</td>
<td>模型中的参数数量。例如，一个 7B 模型有 7 亿参数。</td>
</tr>
<tr>
<td>4B</td>
<td>4 字节，表示每个参数使用的字节数</td>
</tr>
<tr>
<td>32</td>
<td>4 字节中有 32 位</td>
</tr>
<tr>
<td>Q</td>
<td>加载模型时应使用的比特位数，例如 16 位、8 位或 4 位。</td>
</tr>
<tr>
<td>1.2</td>
<td>表示在 GPU 内存中加载额外内容的 20% 开销。</td>
</tr>
</tbody>
</table>
<p>以上图中的第2名<code>gte-Qwen2-1.5B-instruct</code>为例，其参数数量(P)为1.78B，即17.8亿个参数，其模型应用的比特位数(Q)为F32，即32位，那么它占用的内存约为<em>M = (1.78 ∗ 4) / (32 / 32) ∗ 1.2 ≈ 5.93GB</em></p>
<img src="https://img2024.cnblogs.com/blog/1334952/202502/1334952-20250213185124710-744857088.png" alt="image" style="zoom: 40%">
<p>由于<strong>除嵌入模型外，我们还需要运行大模型(LLM)</strong>，因此在总计16GB的内存上运行近6GB的嵌入模型是有些吃力的。那么是否有其他降低模型内存的方式呢？</p>
<p>答案是通过<strong>量化</strong>(Quantization)。</p>
<p>量化是一种降低内存占用的方式，通过将模型参数的精度从浮点数降低到更低的表达方式(如 8 位整数)，显著降低了内存和计算需求，使模型能够更高效地部署在资源有限的设备。但降低精度可能会影响输出的准确性。通常来说，<strong>8bit的量化可以达到16bit的性能</strong>，但4bit的量化可能会显著影响模型性能。</p>
<p>如果想用<strong>2GB左右</strong>的内存运行该模型，那么量化应该设置为多少？</p>
<p>计算<em>(1.78 ∗ 4) / (32 / Q) ∗ 1.2=2</em>，求解<code>Q</code>约为7.5。</p>
<p>这里提供了一个模型内存计算<a href="https://llm-calc.rayfernando.ai/?quant=8-bit&amp;os=8&amp;context=2000" target="_blank" rel="noopener nofollow">工具</a>，我们设置总内存(<code>Custom Ran(GB)</code>)为10GB，系统预留内存(<code>OS Overhead(GB)</code>)为8GB，这样留给嵌入模型的就只有2GB内存。量化级别(<code>Quantization Level</code>)为5-bit，上下文窗口(<code>Context Window(Tokens)</code>)为2048，则该配置下，可以支持1.6B的模型，与预期大致相符：</p>
<img src="https://img2024.cnblogs.com/blog/1334952/202502/1334952-20250213190542710-106143247.png" alt="image" style="zoom: 33%">
<h5 id="选择模型-1">选择模型</h5>
<p>从上面截图中可以看到<code>Alibaba-NLP/gte-Qwen2-7B-instruct</code>模型有28个量化版本，点击进入这28个量化版本的浏览页面</p>
<img src="https://img2024.cnblogs.com/blog/1334952/202502/1334952-20250213162249465-1939654671.png" alt="image" style="zoom: 33%">
<p>点击进入第一个模型<code>tensorblock/gte-Qwen2-7B-instruct-GGUF</code>(<strong>下载量最多</strong>)，在页面右侧选择<code>Use this model</code>-&gt;<code>Ollama</code>:</p>
<img src="https://img2024.cnblogs.com/blog/1334952/202502/1334952-20250213163807138-489549065.png" alt="image" style="zoom: 33%">
<p>可以看到它有很多量化版本，那么该选择哪一个？</p>
<img src="https://img2024.cnblogs.com/blog/1334952/202502/1334952-20250213163846744-2017689464.png" alt="image" style="zoom: 33%">
<p>可以<a href="https://github.com/ggerganov/llama.cpp/discussions/2094#discussioncomment-6351796" target="_blank" rel="noopener nofollow">参考</a>下面描述，<strong>推荐<code>Q4_K_M</code>和<code>Q5_K_S</code>，<code>Q5_K_M</code></strong>，鉴于我们的量化级别不能大于7，因此可以<strong>采用推荐的<code>Q5_K_M</code>模型</strong>。</p>
<pre><code class="language-sh">Allowed quantization types:
   2  or  Q4_0   :  3.50G, +0.2499 ppl @ 7B - small, very high quality loss - legacy, prefer using Q3_K_M
   3  or  Q4_1   :  3.90G, +0.1846 ppl @ 7B - small, substantial quality loss - legacy, prefer using Q3_K_L
   8  or  Q5_0   :  4.30G, +0.0796 ppl @ 7B - medium, balanced quality - legacy, prefer using Q4_K_M
   9  or  Q5_1   :  4.70G, +0.0415 ppl @ 7B - medium, low quality loss - legacy, prefer using Q5_K_M
  10  or  Q2_K   :  2.67G, +0.8698 ppl @ 7B - smallest, extreme quality loss - not recommended
  12  or  Q3_K   : alias for Q3_K_M
  11  or  Q3_K_S :  2.75G, +0.5505 ppl @ 7B - very small, very high quality loss
  12  or  Q3_K_M :  3.06G, +0.2437 ppl @ 7B - very small, very high quality loss
  13  or  Q3_K_L :  3.35G, +0.1803 ppl @ 7B - small, substantial quality loss
  15  or  Q4_K   : alias for Q4_K_M
  14  or  Q4_K_S :  3.56G, +0.1149 ppl @ 7B - small, significant quality loss
  15  or  Q4_K_M :  3.80G, +0.0535 ppl @ 7B - medium, balanced quality - *recommended*
  17  or  Q5_K   : alias for Q5_K_M
  16  or  Q5_K_S :  4.33G, +0.0353 ppl @ 7B - large, low quality loss - *recommended*
  17  or  Q5_K_M :  4.45G, +0.0142 ppl @ 7B - large, very low quality loss - *recommended*
  18  or  Q6_K   :  5.15G, +0.0044 ppl @ 7B - very large, extremely low quality loss
   7  or  Q8_0   :  6.70G, +0.0004 ppl @ 7B - very large, extremely low quality loss - not recommended
   1  or  F16    : 13.00G              @ 7B - extremely large, virtually no quality loss - not recommended
   0  or  F32    : 26.00G              @ 7B - absolutely huge, lossless - not recommended
</code></pre>
<h5 id="量化类型介绍">量化类型介绍</h5>
<p>从上面可以看出不同的后缀（<code>0</code>、<code>K</code>、<code>K_S</code>、<code>K_M</code>、<code>K_L</code>）代表 <strong>不同的量化技术和优化策略</strong>。以下内容来自GPT：</p>
<h6 id="q5_0-vs--q5_k"><strong>Q5_0 vs  Q5_K</strong></h6>
<p><strong>Q5_0</strong></p>
<ul>
<li>Q5_0 是最基础的 5-bit 量化方案，它使用 均匀量化（Uniform Quantization），但<strong>不包含任何额外的优化</strong>。</li>
<li>每个 block（通常 16 或 32 个权重）共享相同的缩放因子（scale）。</li>
<li>误差较大，在某些情况下可能导致 模型精度下降。</li>
</ul>
<p><strong>适用场景</strong></p>
<ul>
<li>适用于对<strong>精度要求不高</strong>的任务。</li>
<li>当设备计算<strong>资源有限但仍需要较好的推理速度</strong>时。</li>
</ul>
<hr>
<p><strong>Q5_K</strong></p>
<ul>
<li>Q5_K（K-Block Quantization）是一种更先进的 5-bit 量化方案，使用 块级（block-wise）非均匀量化（Non-Uniform Quantization） 以 降低量化误差。</li>
<li>相比 Q5_0，Q5_K 在同样的 5-bit 量化下能提供更好的数值精度，因此模型推理质量更高。</li>
<li>Q5_K 进一步引入了优化策略，如动态缩放（dynamic scaling）或非均匀量化方法，<strong>使得量化误差小于 Q5_0</strong>。</li>
</ul>
<p><strong>适用场景</strong></p>
<ul>
<li>适用于 <strong>对精度有较高要求</strong> 的 LLM 任务，如 <strong>聊天机器人、代码生成、翻译</strong> 等。</li>
<li>适用于 <strong>存储受限</strong> 但仍希望保持较好精度的设备，如 <strong>GPU、CPU、移动端</strong>。</li>
</ul>
<h6 id="q5_k-变体q5_k_sq5_k_mq5_k_l">Q5_K 变体(Q5_K_S、Q5_K_M、Q5_K_L)</h6>
<table>
<thead>
<tr>
<th>方案</th>
<th>说明</th>
<th>计算复杂度</th>
<th>精度</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Q5_K_S</strong></td>
<td><strong>"Small" 版本</strong>，更快但精度稍低</td>
<td>✅ 最低</td>
<td>❌ 较低</td>
</tr>
<tr>
<td><strong>Q5_K_M</strong></td>
<td><strong>"Medium" 版本</strong>，折中方案</td>
<td>🔄 适中</td>
<td>🔄 适中</td>
</tr>
<tr>
<td><strong>Q5_K_L</strong></td>
<td><strong>"Large" 版本</strong>，计算稍慢但精度高</td>
<td>❌ 较高</td>
<td>✅ 最佳</td>
</tr>
</tbody>
</table>
<p><strong>适用场景</strong></p>
<ul>
<li><strong>Q5_K_S</strong>：适用于 <strong>推理速度优先</strong> 的场景，例如 <strong>实时聊天机器人</strong> 或 <strong>低端设备</strong>。</li>
<li><strong>Q5_K_M</strong>：适用于 <strong>平衡精度和推理速度</strong> 的场景，是 <strong>最常用的 Q5_K 版本</strong>。</li>
<li><strong>Q5_K_L</strong>：适用于 <strong>对精度要求极高的 LLM 任务</strong>，如 <strong>科学计算、代码理解</strong> 等。</li>
</ul>
<h4 id="选择llm模型">选择LLM模型</h4>
<p>与嵌入模型类似，LLM模型也有自己的性能测试板块：<a href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/" target="_blank" rel="noopener nofollow">open-llm-leaderboard</a>，但该排行榜并未提供语言过滤功能，因此无法直接选择某个模型，还需要判断该模型是否支持中英文。</p>
<p>可以参考<a href="https://huggingface.co/spaces/BAAI/open_cn_llm_leaderboard" target="_blank" rel="noopener nofollow">Open Chinese LLM Leaderboard</a>。</p>
<p>也可以参考<a href="https://github.com/jeinlee1991/chinese-llm-benchmark?tab=readme-ov-file#-%E6%8E%92%E8%A1%8C%E6%A6%9C" target="_blank" rel="noopener nofollow">chinese-llm-benchmark</a>，给出了中文大模型能力评测榜单。例如，我们想要找5B以下的小模型，可以参考该<a href="https://github.com/jeinlee1991/chinese-llm-benchmark/blob/main/leaderboard/opensource1.md" target="_blank" rel="noopener nofollow">榜单</a>，前3名为：</p>
<ol>
<li><a href="https://huggingface.co/Qwen/Qwen2.5-3B-Instruct" target="_blank" rel="noopener nofollow">qwen2.5-3b-instruct</a></li>
<li><a href="https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct" target="_blank" rel="noopener nofollow">Llama-3.2-3B-Instruct</a></li>
<li><a href="https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct" target="_blank" rel="noopener nofollow">qwen2.5-1.5b-instruct</a></li>
</ol>
<p>⚠️在选择模型时，需要在huggingface上再次确认支持的语言，如上面的<code>Llama-3.2-3B-Instruct</code>模型，<a href="https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct" target="_blank" rel="noopener nofollow">官方</a>仅支持8种语言<em>English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported</em>，并没有中文!</p>
<p>这里我们选择的模型为<code>qwen2.5-3b-instruct</code>，根据公式，该模型大概占用的内存为2.55GB。</p>
<h3 id="下载模型">下载模型</h3>
<p>ℹ️也可以直接在Ollama的<a href="https://ollama.com/search?c=embedding&amp;o=newest" target="_blank" rel="noopener nofollow">模型库</a>中直接查找下载。</p>
<h4 id="下载llm">下载LLM</h4>
<pre><code class="language-sh">ollama run hf.co/Qwen/Qwen2.5-3B-Instruct-GGUF:Q5_K_M
</code></pre>
<h4 id="下载embedder">下载Embedder</h4>
<pre><code class="language-sh">ollama pull hf.co/second-state/gte-Qwen2-1.5B-instruct-GGUF:Q5_K_M
</code></pre>
<h3 id="下载anythingllm">下载AnythingLLM</h3>
<ul>
<li>
<p><a href="https://anythingllm.com/" target="_blank" rel="noopener nofollow">下载</a>并安装anythingLLM</p>
</li>
<li>
<p>连接Ollama：分别在anythingLLM的<code>LLM</code>和<code>Embedder</code>种选择Ollama，并将连接地址设置为正确的Ollama服务地址，Ollama的默认监听地址为<code>127.0.0.1:11434</code>(<em>设置为<code>http://localhost:11434</code>好像有问题</em>)。连接好后就可以自动加载模型：<br>
<img src="https://img2024.cnblogs.com/blog/1334952/202502/1334952-20250214105537120-1149611445.png" alt="image" style="zoom: 33%"></p>
<p>⚠️ 使用Ollama时，anythingLLM无法区分模型是LLM还是embedder，因此都会进行加载，即在<code>LLM</code>中出现嵌入模型，而在<code>Embedder</code>中出现LLM，需要手动选择正确的模型，将<code>LLM</code>设置为<code>hf.co/Qwen/Qwen2.5-3B-Instruct-GGUF:Q5_K_M</code>，将<code>Embedder</code>设置为<code>hf.co/second-state/gte-Qwen2-1.5B-instruct-GGUF:Q5_K_M</code>。</p>
<p>官方有如下<a href="https://docs.useanything.com/setup/embedder-configuration/local/ollama" target="_blank" rel="noopener nofollow">描述</a>：</p>
<blockquote>
<p><strong>Heads up!</strong></p>
<p>Ollama's <code>/models</code> endpoint will show both LLMs and Embedding models in the dropdown selection. <strong>Please</strong> ensure you are using an embedding model for embedding.</p>
<p><strong>llama2</strong> for example, is an LLM. Not an embedder.</p>
</blockquote>
</li>
</ul>
<h4 id="配置向量数据库">配置<a href="https://docs.anythingllm.com/features/vector-databases" target="_blank" rel="noopener nofollow">向量数据库</a></h4>
<p>这里就直接采用anythingLLM默认的本地数据库即可：</p>
<img src="https://img2024.cnblogs.com/blog/1334952/202502/1334952-20250214111022713-909031879.png" alt="image" style="zoom: 33%">
<h3 id="测试使用">测试使用</h3>
<p>简单测试一下模型是否生效：</p>
<img src="https://img2024.cnblogs.com/blog/1334952/202502/1334952-20250214125122571-2092548152.png" alt="image" style="zoom: 40%">
<p>现在嵌入一个文档，看是否可以根据嵌入的文档进行回答。将<code>Chat mode</code>设置为Query，这样模型只会根据嵌入的文档进行回答：</p>
<img src="https://img2024.cnblogs.com/blog/1334952/202502/1334952-20250214125314626-1846202931.png" alt="image" style="zoom: 33%">
<p>上传一个文档，文档里面包含一个自定义的成语："<em>空让弄饭是一个成语，意思是有空一起做饭，形容一个人心情好</em>"：</p>
<img src="https://img2024.cnblogs.com/blog/1334952/202502/1334952-20250214125410578-154964474.png" alt="image" style="zoom: 33%">
<p>测试结果如下：<br>
<img src="https://img2024.cnblogs.com/blog/1334952/202502/1334952-20250214141810021-1835797026.png" alt="image" style="zoom: 33%"></p>
<p>如果用<code>Chat</code>模式，其结果如下：</p>
<img src="https://img2024.cnblogs.com/blog/1334952/202502/1334952-20250214141842889-216446065.png" alt="image" style="zoom: 33%">
<h3 id="tips">Tips</h3>
<h4 id="reset向量数据库">Reset向量数据库</h4>
<img src="https://img2024.cnblogs.com/blog/1334952/202502/1334952-20250214115413636-587943269.png" alt="image" style="zoom: 33%">
<h4 id="llm没有使用自己的文档">LLM没有使用自己的文档</h4>
<p>官方给出了一些<a href="https://docs.anythingllm.com/llm-not-using-my-docs" target="_blank" rel="noopener nofollow">解决方式</a></p>
<ul>
<li>
<p>Vector Database Settings &gt; Search Preference中尝试使用<code>Accuracy Optimized</code>,</p>
<img src="https://img2024.cnblogs.com/blog/1334952/202502/1334952-20250214144551626-293976461.png" alt="image" style="zoom: 33%">
</li>
<li>
<p>将Document similarity threshold设置为<code>No Restriction</code>。该属性用于过滤掉可能与查询无关的低分向量数据，默认为20%：<br>
<img src="https://img2024.cnblogs.com/blog/1334952/202502/1334952-20250214144624433-1740101294.png" alt="image" style="zoom: 33%"></p>
</li>
</ul>
<h4 id="ai回答逻辑混乱">AI回答逻辑混乱</h4>
<p>可以适当将低模型的<a href="https://docs.useanything.com/llm-not-using-my-docs#chat-settings--llm-temperature" target="_blank" rel="noopener nofollow">temperature值</a>：<code>Chat Settings &gt; LLM Temperature</code></p>
<h4 id="ollama">Ollama</h4>
<h5 id="日志中出现告警或错误">日志中出现告警或错误</h5>
<p><strong>level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2097 keep=0 new=2048</strong></p>
<p>原因是Ollama默认使用2048的context window，<a href="https://github.com/ollama/ollama/issues/8099#issuecomment-2543316682" target="_blank" rel="noopener nofollow">解决方式</a>是增加模型的<code>num_ctx</code>值，但这种方式比较耗时。</p>
<p>另一种<a href="https://blog.driftingruby.com/ollama-context-window/" target="_blank" rel="noopener nofollow">方式</a>是使用<code>Modefile</code>，下面修改嵌入模型<code>hf.co/second-state/gte-Qwen2-1.5B-instruct-GGUF:Q5_K_M</code>的<code>num_ctx</code>为8192:</p>
<pre><code class="language-dockerfile">cat Modelfile
# Modelfile
FROM hf.co/second-state/gte-Qwen2-1.5B-instruct-GGUF:Q5_K_M
PARAMETER num_ctx 8196
</code></pre>
<p>执行<code>ollama create -f Modelfile gte-Qwen2-1.5B-instruct-GGUF:Q5_K_M</code>将创建一个新的嵌入模型，在AnythingLLM中加载该模型即可。<br>
⚠️重新加载模型之前需要删除嵌入的文档，并reset 向量数据库。</p>
<h5 id="命令行">命令行</h5>
<p>Ollama的命令行有点像docker，常用的命令如下：</p>
<ul>
<li>
<p>ollama server：启动一个ollama服务</p>
</li>
<li>
<p>ollama run：启动一个模型</p>
</li>
<li>
<p>ollama stop：停止一个模型</p>
</li>
<li>
<p>ollama ps：查看运行的模型</p>
</li>
<li>
<p>ollama pull：下载一个模型</p>
</li>
<li>
<p>ollama push：上传一个模型</p>
</li>
<li>
<p>ollama rm：删除一个模型</p>
</li>
<li>
<p>ollama list：查看下载的模型</p>
</li>
<li>
<p>ollama show：查看一个模型的信息</p>
</li>
</ul>
<h5 id="debug"><a href="https://github.com/ollama/ollama/blob/main/docs/troubleshooting.md" target="_blank" rel="noopener nofollow">Debug</a></h5>
<p>查看Ollama日志：<code>cat ~/.ollama/logs/server.log</code></p>
<h5 id="ollama-faq"><a href="https://github.com/ollama/ollama/blob/main/docs/faq.md" target="_blank" rel="noopener nofollow">Ollama FAQ</a></h5>
<h3 id="参考">参考</h3>
<ul>
<li><a href="https://www.substratus.ai/blog/calculating-gpu-memory-for-llm" target="_blank" rel="noopener nofollow">Calculating GPU memory for serving LLMs</a></li>
<li><a href="https://www.53ai.com/news/qianyanjishu/1427.html" target="_blank" rel="noopener nofollow">全民AI时代：手把手教你用Ollama &amp; AnythingLLM搭建AI知识库，无需编程，跟着做就行！</a></li>
<li>ollama支持的<a href="https://github.com/ollama/ollama?tab=readme-ov-file#model-library" target="_blank" rel="noopener nofollow">模型库</a></li>
</ul>
<h3 id="todo">TODO</h3>
<p>尝试一下其他大模型</p>

</div>
<div id="MySignature" role="contentinfo">
    <p>本文来自博客园，作者：<a href="https://www.cnblogs.com/charlieroro/" target="_blank">charlieroro</a>，转载请注明原文链接：<a href="https://www.cnblogs.com/charlieroro/p/18709638" target="_blank">https://www.cnblogs.com/charlieroro/p/18709638</a></p>
</div>
<div class="clear"></div>

		</div>
		<div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="1.042629544412037" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-02-14 19:22">2025-02-14 19:00</span>&nbsp;
<a href="https://www.cnblogs.com/charlieroro">charlieroro</a>&nbsp;
阅读(<span id="post_view_count">129</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18709638" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18709638);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18709638', targetLink: 'https://www.cnblogs.com/charlieroro/p/18709638', title: '使用Ollama和AnythingLLM搭建本地AI' })">举报</a>
</div>
	