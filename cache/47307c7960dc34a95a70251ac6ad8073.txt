
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/bricheersz/p/19025994" title="发布于 2025-08-06 22:31">
    <span role="heading" aria-level="2">你应该懂得AI大模型（十三） 之 推理框架</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body blogpost-body-html">
<p><span data-cke-copybin-start="1"><span data-cke-copybin-start="1">​</span></span></p>
<p><span id="cke_bm_640S">&nbsp;在大语言模型（LLM）技术爆发的今天，从 ChatGPT 到开源的 LLaMA、Qwen 系列，模型能力不断突破，但将这些 “智能大脑” 落地到实际业务中，却面临着效率、成本和部署复杂度的三重挑战。此时，大模型推理框架成为了连接理论与实践的关键桥梁。</span></p>
<h1 id="R3zj-1754488601275">一、什么是大模型推理框架</h1>
<p>大模型推理框架是专门优化预训练大模型 “推理阶段” 的工具集，专注于解决模型部署中的效率、成本和工程化问题。与训练框架（如 PyTorch、TensorFlow）不同，推理框架不参与模型参数的学习过程，而是聚焦于如何让训练好的模型在生产环境中更快速、更经济、更稳定地响应请求。​</p>
<p>简单来说，训练框架负责 “教会模型思考”，而推理框架负责 “让模型高效地回答问题”。</p>
<h1 id="9dBU-1754488619799">二、推理框架的核心作用</h1>
<p>推理框架的核心作用​</p>
<p>1、提升响应速度​</p>
<p>未经优化的大模型推理可能需要数秒甚至数十秒才能生成结果，而推理框架通过注意力机制优化（如 PagedAttention）、动态批处理等技术，可将延迟压缩至毫秒级，满足实时交互场景需求​</p>
<p>2、降低资源消耗​</p>
<p>大模型动辄数十亿甚至千亿参数，原生推理需占用数十 GB 显存。推理框架通过量化技术（如 INT4/INT8）、KV 缓存优化等手段，可将显存占用降低 50%-75%，同时支持单 GPU 部署更大模型。​</p>
<p>3、简化工程落地​</p>
<p>提供开箱即用的 API 服务（REST/gRPC）、负载均衡、动态扩缩容等功能，将复杂的分布式推理逻辑封装成简单接口，让开发者无需深入底层优化即可部署高可用服务。​</p>
<p>4、适配多样化场景​</p>
<p>支持云服务器、边缘设备、端侧终端等多环境部署，兼容 NVIDIA GPU、AMD GPU、CPU 甚至专用 AI 芯片（如昇腾），满足不同企业的硬件条件。</p>
<h1 id="UQHv-1754488646193">三、主流大模型推理框架</h1>
<p>目前市场上的推理框架可分为 “通用型”（适配多模型和硬件）和 “专用型”（针对特定场景深度优化），以下是企业级项目中最常用的几种：​</p>
<p>1. vLLM：高吞吐量的开源明星​</p>
<p>由 UC Berkeley 团队开发的 vLLM，凭借其创新的PagedAttention 技术（借鉴操作系统内存分页机制管理 KV 缓存）成为开源社区的焦点。​</p>
<p>核心优势：​</p>
<ul>
<li>
<p>支持动态批处理（Continuous Batching），吞吐量是原生 Hugging Face Transformers 的 10-20 倍；​</p>
</li>
</ul>
<ul>
<li>
<p>无缝兼容 Hugging Face 模型格式，LLaMA、Mistral、Qwen 等主流模型可直接部署；​</p>
</li>
</ul>
<ul>
<li>
<p>轻量级设计，单条命令即可启动服务，适合快速测试和生产部署。​</p>
</li>
</ul>
<p>适用场景：高并发 API 服务（如客服机器人、内容生成平台），尤其适合 NVIDIA GPU 环境。​</p>
<p>2. Text Generation Inference（TGI）：Hugging Face 生态的官方选择​</p>
<p>作为 Hugging Face 推出的推理框架，TGI 深度集成了 Transformers、Tokenizers 等工具链，是开源模型部署的 “嫡系” 方案。​</p>
<p>核心优势：​</p>
<ul>
<li>
<p>支持张量并行（多 GPU 拆分模型），轻松部署 13B/70B 等大模型；​</p>
</li>
</ul>
<ul>
<li>
<p>内置日志、监控和 A/B 测试工具，便于企业级运维；​</p>
</li>
</ul>
<ul>
<li>
<p>原生支持流式输出（Stream），优化对话交互体验。​</p>
</li>
</ul>
<p>适用场景：依赖 Hugging Face 生态的企业，或需要快速部署开源模型进行验证的场景。​</p>
<p>3. TensorRT-LLM：NVIDIA 硬件的性能王者​</p>
<p>NVIDIA 官方推出的 TensorRT-LLM 是基于 TensorRT 引擎的大模型推理优化框架，专为 NVIDIA GPU 打造。​</p>
<p>核心优势：​</p>
<ul>
<li>
<p>采用编译型优化（将模型转为 TensorRT 引擎），延迟比 vLLM 低 30%-50%；​</p>
</li>
</ul>
<ul>
<li>
<p>支持 INT4/INT8 量化和稀疏性优化，在 H100/A100 等高端 GPU 上性能极致；​</p>
</li>
</ul>
<ul>
<li>
<p>提供 C++/Python 接口，支持多模型并行策略。​</p>
</li>
</ul>
<p>适用场景：对延迟敏感的工业级场景（如金融风控、实时对话），需依赖 NVIDIA GPU。​</p>
<p>4. LMDeploy：轻量高效的多场景适配者​</p>
<p>&nbsp;LMDeploy 以 “轻量、高效、多场景兼容” 为特色，尤其对国产模型支持友好。​</p>
<p>核心优势：​</p>
<ul>
<li>
<p>支持 INT4/INT8/FP16 多种精度，可在消费级 GPU（如 RTX 3090）甚至 CPU 上部署；​</p>
</li>
</ul>
<ul>
<li>
<p>提供模型转换、量化、服务部署全链路工具，降低工程门槛；​</p>
</li>
</ul>
<ul>
<li>
<p>适配 Qwen、LLaMA、Baichuan 等主流模型，国产模型优化更精细。​</p>
</li>
</ul>
<p>适用场景：资源受限的边缘设备、多模型混合部署场景，或国产模型优先的企业。​</p>
<p>5. DeepSpeed-Inference：超大规模模型的分布式专家​</p>
<p>由 Microsoft 和华盛顿大学联合开发的 DeepSpeed-Inference，专为千亿级参数模型设计。​</p>
<p>核心优势：​</p>
<ul>
<li>
<p>支持张量并行、管道并行等多种分布式策略，可拆分 100B + 参数模型；​</p>
</li>
</ul>
<ul>
<li>
<p>集成 ZeRO 优化技术，减少多 GPU 通信开销；​</p>
</li>
</ul>
<ul>
<li>
<p>兼容 PyTorch 生态，便于与训练流程衔接。​</p>
</li>
</ul>
<p>适用场景：需要部署超大规模模型（如 GPT-3 175B、LLaMA 70B）的企业，需多 GPU 集群支持。</p>
<h1 id="j7ol-1754488941192">四、推理框架的核心差异与选型维度</h1>
<p>不同框架的差异主要体现在技术路线、硬件适配和功能侧重上，企业选型时需关注以下维度：​</p>
<p>1. 性能指标：延迟 vs 吞吐量​</p>
<ul>
<li>
<p>低延迟优先：TensorRT-LLM（编译优化）&gt; vLLM（PagedAttention）&gt; TGI；​</p>
</li>
</ul>
<ul>
<li>
<p>高吞吐量优先：vLLM（动态批处理）&gt; TGI &gt; LMDeploy；​</p>
</li>
</ul>
<ul>
<li>
<p>显存效率：LMDeploy（量化优化）&gt; TensorRT-LLM（INT4）&gt; vLLM。​</p>
</li>
</ul>
<p>2. 硬件兼容性​</p>
<ul>
<li>
<p>NVIDIA GPU 专属：TensorRT-LLM（性能最佳）、vLLM（兼容性好）；​</p>
</li>
</ul>
<ul>
<li>
<p>跨硬件支持：LMDeploy（CPU/GPU/ 边缘设备）、ONNX Runtime（多芯片适配）；​</p>
</li>
</ul>
<ul>
<li>
<p>国产芯片适配：LMDeploy（昇腾支持）、ONNX Runtime（寒武纪 / 地平线插件）。​</p>
</li>
</ul>
<p>3. 模型兼容性​</p>
<ul>
<li>
<p>开源模型全覆盖：TGI（Hugging Face 生态）、vLLM（主流模型）；​</p>
</li>
</ul>
<ul>
<li>
<p>国产模型优化：LMDeploy（Qwen/Baichuan）&gt; TGI &gt; TensorRT-LLM；​</p>
</li>
</ul>
<ul>
<li>
<p>超大规模模型：DeepSpeed-Inference（100B + 参数）&gt; TensorRT-LLM（张量并行）。​</p>
</li>
</ul>
<p>4. 部署复杂度​</p>
<ul>
<li>
<p>快速上手：vLLM（单命令启动）、TGI（Docker 一键部署）；​</p>
</li>
</ul>
<ul>
<li>
<p>工业级部署：TensorRT-LLM（需编译模型）、DeepSpeed-Inference（分布式配置）；​</p>
</li>
</ul>
<ul>
<li>
<p>轻量部署：LMDeploy（资源占用低）、ONNX Runtime（跨平台打包）。</p>
</li>
</ul>
<h1 id="bPbl-1754488955464">五、使用LMdeploy部署模型</h1>
<p>笔者使用的是Autodl服务器，因此需要做个SSH隧道。</p>
<h2 id="x1Vz-1754488545969">5.1配置Conda环境</h2>
<div class="cke_widget_wrapper cke_widget_block cke_widget_codeSnippet cke_widget_selected" data-cke-display-name="代码段" data-cke-filter="off" data-cke-widget-id="9" data-cke-widget-wrapper="1">
<pre class="cke_widget_element highlighter-hljs" data-cke-widget-data="%7B%22lang%22%3A%22bash%22%2C%22code%22%3A%22conda%20create%20-n%20lmdeploy%20python%3D3.10%20-y%5Cnconda%20activate%20lmdeploy%5Cnpip%20install%20lmdeploy%20%20modelscope%20gradio%20partial-json-parser%5Cnexport%20LMDEPLOY_USE_MODELSCOPE%3DTrue%22%2C%22classes%22%3Anull%7D" data-cke-widget-keep-attr="0" data-cke-widget-upcasted="1" data-widget="codeSnippet"><code>conda create -n lmdeploy python=3.10 -y
conda activate lmdeploy
pip install lmdeploy  modelscope gradio partial-json-parser
export LMDEPLOY_USE_MODELSCOPE=True</code></pre>
<span class="cke_reset cke_widget_drag_handler_container"><img width="15" height="15" class="cke_reset cke_widget_drag_handler lazyload" data-cke-widget-drag-handler="1" data-src="https://img2024.cnblogs.com/blog/1462902/202508/1462902-20250806222907697-661519337.gif"></span></div>
<p>验证lmdeploy安装--</p>
<div class="cke_widget_wrapper cke_widget_block cke_widget_codeSnippet cke_widget_selected" data-cke-display-name="代码段" data-cke-filter="off" data-cke-widget-id="8" data-cke-widget-wrapper="1">
<pre class="cke_widget_element highlighter-hljs" data-cke-widget-data="%7B%22code%22%3A%22lmdeploy%20--version%22%2C%22classes%22%3Anull%7D" data-cke-widget-keep-attr="0" data-cke-widget-upcasted="1" data-widget="codeSnippet"><code>lmdeploy --version</code></pre>
<span class="cke_reset cke_widget_drag_handler_container"><img width="15" height="15" class="cke_reset cke_widget_drag_handler lazyload" data-cke-widget-drag-handler="1" data-src="https://img2024.cnblogs.com/blog/1462902/202508/1462902-20250806222907697-661519337.gif"></span></div>
<h2 id="bszC-1754488550415">5.2下载模型</h2>
<div class="cke_widget_wrapper cke_widget_block cke_widget_codeSnippet cke_widget_selected" data-cke-display-name="代码段" data-cke-filter="off" data-cke-widget-id="7" data-cke-widget-wrapper="1">
<pre class="cke_widget_element highlighter-hljs" data-cke-widget-data="%7B%22lang%22%3A%22bash%22%2C%22code%22%3A%22%23%E8%BF%99%E4%BA%9B%E5%BA%93%E5%A4%A7%E5%AE%B6%E7%BC%BA%E5%95%A5%E8%A3%85%E5%95%A5%E5%B0%B1%E5%8F%AF%E4%BB%A5%EF%BC%8C%E4%B8%8D%E6%98%AF%E9%83%BD%E5%BE%97%E8%A3%85%EF%BC%8C%E7%9C%8B%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%8F%90%E7%A4%BA%E7%BC%BA%E4%BB%80%E4%B9%88%E3%80%82%5Cnpip%20install%20modelscope%5Cnpip%20install%20openai.%5Cnpip%20install%20tqdm.%5Cnpip%20install%20transformers%5Cnpip%20install%20vllm%22%2C%22classes%22%3Anull%7D" data-cke-widget-keep-attr="0" data-cke-widget-upcasted="1" data-widget="codeSnippet"><code>#这些库大家缺啥装啥就可以，不是都得装，看服务器提示缺什么。
pip install modelscope
pip install openai.
pip install tqdm.
pip install transformers
pip install vllm</code></pre>
</div>
<div class="cke_widget_wrapper cke_widget_block cke_widget_codeSnippet cke_widget_selected" data-cke-display-name="代码段" data-cke-filter="off" data-cke-widget-id="6" data-cke-widget-wrapper="1">
<pre class="cke_widget_element highlighter-hljs" data-cke-widget-data="%7B%22lang%22%3A%22python%22%2C%22code%22%3A%22%23%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD%5Cnfrom%20modelscope%20import%20snapshot_download%5Cnmodel_dir%20%3D%20snapshot_download('Qwen%2FQwen3-4B'%2Ccache_dir%3D%5C%22%2Froot%2Fautodl-tmp%2Fddq%5C%22)%22%2C%22classes%22%3Anull%7D" data-cke-widget-keep-attr="0" data-cke-widget-upcasted="1" data-widget="codeSnippet"><code>#模型下载
from modelscope import snapshot_download
model_dir = snapshot_download('Qwen/Qwen3-4B',cache_dir="/root/autodl-tmp/ddq")</code></pre>
</div>
<h2 id="Gav1-1754488553666">5.3模型转换</h2>
<p>LMDeploy 推荐使用&nbsp;<code>convert</code>&nbsp;命令将模型转换为高效推理格式：</p>
<div class="cke_widget_wrapper cke_widget_block cke_widget_codeSnippet cke_widget_selected" data-cke-display-name="代码段" data-cke-filter="off" data-cke-widget-id="5" data-cke-widget-wrapper="1">
<pre class="cke_widget_element highlighter-hljs" data-cke-widget-data="%7B%22lang%22%3A%22bash%22%2C%22code%22%3A%22lmdeploy%20convert%20qwen3%20Qwen3-7B-Chat%20--dst-path%20qwen3-7b-chat-lmdeploy%22%2C%22classes%22%3Anull%7D" data-cke-widget-keep-attr="0" data-cke-widget-upcasted="1" data-widget="codeSnippet"><code>lmdeploy convert qwen3 Qwen3-7B-Chat --dst-path qwen3-7b-chat-lmdeploy</code></pre>
<span class="cke_reset cke_widget_drag_handler_container"><img width="15" height="15" class="cke_reset cke_widget_drag_handler lazyload" data-cke-widget-drag-handler="1" data-src="https://img2024.cnblogs.com/blog/1462902/202508/1462902-20250806222907697-661519337.gif"></span></div>
<p>转换后的模型将存储在&nbsp;<code>qwen3-7b-chat-lmdeploy</code>&nbsp;目录，包含优化后的权重和配置文件。</p>
<p>此项非必选。</p>
<h2 id="zyek-1754488556548">5.4启动推理服务</h2>
<div class="cke_widget_wrapper cke_widget_block cke_widget_codeSnippet cke_widget_selected" data-cke-display-name="代码段" data-cke-filter="off" data-cke-widget-id="4" data-cke-widget-wrapper="1">
<pre class="cke_widget_element highlighter-hljs" data-cke-widget-data="%7B%22lang%22%3A%22bash%22%2C%22code%22%3A%22lmdeploy%20serve%20api_server%20%2Froot%2Fautodl-tmp%2Fddq%2FQwen%2FQwen3-4B%20--reasoning-parser%20qwen-qwq%20--server-port%2023333%20--session-len%208192%22%2C%22classes%22%3Anull%7D" data-cke-widget-keep-attr="0" data-cke-widget-upcasted="1" data-widget="codeSnippet"><code>lmdeploy serve api_server /root/autodl-tmp/ddq/Qwen/Qwen3-4B --reasoning-parser qwen-qwq --server-port 23333 --session-len 8192</code></pre>
<span class="cke_reset cke_widget_drag_handler_container"><img width="15" height="15" class="cke_reset cke_widget_drag_handler lazyload" data-cke-widget-drag-handler="1" data-src="https://img2024.cnblogs.com/blog/1462902/202508/1462902-20250806222907697-661519337.gif"></span></div>
<ul>
<li>
<p><code>mdeploy serve api_server</code>：LMDeploy 的核心命令，用于启动 API 服务端</p>
</li>
<li>
<p><code>/tmp/code/models/Qwen3-0.6B</code>：指定 Qwen3-0.6B 模型的本地路径</p>
</li>
<li>
<p><code>--reasoning-parser qwen-qwq</code>：指定使用 Qwen 系列的&nbsp;<code>qwq</code>&nbsp;推理解析器，适配 Qwen 模型的对话格式和推理逻辑</p>
</li>
<li>
<p><code>--server-port 23333</code>：设置 API 服务监听的端口为 23333（默认通常是 8000）</p>
</li>
<li>
<p><code>--session-len 8192</code>：设置会话上下文长度为 8192 tokens，控制模型能处理的历史对话 + 当前查询的总长度</p>
<p><br>执行这条命令后，LMDeploy 会加载指定的 Qwen3-0.6B 模型，并在本地 23333 端口启动一个 HTTP API 服务，你可以通过发送 HTTP 请求与模型进行交互。</p>

</li>

</ul>
<h3 id="RPYK-1754488559096">5.4.1 生产环境部署（本文实验未使用这种部署方式）</h3>
<h4 id="NNzm-1754488560899">1、docker部署</h4>
<div class="cke_widget_wrapper cke_widget_block cke_widget_codeSnippet cke_widget_selected" data-cke-display-name="代码段" data-cke-filter="off" data-cke-widget-id="3" data-cke-widget-wrapper="1">
<pre class="cke_widget_element highlighter-hljs" data-cke-widget-data="%7B%22lang%22%3A%22bash%22%2C%22code%22%3A%22%23%20%E6%8B%89%E5%8F%96%20LMDeploy%20%E9%95%9C%E5%83%8F%5Cndocker%20pull%20kube-ai-registry.cn-shanghai.cr.aliyuncs.com%2Fkube-ai%2Flmdeploy%3Alatest%5Cn%5Cn%23%20%E5%90%AF%E5%8A%A8%E5%AE%B9%E5%99%A8%5Cndocker%20run%20-d%20--gpus%20all%20%5C%5C%5Cn%20%20-p%208000%3A8000%20%5C%5C%5Cn%20%20-v%20%2Fpath%2Fto%2Fqwen3-7b-chat-lmdeploy%3A%2Fmodel%20%5C%5C%5Cn%20%20kube-ai-registry.cn-shanghai.cr.aliyuncs.com%2Fkube-ai%2Flmdeploy%20%5C%5C%5Cn%20%20lmdeploy%20serve%20api_server%20%2Fmodel%20--server-port%208000%22%2C%22classes%22%3Anull%7D" data-cke-widget-keep-attr="0" data-cke-widget-upcasted="1" data-widget="codeSnippet"><code># 拉取 LMDeploy 镜像
docker pull kube-ai-registry.cn-shanghai.cr.aliyuncs.com/kube-ai/lmdeploy:latest

# 启动容器
docker run -d --gpus all \
  -p 8000:8000 \
  -v /path/to/qwen3-7b-chat-lmdeploy:/model \
  kube-ai-registry.cn-shanghai.cr.aliyuncs.com/kube-ai/lmdeploy \
  lmdeploy serve api_server /model --server-port 8000</code></pre>
<span class="cke_reset cke_widget_drag_handler_container"><img width="15" height="15" class="cke_reset cke_widget_drag_handler lazyload" data-cke-widget-drag-handler="1" data-src="https://img2024.cnblogs.com/blog/1462902/202508/1462902-20250806222907697-661519337.gif"></span></div>
<h4 id="wKsR-1754488564358">2、在 Kubernetes 中部署（使用 Arena）</h4>
<div class="cke_widget_wrapper cke_widget_block cke_widget_codeSnippet cke_widget_selected" data-cke-display-name="代码段" data-cke-filter="off" data-cke-widget-id="2" data-cke-widget-wrapper="1">
<pre class="cke_widget_element highlighter-hljs" data-cke-widget-data="%7B%22lang%22%3A%22bash%22%2C%22code%22%3A%22arena%20serve%20custom%20%5C%5C%5Cn%20%20--name%3Dlmdeploy-qwen3%20%5C%5C%5Cn%20%20--version%3Dv1%20%5C%5C%5Cn%20%20--gpus%3D1%20%5C%5C%5Cn%20%20--replicas%3D1%20%5C%5C%5Cn%20%20--restful-port%3D8000%20%5C%5C%5Cn%20%20--readiness-probe-action%3D%5C%22tcpSocket%5C%22%20%5C%5C%5Cn%20%20--readiness-probe-action-option%3D%5C%22port%3A%208000%5C%22%20%5C%5C%5Cn%20%20--readiness-probe-option%3D%5C%22initialDelaySeconds%3A%2060%5C%22%20%5C%5C%20%20%23%20Qwen3%E5%90%AF%E5%8A%A8%E8%BE%83%E6%85%A2%EF%BC%8C%E5%BB%B6%E9%95%BF%E5%88%9D%E5%A7%8B%E6%A3%80%E6%9F%A5%E6%97%B6%E9%97%B4%5Cn%20%20--readiness-probe-option%3D%5C%22periodSeconds%3A%2030%5C%22%20%5C%5C%5Cn%20%20--image%3Dkube-ai-registry.cn-shanghai.cr.aliyuncs.com%2Fkube-ai%2Flmdeploy%3Alatest%20%5C%5C%5Cn%20%20--data%3Dqwen3-model%3A%2Fmodel%2FQwen3-7B-Chat%20%5C%5C%20%20%23%20%E6%8C%82%E8%BD%BD%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%8D%B7%5Cn%20%20%5C%22lmdeploy%20serve%20api_server%20%2Fmodel%2FQwen3-7B-Chat%20--server-port%208000%5C%22%22%2C%22classes%22%3Anull%7D" data-cke-widget-keep-attr="0" data-cke-widget-upcasted="1" data-widget="codeSnippet"><code>arena serve custom \
  --name=lmdeploy-qwen3 \
  --version=v1 \
  --gpus=1 \
  --replicas=1 \
  --restful-port=8000 \
  --readiness-probe-action="tcpSocket" \
  --readiness-probe-action-option="port: 8000" \
  --readiness-probe-option="initialDelaySeconds: 60" \  # Qwen3启动较慢，延长初始检查时间
  --readiness-probe-option="periodSeconds: 30" \
  --image=kube-ai-registry.cn-shanghai.cr.aliyuncs.com/kube-ai/lmdeploy:latest \
  --data=qwen3-model:/model/Qwen3-7B-Chat \  # 挂载模型数据卷
  "lmdeploy serve api_server /model/Qwen3-7B-Chat --server-port 8000"</code></pre>
<span class="cke_reset cke_widget_drag_handler_container"><img width="15" height="15" class="cke_reset cke_widget_drag_handler lazyload" data-cke-widget-drag-handler="1" data-src="https://img2024.cnblogs.com/blog/1462902/202508/1462902-20250806222907697-661519337.gif"></span></div>
<h2 id="xFuY-1754488567771">5.4.2 通过MobaXterm建立SSH隧道</h2>
<p>笔者喜欢在vscode中使用remote插件直接进行端口转发，因为笔者没有那么多服务器管理的需求，这个习惯因人而已。</p>
<p><span class="cke_widget_wrapper cke_widget_inline cke_widget_image cke_image_nocaption cke_widget_selected" data-cke-display-name="图像" data-cke-filter="off" data-cke-widget-id="1" data-cke-widget-wrapper="1"><img alt="" class="cke_widget_element lazyload" data-cke-saved-src="https://i-blog.csdnimg.cn/direct/3827fc235b224210b54ea5bf541c624f.png" data-cke-widget-data="%7B%22hasCaption%22%3Afalse%2C%22src%22%3A%22https%3A%2F%2Fi-blog.csdnimg.cn%2Fdirect%2F3827fc235b224210b54ea5bf541c624f.png%22%2C%22alt%22%3A%22%22%2C%22width%22%3A%22%22%2C%22height%22%3A%22%22%2C%22lock%22%3Atrue%2C%22align%22%3A%22none%22%2C%22classes%22%3Anull%7D" data-cke-widget-keep-attr="0" data-cke-widget-upcasted="1" data-widget="image" data-src="https://i-blog.csdnimg.cn/direct/3827fc235b224210b54ea5bf541c624f.png"><img alt="局部截取_20250806_211417" data-src="https://img2024.cnblogs.com/blog/1462902/202508/1462902-20250806223008548-1620486294.png" class="lazyload"><span class="cke_reset cke_widget_drag_handler_container"><span class="cke_image_resizer" title="点击并拖拽以改变尺寸"><span class="cke_widget_edit_container" title="编辑图片"><br></span></span></span></span></p>
<h2 id="daNc-1754488571372">5.4.3 在CherryStudio中测试隧道效果</h2>
<p><span class="cke_widget_wrapper cke_widget_inline cke_widget_image cke_image_nocaption cke_widget_selected" data-cke-display-name="图像" data-cke-filter="off" data-cke-widget-id="0" data-cke-widget-wrapper="1"><span class="cke_reset cke_widget_drag_handler_container"><img width="15" height="15" class="cke_reset cke_widget_drag_handler lazyload" data-cke-widget-drag-handler="1" data-src="https://img2024.cnblogs.com/blog/1462902/202508/1462902-20250806222907697-661519337.gif"><img alt="image" data-src="https://img2024.cnblogs.com/blog/1462902/202508/1462902-20250806223031452-1402094353.png" class="lazyload"></span></span></p>
<p><span data-cke-copybin-start="1"><span data-cke-copybin-end="1">​</span></span></p>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.9027777777777778" data-date-updated="2025-08-07 20:11">2025-08-06 22:31</span>&nbsp;
<a href="https://www.cnblogs.com/bricheersz">BricheersZ</a>&nbsp;
阅读(<span id="post_view_count">243</span>)&nbsp;
评论(<span id="post_comment_count">2</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(19025994);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '19025994', targetLink: 'https://www.cnblogs.com/bricheersz/p/19025994', title: '你应该懂得AI大模型（十三） 之 推理框架' })">举报</a>
</div>
        