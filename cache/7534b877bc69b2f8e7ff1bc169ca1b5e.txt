
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/kelvin-cai/p/18906222" title="发布于 2025-06-01 16:30">
    <span role="heading" aria-level="2">Spring Ai 从Demo到搭建套壳项目（一）初识与实现与deepseek对话模式</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h1 id="前言">前言</h1>
<p>为什么说Java长青，主要是因为其生态圈完善，Spring又做了一款脚手架，把对接各个LLM厂商的sdk做了一遍，形成一系列的spring-ai-starter-** 的依赖。 目前为止版本去到1.0.0.M6，golang跟不上了吧， <em><strong>Make Java Greate Again!!</strong></em></p>
<p>我打算这个系列介绍这个spring-ai-starter和各个LLM的关系，介绍实际操作，演示一下官网的一些关键点和没讲到的细节，还有后续会讲如何使用spring-ai搭建一个套壳项目（啥是套壳项目下一章会讲），从后端，到spring-ai对接，到前端的制作。比较希望大家已经对LLM有些基础的理解。</p>
<h2 id="一技术框架">一、技术框架</h2>
<h3 id="如果没有框架你需要做什么">如果没有框架，你需要做什么？</h3>
<ol>
<li>你要自己写http调用代码，来分别对各个LLM模型接口（或者SDK，例如<a href="https://api-docs.deepseek.com/zh-cn/api/create-chat-completion/%EF%BC%89" target="_blank" rel="noopener nofollow">https://api-docs.deepseek.com/zh-cn/api/create-chat-completion/）</a> 的请求，等待他结果的返回，解析响应。spring-ai就同一个了接口。</li>
<li>还有支持Advisor，就像面向切面那样，发送请求前，检查文本有没有命中禁用词，就例如企业不允许把代码透露出去，也不允许使用某些黑词，就可以这里检查。</li>
<li>还有支持MCP调用能力，如果不对接spring-ai，你就要自己实现MCP协议的调用代码，来达到调用别的服务。例如：后续会实现的使用高德地图mcp，得到高德查看坐标工具，交通工具，路线规划工具，天气工具。</li>
<li>支持会话跟踪，如果你不接spring-ai，你还需要自己记录会话到表，或者让前端把说过的话，一次过传给后端，后端再告诉ai来做context上下文跟踪。</li>
<li>RAG检索增强，可以通过他告诉ai又额外的一套文本，让某个LLM分析这个文本，得到你想要的答案。我觉得这个点可以用于客服话术，先拟定好话术，通过RAG解析话术的文本，当用户跟ai对话的时候，附带这个解析后的spring-ai的Document类对象，得到话术的结果。例如：设定话术用户发送一串数字，就回复“这个订单有什么问题？”</li>
</ol>
<h3 id="那目前市面上有什么框架呢">那目前市面上有什么框架呢？</h3>
<p><img src="https://p0-xtjj-private.juejin.cn/tos-cn-i-73owjymdk6/be14bc802ffe42f4bed4db9ac687f571~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5Zyw6JePS2Vsdmlu:q75.awebp?policy=eyJ2bSI6MywidWlkIjoiMTA0NjM5MDgwMTQ0NDYxNSJ9&amp;rk3s=f64ab15b&amp;x-orig-authkey=f32326d3454f2ac7e96d3d06cdbb035152127018&amp;x-orig-expires=1749370537&amp;x-orig-sign=%2FRuW0Oyrbbhma3YoP29V6h5gmG0%3D" alt="spring ai.jpg" loading="lazy"></p>
<ol>
<li>目前看到的java相关的有org.springframework.ai这个group下的</li>
</ol>

<pre><code>spring-ai-starter-**
</code></pre>
<p>这个前缀的依赖。其后缀包括open-ai, aliyun , deepseek, anthropic等，他就是整合了对多个LLM厂商的接口，统一封装，底层还是调用各个LLM厂商的SDK。</p>
<p>那为什么spring要封装呢？其实你看名字叫starter就知道了，一般叫starter的都是脚手架，就如springcloud，springboot那样，通过yml或者properties的配置，来实现自动装配，生成一个springbean，<br>
方便你去调用。</p>
<pre><code>spring:
  ai:
    openai:
      api-key: ${你的key}
      base-url: ${请求openai的地址}
    deepseek:
      api-key: ${你的key}
      base-url: "https://api.deepseek.com"
      chat:
        options:
          model: deepseek-chat

</code></pre>
<p>如这里就固定了层级格式，让你填写地址和key就可以了。</p>
<ol start="2">
<li>还有alibaba提供的框架，他基于上面这个依赖做的封装，<strong>这种就是套壳</strong>。</li>
</ol>
<p>大家有没有发现上面代码需要对每个大预言模型都生成分别的api-key，所以就有人就想自己做一个统一的使用各个LLM地方，就如Cursor、Pandora，你看这些软件上，可以访问各个LLM的模型而不用对各个模型分别付费，这种统一地方的软件就是给钱包月或者按次数买1个apikey，就可以用各个LLM模型了</p>
<p>所以spring-ai-alibaba-stater-**系列，也是要去阿里百炼（旧名灵积）上生成一个key，就可以去使用不同的模型的一款套壳框架（<a href="https://bailian.console.aliyun.com/%EF%BC%89%EF%BC%8C" target="_blank" rel="noopener nofollow">https://bailian.console.aliyun.com/），</a> 现在也是免费的，但他可以给你免费使用这么多个LLM也是有成本的，以后还是得收费。</p>
<h1 id="二现在就演示一下如何对接spring-ai并使用deepseek模型进行对话">二、现在就演示一下，如何对接spring-ai，并使用deepseek模型进行对话</h1>
<blockquote>
<p>当前先了解spring-ai这个基础框架，第三章再来演示如何对接spring-ai-alibaba版的。因为spring-ai-alibaba现在免费，你当前可以基于这个套壳框架，再套壳后，给商用起来，但是当alibaba开始收费，你又要对接各个厂商了，而且某些公司还不能用alibaba的api-key，所以spring-ai这个基础框架还是要了解的。</p>
</blockquote>
<h2 id="建立maven-工程">建立maven 工程</h2>
<p>引入依赖，一定要使用jdk 17 ，因为springboot已经3了</p>
<pre><code class="language-xml">&lt;properties&gt;
    &lt;maven.compiler.source&gt;17&lt;/maven.compiler.source&gt;
    &lt;maven.compiler.target&gt;17&lt;/maven.compiler.target&gt;
    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
&lt;/properties&gt;

&lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
        &lt;version&gt;3.3.4&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.ai&lt;/groupId&gt;
        &lt;artifactId&gt;spring-ai-starter-model-deepseek&lt;/artifactId&gt;
        &lt;version&gt;1.0.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-webflux&lt;/artifactId&gt; 
        &lt;version&gt;3.3.4&lt;/version&gt;
    &lt;/dependency&gt;

&lt;/dependencies&gt;
</code></pre>
<p><artifactid>spring-boot-starter-webflux</artifactid>这个东西就是响应式接口的关键，没有他，就没有持续输出文字的效果 <artifactid>spring-ai-starter-model-deepseek</artifactid> 这个就是spring对deepseek的封装。</p>
<h2 id="配置applicationxml">配置application.xml</h2>
<pre><code>server:
  port: 8081

spring:
  application:
  name: spring-ai-deepseek-chat-model-example
  ai:
    deepseek: ## 这一行是你选择的LLM模型，如果是openai，这里就填openai， base-url就是填对应厂商的地址
      api-key: ${你申请的apikey}
      base-url: "https://api.deepseek.com"
      chat:
        options:
          model: deepseek-chat
      embedding:
        enabled: false
</code></pre>
<h2 id="创建主类">创建主类</h2>
<pre><code>@SpringBootApplication
public class Application {
    public static void main(String[] args) {
        SpringApplication.run(Application.class,args);
    }
}
</code></pre>
<h2 id="创建controller">创建Controller</h2>
<pre><code>@RequestMapping("/openai")
@ResponseBody
@Controller
public class DeepSeekChatModelController {

    private final ChatModel deepSeekChatModel;
    
    // 主要就是这个地方，springboot已经把yml里的配置，生成好一个叫ChatModel的bean，注入进来controller里就可以直接使用了
    public DeepSeekChatModelController(ChatModel chatModel) {
        this.deepSeekChatModel = chatModel;
    }

    // 这个是同步等待LLM的结果，再回复给前端。
    @GetMapping("/simple/chat/{prompt}")
    public String simpleChat (@PathVariable(value = "prompt") String prompt) {

        return deepSeekChatModel.call(new Prompt(prompt)).getResult().getOutput().getText();
    }
    /**
     * Stream 流式调用。可以使大模型的输出信息实现打字机效果。
     * 这个就是sse方式回复内容给前端，就不用等所有的内容都收到才给前端
     * @return Flux&lt;String&gt; types.
     */
    @GetMapping("/stream/chat/{prompt}")
    public Flux&lt;String&gt; streamChat (@PathVariable(value = "prompt") String prompt,HttpServletResponse response) {
        response.setCharacterEncoding("UTF-8");
        Flux&lt;ChatResponse&gt; stream = deepSeekChatModel.stream(new Prompt(prompt));
        return stream.map(resp -&gt; resp.getResult().getOutput().getText());
    }
}
</code></pre>
<h2 id="实验结果">实验结果</h2>
<p>通过请求与deepseek对话，用localhost:8081/openai/simple/chat/或者localhost:8081/openai/stream/chat/ 两个接口都是可以的。</p>
<p><img src="https://p0-xtjj-private.juejin.cn/tos-cn-i-73owjymdk6/add6a3077d974c1db5b462385b69af00~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5Zyw6JePS2Vsdmlu:q75.awebp?policy=eyJ2bSI6MywidWlkIjoiMTA0NjM5MDgwMTQ0NDYxNSJ9&amp;rk3s=f64ab15b&amp;x-orig-authkey=f32326d3454f2ac7e96d3d06cdbb035152127018&amp;x-orig-expires=1749370537&amp;x-orig-sign=pcF9rdois587ftQZsNx4U6Frdm8%3D" alt="image.png" loading="lazy"></p>
<h1 id="三-但是大家有没有发现拿不到实时数据而是只教你怎么去拿实时数据">三、 但是大家有没有发现，拿不到实时数据，而是只教你怎么去拿实时数据</h1>
<p>因为deepseek只是一个文本类搜索和推荐的工具，他的数据是一年以前的搜索库里的数据，不是最新的。<br>
那么要如何获取最新的数据呢？请看下一章。</p>
<p>公————地藏思维</p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.7835470170567129" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-06-01 16:30">2025-06-01 16:30</span>&nbsp;
<a href="https://www.cnblogs.com/kelvin-cai">地藏Kelvin</a>&nbsp;
阅读(<span id="post_view_count">50</span>)&nbsp;
评论(<span id="post_comment_count">2</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18906222);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18906222', targetLink: 'https://www.cnblogs.com/kelvin-cai/p/18906222', title: 'Spring Ai 从Demo到搭建套壳项目（一）初识与实现与deepseek对话模式' })">举报</a>
</div>
        