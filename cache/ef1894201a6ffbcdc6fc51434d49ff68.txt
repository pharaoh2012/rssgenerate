
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/KendoCross/p/18920456" title="发布于 2025-06-09 11:35">
    <span role="heading" aria-level="2">AI时代Hello World详细教程之LLM微调(SFT)</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body blogpost-body-html">
<p><strong>【00】时代变了</strong></p>
<p>　　移动互联网时代的Hello World(参见<a id="cb_post_title_url" class="postTitle2 vertical-middle" title="发布于 2015-05-08 19:49" href="https://www.cnblogs.com/KendoCross/p/4488706.html">Xamarin 使用极光推送 详细教程</a>&nbsp;)，安装某一套开发工具包(IDE)就够了，AI时代就明显要的就多了。</p>
<p>　　依然直奔主题，无废话，直接上实操步骤。微调基础LLM，使之支持普通话翻译为粤语。</p>
<p>　　</p>
<p><strong>【01】名词解释</strong></p>
<p>　　<img src="https://img2024.cnblogs.com/blog/746628/202506/746628-20250609100736193-987933741.jpg" alt=""></p>
<p><strong>【02】无中生GPU</strong></p>
<p><strong>　　</strong>AI时代没有GPU寸步难行，如何薅到GPU呢？</p>
<p><strong>　　<a href="https://www.kaggle.com/" target="_blank" rel="noopener nofollow">https://www.kaggle.com/&nbsp;&nbsp;</a></strong>&nbsp; 自行注册，通过手机号验证之后，每周可以薅30H的GPU资源。</p>
<p><strong>【03】环境初始化</strong></p>
<p><strong>　　</strong>新建Notebook，就当是VS的项目对待了。</p>
<p><img src="https://img2024.cnblogs.com/blog/746628/202506/746628-20250609102533736-918616019.png" alt="" loading="lazy"></p>
<p>&nbsp;　　初始化基础环境，依赖的python库。</p>
<div class="cnblogs_code">
<pre>%pip <span style="color: rgba(0, 0, 255, 1)">install</span> --no-deps bitsandbytes accelerate xformers==<span style="color: rgba(128, 0, 128, 1)">0.0</span>.<span style="color: rgba(128, 0, 128, 1)">29</span>.post3 peft trl==<span style="color: rgba(128, 0, 128, 1)">0.15</span>.<span style="color: rgba(128, 0, 128, 1)">2</span><span style="color: rgba(0, 0, 0, 1)"> triton cut_cross_entropy unsloth_zoo
</span>%pip <span style="color: rgba(0, 0, 255, 1)">install</span><span style="color: rgba(0, 0, 0, 1)"> sentencepiece protobuf huggingface_hub hf_transfer
</span>%pip <span style="color: rgba(0, 0, 255, 1)">install</span> --no-<span style="color: rgba(0, 0, 0, 1)">deps unsloth
</span>%pip <span style="color: rgba(0, 0, 255, 1)">install</span> -U datasets</pre>
</div>
<p>　　脚本或代码，都可以点击右边的 运行 按钮执行，并且在环境未重启之前，都可以保存上下文变量数据。</p>
<p><img src="https://img2024.cnblogs.com/blog/746628/202506/746628-20250609103739818-1737917862.png" alt="" loading="lazy"></p>
<p>&nbsp;</p>
<p><strong>【04】加载基础模型</strong></p>
<p><strong>　　</strong>基础模型选择：unsloth/Qwen3-0.6B-unsloth-bnb-4bit，模型1G多比较适合用来写HelloWorld，本身也不支持粤语翻译。</p>
<div class="cnblogs_code">
<pre><span style="color: rgba(0, 0, 255, 1)">from</span> unsloth <span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> FastLanguageModel
</span><span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> torch

model, tokenizer </span>=<span style="color: rgba(0, 0, 0, 1)"> FastLanguageModel.from_pretrained(
    model_name</span>=<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">unsloth/Qwen3-0.6B-unsloth-bnb-4bit</span><span style="color: rgba(128, 0, 0, 1)">"</span>,  <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 6亿参数量化模型</span>
    max_seq_length=2048,               <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 支持2048token上下文</span>
    <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> dtype = torch.float16,</span>
    load_in_4bit=True,                             <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 4位量化降低内存占用</span>
    load_in_8bit=False,                            <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 8位模式（需更高显存）</span>
    full_finetuning=False,                         <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 启用参数高效微调（PEFT）</span>
    <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> token="&lt;YOUR_HF_TOKEN&gt;",                     # 访问权限模型需提供令牌</span>
<span style="color: rgba(0, 0, 0, 1)">)

model </span>=<span style="color: rgba(0, 0, 0, 1)"> FastLanguageModel.get_peft_model(
    model,
    r</span>=32,                        <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> LoRA矩阵秩，值越大精度越高</span>
    target_modules=[             <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 需适配的模型层</span>
        <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">q_proj</span><span style="color: rgba(128, 0, 0, 1)">"</span>, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">k_proj</span><span style="color: rgba(128, 0, 0, 1)">"</span>, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">v_proj</span><span style="color: rgba(128, 0, 0, 1)">"</span>, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">o_proj</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">,
        </span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">gate_proj</span><span style="color: rgba(128, 0, 0, 1)">"</span>, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">up_proj</span><span style="color: rgba(128, 0, 0, 1)">"</span>, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">down_proj</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">
    ],
    lora_alpha</span>=64,               <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 缩放因子，通常设为r的2倍</span>
    lora_dropout=0,              <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 关闭 dropout</span>
    bias=<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">none</span><span style="color: rgba(128, 0, 0, 1)">"</span>,                 <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 不微调偏置项</span>
    use_gradient_checkpointing=<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">unsloth</span><span style="color: rgba(128, 0, 0, 1)">"</span>,  <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 支持长上下文</span>
    random_state=3433,           <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 随机种子确保可复现</span>
)</pre>
</div>
<p><strong>【05】微调数据集加载</strong></p>
<p><strong>　　</strong>数据采用以下两个，简单的粤语、普通话对照记录。</p>
<p>　　<a href="https://huggingface.co/datasets/agentlans/cantonese-chinese" target="_blank" rel="noopener nofollow">https://huggingface.co/datasets/agentlans/cantonese-chinese</a>&nbsp;</p>
<p>&nbsp;　&nbsp; &nbsp;<a href="https://huggingface.co/datasets/botisan-ai/cantonese-mandarin-translations" target="_blank" rel="noopener nofollow">https://huggingface.co/datasets/botisan-ai/cantonese-mandarin-translations&nbsp;</a></p>
<div class="cnblogs_code">
<pre><span style="color: rgba(0, 0, 255, 1)">from</span> datasets <span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> load_dataset,Dataset
</span><span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> pandas as pd

</span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 加载推理与对话数据集</span><span style="color: rgba(0, 0, 0, 1)">
ds01 </span>= load_dataset(<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">agentlans/cantonese-chinese</span><span style="color: rgba(128, 0, 0, 1)">"</span>)[<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">train</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">]
</span><span style="color: rgba(0, 0, 255, 1)">print</span>(ds01.column_names)  <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 查看所有列名</span>
<span style="color: rgba(0, 0, 0, 1)">
ds02 </span>= load_dataset(<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">botisan-ai/cantonese-mandarin-translations</span><span style="color: rgba(128, 0, 0, 1)">"</span>)[<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">train</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">]
</span><span style="color: rgba(0, 0, 255, 1)">print</span>(ds02.column_names)  <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 查看所有列名</span>

<span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 标准化推理数据为对话格式</span>
<span style="color: rgba(0, 0, 255, 1)">def</span><span style="color: rgba(0, 0, 0, 1)"> generate01(examples):
    problems </span>= examples[<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">zh</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">]
    solutions </span>= examples[<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">yue</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">]
    </span><span style="color: rgba(0, 0, 255, 1)">return</span><span style="color: rgba(0, 0, 0, 1)"> {
        </span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">messages</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">: [
            {</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">role</span><span style="color: rgba(128, 0, 0, 1)">"</span>: <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">system</span><span style="color: rgba(128, 0, 0, 1)">"</span>, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">content</span><span style="color: rgba(128, 0, 0, 1)">"</span>: <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">你是一个普通话转为粤语的机器人。</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">},
            {</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">role</span><span style="color: rgba(128, 0, 0, 1)">"</span>: <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">user</span><span style="color: rgba(128, 0, 0, 1)">"</span>, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">content</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">: problems},
            {</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">role</span><span style="color: rgba(128, 0, 0, 1)">"</span>: <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">assistant</span><span style="color: rgba(128, 0, 0, 1)">"</span>, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">content</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">: solutions}
        ]
    }

</span><span style="color: rgba(0, 0, 255, 1)">def</span><span style="color: rgba(0, 0, 0, 1)"> generate02(examples):
    problems </span>= examples[<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">translation</span><span style="color: rgba(128, 0, 0, 1)">"</span>][<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">zh</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">]
    solutions </span>= examples[<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">translation</span><span style="color: rgba(128, 0, 0, 1)">"</span>][<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">yue</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">]
    </span><span style="color: rgba(0, 0, 255, 1)">return</span><span style="color: rgba(0, 0, 0, 1)"> {
        </span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">messages</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">: [
            {</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">role</span><span style="color: rgba(128, 0, 0, 1)">"</span>: <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">system</span><span style="color: rgba(128, 0, 0, 1)">"</span>, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">content</span><span style="color: rgba(128, 0, 0, 1)">"</span>: <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">你是一个普通话转为粤语的机器人。</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">},
            {</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">role</span><span style="color: rgba(128, 0, 0, 1)">"</span>: <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">user</span><span style="color: rgba(128, 0, 0, 1)">"</span>, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">content</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">: problems},
            {</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">role</span><span style="color: rgba(128, 0, 0, 1)">"</span>: <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">assistant</span><span style="color: rgba(128, 0, 0, 1)">"</span>, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">content</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">: solutions}
        ]
    }
</span><span style="color: rgba(0, 0, 0, 1)">
dsm01</span>=<span style="color: rgba(0, 0, 0, 1)"> ds01.map(generate01)
dsm02</span>=<span style="color: rgba(0, 0, 0, 1)"> ds02.map(generate02)<br></span><span style="color: rgba(0, 0, 0, 1)"><br>
cvs01 </span>=<span style="color: rgba(0, 0, 0, 1)"> tokenizer.apply_chat_template(
    dsm01[</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">messages</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">],
    tokenize</span>=<span style="color: rgba(0, 0, 0, 1)">False
)<br></span><span style="color: rgba(0, 0, 0, 1)">
cvs02 </span>=<span style="color: rgba(0, 0, 0, 1)"> tokenizer.apply_chat_template(
    dsm02[</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">messages</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">],
    tokenize</span>=<span style="color: rgba(0, 0, 0, 1)">False
)

</span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 设定对话比例</span>
chat_percentage = 0.7<span style="color: rgba(0, 0, 0, 1)">
data </span>=<span style="color: rgba(0, 0, 0, 1)"> pd.concat([
    pd.Series(cvs01),
    pd.Series(cvs02),
])
data.name </span>= <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">text</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">

combined_dataset </span>=<span style="color: rgba(0, 0, 0, 1)"> Dataset.from_pandas(pd.DataFrame(data))
combined_dataset </span>= combined_dataset.shuffle(seed=3407)</pre>
</div>
<p><strong>【06】开始微调</strong></p>
<p><strong>　　</strong>使用trl 库，进行微调。&nbsp;</p>
<div class="cnblogs_code">
<pre><span style="color: rgba(0, 0, 255, 1)">from</span> trl <span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> SFTTrainer, SFTConfig

</span><span style="color: rgba(0, 0, 255, 1)">print</span>(<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">开始训练...</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">)

trainer </span>=<span style="color: rgba(0, 0, 0, 1)"> SFTTrainer(
    model</span>=<span style="color: rgba(0, 0, 0, 1)">model,
    tokenizer</span>=<span style="color: rgba(0, 0, 0, 1)">tokenizer,
    train_dataset</span>=combined_dataset,  <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 结构化数据集</span>
    eval_dataset=<span style="color: rgba(0, 0, 0, 1)">None,
    args</span>=<span style="color: rgba(0, 0, 0, 1)">SFTConfig(
        dataset_text_field</span>=<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">text</span><span style="color: rgba(128, 0, 0, 1)">"</span>,       <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 用于训练的数据集字段</span>
        per_device_train_batch_size=2,   <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 单设备训练批次大小</span>
        gradient_accumulation_steps=4,   <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 梯度累积步数</span>
        warmup_steps=5,                  <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 学习率预热步数</span>
        max_steps=30,                    <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 总训练步数</span>
        learning_rate=2e-4,              <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 学习率</span>
        logging_steps=1,                 <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 日志记录频率</span>
        optim=<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">adamw_8bit</span><span style="color: rgba(128, 0, 0, 1)">"</span>,              <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 优化器</span>
        weight_decay=0.01,               <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 权重衰减</span>
        lr_scheduler_type=<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">linear</span><span style="color: rgba(128, 0, 0, 1)">"</span>,      <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 学习率衰减策略</span>
        seed=3407<span style="color: rgba(0, 0, 0, 1)">,
        report_to</span>=<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">none</span><span style="color: rgba(128, 0, 0, 1)">"</span>,                <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 日志平台（可选wandb）</span>
<span style="color: rgba(0, 0, 0, 1)">    ),
)

trainer_stats </span>= trainer.train()</pre>
</div>
<p><strong>【07】效果检测</strong></p>
<p><strong>　　 </strong>&nbsp;进行简单的对话，看是否有效果。</p>
<div class="cnblogs_code">
<pre>messages =<span style="color: rgba(0, 0, 0, 1)"> [
    {</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">role</span><span style="color: rgba(128, 0, 0, 1)">"</span>: <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">system</span><span style="color: rgba(128, 0, 0, 1)">"</span>, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">content</span><span style="color: rgba(128, 0, 0, 1)">"</span>: <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">你是一个普通话转为粤语的机器人。</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">},
    {</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">role</span><span style="color: rgba(128, 0, 0, 1)">"</span>: <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">user</span><span style="color: rgba(128, 0, 0, 1)">"</span>, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">content</span><span style="color: rgba(128, 0, 0, 1)">"</span>: <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">有个老婆婆在元朗紫荆东路等小巴</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">}
]
text </span>=<span style="color: rgba(0, 0, 0, 1)"> tokenizer.apply_chat_template(
    messages,
    tokenize</span>=<span style="color: rgba(0, 0, 0, 1)">False,
    add_generation_prompt</span>=True,  <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 生成响应必需</span>
    enable_thinking=False,    <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 禁用思考</span>
<span style="color: rgba(0, 0, 0, 1)">)

</span><span style="color: rgba(0, 0, 255, 1)">from</span> transformers <span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> TextStreamer
_ </span>=<span style="color: rgba(0, 0, 0, 1)"> model.generate(
    </span>**tokenizer(text, return_tensors=<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">pt</span><span style="color: rgba(128, 0, 0, 1)">"</span>).to(<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">cuda</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">),
    max_new_tokens</span>=256,         <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 最大生成token数</span>
    temperature=0.7, top_p=0.8, top_k=20<span style="color: rgba(0, 0, 0, 1)">,
    
    streamer</span>=TextStreamer(tokenizer, skip_prompt=<span style="color: rgba(0, 0, 0, 1)">True),
)</span></pre>
</div>
<p><strong>【08】微调后模型保存，并下载</strong></p>
<p><strong>　　&nbsp;</strong>&nbsp;下载时Edge浏览器会先下载完文件之后，才弹出保存对话框。点了Download之后，感觉没有任何反应 需要耐心等待N长时间。</p>
<div class="cnblogs_code">
<pre><span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> shutil

</span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 模型保存在目录</span>
my_model=<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">Qwen3-0.6B-cia-cantonese-chinese</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">
model_dir </span>= <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">/kaggle/working/Models/</span><span style="color: rgba(128, 0, 0, 1)">"</span>+<span style="color: rgba(0, 0, 0, 1)">my_model
output_zip </span>= my_model+<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">.zip</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">

model.save_pretrained_merged(
    model_dir,
    tokenizer,
    save_method</span>=<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">merged_16bit</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">
)


</span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 压缩成 .zip 文件</span>
shutil.make_archive(output_zip.replace(<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">.zip</span><span style="color: rgba(128, 0, 0, 1)">"</span>, <span style="color: rgba(128, 0, 0, 1)">""</span>), <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">zip</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">, model_dir)

</span><span style="color: rgba(0, 0, 255, 1)">print</span>(f<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">✅ 模型已压缩为 {output_zip}</span><span style="color: rgba(128, 0, 0, 1)">"</span>)</pre>
</div>
<p><img src="https://img2024.cnblogs.com/blog/746628/202506/746628-20250609113047648-2002848504.png" alt="" loading="lazy"></p>
<p>&nbsp;</p>
<p><img src="https://img2024.cnblogs.com/blog/746628/202506/746628-20250609113057338-1932212657.png" alt="" loading="lazy"></p>
<p>&nbsp;</p>
<p><strong>【09】本地加载微调 后的模型</strong></p>
<p><strong>　　&nbsp;</strong>&nbsp;微调之后的模型，不需要太多资源就可以运行。可以本地或者继续在Kaggle上运行，验证。</p>
<div class="cnblogs_code">
<pre><span style="color: rgba(0, 0, 255, 1)">from</span> transformers <span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> AutoModelForCausalLM, AutoTokenizer

</span><span style="color: rgba(0, 0, 255, 1)">class</span><span style="color: rgba(0, 0, 0, 1)"> QwenChatbot:
    </span><span style="color: rgba(0, 0, 255, 1)">def</span> <span style="color: rgba(128, 0, 128, 1)">__init__</span>(self, model_name=<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">G:/00LLM/models/Qwen3-0.6B-cia-cantonese-chinese</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">):
    </span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> def __init__(self, model_name="G:/00LLM/models/Qwen3-0.6B-cia-finetuned-002"):</span>
    <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> def __init__(self, model_name="G:/00LLM/models/Qwen3-0.6B-unsloth-cia-finetuned"):</span>
        self.tokenizer =<span style="color: rgba(0, 0, 0, 1)"> AutoTokenizer.from_pretrained(model_name)
        self.model </span>=<span style="color: rgba(0, 0, 0, 1)"> AutoModelForCausalLM.from_pretrained(model_name)
        self.history </span>= [{<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">role</span><span style="color: rgba(128, 0, 0, 1)">"</span>: <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">system</span><span style="color: rgba(128, 0, 0, 1)">"</span>, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">content</span><span style="color: rgba(128, 0, 0, 1)">"</span>: <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">你是一个普通话转为粤语的机器人。</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">}]

    </span><span style="color: rgba(0, 0, 255, 1)">def</span><span style="color: rgba(0, 0, 0, 1)"> generate_response(self, user_input):
        messages </span>= self.history + [{<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">role</span><span style="color: rgba(128, 0, 0, 1)">"</span>: <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">user</span><span style="color: rgba(128, 0, 0, 1)">"</span>, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">content</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">: user_input}]

        text </span>=<span style="color: rgba(0, 0, 0, 1)"> self.tokenizer.apply_chat_template(
            messages,
            tokenize</span>=<span style="color: rgba(0, 0, 0, 1)">False,
            add_generation_prompt</span>=<span style="color: rgba(0, 0, 0, 1)">True
        )

        inputs </span>= self.tokenizer(text, return_tensors=<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">pt</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">)
        response_ids </span>= self.model.generate(**inputs, max_new_tokens=32768<span style="color: rgba(0, 0, 0, 1)">)[0][len(inputs.input_ids[0]):].tolist()
        response </span>= self.tokenizer.decode(response_ids, skip_special_tokens=<span style="color: rgba(0, 0, 0, 1)">True)

        </span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> Update history</span>
        <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> self.history.append({"role": "user", "content": user_input})</span>
        <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> self.history.append({"role": "assistant", "content": response})</span>

        <span style="color: rgba(0, 0, 255, 1)">return</span><span style="color: rgba(0, 0, 0, 1)"> response
    
</span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> Example Usage</span>
<span style="color: rgba(0, 0, 255, 1)">if</span> <span style="color: rgba(128, 0, 128, 1)">__name__</span> == <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">__main__</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">:
    chatbot </span>=<span style="color: rgba(0, 0, 0, 1)"> QwenChatbot()
    </span><span style="color: rgba(0, 0, 255, 1)">print</span>(<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">Welcome to the Qwen Chatbot!</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">)
    </span><span style="color: rgba(0, 0, 255, 1)">print</span>(<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">You can ask questions, and the bot will respond.</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">)
    </span><span style="color: rgba(0, 0, 255, 1)">print</span>(<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">Type 'q' to quit the chat.</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">)
    </span><span style="color: rgba(0, 0, 255, 1)">while</span><span style="color: rgba(0, 0, 0, 1)"> True:
        user_input </span>= input(<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">You: </span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">)
        </span><span style="color: rgba(0, 0, 255, 1)">if</span> user_input.lower() == <span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">q</span><span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(0, 0, 0, 1)">:
            </span><span style="color: rgba(0, 0, 255, 1)">break</span><span style="color: rgba(0, 0, 0, 1)">
        
        response </span>=<span style="color: rgba(0, 0, 0, 1)"> chatbot.generate_response(user_input)
        </span><span style="color: rgba(0, 0, 255, 1)">print</span>(f<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">Bot: {response}</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">)
        </span><span style="color: rgba(0, 0, 255, 1)">print</span>(<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">----------------------</span><span style="color: rgba(128, 0, 0, 1)">"</span>)</pre>
</div>
<p>&nbsp;</p>
<p>以上最简单的 LLM微调详细操作步骤。</p>
<p>&nbsp;</p>
<p>　　</p>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.037070953306712964" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-06-09 11:35">2025-06-09 11:35</span>&nbsp;
<a href="https://www.cnblogs.com/KendoCross">KendoCross</a>&nbsp;
阅读(<span id="post_view_count">0</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18920456);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18920456', targetLink: 'https://www.cnblogs.com/KendoCross/p/18920456', title: 'AI时代Hello World详细教程之LLM微调(SFT)' })">举报</a>
</div>
        