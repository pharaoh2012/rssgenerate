
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/deali/p/18695714" title="发布于 2025-01-31 12:46">
    <span role="heading" aria-level="2">开发者新选择：用DeepSeek实现Cursor级智能编程的免费方案</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h2 id="前言">前言</h2>
<p>去年下半年 cursor 非常火，让很多一行代码都不懂的小白，也可以做自己的网站和搭 App，当时一下子就引爆了独立开发的热潮。</p>
<p>不过每月20dollar的价格也不算便宜，这时候可以使用 cline , aider composer + continue 之类的插件搭配其他大模型后端来实现，也能实现类似的效果。</p>
<h3 id="关于ai编程">关于AI编程</h3>
<p>AI编程现在主要有两种玩法，简单介绍下：</p>
<ol>
<li><strong>全自动模式</strong>（比如Cline、Aider这些工具）：你动嘴，AI动手。直接把需求甩给它就能生成代码，适合搞点小项目——比如做个单页网页、写个命令行工具啥的。不过目前技术还不太行，复杂项目容易翻车。</li>
<li><strong>半自动模式</strong>（Copilot这种经典款）：你和AI组队写代码。你自己握方向盘，AI在旁边帮你补全代码、查漏补缺，相当于副驾驶随时递工具。</li>
</ol>
<p>重点来了：只要不是全交给AI干，生成的代码千万别直接照单全收！最好开着Git这类版本控制工具，多commit几次。每次AI改完代码就拉出来对比看看，这样既能及时发现问题，又能避免代码越堆越乱——这算是用AI编程的保命小技巧了。</p>
<h2 id="cline的问题">cline的问题</h2>
<p>网上说 cline 非常烧 token （使用大模型需要付费，根据消耗的 token数量进行付费），我试了一下还真是，一个页面还没写完，我充了20块的某模型 token 就全都用完了……</p>
<h2 id="使用本地部署的-deepseek-模型">使用本地部署的 DeepSeek 模型</h2>
<p>试了下 cline 自动写代码，但花钱如流水，还不如我自己写呢…</p>
<p>上次部署了 DeepSeek 的蒸馏模型（原版671B，其他模型都是llama/qwen等开源模型蒸馏的），效果还可以，这时候可以拿来当 cline 的后端。</p>
<h3 id="遇到问题">遇到问题</h3>
<p>不过在默认配置下是无法使用的</p>
<p>会提示</p>
<pre><code>Cline is having trouble...
Cline uses complex prompts and iterative task execution that may be challenging for less capable models. For best results, it's recommended to use Claude 3.5 Sonnet for its advanced agentic coding capabilities.
</code></pre>
<p>这个提示的根本原因是 Cline 的工作模式，通常需要模型具备较强的“代理式”推理、分步骤思考、以及对复杂指令的理解和执行能力。相对而言，一些开源大模型（如 DeepSeek-R1 14B）可能在链式推理、代码解释或复杂上下文推断上还不够完备，或者对交互回合数、上下文格式都有所限制，因此在 Cline 这样的多回合 Agentic 插件下就容易“跟不上”，从而触发了“Cline is having trouble…”的警告信息。</p>
<p>从原理上来说，Cline 的交互流程是：</p>
<ol>
<li><strong>Cline 根据你的指令生成比较复杂、带有代理特征的 Prompt</strong>，其中包含“你是谁”“你的目标是什么”“你需要如何自省、复盘”等指导信息。</li>
<li><strong>让模型多回合（多次）迭代思考</strong>，把每一次思考的内容和输出再用来生成后续回合的提示，从而不断 refine 解决方案。</li>
<li>如果模型无法很好地遵循或记住这些多回合的提示，或是对其中的指导指令理解不准确，就会出现答非所问、上下文混乱等情况。Cline 会侦测到这些现象并报错/提示不兼容等。</li>
</ol>
<p>问题的核心<strong>在于默认的上下文长度（Context Length）太小</strong>，而 Cline 插件又需要保留较多的多轮对话上下文，导致模型没办法处理足够长的提示，从而出现 “Cline is having trouble…” 的提示。</p>
<p>在坚持使用 Ollama 的情况下，就只能通过调整上下文长度来解决这个问题了。（参考: <a href="https://github.com/deepseek-ai/DeepSeek-R1/issues/28%EF%BC%89" target="_blank" rel="noopener nofollow">https://github.com/deepseek-ai/DeepSeek-R1/issues/28）</a></p>
<h3 id="调整上下文长度">调整上下文长度</h3>
<p>上下文长度（Context Length）是什么？</p>
<ul>
<li>所谓 “上下文长度” 指的是模型在一次推理过程中能记住并处理的 Token 数量上限（你可以把它简单理解为模型的“记忆容量”）。</li>
<li>如果这个容量不够，当你给模型多轮指令或大段文本时，<strong>后续部分会被截断或遗忘</strong>，所以任务可能无法被完整地理解和执行。</li>
</ul>
<p>许多开源模型初始设置的上下文窗口往往是 2K、4K，如果 Cline 生成了长 Prompt 或者你跟它多次对话累积了很多历史信息，那么原先的 4K 上下文就很容易不足。</p>
<h3 id="在-ollama-里创建带有大上下文长度的模型镜像">在 Ollama 里创建带有大上下文长度的模型镜像</h3>
<p>创建一个名为 <code>deepseek-r1-14b-32k.ollama</code>（文件名自定）的文件，放在哪里都行，Ollama 并不会强制你把它放在某个固定位置，只要命令行能找到该文件即可。</p>
<p>一旦你用 <code>ollama create</code> 命令创建了新的模型，它就会被存储到 Ollama 的模型仓库，后续引用时只需要模型名称，不需要再关心这个文件在哪。</p>
<p>内容大致如下：</p>
<pre><code>FROM deepseek-r1:14b
PARAMETER num_ctx 32768
</code></pre>
<p>然后使用 Ollama 命令行把这个文件转为一个新的模型。例如：</p>
<pre><code>ollama create deepseek-r1-14b-32k -f deepseek-r1-14b-32k.ollama
</code></pre>
<p>完成后，就相当于你有一个带 32k 上下文窗口的 DeepSeek-R1 14B 模型副本。</p>
<p>完成之后可以执行 <code>ollama list</code> 验证一下</p>
<pre><code>$ ollama list
NAME                          ID              SIZE      MODIFIED
deepseek-r1-14b-32k:latest    6ecc115f8ae2    9.0 GB    1 second ago
deepseek-r1:14b               ea35dfe18182    9.0 GB    21 hours ago
</code></pre>
<p>下次在 Cline 中，将模型切换到这个 “deepseek-r1-14b-32k” 就可以使用 32k 的上下文容量，避免因为上下文不足导致的中途报错。</p>
<h2 id="映射ollama服务到本地">映射Ollama服务到本地</h2>
<p>上一篇文章我把模型部署在服务器上</p>
<p>为了让本地能调用，需要使用SSH隧道转发一下。</p>
<pre><code class="language-bash">ssh -L 11434:localhost:11434 用户名@服务器地址 -p 端口
</code></pre>
<h2 id="配置-cline-插件">配置 cline 插件</h2>
<p>点击右上角的齿轮按钮</p>
<p>在 API Provider 选择 Ollama，模型选择我们刚才创建的 deepseek-r1-14b-32k</p>
<p><img src="https://img2024.cnblogs.com/blog/866942/202501/866942-20250131124527355-871280262.png" alt="image" loading="lazy"></p>
<p>设置完成之后点击「Done」保存</p>
<h2 id="火力全开写代码">火力全开写代码</h2>
<p>这里我让AI制作一个「新年祝福生成器」（参考小破站某up主的想法）</p>
<p>这里我用的提示词写得比较详细，把设计的细节都告诉AI，可以减少后续修改次数。</p>
<p>这个提示词比较长，限于篇幅我就不放在正文里了，有需要参考的同学可以在公众号后台回复「提示词」获取完整版。</p>
<p>需要开发其他功能也可以把这篇提示词交给 DeepSeek ，让其参考。</p>
<hr>
<p>输入提示词之后，就开始写代码了，可以看到代码一行行自动生成。</p>
<p><img src="https://img2024.cnblogs.com/blog/866942/202501/866942-20250131124535468-1784016098.png" alt="image" loading="lazy"></p>
<p>完成之后发现有标红的警告，别担心，AI会根据提示信息自动修改</p>
<p><img src="https://img2024.cnblogs.com/blog/866942/202501/866942-20250131124542931-1225806648.png" alt="image" loading="lazy"></p>
<p>几个显卡都跑起来了，Ollama的调度真神奇</p>
<p><img src="https://img2024.cnblogs.com/blog/866942/202501/866942-20250131124551365-1809450263.png" alt="image" loading="lazy"></p>
<h2 id="实现效果">实现效果</h2>
<p>生成出来的代码只能说一般，DeepSeek-14B 应该还是受到模型参数规模限制了。</p>
<p>首先是出现了以下代码</p>
<pre><code class="language-tsx">&lt;p className="text-sm opacity-70 mt-2"&gt;距离春节还有 {new Date().getDaysLeftToChineseNewYear()} 天&lt;/p&gt;
</code></pre>
<p>但是却没有定义这个原型扩展方法。</p>
<p>然后是它对 tailwindcss 的认知很有限，说好的背景颜色并没有生效。</p>
<p>在我手动修复了 bug 之后，运行起来的页面长这样</p>
<p><img src="https://img2024.cnblogs.com/blog/866942/202501/866942-20250131124603662-312893311.png" alt="image" loading="lazy"></p>
<p>但是也不能说 DeepSeek-14B 的代码能力就完全不行</p>
<p>很可能是因为 cline 插件的上下文太长了</p>
<p>我直接与模型对话，同样的需求，生成出来的代码直接就能 run</p>
<p><img src="https://img2024.cnblogs.com/blog/866942/202501/866942-20250131124611430-457800468.png" alt="image" loading="lazy"></p>
<h2 id="部署-localai可选">部署 LocalAI（可选）</h2>
<p><a href="https://github.com/mudler/LocalAI" target="_blank" rel="noopener nofollow">https://github.com/mudler/LocalAI</a></p>
<p>LocalAI 充当一个兼容 OpenAI API 协议的网关，对外提供 <code>/v1/xxx</code> 路由（如 <code>/v1/completions</code>, <code>/v1/chat/completions</code>）。</p>
<ul>
<li>你可以用任何支持 OpenAI API 的客户端（如 Python <code>openai</code> 库、LangChain、或各种开源/商用工具）去调用 LocalAI 的地址。</li>
<li>LocalAI 后台则会在本地加载 .bin 或 .gguf 模型，并用 CPU 或 GPU 进行推理，将结果以 OpenAI 兼容的 JSON 格式返回给客户端。</li>
</ul>
<p>由于我这次使用的 cline 插件支持直接调用 Ollama，所以就不部署 LocalAI 了，有兴趣的同学可以自行尝试。</p>
<h2 id="参考资料">参考资料</h2>
<ul>
<li><a href="https://v2ex.com/t/1102806" target="_blank" rel="noopener nofollow">https://v2ex.com/t/1102806</a></li>
</ul>
<h2 id="小结">小结</h2>
<p>勉强能用，cline 需要消耗的 token 是真的多，我下次试试其他插件有没有改善。</p>

</div>
<div id="MySignature" role="contentinfo">
    微信公众号：「程序设计实验室」
专注于互联网热门新技术探索与团队敏捷开发实践，包括架构设计、机器学习与数据分析算法、移动端开发、Linux、Web前后端开发等，欢迎一起探讨技术，分享学习实践经验。
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.3937654057592593" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-01-31 12:47">2025-01-31 12:46</span>&nbsp;
<a href="https://www.cnblogs.com/deali">程序设计实验室</a>&nbsp;
阅读(<span id="post_view_count">305</span>)&nbsp;
评论(<span id="post_comment_count">3</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18695714" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18695714);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18695714', targetLink: 'https://www.cnblogs.com/deali/p/18695714', title: '开发者新选择：用DeepSeek实现Cursor级智能编程的免费方案' })">举报</a>
</div>
        