
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/Big-Yellow/p/18962444" title="发布于 2025-07-02 22:16">
    <span role="heading" aria-level="2">深入浅出了解生成模型-4：一致性模型（consistency model）</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                    <div id="cnblogs_post_description" style="display: none">
        
        一致性模型（consistency model）是扩散模型（Diffusion Models）的图像生成加速方法，通过将随机过程转化为常微分方程（ODE），引入Consistency Regularization实现一步或少数几步生成。LCM/LCM-Lora进一步通过Skipping-Step和Classifier-free guidance（CFG）优化，代码可参考diffusers库实践。
    </div>
<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p>更加好的排版: <a href="https://www.big-yellow-j.top/posts/2025/06/17/CM.html" target="_blank" rel="noopener nofollow">https://www.big-yellow-j.top/posts/2025/06/17/CM.html</a></p>
<p>前面已经介绍了<a href="https://www.big-yellow-j.top/posts/2025/05/19/DiffusionModel.html" target="_blank" rel="noopener nofollow">扩散模型</a>，在最后的结论里面提到一点：扩散模型往往需要多步才能生成较为满意的图像。不过现在有一种新的方式来加速（旨在通过少数迭代步骤）生成图像：<strong>一致性模型（consistency model）</strong>，因此这里主要是介绍一致性模型（consistency model）基本原理以及代码实践，值得注意的是本文不会过多解释数学原理，数学原理推导可以参考：</p>
<iframe src="//player.bilibili.com/player.html?isOutside=true&amp;aid=113086069474472&amp;bvid=BV1w1p3eHEtB&amp;cid=28321842065&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="=50" allowfullscreen="false"></iframe>
<p>介绍一致性模型之前需要了解几个知识：在传统的扩散模型中无论是加噪还是解噪过程都是随机的，在论文<sup class="footnote-ref"><a href="#fn1" id="fnref1" rel="noopener nofollow">[1]</a></sup>中（也就是CM作者宋博士的另外一篇论文）将这个随机过程（也就是随机微分方程SDE）转化成“固定的”过程（也就是常微分方程ODE），只有过程可控才能保证下面公式成立。</p>
<p><img src="https://s2.loli.net/2025/06/21/RxYJFlc3BUbntaE.webp" alt="" loading="lazy"></p>
<h2 id="一致性模型consistency-model">一致性模型（Consistency Model）</h2>
<p><img src="https://s2.loli.net/2025/06/21/HnPuMUNaSq18jQG.webp" alt="" loading="lazy"></p>
<blockquote>
<p>其中<code>ODE</code>（常微分方程），在传统的扩散模型（Diffusion Models, DM）中，前向过程是从原始图像 <span class="math inline">\(x_0\)</span>开始，不断添加噪声，经过 <span class="math inline">\(T\)</span>步得到高斯噪声图像 <span class="math inline">\(x_T\)</span>。反向过程（如 DDPM）通常通过训练一个逐步去噪的模型，将 <span class="math inline">\(x_T\)</span>逐步还原为 <span class="math inline">\(x_0\)</span> ，每一步估计一个中间状态，因此推理成本高（需迭代 T 步）。而在 <strong>Consistency Models（CM）</strong> 中，模型训练时引入了 <strong>Consistency Regularization</strong>，使得模型在不同的时间步 <span class="math inline">\(t\)</span>都能一致地预测干净图像。这样在推理时，无需迭代多步，而是可以通过一个单一函数<span class="math inline">\(f(x ,t)\)</span> 直接将任意噪声图像<span class="math inline">\(x_t\)</span> 还原为目标图像<span class="math inline">\(x_0\)</span> 。这大大减少了推理时间，实现了一步（或少数几步）生成。</p>
</blockquote>
<p>一致性模型（consistency model）在论文<sup class="footnote-ref"><a href="#fn2" id="fnref2" rel="noopener nofollow">[2]</a></sup>里面主要是通过使用常微分方程角度出发进行解释的。Consistency Model 在 Diffusion Model 的基础上，新增了一个约束：<strong>从某个样本到某个噪声的加噪轨迹上的每一个点，都可以经过一个函数 <span class="math inline">\(f\)</span> 映射为这条轨迹的起点</strong>（也就是通过扩散处理的图像在不同的时间 <span class="math inline">\(t\)</span>都可以直接转化为最开始的图像 <span class="math inline">\(x_0\)</span>），用数学描述就是：<span class="math inline">\(f:(x_t, t)\rightarrow x_\epsilon\)</span>，换言之就是需要满足： <span class="math inline">\(f(x_t,t)=f(x_{t^\prime},t^\prime)\)</span> 其中 <span class="math inline">\(t,t^\prime \in [\epsilon,T]\)</span>，正如论文里面的图片描述：<br>
<img src="https://s2.loli.net/2025/06/21/cXk2KYJA78PbdIW.webp" alt="" loading="lazy"></p>
<p>要满足上面的计算关系，作者在论文里面定义如下的等式关系：</p>
<p></p><div class="math display">\[f_\theta(x,t)=c_{skip}(t)x+ c_{out}(t)F_\theta(x,t)
\]</div><p></p><p>其中等式需要满足：<span class="math inline">\(c_{skip}(\epsilon)=1,c_{out}(\epsilon)=0\)</span> （<span class="math inline">\(c_{skip}(t)=\frac{\sigma_{data}^2}{(t- \epsilon)^2+ \sigma_{data}^2}\)</span>， <span class="math inline">\(c_{out}(t)=\frac{\sigma_{data}(t-\epsilon)}{\sqrt{\sigma_{data}^2+ t^2}}\)</span>），随着解噪过程（时间从：<span class="math inline">\(T \rightarrow  \epsilon\)</span> 其中 <span class="math inline">\(c_{skip}\)</span> 的值逐渐增大，也就是当前的解噪图像占比权重增加），其中我的 <span class="math inline">\(F_\theta\)</span> 就是我们的神经网络模型（比如Unet）。既然使用了神经网络那么必定就需要设计一个损失函数，在论文里面作者设计的损失函数为：<strong>两个时间步之间生成得到的图像距离</strong>通过最小化这个值（比如说 <span class="math inline">\(\Vert x_{t+1} - x_t \Vert_2\)</span>）来优化模型参数。作者对于模型训练给出两种训练方式</p>
<h3 id="直接通过蒸馏模型进行优化">直接通过蒸馏模型进行优化</h3>
<p>通过直接蒸馏的方式对模型参数进行优化，其中设计的损失函数为：</p>
<p></p><div class="math display">\[\mathcal{L}_{CD}^N(\boldsymbol{\theta},\boldsymbol{\theta}^-;\phi) = \mathbb{E}[\lambda(t_n)d(\boldsymbol{f}_{\boldsymbol{\theta}}(\mathbf{x}_{t_{n+1}},t_{n+1}),\boldsymbol{f}_{\boldsymbol{\theta}^-}(\hat{\mathbf{x}}_{t_n}^{\boldsymbol{\phi}},t_n))]
\]</div><p></p><p>其中 <span class="math inline">\(d\)</span>代表距离（比如<span class="math inline">\(l_1\)</span>或者 <span class="math inline">\(l_2\)</span>）对于上面公式中几个参数：<span class="math inline">\(\theta, \theta^-\)</span>，其中 <span class="math inline">\(\hat{x}_{t_n}^\phi\)</span> 代表的是一个预训练的 score model。虽然在CM中损失函数设计上一下子又3个模型，但是实际训练过程中更新的只有一个参数：<span class="math inline">\(\theta\)</span>。另外一个参数是直接通过：$\theta^- \leftarrow \mu \theta^-+ (1-\mu \theta) $ 通过指数滑动平均方式进行训练。而另外一个参数 <span class="math inline">\(\phi\)</span>是一个确定的函数直接通过ODE solver来进行计算得到，比如在论文<sup class="footnote-ref"><a href="#fn3" id="fnref3" rel="noopener nofollow">[3]</a></sup>的使用的欧拉求解法：</p>
<p></p><div class="math display">\[\hat{x}_{t_n}^\phi= x_{t_{n+1}}- (t_n- t_{n+1})t_{n+1}\nabla_{x_{t_{n+1}}}\log p_{t_{n+1}}(x_{t_{n+1}})
\]</div><p></p><blockquote>
<p>欧拉法： <span class="math inline">\(y_{n+1}= y_n+h*f(t_n, y_n)\)</span>  其中h代表时间步长，f代表当前导数估计。不过值得进一步了解的是，在DL中大部分函数都是直接通过神经网络进行“估算的”，也就是说对于上面的 <span class="math inline">\(\nabla_{x_{t_{n+1}}}\log p_{t_{n+1}} \textcolor{red}{≈} s_\theta(x_{t_{n+1}},t_{n+1})\)</span> 其中 <span class="math inline">\(s_\theta\)</span>代表的是训练好的去噪网络。</p>
</blockquote>
<p>那么这样一来整个过程就变成了：<br>
<img src="https://s2.loli.net/2025/06/21/ZpA3D7iqJcI5KdV.webp" alt="" loading="lazy"></p>
<p>回顾整个过程（直接借鉴上面的流程图），算法比较简单（只不过背后的数学原理蛮复杂），简单描述上面过程就是：对于输入图片通过加噪处理之后得到加噪的图像，损失函数设计就是直接通过计算相邻的两步之间的“距离”最小，对于 <span class="math inline">\(x_{t_{n+1}}\)</span> 我们是已经知道的，但是对于当前时间 <span class="math inline">\(t_n\)</span> 是未知的，因此可以直接通过ODE solver的方式去进行估计，而后再去计算loss并且更新参数，对于students模型参数 <span class="math inline">\(\theta\)</span>就可以直接通过计算梯度而后进行更新参数，而对于教师模型参数 <span class="math inline">\(\theta^-\)</span> 可以直接可通过EMA进行更新</p>
<h3 id="直接训练模型进行优化">直接训练模型进行优化</h3>
<p>直接训练模型进行优化，其中具体的过程为：<br>
<img src="https://s2.loli.net/2025/06/21/Y8QCsmnaqiRlkbP.webp" alt="" loading="lazy"></p>
<h2 id="lcmlcm-lora">LCM/LCM-Lora</h2>
<p>潜在一致性模型（Latent Consistency Model）<sup class="footnote-ref"><a href="#fn4" id="fnref4" rel="noopener nofollow">[4]</a></sup>以及LCM-Lora<sup class="footnote-ref"><a href="#fn5" id="fnref5" rel="noopener nofollow">[5]</a></sup>（LCM的Lora优微调）通过再latent space中使用一致性模型（stable diffusion model通过VAE将图像进行压缩到latent sapce而后通过DF模型训练并且最后再通过VAE decoder输出），在LCM中主要提出两点：<br>
1、<strong>Skipping-Step</strong>：因为在最开始的CM中计算两个相邻的时间步之间的loss由于时间步过于接近，就会导致loss很小，因此通过跳步解决这个问题，这样loss就会变成：<span class="math inline">\(d(f(x_{t_{n+\textcolor{red}{k}}}, t_{n+\textcolor{red}{k}}), f(x_{t_n}, t_n))\)</span>。<br>
2、引入<strong>Classifier-free guidance (CFG)</strong> 那么整个loss计算就会变成：<span class="math inline">\(d(f(x_{t_{n+\textcolor{red}{k}}}, \textcolor{red}{w}+ \textcolor{red}{c}, t_{n+\textcolor{red}{k}}), f(x_{t_n}, \textcolor{red}{w}+ \textcolor{red}{c}+ t_n))\)</span>，公式中c代表文本，对于CFG而言其实就是一个改进的ODE solver（见下面算法流程中的蓝色部分）</p>
<p>对于LCD算法流程，其中蓝色部分为LCM所修改的内容：<br>
<img src="https://s2.loli.net/2025/06/21/bftKAHLBJW21QFv.webp" alt="GZ7hs3blVFiJpfN.webp" loading="lazy"></p>
<p>对于最后得到的实验结果分析：</p>
<ul>
<li>不同的k对结果的影响</li>
</ul>
<p><img src="https://s2.loli.net/2025/06/21/JsrT8CbifgUaxv1.webp" alt="download.webp" loading="lazy"></p>
<p>在DPM-solver++和DPM-Solver中基本只需要 2000 步迭代，LCM 4 步采样的 FID 就已经基本收敛了</p>
<ul>
<li>不同的Guidance Scale对结果的影响</li>
</ul>
<p><img src="https://s2.loli.net/2025/06/21/Uz29VWDdXb7hYHx.webp" alt="" loading="lazy"></p>
<p>LCM 作者用不同 LCM 的迭代次数与不同 Guidance Scale 做了对比。发现 <span class="math inline">\(w\)</span> 增加有助于提升 CLIP Score，但是损失了 FID 指标（即多样性）的表现。另外，LCM 迭代次数为 2、4、8 时，CLIP Score 和 FID 相差都不大，说明了 LCM 的蒸馏性能确实非常强悍，两步前向的效果可能都足够好了，只是一步前向的结果还差些。<br>
<strong>总得来说</strong>，在LCM中主要是做了如下几点改进：1、使用skipping-step来“拉大”相邻点之间的距离计算；2、改进了ODE solver。</p>
<h2 id="代码操作">代码操作<sup class="footnote-ref"><a href="#fn6" id="fnref6" rel="noopener nofollow">[6]</a></sup></h2>
<blockquote>
<p>直接阅读<a href="#%E6%80%BB%E7%BB%93" rel="noopener nofollow">最后总结</a></p>
</blockquote>
<p>直接使用diffuser里面给的<a href="https://github.com/huggingface/diffusers/tree/main/examples/consistency_distillation" target="_blank" rel="noopener nofollow">案例</a>使用LCM/LCM-lora，代码分析如下：<br>
1、时间步处理以及 <span class="math inline">\(c_{skip}\)</span> 和 <span class="math inline">\(c_{out}\)</span><br>
在代码中实现Skipping-Step因此在代码中处理方法为：首先通过DDPM（因为在代码中使用的教师模型是：<code>stable-diffusion-v1-5/stable-diffusion-v1-5</code> 而他使用的就是DDPM方式）而后计算得到topk（1000//30=33），而后通过构建随机索引（通过DDIM采样步：50）从DDIM的time steps（<code>(np.arange(1, ddim_timesteps + 1) * step_ratio).round().astype(np.int64) - 1</code>）中进行索引。而后计算边界缩放：<span class="math inline">\(c_{skip}(t)=\frac{\sigma_{data}^2}{t^2+ \sigma_{data}^2}\)</span>， <span class="math inline">\(c_{out}(t)=\frac{t}{\sqrt{\sigma_{data}^2+ t^2}}\)</span></p>
<pre><code class="language-python">noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_teacher_model, subfolder="scheduler", revision=args.teacher_revision)
...
topk = noise_scheduler.config.num_train_timesteps // args.num_ddim_timesteps
index = torch.randint(0, args.num_ddim_timesteps, (bsz,), device=latents.device).long()
start_timesteps = solver.ddim_timesteps[index]
timesteps = start_timesteps - topk
timesteps = torch.where(timesteps &lt; 0, torch.zeros_like(timesteps), timesteps)

# 3. Get boundary scalings for start_timesteps and (end) timesteps.
c_skip_start, c_out_start = scalings_for_boundary_conditions(
    start_timesteps, timestep_scaling=args.timestep_scaling_factor
)
c_skip_start, c_out_start = [append_dims(x, latents.ndim) for x in [c_skip_start, c_out_start]]
c_skip, c_out = scalings_for_boundary_conditions(
    timesteps, timestep_scaling=args.timestep_scaling_factor
)
c_skip, c_out = [append_dims(x, latents.ndim) for x in [c_skip, c_out]]
</code></pre>
<p>2、加噪、模型处理<br>
图像通过VAE处理之后然后会直接通过<code>noise_scheduler.add_noise</code>处理，最后通过模型预测得到<code>noise_pred</code>，因为加噪过程是 <span class="math inline">\(z_t=\sqrt{\alpha_t}x_0+ \sqrt{1-\alpha_t}\epsilon\)</span> 通过模型得到了 <span class="math inline">\(z_t\)</span>因此反推（<code>get_predicted_original_sample</code>）得到加噪前图像 <span class="math inline">\(x_0\)</span>，<strong>最后的模型预测就是</strong>：<span class="math inline">\(f_\theta(x,t)=c_{skip}(t)x+ c_{out}(t)F_\theta(x,t)\)</span></p>
<pre><code class="language-python">noise = torch.randn_like(latents)
noisy_model_input = noise_scheduler.add_noise(latents, noise, start_timesteps)

# 5. Sample a random guidance scale w from U[w_min, w_max] and embed it
w = (args.w_max - args.w_min) * torch.rand((bsz,)) + args.w_min
w_embedding = guidance_scale_embedding(w, embedding_dim=time_cond_proj_dim)
w = w.reshape(bsz, 1, 1, 1)
# Move to U-Net device and dtype
w = w.to(device=latents.device, dtype=latents.dtype)
w_embedding = w_embedding.to(device=latents.device, dtype=latents.dtype)

# 6. Prepare prompt embeds and unet_added_conditions
prompt_embeds = encoded_text.pop("prompt_embeds")

# 7. Get online LCM prediction on z_{t_{n + k}} (noisy_model_input), w, c, t_{n + k} (start_timesteps)
noise_pred = unet(
    noisy_model_input,
    start_timesteps,
    timestep_cond=w_embedding,
    encoder_hidden_states=prompt_embeds.float(),
    added_cond_kwargs=encoded_text,
).sample

pred_x_0 = get_predicted_original_sample(
    noise_pred,
    start_timesteps,
    noisy_model_input,
    noise_scheduler.config.prediction_type,
    alpha_schedule,
    sigma_schedule,
)

model_pred = c_skip_start * noisy_model_input + c_out_start * pred_x_0
</code></pre>
<p>3、计算CFG、计算loss<br>
因为LCM是一个文本引导的模型，因此在CFG计算中：<br>
<img src="https://s2.loli.net/2025/06/25/kajBsrchNvH7Z9b.webp" alt="image.png" loading="lazy"></p>
<p>其中就存在计算有条件文本<span class="math inline">\(c\)</span> 和无条件文本 <span class="math inline">\(\emptyset\)</span>（直接用空文本<code>uncond_input_ids = tokenizer([""] * args.train_batch_size, return_tensors="pt", padding="max_length", max_length=77).input_ids.to(accelerator.device)uncond_prompt_embeds = text_encoder(uncond_input_ids)[0]</code>进行表示即可）<a href="https://github.com/huggingface/diffusers/blob/80f27d7e8db9a6d9a79a320171446963660d8cdf/examples/consistency_distillation/train_lcm_distill_sdxl_wds.py#L1363" target="_blank" rel="noopener nofollow">代码</a>。最后计算loss值，更新参数并且通过EMA更新教师模型参数：</p>
<pre><code class="language-python">with torch.no_grad():
    if torch.backends.mps.is_available():
        autocast_ctx = nullcontext()
    else:
        autocast_ctx = torch.autocast(accelerator.device.type, dtype=weight_dtype)

    with autocast_ctx:
        target_noise_pred = target_unet(
            x_prev.float(),
            timesteps,
            timestep_cond=w_embedding,
            encoder_hidden_states=prompt_embeds.float(),
        ).sample
    pred_x_0 = get_predicted_original_sample(
        target_noise_pred,
        timesteps,
        x_prev,
        noise_scheduler.config.prediction_type,
        alpha_schedule,
        sigma_schedule,
    )
    target = c_skip * x_prev + c_out * pred_x_0

# 10. Calculate loss
if args.loss_type == "l2":
    loss = F.mse_loss(model_pred.float(), target.float(), reduction="mean")
elif args.loss_type == "huber":
    loss = torch.mean(
        torch.sqrt((model_pred.float() - target.float()) ** 2 + args.huber_c**2) - args.huber_c
    )
</code></pre>
<h2 id="总结">总结</h2>
<p>总的来说consistency model作为一种diffusion model生成（区别与DDPM/DDIM）加速操作，在理论上首先<strong>将随机生成过程变成“确定”过程，这样一来生成就是确定的</strong>，从 <span class="math inline">\(T\rightarrow t_0\)</span> 所有的点都在“一条线”上等式 <span class="math inline">\(f(x_t,t)=f(x_{t^\prime},t^\prime)\)</span> 其中 <span class="math inline">\(t,t^\prime \in [\epsilon,T]\)</span> 成立那么就保证了模型不需要再去不断依靠 <span class="math inline">\(t+1\)</span> 生成内容去推断 <span class="math inline">\(t\)</span>时刻内容（具体可以参考算法流程图）。而后续的LCM/LCM-Lora/TCD<sup class="footnote-ref"><a href="#fn7" id="fnref7" rel="noopener nofollow">[7]</a></sup>则是基于CM的原理进行改进，回顾一下LCM的过程，理解代码（参考Huggingface）操作：<br>
<img src="https://s2.loli.net/2025/06/21/bftKAHLBJW21QFv.webp" alt="GZ7hs3blVFiJpfN.webp" loading="lazy"></p>
<p>（LCM蒸馏）训练过程中主要使用了3个模型：1、<code>teacher_model</code>；2、<code>unet</code>；3、<code>student_model</code>。其中后面两个模型是相同的，第一个模型可以直接使用训练好的SD模型。<br>
1、首先是构建跳步迭代过程，而后去计算（公式-1） <span class="math inline">\(c_{skip}\)</span>和 <span class="math inline">\(c_{out}\)</span>以及：<span class="math inline">\(f_\theta(x,t)=c_{skip}(t)x+ c_{out}(t)F_\theta(x,t)\)</span> 对于其中的<span class="math inline">\(x\)</span> 以及 <span class="math inline">\(F_\theta\)</span> （代码中）分别表示的是<span class="math inline">\(t\)</span>时刻 模型预测的输出和 <span class="math inline">\(t-1\)</span>时刻的噪声图像（加噪反推可以直接计算出来）这样一来就得到（ <code>unet</code>模型计算 ）：<code>model_pred</code>（对应公式中的：<span class="math inline">\(f_\theta(z_{t_{n+k}},w,c,t_{n+k})\)</span>）<br>
2、对于<strong>ODE solver计算</strong>就和公式-1计算过程相似，只不过需要区分有文本编码和没有文本编码两种，最后得到（这个过程通过教师模型 <code>teacher_model</code> 处理）<code>pred_x0 = cond_pred_x0 + w * (cond_pred_x0 - uncond_pred_x0)</code> 以及 <code>pred_noise = cond_pred_noise + w * (cond_pred_noise - uncond_pred_noise)</code> 而后再通过扩散解噪过程得到<span class="math inline">\(t\)</span>时刻的（<strong>ODE Solver结果</strong>）： <code>x_prev</code>（对应公式中的：<span class="math inline">\(z_{t_n}^{\Psi,w}\)</span>）<br>
3、计算loss通过<code>student_model</code>处理 <code>x_prev</code>而后反推得到 <code>pred_x_0</code>再去计算<span class="math inline">\(f_\theta(x,t)=c_{skip}(t)x+ c_{out}(t)F_\theta(x,t)\)</span>（<code>c_skip * x_prev + c_out * pred_x_0</code>）得到target。最后去计算model_pred和 target的loss值，而后去通过EMA更新 <code>student_model</code>参数（通过<code>unet</code>来更新他的参数）</p>
<h2 id="参考">参考</h2>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p><a href="https://arxiv.org/pdf/2011.13456" target="_blank" rel="noopener nofollow">https://arxiv.org/pdf/2011.13456</a> <a href="#fnref1" class="footnote-backref" rel="noopener nofollow">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p><a href="https://arxiv.org/abs/2303.01469" target="_blank" rel="noopener nofollow">https://arxiv.org/abs/2303.01469</a> <a href="#fnref2" class="footnote-backref" rel="noopener nofollow">↩︎</a></p>
</li>
<li id="fn3" class="footnote-item"><p><a href="https://arxiv.org/pdf/2406.14548v2" target="_blank" rel="noopener nofollow">https://arxiv.org/pdf/2406.14548v2</a> <a href="#fnref3" class="footnote-backref" rel="noopener nofollow">↩︎</a></p>
</li>
<li id="fn4" class="footnote-item"><p><a href="https://arxiv.org/abs/2310.04378" target="_blank" rel="noopener nofollow">https://arxiv.org/abs/2310.04378</a> <a href="#fnref4" class="footnote-backref" rel="noopener nofollow">↩︎</a></p>
</li>
<li id="fn5" class="footnote-item"><p><a href="https://arxiv.org/abs/2311.05556" target="_blank" rel="noopener nofollow">https://arxiv.org/abs/2311.05556</a> <a href="#fnref5" class="footnote-backref" rel="noopener nofollow">↩︎</a></p>
</li>
<li id="fn6" class="footnote-item"><p><a href="https://github.com/huggingface/diffusers/tree/main/examples/consistency_distillation" target="_blank" rel="noopener nofollow">https://github.com/huggingface/diffusers/tree/main/examples/consistency_distillation</a> <a href="#fnref6" class="footnote-backref" rel="noopener nofollow">↩︎</a></p>
</li>
<li id="fn7" class="footnote-item"><p><a href="https://arxiv.org/abs/2402.19159" target="_blank" rel="noopener nofollow">https://arxiv.org/abs/2402.19159</a> <a href="#fnref7" class="footnote-backref" rel="noopener nofollow">↩︎</a></p>
</li>
</ol>
</section>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0" data-date-updated="2025-07-02 22:17">2025-07-02 22:16</span>&nbsp;
<a href="https://www.cnblogs.com/Big-Yellow">Big-Yellow-J</a>&nbsp;
阅读(<span id="post_view_count">1</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18962444);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18962444', targetLink: 'https://www.cnblogs.com/Big-Yellow/p/18962444', title: '深入浅出了解生成模型-4：一致性模型（consistency model）' })">举报</a>
</div>
        