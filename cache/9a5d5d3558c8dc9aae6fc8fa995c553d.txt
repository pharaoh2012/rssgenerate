
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/orion-orion/p/18660368" title="发布于 2025-01-08 18:55">
    <span role="heading" aria-level="2">贝叶斯机器学习：共轭先验</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                    <div id="cnblogs_post_description" style="display: none">
        <img src="https://img2024.cnblogs.com/blog/1784958/202501/1784958-20250108185428814-1274995949.png" alt="贝叶斯机器学习：共轭先验" class="desc_img">
        离散随机变量的二项分布和多项式分布，以及连续随机变量的高斯分布，这些都是参数分布（parmetric distribution）的具体例子。之所以被称为参数分布，是因为少量可调节的参数控制了整个概率分布。在频率派的观点中，我们通过最优化某些准则（例如似然函数）来确定参数的具体值。而在贝叶斯派的观点中，给定观测数据，我们引入参数的先验分布，然后使用贝叶斯定理来计算对应后验概率分布。我们会看到，对于贝叶斯参数估计而言，共轭先验（conjugate prior）有着很重要的作用。它使得后验概率分布的函数形式与先验概率相同，因此使得贝叶斯分析得到了极大的简化。例如，二项分布的参数的共轭分布为Beta分布，多项式分布的参数的共轭分布为狄利克雷分布（Dirichlet distribution），而高斯分布的均值的共轭先验是另一个高斯分布。所有这些分布都是指数族（exponential family）分布的特例。在本篇博客中我们将会介绍二项分布与多项式分布的共轭先验，高斯分布的共轭先验留在下一篇博客中进行介绍。
    </div>
<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p>离散随机变量的二项分布和多项式分布，以及连续随机变量的高斯分布，这些都是<strong>参数分布（parmetric distribution）</strong> 的具体例子。之所以被称为参数分布，是因为少量可调节的参数控制了整个概率分布。</p>
<p>在频率派的观点中，我们通过最优化某些准则（例如似然函数）来确定参数的具体值。而在贝叶斯派的观点中，给定观测数据，我们引入参数的先验分布，然后使用贝叶斯定理来计算对应后验概率分布。</p>
<blockquote>
<p><strong>注</strong> 参数方法的另一个限制是它假定分布有一个具体的函数形式，这对于一个具体应用来说是不合适的。另一种替代的方法是<strong>非参数（nonparametric）</strong> 估计方法。这种方法中分布的形式通常依赖于数据集的规模。这些模型仍然具有参数，但是这些参数控制的是模型的复杂度而不是分布的形式。</p>
</blockquote>
<p>我们会看到，对于贝叶斯参数估计而言，<strong>共轭先验（conjugate prior）</strong> 有着很重要的作用。它使得后验概率分布的函数形式与先验概率相同，因此使得贝叶斯分析得到了极大的简化。例如，二项分布的参数的共轭分布为<strong>Beta分布</strong>，多项式分布的参数的共轭分布为<strong>狄利克雷分布（Dirichlet distribution）</strong>，而高斯分布的均值的共轭先验是另一个高斯分布。所有这些分布都是<strong>指数族（exponential family）</strong> 分布的特例。在本篇博客中我们将会介绍二项分布与多项式分布的共轭先验，高斯分布的共轭先验留在下一篇博客中进行介绍。</p>
<h1 id="1-二项分布的共轭先验beta分布">1 二项分布的共轭先验：Beta分布</h1>
<h2 id="11-伯努利分布">1.1 伯努利分布</h2>
<p>考虑一个二元随机变量<span class="math inline">\(x\in \{0, 1\}\)</span>，它服从<strong>伯努利分布（Bernoulli distribution）</strong>：</p>
<p></p><div class="math display">\[\text{Bern}(x\mid \mu) = \mu^x (1 - \mu)^{1 - x}
\]</div><p></p><p>其中<span class="math inline">\(\mu\)</span>为<span class="math inline">\(x=1\)</span>的概率（<span class="math inline">\(0\leqslant \mu \leqslant 1\)</span>）。这个分布是归一化的（<span class="math inline">\(\sum_xp(x\mid \mu) = (1 - \mu) + \mu = 1\)</span>），并且均值和方差为</p>
<p></p><div class="math display">\[\begin{aligned}
    \mathbb{E}[x] &amp;= \sum_{x}x\text{Bern}(x\mid\mu) = 0 \cdot x(1 - \mu) + 1 \cdot \mu = \mu,\\
    \quad \mathrm{Var}[x] &amp;= \sum_{x}(x - \mu)^2\text{Bern}(x\mid\mu) = (-\mu)^2(1 - \mu) + (1 - \mu)^2\mu = \mu (1 - \mu)
\end{aligned}
\]</div><p></p><p>现在我们假设我们有一个<span class="math inline">\(x\)</span>的观测值的数据集<span class="math inline">\(\mathcal{D}=\{x_1, x_2, \cdots, x_N\}\)</span>。假设每个观测都是独立地从<span class="math inline">\(\text{Bern}(x\mid \mu)\)</span>中抽取的，因此我们可以构造关于<span class="math inline">\(\mu\)</span>的似然函数如下：</p>
<p></p><div class="math display">\[p(\mathcal{D}\mid \mu) = \prod_{i=1}^N\mu^{x_i}(1 - \mu)^{1 - x_i}
\]</div><p></p><p>依频率派的做法，我们可以通过最大化似然函数（或等价地最大化对数似然函数）来估计<span class="math inline">\(\mu\)</span>的值。这种参数估计方法被称为<strong>最大似然估计（maximum likelihood estimation, MLE）</strong>（参见博客<a href="https://www.cnblogs.com/orion-orion/p/15888493.html" target="_blank">《统计推断：最大似然估计、贝叶斯估计与方差偏差分解》</a>）。如果我们令<span class="math inline">\(\frac{\mathrm{d}\ln p(\mathcal{D}\mid \mu)}{\mathrm{d}\mu} = 0\)</span>，我们就得到了最大似然的估计值<span class="math inline">\(\mu_{\text{ML}} = \frac{1}{N}\sum_{i}x_i\)</span>。这也被称为<strong>样本均值（sample mean）</strong>。如果我们把数据集里<span class="math inline">\(x=1\)</span>的观测的数量记作<span class="math inline">\(m\)</span>，那么我们可以把上式写成下面的形式：</p>
<p></p><div class="math display">\[\mu_{\text{ML}} = \frac{m}{N}
\]</div><p></p><p>此时可以理解为数据集里<span class="math inline">\(x=1\)</span>的观测所占的比例。</p>
<blockquote>
<p><strong>注</strong> 上述是频率派的参数估计方法，如果采用贝叶斯派的参数估计方法，在将参数<span class="math inline">\(\mu\)</span>的先验分布选为均匀分布的情况下，会得到其后验概率分布近似为高斯分布<span class="math inline">\(\mathcal{N}(\mu_{\text{B}}, \frac{\mu_{\text{B}}(1 - \mu_{\text{B}})}{N})\)</span>，这里<span class="math inline">\(\mu_{\text{B}}\)</span>和<span class="math inline">\(\mu_{\text{ML}}\)</span>一样也是<span class="math inline">\(\frac{m}{N}\)</span>（参见博客<a href="https://www.cnblogs.com/orion-orion/p/18621496" target="_blank">《概率论沉思录：初等假设检验》</a>）。</p>
</blockquote>
<h2 id="12-二项分布">1.2 二项分布</h2>
<p>我们也可以求解给定数据集规模<span class="math inline">\(N\)</span>的条件下，<span class="math inline">\(x=1\)</span>的观测出现的数量<span class="math inline">\(m\)</span>的概率分布。该分布即是我们在博客<a href="https://www.cnblogs.com/orion-orion/p/18519155" target="_blank">《概率论沉思录：初等抽样论》</a>）中提到过的<strong>二项分布（binomial distribution）</strong>：</p>
<p></p><div class="math display">\[\text{Bin}(m\mid N, \mu) = \binom{N}{m}\mu^m (1 - \mu)^{N - m}
\]</div><p></p><p>二项分布的均值和方差如下所示：</p>
<p></p><div class="math display">\[\begin{aligned}
    \mathbb{E}[m] &amp;= \sum_{m=0}^Nm \text{Bin}(m\mid N, \mu) \\
    &amp;= \sum_{m=0}^Nm\frac{N\mu}{m} \text{Bin}(m - 1\mid N - 1, \mu) \\
    &amp;= N\mu \underbrace{\sum_{m=1}^N\text{Bin}(m - 1\mid N - 1, \mu)}_{1} \\
    &amp;= N\mu
\end{aligned}
\]</div><p></p><p></p><div class="math display">\[\begin{aligned}
    \mathrm{Var}[m] &amp;= \sum_{m=0}^N(m - N\mu)^2\text{Bin}(m\mid N, \mu) \\ &amp;= N\mu \sum_{m=0}^N \left(\underbrace{m}_{(m - 1) + 1} \text{Bin}(m - 1\mid N - 1, \mu)\right) - 2N^2\mu^2 \cdot 1 + N^2\mu^2 \cdot 1 \\
    &amp; = N\mu \left(\mathbb{E}[m - 1] + 1\right) - 2N^2\mu^2 \cdot 1 + N^2\mu^2 \cdot 1 \\
    &amp; = N\mu\left[(N - 1)\mu + 1\right] - N^2\mu^2\\
    &amp; = N\mu (1 - \mu)
\end{aligned}
\]</div><p></p><blockquote>
<p><strong>注</strong> 这些结果也可以使用结论<span class="math inline">\(\mathbb{E}[x_1 + \cdots  + x_N] = \mathbb{E}[x_1] + \cdots \mathbb{E}[x_N]\)</span>（对随机变量<span class="math inline">\(x_i\)</span>），与结论<span class="math inline">\(\mathrm{Var}[x_1 + \cdots  + x_N] = \mathrm{Var}[x_1] + \cdots \mathrm{Var}[x_N]\)</span>（对相互独立的随机变量<span class="math inline">\(x_i\)</span>）得到。由于<span class="math inline">\(m = x_1 + \cdots + x_N\)</span>，并且对于每次观察有<span class="math inline">\(\mathbb{E}[x_i] = \mu\)</span>和<span class="math inline">\(\mathrm{Var}[x_i] = \mu (1 - \mu)\)</span>，应用该结论得到<span class="math inline">\(\mathbb{E}[m] = N\mu, \mathrm{Var}[m] = N\mu(1 - \mu)\)</span>。</p>
</blockquote>
<p>类似地，关于数据集<span class="math inline">\(\mathcal{D}\)</span>，对于二项分布而言我们相当于拥有了单个样本点<span class="math inline">\(m\)</span>，我们也可以构造关于<span class="math inline">\(\mu\)</span>的似然函数如下：</p>
<p></p><div class="math display">\[p(\mathcal{D}\mid \mu) = p(m\mid \mu) = \binom{N}{m}\mu^m(1 - \mu)^{N - m}
\]</div><p></p><p>再次采用频率派的做法，令<span class="math inline">\(\frac{\mathrm{d}\ln p(\mathcal{D}\mid \mu)}{\mathrm{d}\mu} = 0\)</span>，我们就得到了二项分布参数<span class="math inline">\(\mu\)</span>的最大似然估计值</p>
<p></p><div class="math display">\[\mu_{\text{ML}} = \frac{m}{N}
\]</div><p></p><p>可以看到，在二项分布中，参数<span class="math inline">\(\mu\)</span>的最大似然解和伯努利分布同样为<span class="math inline">\(\mu_{\text{ML}} = \frac{m}{N}\)</span>。</p>
<h2 id="13-beta分布">1.3 Beta分布</h2>
<p>我们前面得到了在二项分布中参数<span class="math inline">\(\mu\)</span>的最大似然解<span class="math inline">\(\mu_{\text{ML}} = \frac{m}{N}\)</span>。而我们之前提到过，我们使用的数据集对于想要拟合的二项分布而言相当于只有单个样本点，会给出严重的过拟合结果。为了用贝叶斯的观点看待这个问题，我们需要引入一个关于参数<span class="math inline">\(\mu\)</span>的先验分布<span class="math inline">\(p(\mu)\)</span>。我们注意到二项分布的似然函数是某个因子与<span class="math inline">\(\mu^m (1 - \mu)^{N - m}\)</span>的乘积的形式。如果我们选择一个正比于<span class="math inline">\(\mu\)</span>和<span class="math inline">\((1 - \mu)\)</span>的幂指数的先验概率分布，那么后验概率分布（正比于先验和似然函数的乘积）就会有着与先验分布相同的函数形式（这个性质被叫做<strong>共轭性（conjugacy）</strong>）。这样，先验分布的校正将表现为其参数的校正。这样在处理上是非常方便的，它通常使得计算相当容易。因此，我们把先验分布选择为<strong>Beta分布</strong><sup>[2]</sup>，定义为</p>
<p></p><div class="math display">\[\text{Beta}(\mu\mid a, b) = \frac{1}{\Beta(a, b)}\mu^{a - 1}(1 - \mu)^{b - 1}
\]</div><p></p><p>其中归一化因子<span class="math inline">\(\Beta(a, b)\)</span>经由我们在博客<a href="https://www.cnblogs.com/orion-orion/p/18621496" target="_blank">《概率论沉思录：初等假设检验》</a>中提到过的Beta函数计算得到：</p>
<p></p><div class="math display">\[\Beta(a, b) = \int_{0}^1\mu^{a - 1}(1 - \mu)^{b - 1}\mathrm{d}\mu = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a + b)}
\]</div><p></p><p>其中</p>
<p></p><div class="math display">\[\Gamma(x)\equiv \int_{0}^{\infty}t^{x - 1}e^{-t}\mathrm{d}t,\quad x &gt; 0
\]</div><p></p><p>为Gamma函数。对<span class="math inline">\(\Gamma(x + 1) = \int_{0}^{\infty}t^{x}e^{-t}\mathrm{d}t\)</span>使用一次分部积分法<sup>[3]</sup>我们有<span class="math inline">\(\Gamma(x + 1) = \left[t^{x}(-e^{-t})\right]_{0}^{\infty} - \int_{0}^{\infty}xt^{x - 1}\left(-e^{-t}\right)\mathrm{d}t = x \Gamma(x)\)</span>，又因为<span class="math inline">\(\Gamma(1)=\int_{0}^{\infty}e^{-t}\mathrm{d}t=\left[-e^{-t}\right]_{0}^{\infty}=1\)</span>，因此用归纳法可证明当<span class="math inline">\(x\)</span>为正整数时<span class="math inline">\(\Gamma(x) = (x - 1)!\)</span>（参见博客<a href="https://www.cnblogs.com/orion-orion/p/18621496" target="_blank">《概率论沉思录：初等假设检验》</a>）。</p>
<p>Beta分布的均值和方差如下所示：</p>
<p></p><div class="math display">\[\begin{aligned}
\mathbb{E}[\mu] &amp;= \int_{0}^1\mu \text{Beta}(\mu\mid a, b)\mathrm{d}\mu \\
&amp;= \int_{0}^1\mu \frac{1}{\Beta(a, b)}\mu^{a - 1}(1 - \mu)^{b - 1}\mathrm{d}\mu \\
&amp; = \frac{\Beta(a + 1, b)}{\Beta(a, b)}\\
&amp;= \frac{a}{a + b}
\end{aligned}
\]</div><p></p><p></p><div class="math display">\[\begin{aligned}
\mathrm{Var}[\mu] &amp;= \int_{0}^{1}(\mu - \frac{a}{a + b})^2\text{Beta}(\mu\mid a, b)\mathrm{d}\mu\\
&amp;= \int_{0}^{1}(\mu - \frac{a}{a + b})^2\frac{1}{\Beta(a, b)}\mu^{a - 1}(1 - \mu)^{b - 1}\mathrm{d}\mu\\
&amp;= \frac{1}{\Beta(a, b)}\left[\Beta(a + 2, b) - \frac{2a}{a + b}\Beta(a + 1, b) + \frac{a^2}{(a + b)^2}\Beta(a, b)\right]\\
&amp;= \frac{ab}{(a + b)^2(a + b + 1)}\\
\end{aligned}
\]</div><p></p><p>参数<span class="math inline">\(a\)</span>和<span class="math inline">\(b\)</span>经常被称为<strong>超参数（hyperparameter）</strong>，因为它们控制了参数<span class="math inline">\(\mu\)</span>的概率分布。</p>
<p>根据Beta先验<span class="math inline">\(\text{Beta}(\mu\mid a, b) = \frac{1}{\Beta(a, b)}\mu^{a - 1}(1 - \mu)^{b - 1}\)</span>与二项分布的似然函数<span class="math inline">\(p(m\mid \mu) = \binom{N}{m}\mu^m(1 - \mu)^{N - m}\)</span>，我们可得到<span class="math inline">\(\mu\)</span>的后验分布为：</p>
<p></p><div class="math display">\[\begin{aligned}
p(\mu\mid m) &amp;= \frac{p(m\mid \mu)\text{Beta}(\mu\mid a, b)}{\int_{0}^1p(m\mid \mu)\text{Beta}(\mu\mid a, b)\mathrm{d}\mu}\\
&amp;= \frac{\binom{N}{m}\mu^m(1 - \mu)^{N - m}\frac{1}{\Beta(a, b)}\mu^{a - 1}(1 - \mu)^{b - 1}}{\int_{0}^1 \binom{N}{m}\mu^m(1 - \mu)^{N - m}\frac{1}{\Beta(a, b)}\mu^{a - 1}(1 - \mu)^{b - 1}\mathrm{d}\mu}\\
&amp;= \frac{\binom{N}{m}\frac{1}{\Beta(a, b)}\mu^{m + a - 1}(1 - \mu)^{N - m + b -1}}{\binom{N}{m}\frac{\Beta(m + a, N - m + b)}{\Beta(a, b)}}\\
&amp; = \frac{1}{\Beta(m + a, N - m + b)}\mu^{m + a - 1}(1 - \mu)^{N - m + b -1}
\end{aligned}
\]</div><p></p><p>这是<span class="math inline">\(\text{Beta}(\mu\mid m + a, N - m + b)\)</span>分布。</p>
<p>我们看到，<span class="math inline">\(\mu\)</span>的后验分布是另一个Beta分布，这反映出先验关于似然函数的共轭性质。我们还看到，如果一个数据集里有<span class="math inline">\(m\)</span>次观测为<span class="math inline">\(x=1\)</span>，有<span class="math inline">\(N - m\)</span>次观测为<span class="math inline">\(x = 0\)</span>，那么从先验概率到后验概率，<span class="math inline">\(a\)</span>的值变大了<span class="math inline">\(m\)</span>，<span class="math inline">\(b\)</span>的值变大了<span class="math inline">\(N - m\)</span>。这让我们可以简单地把先验概率中的超参数<span class="math inline">\(a\)</span>和<span class="math inline">\(b\)</span>分别看成<span class="math inline">\(x = 1\)</span>和<span class="math inline">\(x = 0\)</span>的<strong>有效观测数（effective number of observation）</strong>。注意，<span class="math inline">\(a\)</span>和<span class="math inline">\(b\)</span>不一定是整数。</p>
<p>另外，如果我们接下来观测到更多的数据，那么后验概率分布可以扮演先验概率的角色。为了说明这一点，我们可以假想每次只取一个观测值，然后在每次观测之后更新当前的后验分布。在每个阶段，后验概率都可以视为一个<span class="math inline">\(\text{Beta}(\mu\mid a, b)\)</span>分布，参数<span class="math inline">\(a\)</span>和<span class="math inline">\(b\)</span>分别表示对于<span class="math inline">\(x=1\)</span>和<span class="math inline">\(x=0\)</span>的观测总数（先验的和实际的）。观测到一个<span class="math inline">\(x = 1\)</span>对应于把<span class="math inline">\(a\)</span>的值增加<span class="math inline">\(1\)</span>，而观测到<span class="math inline">\(x=0\)</span>会使<span class="math inline">\(b\)</span>增加<span class="math inline">\(1\)</span>。下图说明了这个过程中的一个步骤：</p>
<p align="center">
<img src="https://images.cnblogs.com/cnblogs_com/blogs/538207/galleries/2015781/o_250105032612_%E8%B4%9D%E5%8F%B6%E6%96%AF%E9%A1%BA%E5%BA%8F%E6%8E%A8%E6%96%AD%E4%B8%AD%E7%9A%84%E4%B8%80%E4%B8%AA%E6%AD%A5%E9%AA%A4%E7%9A%84%E4%BE%8B%E5%AD%90.png" width="700" height="190" alt="" align="center">        
</p>
<p>在该图中，先验分布为<span class="math inline">\(\text{Beta}(\mu\mid a=2, b=2) = 6\mu(1 - \mu)\)</span>，似然函数由公式<span class="math inline">\(p(m=1\mid \mu) = \mu\)</span>给出（其中<span class="math inline">\(N=1\)</span>），对应于<span class="math inline">\(x=1\)</span>的一次观测，从而后验概率分布为<span class="math inline">\(\text{Beta}(\mu\mid a=3, b=2) = 12\mu^2(1 - \mu)\)</span>。</p>
<p>我们看到，如果我们接受了贝叶斯观点，那么学习过程中的<strong>顺序（sequential）</strong> 方法可以自然而然地得出。它与先验和似然函数的选择无关，只取决于数据独立同分布的假设。顺序方法每次使用一个观测值，或者每次使用一小批观测值。然后再使用下一个观测值之前丢掉它们。</p>
<blockquote>
<p><strong>注</strong> 顺序方法可以被用于实时学习的场景中。在实时学习的场景中，输入为一个持续稳定的数据流，模型必须在观测到所有数据之前就进行预测。由于顺序学习的方法不需要把所有的数据都存储到内存里，因此顺序方法对于大的数据集也很有用。最大似然方法也可以转化成顺序的框架。</p>
</blockquote>
<p>对于<span class="math inline">\(\mu\)</span>的后验分布<span class="math inline">\(\text{Beta}(m + a, N - m + b)\)</span>，如果我们想在此基础上使用点估计（而不同于直接进行最大似然估计），一个很自然的做法就是采用这个后验分布的均值做为<strong>贝叶斯估计量（Bayes estimator）</strong>，如下式所示：</p>
<p></p><div class="math display">\[\mu_{\text{B}} = \mathbb{E}[\mu \mid \mathcal{D}] = \frac{m + a}{a + b + N}
\]</div><p></p><blockquote>
<p><strong>注</strong> 这里我们也可以采用<strong>最大后验点估计（maximum a posteriori, MAP）</strong><sup>[4]</sup>，该方法选择后验概率最大的点（在这里<span class="math inline">\(\mu\)</span>是连续值的情况下，也即概率密度最大的点）：</p>
</blockquote>
<p></p><div class="math display">\[\mu_{\text{MAP}} = \text{arg max}_{\mu} p(\mu\mid m)
\]</div><p></p><blockquote>
<p>令<span class="math inline">\(\frac{\mathrm{d} p(\mu\mid m)}{\mathrm{d}\mu}=0\)</span>可以得到其最大值是在<span class="math inline">\(\mu_{\text{MAP}}=\frac{m + a - 1}{a + b + N - 2}\)</span>处。</p>
</blockquote>
<p>现在我们来考虑一下<span class="math inline">\(\mu\)</span>的贝叶斯估计量<span class="math inline">\(\mu_{\text{B}}\)</span>是怎样形成的。先验分布<span class="math inline">\(\text{Beta}(\mu\mid a, b)\)</span>有均值<span class="math inline">\(\frac{a}{a + b}\)</span>，它是我们在没有见到数据时对<span class="math inline">\(\mu\)</span>的最好估计。当不考虑先验信息，我们可能会使用极大似然估计量<span class="math inline">\(\mu_{\text{ML}}=\frac{m}{N}\)</span>当作对于<span class="math inline">\(\mu\)</span>的估计，而<span class="math inline">\(\mu\)</span>的贝叶斯估计量<span class="math inline">\(\mu_{\text{B}}\)</span>则结合进了所有这些信息。如果把<span class="math inline">\(\mu_{\text{B}}\)</span>写成下式，可以清楚地看到这些信息被结合进去的方式：</p>
<p></p><div class="math display">\[\mu_{\text{B}} = \left(\frac{a + b}{a + b + N}\right)\underbrace{\left(\frac{a}{a + b}\right)}_{\mathbb{E}[\mu]} + \left(\frac{N}{a + b + N}\right)\underbrace{\left(\frac{m}{N}\right)}_{\mu_{\text{ML}}}
\]</div><p></p><p>这样，<span class="math inline">\(\mu_{\text{B}}\)</span>就表示成先验均值<span class="math inline">\(\mathbb{E}[\mu] = \frac{a}{a + b}\)</span>和最大似然估计量<span class="math inline">\(\mu_{\text{ML}} = \frac{m}{N}\)</span>（也即样本均值）的一个线性组合，其组合的权重由<span class="math inline">\(a\)</span>，<span class="math inline">\(b\)</span>和<span class="math inline">\(N\)</span>决定。可以看到，在数据集无限大的极限情况下，<span class="math inline">\(m, N \rightarrow \infty\)</span>，此时<span class="math inline">\(\mu_{\text{B}}\)</span>变成了最大似然的结果<span class="math inline">\(\mu_{\text{ML}}\)</span>。实际上，有个很普遍的情况是，贝叶斯的结果和最大似然的结果在数据集的规模趋于无穷的情况下会统一到一起。对于有限规模的数据集，<span class="math inline">\(\mu_{\text{B}}\)</span>总是位于先验均值<span class="math inline">\(\mathbb{E}[\mu]\)</span>和最大似然估计量<span class="math inline">\(\mu_{\text{ML}}\)</span>之间。</p>
<p>从之前的图中我们可以看到，<span class="math inline">\(\mu\)</span>的后验分布的图像相比其先验分布更尖。进一步地，随着观测数量的不断增加，后验分布的参数<span class="math inline">\(a\)</span>或<span class="math inline">\(b\)</span>也会不断增大，后验分布的数量还会越来越尖。下图展示了当<span class="math inline">\(a\)</span>和<span class="math inline">\(b\)</span>变化时，Beta分布<span class="math inline">\(\text{Beta}(\mu\mid a, b)\)</span>关于<span class="math inline">\(\mu\)</span>的函数图像的变化。</p>
<p align="center">
<img src="https://images.cnblogs.com/cnblogs_com/blogs/538207/galleries/2015781/o_14b33f78.png" width="600" height="420" alt="" align="center">        
</p>
<p>我们也能够通过Beta分布的方差公式<span class="math inline">\(\mathrm{Var}[\mu] = \frac{ab}{(a + b)^2(a + b + 1)}\)</span>看出这种趋势。根据该公式，如果<span class="math inline">\(a\rightarrow\infty\)</span>或<span class="math inline">\(b\rightarrow\infty\)</span>，那么方差就趋于0。我们可能想知道，下面这个性质是不是贝叶斯学习的一个共有属性：随着我们观测到越来越多的数据，后验概率表示的不确定性将会持续下降。</p>
<p>为了说明这一点，我们可以用频率学家的观点考虑贝叶斯学习问题。我们可以证明，平均来看，这种性质确实成立。考虑一个一般的贝叶斯推断问题，参数为<span class="math inline">\(\boldsymbol{\theta}\)</span>，并且我们观测到了一个数据集<span class="math inline">\(\mathcal{D}\)</span>，由联合概率分布<span class="math inline">\(p(\boldsymbol{\theta}, \mathcal{D})\)</span>描述。我们有下列结果：</p>
<p></p><div class="math display">\[\mathbb{E}_{\boldsymbol{\theta}}[\boldsymbol{\theta}] = \mathbb{E}_{\mathcal{D}}\left[\mathbb{E}_{\boldsymbol{\theta}}\left[\boldsymbol{\theta}\mid \mathcal{D}\right]\right]
\]</div><p></p><p>其中</p>
<p></p><div class="math display">\[\begin{aligned}
    \mathbb{E}_{\boldsymbol{\theta}}[\boldsymbol{\theta}]&amp;\equiv \int p(\boldsymbol{\theta})\boldsymbol{\theta}\mathrm{d}\boldsymbol{\theta}\\
    \mathbb{E}_{\mathcal{D}}\left[\mathbb{E}_{\boldsymbol{\theta}}\left[\boldsymbol{\theta}\mid\mathcal{D}\right]\right]&amp;\equiv \int \left\{\int \boldsymbol{\theta} p(\boldsymbol{\theta}\mid \mathcal{D})\mathrm{d}\boldsymbol{\theta}\right\}p(\mathcal{D})\mathrm{d}\mathcal{D}
\end{aligned}
\]</div><p></p><p>该结果表明，<span class="math inline">\(\boldsymbol{\theta}\)</span>的后验均值在产生数据集的整个分布上做平均后，等于<span class="math inline">\(\boldsymbol{\theta}\)</span>的先验均值。类似地，我们可以证明</p>
<p></p><div class="math display">\[\mathrm{Var}_{\boldsymbol{\theta}}[\boldsymbol{\theta}] = \mathbb{E}_{\mathcal{D}}\left[\mathrm{Var}_{\boldsymbol{\theta}}\left[\boldsymbol{\theta}\mid \mathcal{D}\right]\right] + \mathrm{Var}_{\mathcal{D}}\left[\mathbb{E}_{\boldsymbol{\theta}}[\boldsymbol{\theta}\mid \mathcal{D}]\right]
\]</div><p></p><p>上式左侧的项是<span class="math inline">\(\boldsymbol{\theta}\)</span>的先验方差。在右侧，第一项是<span class="math inline">\(\boldsymbol{\theta}\)</span>的平均后验方差，第二项是<span class="math inline">\(\boldsymbol{\theta}\)</span>的后验均值的方差。由于这个方差非负，因此这个结果表明，平均来看，<span class="math inline">\(\boldsymbol{\theta}\)</span>的后验方差小于或等于先验方差。后验均值的方差越大，方差减小得就越多。但是需要注意的是，这个结果只在平均情况下成立，对于一个特定的观测数据集，有可能后验方差大于先验方差。</p>
<h1 id="2-多项分布的共轭先验狄利克雷分布">2 多项分布的共轭先验：狄利克雷分布</h1>
<h2 id="21-类别分布">2.1 类别分布</h2>
<p>二元随机变量可以用来描述只能取两种可能值中的一种的这样的量。然而，我们经常会遇到可以取<span class="math inline">\(K\)</span>个互斥状态中的某一种的离散变量。一种比较方便的表示方法是“<span class="math inline">\(1\)</span>-of-<span class="math inline">\(K\)</span>”表示法。这种表示方法中，变量被表示成一个<span class="math inline">\(K\)</span>维向量<span class="math inline">\(\boldsymbol{x}\)</span>，向量中的一个元素<span class="math inline">\(x_k\)</span>等于<span class="math inline">\(1\)</span>，剩余元素等于<span class="math inline">\(0\)</span>（注意，这样的向量满足<span class="math inline">\(\sum_{k}x_k = 1\)</span>）。例如，如果我们有一个能够取<span class="math inline">\(K=6\)</span>种状态的变量，这个变量的某次特定的观测恰好对应于<span class="math inline">\(x_3=1\)</span>的状态，那么<span class="math inline">\(\boldsymbol{x}\)</span>就可以表示为<span class="math inline">\(\boldsymbol{x} = (0, 0, 1, 0, 0, 0)^T\)</span>。<span class="math inline">\(\boldsymbol{x}\)</span>的分布为<strong>类别分布（categorical distribution）</strong>：</p>
<p></p><div class="math display">\[p(\boldsymbol{x}\mid \boldsymbol{\mu}) = \prod_{k=1}^K\mu_{k}^{x_k}
\]</div><p></p><p>其中<span class="math inline">\(\boldsymbol{\mu} = (\mu_1, \cdots, \mu_K)^T\)</span>，参数<span class="math inline">\(\mu_k\)</span>表示<span class="math inline">\(x_k=1\)</span>的概率（<span class="math inline">\(0 \leqslant \mu_k \leqslant 1, \sum_{k}\mu_k = 1\)</span>）。上述概率分布可以被看做是伯努利分布对于多个输出的一个推广。这个分布是归一化的（<span class="math inline">\(\sum_{\boldsymbol{x}}p(\boldsymbol{x}\mid \boldsymbol{u}) = \sum_{k}\mu_k = 1\)</span>），并且均值为</p>
<p></p><div class="math display">\[    \mathbb{E}[\boldsymbol{x}] = \sum_{\boldsymbol{x}}p(\boldsymbol{x}\mid \boldsymbol{\mu})\boldsymbol{x} = \mu_1\left(\begin{matrix}
        1 \\
        \vdots \\
        0 \\
    \end{matrix}\right) + \cdots + \mu_K\left(\begin{matrix}
        0 \\
        \vdots \\
        1 \\
    \end{matrix}\right) = \left(\begin{matrix}
        \mu_1 \\
        \vdots \\
        \mu_K \\
    \end{matrix}\right) = \boldsymbol{\mu}
\]</div><p></p><p>类似地，我们考虑一个有<span class="math inline">\(x\)</span>的独立观测值的数据集<span class="math inline">\(\mathcal{D}=\{x_1, x_2, \cdots, x_N\}\)</span>。对应的似然函数形式为</p>
<p></p><div class="math display">\[p(\mathcal{D}\mid \boldsymbol{\mu}) = \prod_{i=1}^N\prod_{k=1}^K\mu_{k}^{x_{ik}} = \prod_{k=1}^K\mu_k^{\sum_ix_{ik}} = \prod_{k=1}^K\mu_k^{m_k}
\]</div><p></p><p>依频率派的做法，为了找到<span class="math inline">\(\boldsymbol{\mu}\)</span>的最大似然解，我们可以关于<span class="math inline">\(\mu_k\)</span>最大化<span class="math inline">\(\ln p(\mathcal{D}\mid \boldsymbol{\mu})\)</span>，并且要满足约束<span class="math inline">\(\sum_{k}\mu_k=1\)</span>（这可以通过拉格朗日乘数<span class="math inline">\(\lambda\)</span>实现），此时即最大化：</p>
<p></p><div class="math display">\[\sum_{k=1}^Km_k\ln \mu_k + \lambda(\sum_{k=1}^K\mu_k - 1)
\]</div><p></p><p>令上式关于<span class="math inline">\(\mu_k\)</span>的导数等于<span class="math inline">\(0\)</span>，我们有<span class="math inline">\(\mu_k = -\frac{m_k}{\lambda}\)</span>。把该结果代入到约束<span class="math inline">\(\sum_{k}\mu_k=1\)</span>中，解得<span class="math inline">\(\lambda = -N\)</span>。于是我们得到了最大似然解</p>
<p></p><div class="math display">\[\mu_k^{\text{ML}} = \frac{m_k}{N}
\]</div><p></p><p>它是在<span class="math inline">\(N\)</span>次观测中，<span class="math inline">\(x_k=1\)</span>的观测所占的比例。</p>
<h2 id="22-多项分布">2.2 多项分布</h2>
<p>类比于二项分布之于伯努利分布，我们也可以考虑<span class="math inline">\(m_1, \cdots, m_K\)</span>在参数<span class="math inline">\(\boldsymbol{\mu}\)</span>和观测总数<span class="math inline">\(N\)</span>下的联合分布。该分布即是我们在博客<a href="https://www.cnblogs.com/orion-orion/p/18519155" target="_blank">《概率论沉思录：初等抽样论》</a>）中提到过的<strong>多项分布（multinomial distribution）</strong>：</p>
<p></p><div class="math display">\[\text{Mult}(m_1,\cdots, m_K\mid \boldsymbol{\mu}, N) = \frac{N!}{m_1!\cdots m_K!}\prod_{k=1}^K\mu_k^{m_k}
\]</div><p></p><p>其中<span class="math inline">\(\sum_{k=1}^Km_k=N\)</span>。</p>
<p>类似地，关于数据集<span class="math inline">\(\mathcal{D}\)</span>，对于多项分布而言我们相当于拥有了单个样本点<span class="math inline">\((m_1, m_2, \cdots, m_K)\)</span>，我们也可以构造关于<span class="math inline">\(\boldsymbol{\mu}\)</span>的似然函数如下：</p>
<p></p><div class="math display">\[p(\mathcal{D}\mid \boldsymbol{\mu}) = p(m_1,\cdots, m_K\mid \boldsymbol{\mu}) = \frac{N!}{m_1!\cdots m_K!}\prod_{k=1}^K\mu_k^{m_k}
\]</div><p></p><p>再次采用频率派的做法，为了找到<span class="math inline">\(\boldsymbol{\mu}\)</span>的最大似然解，我们可以关于<span class="math inline">\(\mu_k\)</span>最大化<span class="math inline">\(\ln p(\mathcal{D}\mid \boldsymbol{\mu})\)</span>，并且要满足约束<span class="math inline">\(\sum_{k}\mu_k=1\)</span>（这也可以通过拉格朗日乘数<span class="math inline">\(\lambda\)</span>实现），这样我们就得到了多项分布参数<span class="math inline">\(\mu\)</span>的最大似然估计值</p>
<p></p><div class="math display">\[\mu^{\text{ML}}_k = \frac{m_k}{N}
\]</div><p></p><p>可以看到，在多项分布中，参数<span class="math inline">\(\mu_k\)</span>的最大似然解和“<span class="math inline">\(1\)</span>-of-<span class="math inline">\(K\)</span>”随机变量的分布同样为<span class="math inline">\(\mu^{\text{ML}}_k = \frac{m_k}{N}\)</span>。</p>
<h2 id="23-狄利克雷分布">2.3 狄利克雷分布</h2>
<p>现在我们介绍多项分布<span class="math inline">\(\text{Mult}(m_1,\cdots, m_K\mid \boldsymbol{\mu}, N)\)</span>的参数<span class="math inline">\(\boldsymbol{\mu}\)</span>的一族先验分布。观察多项式分布的形式，我们将其对应的共轭先验选择为<strong>狄利克雷分布（Dirichlet distribution）</strong><sup>[5]</sup>，定义为</p>
<p></p><div class="math display">\[\text{Dir}(\boldsymbol{\mu}\mid \boldsymbol{\alpha})  = \frac{1}{\Beta(\boldsymbol{\alpha})} \prod_{k=1}^K\mu_k^{\alpha_k - 1}
\]</div><p></p><p>其中归一化因子<span class="math inline">\(\Beta(\boldsymbol{\alpha})\)</span>经由多项Beta函数<sup>[5]</sup>计算得到：</p>
<p></p><div class="math display">\[\Beta(\boldsymbol{\alpha}) = \int_{\Omega}\prod_{k=1}^K\mu_k^{\alpha_k - 1}\mathrm{d}\boldsymbol{\mu} = \frac{\prod_{k=1}^K\Gamma(\alpha_k)}{\Gamma(\sum_{k=1}^K\alpha_k)}
\]</div><p></p><p>其中积分区域<span class="math inline">\(\Omega\)</span>也即<span class="math inline">\(\boldsymbol{\mu}\)</span>的值域（<span class="math inline">\(0\leqslant\mu_k\leqslant 1\)</span>且<span class="math inline">\(\sum_{k}\mu_k = 1\)</span>），为<span class="math inline">\(\mathbb{R}^K\)</span>中的<span class="math inline">\(K - 1\)</span>维的<strong>单纯形（simplex）</strong>。下图展示了<span class="math inline">\(\boldsymbol{\mu} = (\mu_1, \mu_2, \mu_3)^T\)</span>在<span class="math inline">\(\mathbb{R}^3\)</span>中的分布情况，可以看到其被限制在了一个二维单纯形平面中：</p>
<p align="center">
<img src="https://images.cnblogs.com/cnblogs_com/blogs/538207/galleries/2015781/o_250107031737_mu%20%3D%20(mu_1,%20mu_2,%20mu_3)%5ET%E5%9C%A8R%5E3%E4%B8%AD%E7%9A%84%E5%88%86%E5%B8%83%E6%83%85%E5%86%B5.png" width="220" height="220" alt="" align="center">        
</p>
<p>参数<span class="math inline">\(\boldsymbol{\alpha} = (\alpha_1, \cdots, \alpha_K)^T\)</span>。下图展示了在不同的参数<span class="math inline">\(\boldsymbol{\alpha} = (\alpha_1, \alpha_2, \alpha_3)^T\)</span>的情况下，单纯形上的狄利克雷分布的图像：</p>
<p align="center">
<img src="https://images.cnblogs.com/cnblogs_com/blogs/538207/galleries/2015781/o_250107074848_%E4%B8%8D%E5%90%8C%E7%9A%84%E5%8F%82%E6%95%B0%E7%9A%84%E6%83%85%E5%86%B5%E4%B8%8B%EF%BC%8C%E5%8D%95%E7%BA%AF%E5%BD%A2%E4%B8%8A%E7%9A%84%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83%E7%9A%84%E5%9B%BE%E5%83%8F.png" width="720" height="220" alt="" align="center">        
</p>
<p>其中两个水平轴是单纯形平面上的坐标轴，垂直轴对应于概率密度的值。这里<span class="math inline">\(\boldsymbol{\alpha}=(0.1, 0.1, 0.1)^T\)</span>对应于左图，<span class="math inline">\(\boldsymbol{\alpha}=(1, 1, 1)^T\)</span>对应于中图，<span class="math inline">\(\boldsymbol{\alpha}=(10, 10, 10)^T\)</span>对应于右图。</p>
<p>根据狄利克雷先验<span class="math inline">\(\text{Dir}(\boldsymbol{\mu}\mid \boldsymbol{\alpha})  = \frac{1}{\Beta(\boldsymbol{\alpha})} \prod_k\mu_k^{\alpha_k - 1}\)</span>与多项分布的似然函数<span class="math inline">\(p(\mathcal{D}\mid \boldsymbol{\mu}) = \prod_k\mu_k^{m_k}\)</span>，我们可以可得到<span class="math inline">\(\boldsymbol{\mu}\)</span>的后验分布为：</p>
<p></p><div class="math display">\[\begin{aligned}
p(\boldsymbol{\mu}\mid \mathcal{D}) &amp;= \frac{p(\mathcal{D}\mid \boldsymbol{\mu})\text{Dir}(\boldsymbol{\mu}\mid \boldsymbol{\alpha})}{\int_{\Omega}p(\mathcal{D}\mid \boldsymbol{\mu})\text{Dir}(\boldsymbol{\mu}\mid \boldsymbol{\alpha})\mathrm{d}\boldsymbol{\mu}}\\
&amp;= \frac{\prod_k^K\mu_k^{m_k}\frac{1}{\Beta(\boldsymbol{\alpha})} \prod_k^K\mu_k^{\alpha_k - 1}}{\int_{\Omega} \prod_k^K\mu_k^{m_k}\frac{1}{\Beta(\boldsymbol{\alpha})} \prod_k^K\mu_k^{\alpha_k - 1}\mathrm{d}\boldsymbol{\mu}}\\
&amp;= \frac{\frac{1}{\Beta(\boldsymbol{\alpha})}\prod_{k}^K\mu^{m_k + \alpha_k - 1}}{\frac{\Beta(\boldsymbol{m} + \boldsymbol{\alpha})}{\Beta(\boldsymbol{\alpha})}}\\
&amp; = \frac{1}{\Beta(\boldsymbol{m} + \boldsymbol{\alpha})}\prod_{k}^K\mu^{m_k + \alpha_k - 1}
\end{aligned}
\]</div><p></p><p>这是<span class="math inline">\(\text{Dir}(\boldsymbol{\mu}\mid \boldsymbol{m} + \boldsymbol{\alpha})\)</span>分布，其中<span class="math inline">\(\boldsymbol{m} = (m_1, \cdots, m_K)^T\)</span>。</p>
<p>类比于我们之前为二项分布选择先验分布<span class="math inline">\(\text{Beta}(\mu\mid a, b)\)</span>，将参数<span class="math inline">\(a\)</span>和<span class="math inline">\(b\)</span>分别看成<span class="math inline">\(x = 1\)</span>和<span class="math inline">\(x = 0\)</span>的有效观测数，这里我们为多项分布选择先验分布<span class="math inline">\(\text{Dir}(\boldsymbol{\mu}\mid \boldsymbol{\alpha})\)</span>，也可以将参数<span class="math inline">\(\alpha_k\)</span>看成<span class="math inline">\(x_k = 1\)</span>的有效观测数。</p>
<p>需要注意的是，具有两个状态的量既可以表示为二元变量然后使用二项分布建模，也可以表示为类别变量然后使用多项分布建模。</p>
<h1 id="参考">参考</h1>
<ul>
<li>[1] Bishop C M, Nasrabadi N M. Pattern recognition and machine learning[M]. New York: springer, 2006.</li>
<li>[2] Casella G, Berger R. Statistical inference[M]. CRC press, 2024.</li>
<li>[3] Rudin W. Principles of mathematical analysis[M]. New York: McGraw-hill, 1964.</li>
<li>[4] Bengio Y, Goodfellow I, Courville A. Deep learning[M]. Cambridge, MA, USA: MIT press, 2017.</li>
<li>[5] <a href="https://zh.wikipedia.org/wiki/%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83" target="_blank" rel="noopener nofollow">《维基百科：狄利克雷分布》</a></li>
</ul>

</div>
<div id="MySignature" role="contentinfo">
    数学是符号的艺术，音乐是上界的语言。
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.026645976543981483" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-01-08 19:49">2025-01-08 18:55</span>&nbsp;
<a href="https://www.cnblogs.com/orion-orion">orion-orion</a>&nbsp;
阅读(<span id="post_view_count">13</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18660368" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18660368);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18660368', targetLink: 'https://www.cnblogs.com/orion-orion/p/18660368', title: '贝叶斯机器学习：共轭先验' })">举报</a>
</div>
        