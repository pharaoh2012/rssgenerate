
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/rossiXYZ/p/18706134" title="发布于 2025-02-15 09:37">
    <span role="heading" aria-level="2">探秘Transformer系列之（2）---总体架构</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                    <div id="cnblogs_post_description" style="display: none">
        
        从零开始解析Transformer，目标是：(1) 解析Transformer如何运作，以及为何如此运作，让新同学可以入门；(2) 力争融入一些比较新的或者有特色的论文或者理念，让老鸟也可以有所收获。
    </div>
<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p>探秘Transformer系列之（2）---总体架构</p>
<h2 id="0x00-概述">0x00 概述</h2>
<h3 id="01-流程">0.1 流程</h3>
<p>使用Transformer来进行文本生成其实就是用模型来预测下一个词，完整流程包括多个阶段，如分词、向量化、计算注意力和采样，具体运作流程如下：</p>
<ul>
<li>分词（tokenize）。把用户的输入文本（此处假设是“Data visualization empowers users to”）拆解为若干独立的词汇单元，即token。</li>
<li>编码。借助词表把token映射为数字，每个token由一个唯一的数字表示。</li>
<li>embedding（嵌入）化。embedding模块将token代表的数字转换为embedding向量，即将词映射到一个向量空间，这样LLM才能处理。此时还会加上位置编码信息，因为理解语言不仅关乎单词，还关乎单词的顺序。位置编码可以确保单词的顺序不会丢失。所有嵌入向量组合在一起形成嵌入矩阵。</li>
<li>注意力计算。这是语境化操作，若干堆叠的Transformer Block通过注意力机制将这些Embedding向量转换成若干特征向量，构建词和词之间的关系。在注意力计算过程中，每个token可以了解自己与其它token的相关性。最终每个token流经Transformer最后一层之后得到的是一个代表语义的特征向量。</li>
<li>计算概率。将最后一个token（”to“）对应的特征向量映射为下一个待预测词的概率分布（logits）。具体操作是通过一个线性层把特征向量升维到词表维度（即把解码器的输出转化为与词典大小相同的向量），并且通过softmax进行归一化，最终输出一个概率分布。该分布表示对词表中每个词匹配这个特征向量的概率。</li>
<li>采样。依据这些概率，按照一定的采样规则来采样下一个token，比如选取概率最高的”visualize“作为最有可能出现的下一个单词。</li>
<li>再次使用分词表将”visualize“对应的整数转换回原始的词汇，形成推理结果句子。</li>
<li>不断重复上述过程。直到LLM输出结束流（EOS）标记表示解码结束或者已经生成所需数量的token。</li>
</ul>
<p>下图将上述流程的核心部分作了可视化，也是本篇讲解的基础，后续将对模型结构和执行流程进行逐步细化。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209144135124-25381119.jpg" alt="" loading="lazy"></p>
<h3 id="02-说明">0.2 说明</h3>
<p>本系列主要以下面几项为基础：</p>
<ul>
<li>Transformer论文：Attention Is All You Need  <a href="https://arxiv.org/abs/1706.03762v7%E3%80%82" target="_blank" rel="noopener nofollow">https://arxiv.org/abs/1706.03762v7。</a></li>
<li>其它相关经典论文和精彩博客，参考将在各个篇幅的具体部分中给出。</li>
<li>“The Annotated Transformer” 博客以及其源码（后续简称为哈佛源码）。“The Annotated Transformer” 是Transformer论文的读书笔记，而且博客作者用代码实现了论文的模型，并且结合实现的模型对原始论文做了详细解读。与互联网上可以获取的其他Transformer的模型实现相比较，“The Annotated Transformer” 更适合学习和解读。其地址为：
<ul>
<li><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener nofollow">http://nlp.seas.harvard.edu/2018/04/03/attention.html</a></li>
<li><a href="https://github.com/harvardnlp/annotated-transformer.git" target="_blank" rel="noopener nofollow">https://github.com/harvardnlp/annotated-transformer.git</a>  2025年-2月4日拉取其代码。</li>
</ul>
</li>
</ul>
<p>另外，本篇以文本翻译功能为例来进行说明。</p>
<h2 id="0x01-总体架构">0x01 总体架构</h2>
<h3 id="11-设计动机">1.1 设计动机</h3>
<p>Transformer的新颖之处在于它是一个完全基于注意力机制实现的序列转换架构，我们对Transformer的主要设计动机分析如下：</p>
<ul>
<li>解决长距离依赖关系。论文希望解决RNN在序列长距离上的限制，而注意力机制可以将序列中的任意两个位置之间的距离是缩小为一个常量，从而在长文本分析时可以捕获更多的语义关联关系。</li>
<li>提升训练并行度。论文希望克服RNN不能并行的缺点，而注意力机制可以无视序列的先后顺序来捕捉序列间的关系，因此具有更好的并行性，符合现有的GPU框架，能够进行分布式训练，提升模型训练效率。</li>
</ul>
<p>因此，Jakob Uszkoreit（Transformer作者之一）提出了用自注意力机制来替换RNN对序列的编解码过程。而Noam Shazeer（Transformer作者之一）在此基础上提出了scaled dot-product attention、多头注意力和位置表示。</p>
<h3 id="12-模型结构">1.2 模型结构</h3>
<p>首先我们来看看原始论文里面的架构图，接下来就以它为源头进行分析。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209144149852-1509103977.jpg" alt="" loading="lazy"></p>
<h4 id="主体模块">主体模块</h4>
<p>从网络结构来分析，Transformer 包括了四个主体模块。</p>
<ul>
<li>输入模块，对应下图的绿色圈。</li>
<li>编码器（Encoder），对应下图的蓝色圈。</li>
<li>解码器（Decoder），对应下图的红色圈。编码器和解码器都有自己的输入和输出，编码器的输出会作为解码器输入的一部分（位于解码器的中间的橙色圈）。</li>
<li>输出模块，对应下图的紫色圈。</li>
</ul>
<p>确切的说，蓝色圈是编码器层（Encoder layer），红色圈是解码器层（Decoder layer）。图中的 <span class="math inline">\(N\times\)</span> 代表把若干具有相同结构的层堆叠起来，这种将同一结构重复多次的分层机制就是栈。为了避免混淆，我们后续把单个层称为编码器层或解码器层，把堆叠的结果称为编码器或解码器。在Transformer论文中，Transformer使用了6层堆叠来进行学习。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209144200970-309684502.jpg" alt="" loading="lazy"></p>
<h4 id="多层">多层</h4>
<p>在Transformer中，第一层的输入是嵌入矩阵。第一层的输出随后被用作第二层的输入，依此类推。每一层都生成了一组嵌入，但这些嵌入不再直接与单个词元相关，而是与某种更复杂的词元关系的理解相关联。比如下图给出了一个模型中的第6层和第7层之间的关系，该模型每层有12个注意头。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209144211526-1312823816.jpg" alt="" loading="lazy"></p>
<p>对于多层的作用，目前也有不同的解释。比较常见的解释是分层的本质是由下往上从不同上下文中逐步构建不同层次的特征。比如底层学习单词特征，中间层学习句法特征，高层学习语义特征等，每一层做各自的事情，不会相互影响。</p>
<p>输入文本对应的embedding在Transformer内部各层流通时会不断演变，这个过程类似于逐层“精炼”和“抽象”输入的信息。每一层都会对输入进行不同级别的变换和抽象，都会在其输入的基础之上吸收更多上下文信息来丰富自己的表示，逐层提取出更高层次的特征，从而在综合多层之后就会获得更加强大的表达能力。随着深度学习模型进行训练，这些网络层会逐渐学习到各种范畴之间的关系和相似性，从而在推理和回答问题时能够利用这些知识。当embedding到达最后一层时，其不仅仅代表对应token独立的含义，而是具备深刻的语境信息，反应了该token与序列中其它token的综合关系。我们可以把多层加工理解为工厂的流水线，假定要生产一件瓷器，我们要先通过印坯和修坯来确定器物形状，然后通过刻花来在已经干了的坯体上刻画出各种精美的花纹或者图案。接下来进行施釉，在成型的陶瓷坯体表面施以釉浆；最后将瓷坯装入匣钵，高温入窑烧造。最终才能得到一件精美的瓷器。</p>
<p>针对分层中的每一层可能都会起到不同的作用这点，研究人员做了深入的研究。</p>
<p>论文“What Does BERT Learn about the Structure of Language?”剖析了 BERT 所理解的英语结构的复杂性。他们的研究发现，BERT 的短语表示主要在神经网络的较低层捕捉短语级别的信息，并在中间层中编码了语言要素的复杂层次结构。这个层次结构以表层特征作为基础，中间层提取语法特征，最上层呈现语义特征。</p>
<p>论文"Analyzing Memorization in Large Language Models through the Lens of Model Attribution"指出：</p>
<ul>
<li>较深层的注意力模块（最后25%的层）主要负责记忆。</li>
<li>较浅层的注意力模块对模型的泛化和推理能力至关重要。</li>
<li>在深层注意力模块应用短路（short-circuit）干预可以显著降低记忆所需内存，同时保持模型性能。</li>
</ul>
<p>论文”Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models"则发现。语言模型存在一种普遍机制：防止过度自信（anti-overconfidence）：在模型的最后若干层，语言模型总是在抑制正确答案的输出。这种抑制具体又分为两种：</p>
<ul>
<li>通过注意力头将输入起始位置的信息复制到了最末位置，我们发现起始位置的信息似乎包含了很多高频的token，模型可以通过这种方法来让高频token稀释残差流中的正确回答，降低回答的自信度。</li>
<li>末层的MLP似乎在将残差流引导向一个“平均”token的方向（平均token是基于训练数据的词频，对token embedding加权平均得到的结果）。</li>
</ul>
<p>另外，模型的效果往往和模型的参数量成正比，Transformer就是通过增加模型的层数来加大模型可学习的参数量，让更多的参数来承载文本中深层次的信息。</p>
<h3 id="13-注意力模块">1.3 注意力模块</h3>
<p>注意力机制是Transformer模型的心脏，它赋予模型洞察句子中每个单词与其它单词间错综复杂关系的超能力。</p>
<h4 id="分类">分类</h4>
<p>在Transformer中有三种注意力结构：全局自注意力，掩码自注意力和交叉注意力，具体如下图所示。交叉注意力主要用于处理两个不同序列之间的关系；全局自注意力主要用于处理单个序列内元素之间的关系；掩码自注意力（也被称做因果自注意力）通过掩码来控制模型在计算注意力分数时的关注范围，从而确保在解码时不会受到未来信息的影响。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209144227590-945674250.jpg" alt="" loading="lazy"></p>
<h4 id="位置">位置</h4>
<p>三种注意力模块在Transformer网络对应位置如下图所示。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209144236400-880372719.jpg" alt="" loading="lazy"></p>
<h4 id="作用">作用</h4>
<p>Transformer实际上是通过三重注意力机制建立起了序列内部以及序列之间的全局联系。论文中对这三种注意力作用的解释如下图所示。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209144246216-890349384.jpg" alt="" loading="lazy"></p>
<p>我们具体分析下这三种注意力。</p>
<h5 id="全局自注意力层">全局自注意力层</h5>
<p>全局自注意力层（Global self attention layer）位于编码器中，它负责处理整个输入序列。在全局自注意力机制中，序列中的每个元素都可以直接访问序列中的其它元素，从而与序列中的其他元素建立动态的关联，这样可以使模型更好地捕捉序列中的重要信息。自注意力的意思就是关注于序列内部关系的注意力机制，那么是如何实现让模型关注序列内部之间的关系呢？自注意力将query、key、value设置成相同的东西，都是输入的序列，就是让注意力机制在序列的本身中寻找关系，注意到不同部分之间的相关性。</p>
<p>对于全局自注意力来说，Q、K、V有如下可能：</p>
<ul>
<li>Q、K、V都是输入序列。</li>
<li>Q、K、V都来自编码器中前一层的输出。编码器中的每个位置都可以关注编码器前一层输出的所有位置。</li>
</ul>
<p>再细化来说，Q是序列中当前位置的词向量，K和V是序列中的所有位置的词向量。</p>
<h5 id="掩码自注意力">掩码自注意力</h5>
<p>掩码自注意力层或者说因果自注意力层（Causal attention layer）可以在解码阶段捕获当前词与已经解码的词之间的关联。它是对解码器的输入序列执行类似全局自注意力层的工作，但是又有不同之处。</p>
<p>Transformer是自回归模型，它逐个生成文本，然后将当前输出文本附加到之前输入上变成新的输入，后续的输出依赖于前面的输出词，具备因果关系。这种串行操作会极大影响训练模型的时间。为了并行提速，人们引入了掩码，这样在计算注意力时，通过掩码可以确保后面的词不会参与前面词的计算。</p>
<p>对于掩码自注意力来说，Q、K、V有如下可能：</p>
<ul>
<li>Q、K、V都是解码器的输入序列。</li>
<li>Q、K、V都来自解码器中前一层的输出。解码器中的每个位置都可以关注解码器前一层的所有位置。</li>
</ul>
<p>再细化来说，Q是序列中当前位置的词向量，K和V是序列中的所有位置的词向量。</p>
<h5 id="交叉注意力层">交叉注意力层</h5>
<p>交叉注意力层（Cross attention layer）其实就是传统的注意力机制。交叉注意力层位于解码器中，但是其连接了编码器和解码器，这样可以刻画输入序列和输出序列之间的全局依赖关系，完成输入和输出序列之间的对齐。因此它需要将目标序列作为Q，将上下文序列作为K和V。</p>
<p>对于交叉注意力来说，Q、K、V来自如下：</p>
<ul>
<li>Q来自前一个解码器层，是因果注意力层的输出向量。</li>
<li>K和V来自编码器输出的注意力向量。</li>
</ul>
<p>这使得解码器中的每个位置都能关注输入序列中的所有位置。另外，编码器并非只传递最后一步的隐状态，而是把所有时刻（对应每个位置）产生的所有隐状态都传给解码器，这就解决了中间语义编码上下文的长度是固定的问题。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209144308855-979033684.jpg" alt="" loading="lazy"></p>
<p>或者从另一个角度来理解，交叉注意力是序列到序列模式；双向自注意力是自编码模式；单向自注意力是自回归模式。</p>
<h3 id="14-执行流程">1.4 执行流程</h3>
<p>我们再来结合模型结构图来简述推理阶段的计算流程，具体如下图所示。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209144319485-2071213191.jpg" alt="" loading="lazy"></p>
<p>假设我们进行机器翻译工作，把中文”我吃了一个苹果“翻译成英文”I ate an apple“，在假设模型只有一层，执行步骤如下：</p>
<ol>
<li>处理输入。用户输入自然语言句子”我吃了一个苹果“；tokenizer先把序列转换成token序列；然后Input Embedding层对每个token进行embedding编码，再加入Positional Encoding（位置编码），最终形成带有位置信息的embedding编码矩阵。编码矩阵用 <span class="math inline">\(X_{n*d}\)</span> 表示， n 是句子中单词个数，d 是表示向量的维度（论文中 d=512）。注：原论文图上的输入是token，本篇为了更好的说明，把输入设置为自然语言句子。</li>
<li>编码器进行编码。编码矩阵首先进入MHA（Multi-Head Attention，多头注意力）模块，在这里每个token会依据一定权重把自己的信息和其它token的信息进行交换融合；融合结果会进入FFN（Feed Forward Network）模块做进一步处理，最终得到整个句子的数学表示，句子中每个字都会带上其它字的信息。整个句子的数学表示就是Encoder的输出。</li>
<li>通过输入翻译开始符<begin>来启动解码器。</begin></li>
<li>解码器进行解码。解码器首先进入Masked Multi-Head Attention模块，在这里解码器的输入序列会进行内部信息交换；然后在Multi-Head Attention模块中，解码器把自己的输入序列和编码器的输出进行融合转换，最终输出一个概率分布，表示词表中每个单词作为下一个输出单词的概率；最终依据某种策略输出一个最可能的单词。这里会预测出第一个单词”I“。</li>
<li>把预测出的第一个单词”I“和<begin>一起作为解码器的输入，进行再次解码。</begin></li>
<li>解码器预测出第二个单词”ate“。</li>
</ol>
<p>针对本例，解码器的每一步输入和输出具体如下表所示。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209144327875-1762767210.jpg" alt="" loading="lazy"></p>
<h3 id="16-小结">1.6 小结</h3>
<p>Transformer总体架构是一个有机整体，难以分割。组合的意义不在于构成它的基本单元，而在于这些单元之间形成的复杂关系和涌现的行为。比如集体智能来自个体的组合，却产生了所有个体都不具备的高阶能力。</p>
<p>有些工作就将焦点转移到 transformer 模块的高级架构上，并认为其完整结构，而不仅仅是标记混合注意力操作，对Transformer实现具有竞争力的性能至关重要。</p>
<p>论文"Attention is not all you need"指出如果没有skip connection（residual connection-残差链接）和MLP，自注意力网络的输出会朝着一个rank-1的矩阵收缩。即，skip connection和MLP可以很好地阻止自注意力网络的这种”秩坍塌（秩坍塌）退化“。这揭示了skip connection，MLP对self-attention的不可或缺的作用；</p>
<p>论文”MetaFormer is Actually What You Need for Vision“则描述了一种通用架构，在该结构中，输入首先经过embedding，得到 𝑋。然后embedding送入重复的blocks中，第一个block主要包含了token mixer，使得不同的token能够相互信息通信（Y = TokenMixer(Norm(X)) + X,）；第二个block包含两层MLP。该架构通过指定token mixer的具体设计，可以获得不同的模型。如果将token mixer指定为注意力或spatial MLP，则MetaFormer将分别成为一个transformer或类似MLP的模型。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209144339509-511092828.jpg" alt="" loading="lazy"></p>
<h2 id="0x02-构建">0x02 构建</h2>
<p>我们接下来结合哈佛源码进行分析和学习。哈佛代码中的make_model()函数是Transformer模型的构建函数。</p>
<h3 id="21-参数">2.1 参数</h3>
<p>make_model()函数的参数有如下7个：</p>
<ul>
<li>src_vocab：源语言词表中单词数目，即源词典的大小。</li>
<li>tgt_vocab：目标语言词表中单词数目，即目标词典的大小。</li>
<li>N=6：编码器和解码器堆叠数，即编码器层数和解码器层数。</li>
<li>d_model=512：模型所处理数据的维度，即词向量（word embedding）的大小。</li>
<li>d_ff=2048：FFN（前馈全连接层）中变换矩阵的维度，即隐层神经元的数量。</li>
<li>head=8：多头注意力层中的注意力头数。</li>
<li>dropout=0.1：防止过拟合。</li>
</ul>
<h3 id="22-构建逻辑">2.2 构建逻辑</h3>
<p>make_model()函数的主要思路就是用从小到大搭建积木的方式来构建Transformer。我们先脱离代码来构思下，看看架构图上的哪些模块可以作为积木。</p>
<ul>
<li>输入模块：Input Embedding和Positional Encoding分别可以作为单独的积木块，它们结合在一起又可以作为一个新的积木块。</li>
<li>编码器层和解码器层可以作为两个单独的大积木块。其内部的Masked Multi-Head Attention、Multi-Head Attention、Feed Forward和Add &amp; Norm也都可以作为单独的小积木块。</li>
<li>输出模块：Linear和Softmax分别可以作为单独的积木块，它们结合在一起又可以作为一个新的积木块。</li>
</ul>
<p>有了这些积木块，我们就可以很容易的构建起Tranformer了。当然，在make_model()函数中做了一定的抽象，有些小积木块被用某些类进行了封装（比如Linear和Softmax被封装在Generator类中，细节没有在make_model()函数中展示出来）。我们把这些模块和代码中一一对应起来看，下图中的数字代表代码中某模块出现的顺序，这些数字在下面代码的注释中也有标明。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209144352681-1243535141.jpg" alt="" loading="lazy"></p>
<p>具体的代码逻辑如下。</p>
<ul>
<li>把copy.deepcopy()这个深度拷贝函数重新命名为c，这样后续代码会比较简洁。deepcopy()函数会开辟一个新内存并将源实例完全复制过来，复制过来的对象和源对象没有任何关联。后续各种类的构造函数中会调用copy.deepcopy()来重新生成一个对应实例，比如上面图中的标号1，2，3在解码器端就分别被做了深度拷贝。这样，两个标号1对应的实例彼此之间相互独立，不受干扰。</li>
<li>构建 MultiHeadedAttention，PositionwiseFeedForward 和 PositionalEncoding 对象。</li>
<li>构造 EncoderDecoder 对象，这是Transformer主体类，其参数是Encoder、Decoder、src-embed、tgt-embed和 Generator。我们先分析后面三个参数。
<ul>
<li>src-embed是nn.Sequential(Embeddings(d_model, src_vocab), c(position))的返回结果，其意义是输入编码，对应上图的编号7（由编码3和6组成）。nn.Sequential()函数构建了顺序容器，容器内模块的顺序就是模型处理数据的顺序；</li>
<li>tgt-embed是nn.Sequential(Embeddings(d_model, tgt_vocab), c(position))的返回结果，其意义是输入编码，对应上图的编号9（由编码3和8组成）。</li>
<li>Generator对应上图的编号10，其包括了Linear和Softmax。Generator会把Decoder的输出变成输出词的概率。</li>
</ul>
</li>
<li>Encoder和Decoder两个类很像，我们以Encoder为例来说明。Encoder由N个EncoderLayer构成，EncoderLayer的参数是d_model, c(attn), c(ff), dropout，即word embedding维度、多头注意力、FFN层和Dropout。可以看到，Encoder和Decoder类中的注意力都是MultiHeadedAttention的实例，只是因为传递参数的不同，才决定某个注意力是交叉注意力还是掩码多头注意力。</li>
<li>初始化模型参数。Xavier初始化可以参考论文"Understanding the difficulty of training deep feedforward neural networks"。</li>
</ul>
<p>具体代码如下。</p>
<pre><code class="language-python">def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):
    "Helper: Construct a model from hyperparameters."
    # copy.deepcopy是深度拷贝函数，即重新生成一个新实例。重新命名可以让后续代码比较简洁
    c = copy.deepcopy
    # 构建多头注意力层的实例，对应上图的数字标号1
    attn = MultiHeadedAttention(h, d_model)
    # 构建前馈神经网络层的实例，对应上图的数字标号2
    ff = PositionwiseFeedForward(d_model, d_ff, dropout)
    # 构建位置编码模块的实例，对应上图的数字标号3
    position = PositionalEncoding(d_model, dropout)
    # 总的Transformer模型
    model = EncoderDecoder(
        # EncoderLayer只包含一个Attention层，对应上图的数字标号4。Encoder则包括外面的N
        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),
        # DecoderLayer包含两个Attention层，对应上图的数字标号5，Decoder则包括外面的N
        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),
        # 输入的Embedding和位置编码，Embeddings对应上图的数字标号6，Sequential就是两个编码合并的结果，对应上图的数字标号7
        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),
        # 输出的Embedding和位置编码，Embeddings对应上图的数字标号8，Sequential就是两个编码合并的结果，对应上图的数字标号9
        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),
        # Generator类包括Linear层和Softmax层，对应上图的数字标号10，负责依据Decoder的输出来预测下一个token
        Generator(d_model, tgt_vocab),
    )

    # This was important from their code.
    # Initialize parameters with Glorot / fan_avg.
    # 初始化模型参数，这里采用xavier初始化，即如果参数的维度大于1，则将其初始化成一个服从均匀分布的矩阵
    for p in model.parameters():
        if p.dim() &gt; 1:
            nn.init.xavier_uniform_(p)
    return model
</code></pre>
<h3 id="23-主体类">2.3 主体类</h3>
<p>EncoderDecoder类就是基于Transformer架构的编码器-解码器实现，其成员变量如下：</p>
<ul>
<li>encoder：Encoder类实例，这是编码器的实现。</li>
<li>decoder：Decoder类实例，这是解码器的实现。</li>
<li>src_embed：源语言的word embedding生成模块，是一个nn.Sequential对象，包括Embebddings和PositionalEncoding。src_embed将对输入进行Embedding和位置编码</li>
<li>tgt_embed：目标语言的word embedding生成模块，是一个nn.Sequential对象，包括Embebddings和PositionalEncoding。tgt_embed将对再传入的输出进行Embedding和位置编码</li>
<li>generator：Generator类的对象，包括Linear层和Softmax层，负责对Decoder的输出做预测，即依据Decoder的隐状态输出来预测当前时刻的词。隐状态会输入到全连接层（全连接层的输出大小是词典的大小），全连接层会接上一个softmax得到预测词的概率。</li>
</ul>
<p>EncoderDecoder类的forward()函数完成了编码和解码的工作，它接受四个函数：</p>
<ul>
<li>src：源序列，其内容是token在词表对应的编号。src的形状是[batch_size, seq_len]，举例是[[ 0, 2, 4, 8, 1, 2, 2 ]] ，即批量大小为1，句子长度是7，其中0为bos，1为eos，2为pad。</li>
<li>tgt：目标序列，具体含义类似src。</li>
<li>src_mask：源序列掩码，具体作用是对填充符号进行掩码。以[[ 0, 2, 4, 8, 1, 2, 2 ]]为例，其掩码是[[True,True,True,True,True,False,False]]，即对两个填充的pad进行掩码。</li>
<li>tgt_mask：目标序列掩码，其作用有两种：不让注意力计算看到未来的单词；对填充符号进行掩码。其形状是[batch_size, seq_len, seq_len]，上面对应的掩码如下：</li>
</ul>
<pre><code class="language-python">[True,False,False,False,False,False,False],
[True,True,False,False,False,False,False],
[True,True,True,False,False,False,False],
[True,True,True,True,False,False,False],
[True,True,True,True,True,False,False], # 例句是5个正式token，后面两个pad被置为False
[True,True,True,True,True,False,False],
[True,True,True,True,True,False,False],
</code></pre>
<p>EncoderDecoder代码具体如下：</p>
<pre><code class="language-python"># 继承nn.Module
class EncoderDecoder(nn.Module):
    """
    A standard Encoder-Decoder architecture. Base for this and many other models.
    标准的编码器-解码器架构，这是很多模型的基础。
    """

    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):
        """
        初始化函数有5个参数，从外部传入参数的目的是更加灵活，可以更换组件
        """
        super(EncoderDecoder, self).__init__()
        self.encoder = encoder # 编码器对象
        self.decoder = decoder # 解码器对象
        # 源语言input embedding和position embedding的组合
        self.src_embed = src_embed 
        # 目标语言output embedding和position embedding的组合
        self.tgt_embed = tgt_embed 
        self.generator = generator # 类别生成器对象

    def forward(self, src, tgt, src_mask, tgt_mask):
        # 前向传播函数有四个参数：源序列，目标序列，源序列掩码，目标序列掩码
        "Take in and process masked src and target sequences."
        # 1. 将source, source_mask传入编码函数encode()，让编码器对源序列进行编码，得到编码结果memory
        # 2. 将memory，source_mask，target，target_mask一同传给解码函数decode()进行解码
        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)

    # 编码函数，接受参数是源序列和掩码
    def encode(self, src, src_mask):
        # 1. 对src编码，得到input embedding
        # 2. 计算位置编码，将input embedding和位置编码相加，得到word embedding
        # 3. 使用编码器encoder进行编码，编码结果记作memory
        return self.encoder(self.src_embed(src), src_mask)

    # 解码函数，参数为：编码器输出（memory）、源序列掩码、目标序列和目标序列掩码
    def decode(self, memory, src_mask, tgt, tgt_mask):
        # 1. 对tgt编码，得到得到input embedding
        # 2. 计算位置编码，将input embedding和位置编码相加，得到word embedding
        # 3. 使用编码器decoder进行解码，解码器输出可以使用self.generator进行最后的预测
        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)
</code></pre>
<h3 id="24-如何调用">2.4 如何调用</h3>
<p>模型调用的方式如下面代码所示，forward()函数负责把输入编码成隐状态，然后把隐状态解码称输出logits（对数几率）。其参数都从batch类的实例中获取，具体如下：</p>
<ul>
<li>src：源句子列表。形状是[batch size, max sequence length]。每个句子是从数据集提取出来，经过词典处理过的，举例为：[ 0,  5, 12,..., 1, 2,  2]。其中0为bos，1为eos，2为pad。</li>
<li>tgt：目标句子列表。具体意义同上。</li>
<li>src_mask：注意力层要用的掩码（后面章节会详细分析）。</li>
<li>tgt_mask：解码器的掩码自注意力层要用的掩码（后面章节会详细分析）。</li>
</ul>
<pre><code class="language-python">out = model.forward(batch.src, batch.tgt, batch.src_mask, batch.tgt_mask)
</code></pre>
<h2 id="0x03-输入">0x03 输入</h2>
<h3 id="31-输入分类">3.1 输入分类</h3>
<p>我们还是用把中文”我吃了一个苹果“翻译成英文”I ate an apple“为例。模型的总体输入是若干个单词组成的句子，不同模型支持的句子最大长度不同，如果句子较短，则会用某些特殊词填充多余的位置。而源语言和目标语言（或者说编码器和解码器）分别对应了两种不同的独立的输入。</p>
<ul>
<li>
<p>编码器的输入对应图上的Inputs，这是从原始的源序列文本得到的token列表。在机器翻译业务中，Encoder是一次性接受一个完整的句子，然后进行处理。比如”我吃了一个苹果“这几个字经过tokenizer之后，得到了每个字对应的唯一序号，假设序号为[5, 4, 6,7, 8, 9, 10]，然后会把[5, 4, 6,7, 8, 9, 10]作为inputs传给Encoder层。</p>
</li>
<li>
<p>解码器的输入其实有两种：</p>
<ul>
<li>Outputs(shifted right)。Outputs实际上是解码器之前输出的拼接，shifted right的目的是将序列整体右移一位。解码器并不能一次性全部输出"I ate an apple”，而是一个单词一个单词进行输出，或者说是像RNN一样循环执行的，也就是这次的输出（作为Outputs）会加到上次的输入后面，作为下一次的输入，以便生成后续的单词。</li>
<li>编码器的输出。编码器层把Inputs编码成一个中间隐状态（在Transformer实现中叫做memory，对应了RNN的Hidden State）输出给解码器，或者说是编码器把源语言的完整句子一次性编码输出给解码器。</li>
</ul>
</li>
</ul>
<p>具体如下图所示。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209144529875-2043657157.jpg" alt="" loading="lazy"></p>
<h3 id="32-输入模块">3.2 输入模块</h3>
<p>输入模块具体包括如下：</p>
<ul>
<li>Tokenizer（词元分析器）。注：原论文图上的输入是token，本篇为了更好的说明，把输入设置为自然语言句子，也加入了tokenizer。</li>
<li>源语言文本嵌入层（对应图上的 Input Embedding）和位置编码器（对应图上的Positional Encoding）。</li>
<li>目标语言文本嵌入层（对应图上的Output Embedding）和位置编码器（对应图上的Positional Encoding）。</li>
</ul>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209144538482-647651009.jpg" alt="" loading="lazy"></p>
<h3 id="33-文字转换">3.3 文字转换</h3>
<p>回到本篇最开始图1的例子。“Data visualization empowers users to”这个句子并不能被模型理解，因此我们需要把自然语言进行编码，也就是对文字进行向量化。具体分为以下几个步骤：</p>
<ul>
<li>对输入文字进行tokenize（分词），得到token。</li>
<li>在词表（假设词表大小是10000）中找到每个token对应的token id。</li>
<li>把每个token id转化成一个token embedding（假设维度是512）。</li>
<li>给序列中的每个位置添加一个位置编码。</li>
<li>把input embedding和位置编码相加，得到最终的word embedding。</li>
</ul>
<p>具体流程如下。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209144627237-1114836463.jpg" alt="" loading="lazy"></p>
<p>让我们看看每个步骤是如何完成的。</p>
<h4 id="分词">分词</h4>
<p>分词是将输入文本分解为更小、更易于管理的语义片段的过程，这些片段被成为token。token是模型词汇表的一部分，词汇表是LLM在训练时使用的词元列表。tokenizer（分词器）会做两件事：</p>
<ul>
<li>
<p>首先，tokenizer会将输入文本切分为更小、更易于管理的token。token可以是单词或子词（sub-word），比如单词<code>"Data"</code> 就被映射成token，而单词 <code>"empowers"</code> 则被分为两个token：“em”和“powers”。</p>
</li>
<li>
<p>接下来，tokenizer会把token映射成不同的整数，这些整数就是词表的索引。可以认为这是一种one-hot编码形式。</p>
</li>
</ul>
<h4 id="embedding化">embedding化</h4>
<p>我们接下来看看在NLP领域中，词嵌入向量生成过程，即如何把单词在词表里面的索引转换为一个Transformer可以使用的向量，也就是embedding（嵌入）。embedding是每个词元的固定向量表示，它比纯整数更适合深度学习，因为它捕捉到了单词的语义意义。embedding向量的大小取决于模型维度。当输入中每个token的embedding堆叠在一起时，它们构成了输入的嵌入矩阵。</p>
<p>在Transformer架构图中，Inputs和Outputs的上面分别有一个Embedding模块，每个模块都是由两个子模块组合而成。</p>
<ul>
<li>Inputs相关的embedding模块包括Input Embedding和Positional Encoding。
<ul>
<li>Input Embedding负责把token编码。</li>
<li>Positional Encoding负责给token加入位置信息。实际操作中，Transformer会一次性接收整个输入句子的嵌入矩阵。这样做的好处是可以并行操作，但是劣势是缺少位置信息，比如模型无法区分“我爱你”和“爱你我” 。Positional Encoding就负责给每个词增加位置信息。</li>
</ul>
</li>
<li>Outputs相关的embedding模块包括Output Embedding + Positional Encoding：与上面类似，不再赘述。</li>
</ul>
<h5 id="token-embedding">Token Embedding</h5>
<p>Token Embedding会将文字转换成模型可以理解和处理的数学表示，即将每个token id（one-hot）和一个高维向量相关联，向量的每个维度对应语义的某个方面。embedding通常是查表操作，即根据token_id的值，去embedding矩阵中查找第token_id行的数据作为embedding。向量的维度取决于模型，Transformer论文将每个token表示为512维向量。Token Embedding也叫word embedding。</p>
<h5 id="positional-encoding">Positional Encoding</h5>
<p>一个句子中的文字先后顺序很重要，比如以下两句话的文字完全相同，但因为文字顺序不同，其语义完全不同。</p>
<ul>
<li>买一张从上海到北京的车票。</li>
<li>买一张从北京到上海的车票。</li>
</ul>
<p>因此，模型还需要对输入句子中每个token的位置信息进行编码。</p>
<h5 id="word-embedding">Word Embedding</h5>
<p>最后，模型将token embedding和Positional Encoding相加，得到最终的嵌入表示。这种组合表示捕获了标记的语义及其在输入序列中的位置。在 LLM（大型语言模型）中，将 word embedding（词嵌入）加上位置编码后的结果通常仍可以称为 “输入嵌入（Input Embeddings）” 或 “带位置信息的词嵌入”。为了讲述方便，后续我们依然称之为Word Emebdding 。</p>
<p>本阶段输出张量的形状是[batch size, sequence length,embedding dimension]。以“Data visualization empowers users to”为例，5个单词被切分成6个token，最终得到的word embedding是一个[batch size， sequence length, embedding dimension]的矩阵。因为只有一句话，所以batch size是1，sequence length是6，假设embedding dimension是512，则矩阵维度是[1,6,512]。</p>
<p>实际上，我们可以认为embedding是LLM自己的语言系统（包括文本信息特征空间与位置信息特征空间）。输入层的作用就是把自然语言、程序语言、视觉听觉语言等信息都映射（或者叫编码）到这个高维的语言空间中。接下来会通过注意力机制从高维语言空间中提取各种丰富的知识和结构，加权积累与关联生成自己的语言，最后再“编码”回人类的语言。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209144643573-408545283.jpg" alt="" loading="lazy"></p>
<h2 id="0x04-transformer-layer">0x04 Transformer Layer</h2>
<p>Transformer处理的核心在于Transformer块，它包括多头注意力机制和多层感知器层。大多数模型由多个这样的块组成，这些块一个接一个地顺序堆叠。</p>
<p>从Transformer的构造代码可以看出来，Encoder类实例是由N个EncoderLayer类实例构建而成，Decoder类实例由N个DecoderLayer类实例构建而成，这和论文相符合。</p>
<pre><code class="language-python"># EncoderLayer只包含一个注意力层，Encoder则包括外面的N参数
Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),
# DecoderLayer包含两个注意力层，Decoder则包括外面的N参数
Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),
</code></pre>
<p>EncoderLayer和DecoderLayer是基础构建块，每个块主要包括如下：</p>
<ul>
<li>多头注意力机制。它允许token与其他token进行通信，彼此交流信息，从而捕获上下文信息和单词之间的关系。</li>
<li>FFN层。一个对每个token 独立运行的前馈网络。注意力层的目标是在 token 之间路由信息，而 MLP 的目标是细化每个 token 的表示。</li>
</ul>
<p>本小节先概要介绍多头自注意力机制和FFN层，后续篇幅会对EncoderLayer和DecoderLayer进行详细介绍。</p>
<h3 id="41-多头自注意力机制">4.1 多头自注意力机制</h3>
<p>自注意力机制使模型能够专注于输入序列的相关部分，从而使其能够捕获数据内的复杂关系和依赖关系。多头注意力机制借鉴了CNN中multi-kernel 的思想，对不同头使用不同的线性变换。多头自注意力机制本质上是构造多个子空间，在这些子空间上再构建多个注意力来替代单个注意力，这样可以获取更多维度的信息和相互关系。自注意力机制是LLM架构中唯一计算序列中词元间关系的地方，因此它构成了语言理解的核心，涵盖了对词汇关系的理解。</p>
<p>让我们看看多头自注意力机制如何计算。为了简化说明，这里假设编码器也用到了掩码，对于某些复合操作也分解说明。</p>
<h5 id="第-1-步根据原始嵌入计算查询键和值矩阵">第 1 步：根据原始嵌入计算查询、键和值矩阵</h5>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209144655019-1620461538.jpg" alt="" loading="lazy"></p>
<p>自注意力机制的输入是<code>n_tokens x n_embd</code>的嵌入矩阵，其中每一行或向量表示一个独立的词元。LLM计算的第一部分从词元嵌入矩阵中提取每个词元的相关行。每个 token 的word embedding被转换为三个不同的向量，分别称为query、 key和 value。这些向量是通过将输入嵌入矩阵与学习到的<span class="math inline">\(W^Q\)</span>、 <span class="math inline">\(W^K\)</span>和 <span class="math inline">\(W^V\)</span>权重矩阵相乘（这些矩阵是模型参数的一部分）得出的。query和key的点积被用来来判断两个向量之间的相似性，这也是论文中提到的点积注意力。</p>
<p>我们用淘宝搜索来类比，可以帮助我们对这些矩阵有更好的理解。假如我们在淘宝上进搜索”李宁鞋“。</p>
<ul>
<li>query是你在搜索栏输入的查询内容。</li>
<li>key是在页面上返回的商品描述、标题，其实就是淘宝商品数据库中与候选商品相关的关键字。</li>
<li>value是李宁鞋商品本身。因为一旦依据搜索词（query）搜到了匹配的商品描述、标题（key），我们就希望具体看看商品内容。</li>
</ul>
<p>通过使用这些 QKV 值，模型可以计算注意力分数，从而确定每个token在生成预测时应从其它token那里获得多少关注。</p>
<h5 id="第-2-步掩码自我注意力">第 2 步：掩码自我注意力</h5>
<p>掩码自注意力允许模型通过关注输入的相关部分来生成序列，同时防止访问未来的token。下图展示了如何使用查询、键和值矩阵来计算掩码自注意力。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209144704027-1053846269.jpg" alt="" loading="lazy"></p>
<p>图上具体分为三步：</p>
<ul>
<li>使用点积来计算注意力分数。使用Q和K矩阵的点积来决定了每个Q与每个K的对齐程度，点积结果是一个反映所有输入token之间关系的矩阵。</li>
<li>对注意力分数施加scaling（缩放）和掩码。首先对注意力分数进行缩放，其次将掩码应用于注意力矩阵的上三角，以防止模型访问未来的标记，因为模型需要学习如何在不“窥视”未来的情况下来预测下一个token。</li>
<li>施加Softmax和dropout操作。注意力分数会通过 softmax 运算转换为概率，然后会施加dropout操作来随机丢弃一些元素。</li>
</ul>
<p>此时得到的结果是注意力权重。</p>
<h5 id="第3步拼接">第3步：拼接</h5>
<p>使用第二步产生的权重和V矩阵进行相乘以获得自注意力机制的最终输出，因为是多头注意力的结果，所以需要把这些头的输出进行拼接并且通过线性层来融合。</p>
<h3 id="42-ffn层">4.2 FFN层</h3>
<p>在多个自注意力头捕获输入token之间的不同关系后，拼接的输出将通过FFN（feed-forward network）层进行处理，以增强模型的表示能力。下图展示了如何使用 FFN层将自注意力表示投影到更高的维度，以增强模型的表示能力。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209144714057-1754818614.jpg" alt="" loading="lazy"></p>
<p>FFN层由两个线性变换组成，线性变换 中间有一个激活函数。第一个线性变换将输入的维度从<code>512</code> 增加到四倍<code>2048</code>。第二个线性变换将维度降回到 的原始大小<code>512</code>，以确保后续层接收到维度一致的输入。</p>
<p>与自注意力机制在序列中彼此交流不同，FFN对序列中每个元素都独立计算，因此不会进行元素间的信息交换（元素间的互动完全靠自注意力）。这样有助于在注意力层进行元素间的信息交换之后，让每个元素消化整合自己的信息，为下一层再次通过自注意力交换信息做好准备。</p>
<h3 id="43-辅助架构">4.3 辅助架构</h3>
<p>除了上述主要模块之外，Transformer模型中还应用了LayerNorm（层归一化）和ResNet（残差连接）等设计方法。虽然在上面源码中的构造函数中没有提及，但这些模块对提高模型的整体表示能力非常重要。</p>
<h4 id="layernorm">LayerNorm</h4>
<p>LayerNorm有助于稳定训练过程并提高收敛性。它的工作原理是对输入的各个特征进行归一化，确保激活的均值和方差一致。普遍认为这种归一化有助于缓解与内部协变量偏移相关的问题，使模型能够更有效地学习并降低对初始权重的敏感性。从架构图上看，LayerNorm在每个Transformer 块中应用两次，一次在自注意力机制之后，一次在FFN层之后，但是在实际工作中不一定如此。</p>
<h4 id="dropout">Dropout</h4>
<p>Dropout 是一种正则化技术，用于通过在训练期间随机将一部分模型权重设置为零来防止过拟合。这鼓励模型学习更强大的特征并减少对特定神经元的依赖，帮助网络更好地泛化新的、未见过的数据。</p>
<h4 id="残差连接">残差连接</h4>
<p>残差连接（Residual Connection）于 2015 年首次由何凯明大神在ResNet论文中引入。残差连接就是把网络的输入和输出相加，得到网络的新输出为F(x)+x。其本质思想是允许网络中的信息和梯度直接跨过一个或多个层进行传播，这样能够保留原始的一些信息。这种架构创新可以有助于缓解梯度消失问题，从而训练更深的神经网络，彻底改变了深度学习Transformer论文中，残差连接在每个 Transformer 块内使用两次：一次在FFN之前，一次在 FFN之后。</p>
<h2 id="0x05-概率输出output-probabilities">0x05 概率输出（Output Probabilities）</h2>
<p>经过一系列Transformer模块的精心处理，输入数据最终来到了它的归宿——最后的输出层。输出模块包括两部分：线性层和softmax层。哈佛代码用Generator类对这两部分进行了封装，将模型输出的 embeding 转换为对下一个词的预测（在整体词表上的概率分布）。</p>
<h3 id="51-解码器结果">5.1 解码器结果</h3>
<p>输入经过所有解码块处理后，最终输出依然是若干token对应的向量，其代表在Transformer视角下的，用高维概率向量编织起来事物之间的各种复杂关系。这些关系在本质上就是范畴论概念下事物的米田嵌入（米田嵌入采用对象的所有关系来表征该对象）。Transformer 学习的过程，是核函数选择与参数化的过程，也是寻找米田嵌入的过程：提取对象的所有关系，形成其关系图 -- 即概率化的内部世界模型。</p>
<p>我们首先看看哈佛代码。在推理时，generator使用的并不是编码器的所有输出，而是最后一个token对应的向量out[:, -1]，即只使用输出序列中最后一个单词的猜测结果。训练时，generator则使用编码器的全部输出。下面是推理代码示例。</p>
<pre><code class="language-python">def inference_test():
    test_model = make_model(11, 11, 2)
    test_model.eval()
    src = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])
    src_mask = torch.ones(1, 1, 10)

    memory = test_model.encode(src, src_mask)
    ys = torch.zeros(1, 1).type_as(src)

    for i in range(9):
        out = test_model.decode(
            memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)
        )
        prob = test_model.generator(out[:, -1])
        _, next_word = torch.max(prob, dim=1)
        next_word = next_word.data[0]
        ys = torch.cat(
            [ys, torch.empty(1, 1).type_as(src.data).fill_(next_word)], dim=1
        )
</code></pre>
<p>下面是训练代码示例。</p>
<pre><code class="language-python">class SimpleLossCompute:
    "A simple loss compute and train function."

    def __init__(self, generator, criterion):
        self.generator = generator
        self.criterion = criterion

    def __call__(self, x, y, norm):
        x = self.generator(x)
        sloss = (
            self.criterion(
                x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)
            )
            / norm
        )
        return sloss.data * norm, sloss
</code></pre>
<h3 id="52-转换">5.2 转换</h3>
<p>Transformer的输出是最有可能放在输入序列末尾的单词。但输出的最后一个token对应的向量是512维的向量，无法直接用来推理。另外，词表中每个单词都有可能成为下一个单词， 所以模型需要对所有它知道的单词均按可能性打分，最终选出其中最合适的单词推荐给用户。因此我们最终需要得到词表中所有单词作为下一个单词的概率。这需要经历几个阶段才能完成从512维向量得到10000个单词概率的转换。</p>
<ul>
<li>
<p>在 "Embedding"部分，我们已经学习了一种映射，它可以将给定的（one-hot）单词转换为 512 向量，然后才能被Transformer处理。现在编码器-解码器已经处理结束，所以需要反转这个映射（求逆），将输出的 512 向量转换回词表对应的10000 维度空间中。可以理解为，对于最后一个token向量来说有一个分类任务，需要把该向量分类到词表中的正确token，即下一个token的分类。Transformer通过一个线性层达到了这个目的，从向量维度投影到词表长度（[1，embed_size]-&gt;[1，vocab_size]）。其中词汇表中的每个token都有一个对应的值，称为 <code>logit</code>。该过程类似CNN中，卷积层之后再接一个线性层做分类。</p>
</li>
<li>
<p>因为是要预测，所以需要根据模型的输出 logits 为词汇表中的每个token分配一个概率。这些概率决定了每个token成为序列中下一个单词的可能性。具体操作是应用 softmax 函数将 logits 转换为总和为 1 的概率分布。</p>
</li>
</ul>
<p>下图中的Output Probabilities就是线性层后经过Softmax的概率分布，红圈对应就是Generator类。后续会根据这些标记成为下一个单词的可能性对其进行采样。比如我们可以简单地根据对它们进行排名。从中得到最大的值对应的index，然后再去字典中查询，就知道预测的下一个词是什么了，即得到了下一个词在词典中的编号。</p>
<p>简而言之，这最后的线性层和softmax函数共同作用，为模型预测下一个词提供了数学基础和概率指导。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209144729042-1704342798.jpg" alt="" loading="lazy"></p>
<p>在后续的文章中，我们会逐步深入Transformer的各个组成模块。</p>
<h2 id="0x06-解释">0x06 解释</h2>
<p>对于Transformer，迄今为止，我们所了解的更多是实践的成果和经验性的积累，而对于其理论和理解非常缺乏甚至是空白。虽然有些东西我们直观上操作没问题，但是我们有必要看看研究人员如何从理论（机器学习、生物学和数学）角度进行的探索。这些探索试图从本质上理解神经网络和Transformer内部运作的机理，看看其为何是一个好的结构，从而从这个理解角度出发来帮助分析失败案例，探索设计出更好的网络结构和有效的训练方法，减少模型的偏见、幻觉等。也可以使Transformer能够更好的应对提示的长度和复杂性，可以在更有限的计算资源下完成复杂任务，进而为 NLP 领域的发展提供新的思路和方法。</p>
<p>注：以下分类并非是正交的，可能会有彼此交叉。因为数学的具象化为物理，而物理的尽头则为数学，数学与物理相辅相成，成为理解神经网络乃至智能本质不可或缺的手段。</p>
<h3 id="61-机械可解释性">6.1 机械可解释性</h3>
<p>近年来，机械可解释性（Mechanistic Interpretability）成为AI可解释性研究的一个重要方向，该可解释性研究旨在以逆向工程方式剖析AI模型（尤其是黑盒子的神经网络模型），希望理解LLM内部的运行机制，以及定位参数的存储位置。机械可解释性的几个主要流派如下：</p>
<ul>
<li>causal tracing。主要目标是理解LLM的信息流机制。基本思想是当改变模型的某个位置的参数/向量时，查看最终预测的变化情况，通过变化程度定位到对最终输出最重要的位置。</li>
<li>circuit analysis。主要目标是以注意力头和FFN作为基本单元，构建输入到输出的circuit。它的潜在假设是：对于特定的输入/输出，只有一小部分参数很重要。比如该研究中有一部分是探索不同注意力头的功能，另一部分专注于构建整个circuit。</li>
<li>logit lens &amp; neuron analysis。更强调从单个神经元角度进行分析。logit lens把LM head提前加在每个中间layer上，以此观察latent embeddings的特性。也有其它工作把神经元投影到unembedding space中来获取可解释性。</li>
<li>SAE（sparse autoencoder）。这是Anthropic的工作，目的是使用SAE来增加神经元的可解释性。</li>
</ul>
<p>我们以causal tracing和circuit analysis为例来看看有关研究思路。</p>
<h4 id="causal-tracing">causal tracing</h4>
<p>理解语言模型的内部运作意味着定位前向传播中的哪些元素（输入元素、表示和模型组件）负责特定的预测。我们接下来介绍几种定位模型行为的不同方法。</p>
<h5 id="稀疏探测">稀疏探测</h5>
<p>论文"Finding Neurons in a Haystack: Case Studies with Sparse" 提出了稀疏探测，这是一种旨在识别与特定特征相关的 LLM 神经元的技术或概念，并有助于理解高级人类可解释的特征如何在此类模型的神经元激活中表示。</p>
<p>该团队使用自回归LLM，针对 k 个神经元来施加探针来看它们的分类性能。他们将主要发现总结如下：</p>
<ul>
<li>
<p>LLM 的神经元内有大量可解释的结构，稀疏探测是定位此类神经元（即使处于叠加状态）的有效方法，但需要仔细使用和后续分析才能得出严格的结论。</p>
</li>
<li>
<p>许多早期层神经元处于叠加状态，其中特征表示为多语义神经元的稀疏线性组合，每个神经元都会激活大量不相关的 n-gram 和局部模式。此外，根据权重统计和玩具模型的见解，论文得出结论，前 25% 的全连接层比其余层使用更多的叠加。</p>
</li>
<li>
<p>更高层次的上下文和语言特征（例如，is_python_code）似乎是由单语义神经元编码的，而且主要发生在中间层。</p>
</li>
<li>
<p>表示稀疏性随着模型规模的增加而增加，但不同的特征服从不同的动态：一些具有专用神经元的特征随着规模增加而出现；有的特征分裂成具有规模的更细粒度的特征；许多特征保持不变或随机出现。</p>
</li>
</ul>
<h5 id="输入归因">输入归因</h5>
<p>输入归因方法通常用于通过估计输入元素（在LM的情况下为token）在模型预测中的贡献来定位模型行为。</p>
<p>下图给出了在注意力头间计算令牌间贡献的三种方法。仅依赖注意力权重会忽略它们所操作的向量的大小。这种限制可以通过考虑值加权或输出值加权向量（<span class="math inline">\(x′_j\)</span>）的范数来解决，即第二种方法。最后，基于距离的分析根据加权向量与注意力输出的接近程度来估计加权向量的贡献。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209144740954-660378969.jpg" alt="" loading="lazy"></p>
<h5 id="模型组件归因">模型组件归因</h5>
<p>直接逻辑归因（DLA，Direct Logit Attributions）是一种解释词汇空间中模型组件输出激活的技术。DLA应用未嵌入矩阵来模拟内部激活，有效地跳过了下游组件的进一步计算。下图给出了在输出token w上应用DLA 的案例。 分别是(a)注意头的DLA；(b)通过注意头的中间表示的DLA；(c)FFN块的DLA和(d)单个神经元的DLA。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209144749243-1981944358.jpg" alt="" loading="lazy"></p>
<h4 id="circuit-analysis">circuit analysis</h4>
<p>论文"Chain of Thought Empowers Transformers to Solve Inherently Serial Problems“ 将Transformer看作一定深度的复杂电路，分析其可以解决问题的复杂度。电路复杂度分析用 <span class="math inline">\(TC^0\)</span> 表示可以通过一个固定深度的电路解决的计算问题，而足够长的思维链，能将Transformer的表达能力扩展到<span class="math inline">\(TC^0\)</span>之外。</p>
<p>论文指出，从概念上讲，CoT赋予模型执行本质上串行计算的能力，这是Transformer所缺乏的，尤其是在深度较低的情况下。并行处理可以增加填充信息，在宽度上有机会影响采样的概率分布，进而影响最后的推理效果，但是简单的并行推理会导致模型无法提供深度信息。串行处理则通过引入中间信息，加深LLM在范畴对象和态射中遍历的深度，逐步调整采样概率分布，实现更精确的推理。CoT 则提高了低深度Transformer在内在串行问题上的表达能力，让Transformer避免简单并行推理，通过串行的方式去一步步推理。</p>
<p>论文进一步论证，通过T步CoT，使用固定位精度和O(logn) 嵌入大小的固定深度Transformer可以解决任何可由大小为T的布尔电路解决的问题。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209144801619-2019268395.jpg" alt="" loading="lazy"></p>
<p>上图是不同嵌入大小d(n)和CoT长度T(n)的共复杂度类之间的关系图。</p>
<h3 id="62-机器学习角度">6.2 机器学习角度</h3>
<h4 id="前向传播角度">前向传播角度</h4>
<p>有些方法着重研究前向传播的隐状态和权重的映射上，试图通过可视化权重和隐状态来解读语言模型的内部运作。此处我们主要介绍 logit lens。logit lens的作用是通过将 LLM 的隐状态转换为词汇概率来展示模型在生成过程中的表现，这种投影有助于理解 LLM 在生成过程中逐渐构建输出的模式。</p>
<p>Logit Lens的思路和原理是：</p>
<ul>
<li>语言模型在逐层为输入分配特征。那么我们可不可以观察特征是怎么逐层变化的？</li>
<li>既然解码新token的过程是把最终的hidden states用线性层变换一次，然后经过softmax转换为词典的概率分布。那么把LM head提前加在每个中间层上，通过将激活值先经过transformer的最终归一化层，然后与输出嵌入矩阵相乘，就可以将激活值转换为词汇表中每个词的logit。</li>
</ul>
<h4 id="反向传播角度">反向传播角度</h4>
<p>论文"Backward Lens: Projecting Language Model Gradients into the Vocabulary Space"从反向传播矩阵来理解Transformer的运作机制。</p>
<h5 id="动机">动机</h5>
<p>反向传播是将链式法则应用于计算导数并更新深度学习网络模型权重的过程。该过程始于模型执行前向传播，生成预测<span class="math inline">\(\hat y\)</span> 后与期望目标比较，通过损失函数进行量化差异。在此之后，模型开始反向传播，逐层计算梯度。反向传播算法通过计算每一层的梯度来更新模型中的权重。这一机制不仅使模型能够学习新的信息，也为研究人员提供了解释模型行为的机会。目前，关于反向传播的梯度如何影响模型学习和知识存储的探讨仍然较为稀缺。</p>
<p>该研究的动机在于扩展现有的可解释性方法，尤其是将其应用于 LM 的反向传播过程，比如如何将梯度信息有效地应用于模型的知识更新与编辑中。通过分析反向传播中的梯度矩阵，研究者能够更全面地理解信息在模型中的流动。</p>
<p>此外，该论文还提出了一种新的思路：通过将梯度矩阵映射到词汇空间，揭示 LM 在学习新知识时的内在机制。通过这一方法，研究者希望能够明确地理解模型如何在多层次上进行信息存储和记忆。</p>
<h5 id="方案">方案</h5>
<p><strong>将Logit Lens应用于梯度矩阵</strong></p>
<p>在分析中，研究者专注于MLP层，这是识别和编辑存储知识的重要领域。MLP模块由两个紧密连接的矩阵（<span class="math inline">\(FF_1\)</span>和 <span class="math inline">\(FF_2\)</span>）构成。</p>
<p>具体来说，<span class="math inline">\(FF_1\)</span>将输入从 <span class="math inline">\(R^d\)</span>映射到<span class="math inline">\(R^{d_m}\)</span> ，而 <span class="math inline">\(FF_2\)</span>则将其映射回 <span class="math inline">\(R^d\)</span>。由于梯度矩阵的维度高且难以全面分析，因此研究者将每个梯度矩阵的外积形式转换为一组较小的向量。每个由 <span class="math inline">\(x_i^\top \cdot \delta_i\)</span> 形成的矩阵可以同时从两个视角进行解释：一方面作为 <span class="math inline">\(x_i\)</span> 的跨度（线性组合），另一方面作为 <span class="math inline">\(\delta_i\)</span> 的跨度。研究者利用这种双重性，通过聚焦于 n 个向量的线性组合来分析梯度。此外，研究者也指出 <span class="math inline">\(FF_1\)</span>的梯度使用 <span class="math inline">\(x_i\)</span>作为其跨度集合， <span class="math inline">\(FF_2\)</span>的梯度则使用<span class="math inline">\(\delta_i\)</span> 作为其跨度集合。通过这种分析，研究者能够更深入地理解 MLP 层中存储信息的复杂机制，也可以通过构建特定的跨度集合来提高对梯度矩阵的解释能力。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209144816822-468973999.jpg" alt="" loading="lazy"></p>
<p>上图展示了通过<span class="math inline">\(x^⊤·δ\)</span>的外积来计算梯度矩阵的过程。矩阵的每一行由相同的值组成。在图的上方，我们将矩阵描述为δ的跨度（span），而在图的下方，我们将其描述为x的跨度。矢量被转置展示以强调跨度效果。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209144823604-1348156595.jpg" alt="" loading="lazy"></p>
<p>上图展示了依据“莱昂内尔·梅西效力”的prompt，给出”巴黎“这个回答时，LM的一个 MLP 层的前向与反向传播过程，以及梯度对模型更新的影响。具体表现为梯度（以绿色表示）和权重（以蓝色表示）之间的相互作用。MLP的第一个矩阵<span class="math inline">\(FF_1\)</span>试图将在前向传播过程中遇到的信息合并在模型的权重（蓝色）中。利用词汇投影（vocabulary projection）方法，论文作者发现这些信息代表了token“团队”。第二个矩阵<span class="math inline">\(FF_2\)</span>的梯度则旨在将<span class="math inline">\(FF_2\)</span>编码的信息向新目标的embedding 方向移动。</p>
<p><strong>知识存储与模型编辑的机制</strong></p>
<p>我们接下来看看如何利用反向传播中的梯度更新 MLP 层的权重。论文提出了一种称为“印记与偏移”（imprint and shift）机制的双阶段过程。该机制通过结合前向传播的输入和目标嵌入，利用梯度信息在 MLP 层中存储信息。每个 MLP 层的梯度可以表示为正向传播的输入向量和反向传播的 VJP（向量雅可比乘积）的组合。具体地，梯度在更新过程中的表现可以表示为：</p>
<p></p><div class="math display">\[\frac{\partial L}{\partial W} = x_i^\top \cdot \delta_i
\]</div><p></p><p>在这个表达式中， <span class="math inline">\(x_i\)</span> 是前向传播的输入，而 <span class="math inline">\(\delta_i\)</span> 是相应的 VJP。当使用反向传播更新 LM 的 MLP 层时，会发生以下两个主要阶段的变化：</p>
<p>印记阶段：这个过程将给定输入的“印记”附加到 MLP 层。输入 <span class="math inline">\(x_i\)</span>  被加入或减去到  <span class="math inline">\(FF_1\)</span>的神经元中，从而调整每个与输入对应的  <span class="math inline">\(FF_1\)</span>神经元的激活程度。</p>
<p>偏移阶段：此阶段涉及对 <span class="math inline">\(FF_2\)</span>的输出进行调整，具体表现为从 <span class="math inline">\(FF_2\)</span> 的神经元中减去 <span class="math inline">\(\delta_i\)</span>，以放大在启用 VJP 值后对输出的影响。这相当于将之前概率较低的词汇提升为预测可能性更高的目标。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209144848471-1737654682.jpg" alt="" loading="lazy"></p>
<p>上图展示了反向传播的印记和偏移机制。“grad”表示梯度矩阵中的单个神经元。<span class="math inline">\(FF_1\)</span> 的grad的颜色与前向传播输入相同，而<span class="math inline">\(FF_2\)</span>的梯度则与新的目标嵌入相同，这表明它们彼此相似。</p>
<h3 id="63-生物学角度">6.3 生物学角度</h3>
<p>随着神经网络的诞生及后续的辉煌发展，研究者们一直在为神经网络寻找生物学上的解释，生物学上的进展也在启发AI研究人员开发新模型。</p>
<h4 id="星形胶质细胞">星形胶质细胞</h4>
<p>论文"Building transformers from neurons and astrocytes"指出，由神经元和其他称为星形胶质细胞（astrocyte ）的脑细胞组成的生物网络可以执行与 Transformer 相同的核心计算。论文从计算角度探讨了星形胶质细胞在大脑中发挥的作用，并制作了一个数学模型，展示了如何将它们与神经元一起构建一个生物学上合理的 Transformer。</p>
<h5 id="动机-1">动机</h5>
<p>Transformer 会比较句子中的所有单词以生成预测，这个过程称为自注意力。为了让自注意力发挥作用，Transformer 必须以某种形式的记忆保存所有单词，但由于神经元的交流方式，这在生物学上似乎是不可能的。</p>
<p>然而，研究一种略有不同类型的机器学习模型（Dense Associated Memory）的科学家意识到，这种自注意机制可能发生在大脑中，但前提是至少三个神经元之间存在通信。而星形胶质细胞（不是神经元）可以与神经元形成三向连接，即所谓的三方突触。星形胶质细胞可以向神经元发出信号。因为星形胶质细胞的运作时间比神经元长得多——它们通过缓慢升高然后降低钙反应来产生信号——这些细胞可以保存并整合从神经元传递给它们的信息。通过这种方式，星形胶质细胞可以形成一种记忆缓冲区。</p>
<p>因此，论文作者假设星形胶质细胞可以在 Transformer 的计算方式中发挥作用。</p>
<h5 id="方案-1">方案</h5>
<p>论文作者从计算角度探讨了星形胶质细胞在大脑中发挥的作用，建立了神经元-星形胶质细胞网络的数学模型，展示了如何将它们与神经元一起，构建一个生物学上合理的 Transformer，即该模型可以像 Transformer 一样运行。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209144858803-655808281.jpg" alt="" loading="lazy"></p>
<p>上图中，A部分给出了神经元-星形胶质细胞网络的概述。Transformer块由一个前馈网络近似，该前馈网络具有一个星形胶质细胞单元，该单元覆盖了隐藏层和最后一层之间的突触（矩阵H）。B部分则展示了在写入阶段，可以使用Hebbian学习规则更新神经元之间的权重，使用突触前可塑性规则更新神经元与星形胶质细胞之间的权重。在读取阶段，星形胶质细胞在数据流经网络时会调节突触权重H。</p>
<p>通过分析，论文作者表明，他们的生物物理神经元-星形胶质细胞网络理论上与 Transformer 相匹配。此外，他们通过将图像和文本段落输入 Transformer 模型和模拟神经元星形胶质细胞网络，发现两者都以类似的方式回应提示，这证实了论文作者的理论模型。</p>
<h4 id="海马体">海马体</h4>
<p>论文”RELATING TRANSFORMERS TO MODELS AND NEURAL REPRESENTATIONS OF THE HIPPOCAMPAL FORMATION“则从负责记忆的海马体（Hippocampal ）角度做出了分析。</p>
<p>虽然Transformer模型是在完全没有生物学知识辅助的情况下开发出来的，但在数学上，Transformer的架构却和目前神经科学中的海马体模型极其相似，尤其是网格细胞（grid cell）和位置细胞（place cell）。所以基于transformer的大语言模型（比如GPT、Bard等）实际上在模仿海马及内嗅皮层处理信息的方式。采用递归位置编码的Transformer可以精确复制海马结构。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209144918200-1231149351.jpg" alt="" loading="lazy"></p>
<p>上图显示了Transformer如何准确地复制了在海马体中观察到的那些模式。</p>
<p>TEM（Tolman-Eichenbaum Machine）模型是一种神经科学模型，这种序列学习器可以捕捉海马体和内嗅皮层（内侧/外侧；MEC/LEC）中的许多已知神经现象。下图给出了TEM模型的结构，以及它和Transformer的对比。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209144929567-706262674.jpg" alt="" loading="lazy"></p>
<p>该论文的研究似乎也说明了，我们的人脑中存在类似自然语言处理中常用的词向量（包含了词的语义信息）并且编码了词的位置，而且，大脑中似乎也存在一个类 transformer 模型。</p>
<h3 id="64-数学角度">6.4 数学角度</h3>
<h4 id="ode视角">ODE视角</h4>
<p>以微分方程的概念来审视和解释神经网络是近年来兴起的一个新的研究方向，深度神经网络（DNNs）有一个共同特征：输入数据按照顺序被逐层处理，形成一个时间离散的动态系统。基于此，研究者们假设特定类型的神经网络可以看作是离散的微分方程，所以可以使用现成的微分方程求解器来进行计算，希望可以得到效果更好且具有强解释性的结果。</p>
<h5 id="神经常微分方程">神经常微分方程</h5>
<p>论文"Neural Ordinary Differential Equations"提出了一种名为神经常微分方程的模型，这是新一类的深度神经网络。神经常微分方程不拘于对已有架构的修修补补，它完全从另外一个角度考虑如何以连续的方式借助神经网络对数据建模。</p>
<h6 id="推导">推导</h6>
<p>目前较为常用的神经网络，例如残差网络都是通过堆叠一系列的转换块（transformations）或残差块来形成一个隐层状态以建立复杂的转换。当随着网络的层数不断加深，网络推理计算的每一步都足够小时，即接近极限时，我们对网络的隐藏层神经元的连续动态进行参数化，就可以得到对应的Neural ODE。具体推导如下图所示。我们可以将输出层 定义为在某时刻 上常微分方程（ODE）初值问题的解，这个值可以通过一个常微分方程求解器进行计算。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209144939767-877815598.jpg" alt="" loading="lazy"></p>
<h6 id="优势">优势</h6>
<p>其实无论是什么结构的神经网络，本质上都是在拟合一个复杂的非线性复合函数，其中复合的次数其实等价于神经网络的层数。要对网络进行求解，首先需要找到网络参数的梯度，这就涉及到链式法则，其要求在网络前向传播的过程中保留所有层的激活值，并且在反向传播的时候再利用这些激活值进行计算，这对设备内存或显存的占用非常大，因此一般情况下无法训练很深层数的网络。</p>
<p>而Neural ODE来可以直接通过方程求解器来计算网络梯度，但是当网络的层数较深时，计算的误差会逐渐累加，因此引入了一个伴随状态方法（Adjoint State Method）来计算ODE的梯度。该方法将网络梯度的计算转化为解一个ODE，随后可以将隐藏层状态的导数作为一个参数，这样参数就不是原本的离散序列，而是一个连续的向量场（vector field），因此就不需要前向传播去一一计算，也就不需要耗费大量空间来保存中间结果了。</p>
<p>综上所述，Neural ODE框架可以使用伴随状态方法在不存储激活值 的情况下进行网络学习，因此显著减少了原本反向传播时的大量内存使用空间，同时也提供了一个理论框架，从ODE的连续视角来研究深度学习模型。</p>
<h6 id="示例">示例</h6>
<p>神经常微分方程（Neural ODE）的核心操作是对网络隐藏层状态的导数进行参数化，进而建立起与隐藏层强相关的微分方程，如果可以使用某种手段直接将中间层的结果求解出来。比如，论文指出：残差网络与常微分方程（ODE）之间存在着密切的关系。具体来说，残差网络可以被看作是求解常微分方程的欧拉方法的离散化版本，即ResNets可以被视为一组特定Neural ODE的离散化。所以可以使用现成的微分方程求解器来进行计算。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209144948462-67000527.jpg" alt="" loading="lazy"></p>
<p>上图对两种网络进行对比，可以让我们更直观地理解Neural ODE。右侧上方的残差网络定义了有限变换的离散序列。从0到1再到5的转换代表了离散的网络层，每一层都会通过一个激活函数进行非线性转换。我们可以将其中的黑色评估位置视为神经元，它会对其输入进行转换以调整传递的信息。而ODE网络则定义了一个向量场，隐藏状态在其中进行连续转换，黑色的评估点会根据预设的误差容忍度自动调整其位置。</p>
<h5 id="do-residual-neural-networks-discretize-neural-ordinary-differential-equations">Do Residual Neural Networks discretize Neural Ordinary Differential Equations?</h5>
<p>论文”Do Residual Neural Networks discretize Neural Ordinary Differential Equations?“对ResNets与Neural ODEs之间的联系进行进一步的深入研究。</p>
<p>论文首先量化ResNets的隐藏层状态轨迹与其对应的Neural ODE的解之间的距离，随后发现使用梯度下降算法优化ResNets得到的平滑性，可以以一定的速率对Neural ODE进行正则化，并且其能达到的深度以及所需的优化时间与梯度下降算法一致。基于该发现，论文提出可以使用无记忆（memory-free）的离散邻接法（adjoint method）来训练ResNets，并表明如果残差函数与输入符合李普希茨（Lipschitz）条件，这种方法在理论上可以支持较深层ResNets的训练。最后，论文成功地用邻接法在在残差层中没有内存消耗的情况下对非常深的ResNets进行微调。</p>
<h4 id="多粒子动态系统视角">多粒子动态系统视角</h4>
<p>论文"Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View"从多粒子动态系统（MPDS/Multi-Particle Dynamic System）的角度提出了对Transformer架构的新理解，将其在数学上解释为一种数值常微分方程（ODE）求解器。其实，这也是ODE的另一种视角。</p>
<p>多粒子的动态系统是在物理中常见的一种动态系统，在流体力学中通常以动态系统的常微分方程来模拟，这种动态系统的常微分方程叫做对流扩散方程（Convection-diffusion Equation）。比如粒子位置对时间的导数是两个函数的和，第一个函数是 F，这是N个粒子共同作用的一个N元函数（通常被称为Diffusion）；第二个函数是G函数，是对每个粒子单独作用的阐述（通常被称为Convection）。这与 Transformer 结构中的 MHA 与 FFN 形成了一种很自然的契合。因为MHA 考虑到句子中不同单词的语义和依赖关系，从而用这些信息来捕获句子的内在结构和表示。FFN则是独立应用于句子中每个位置的单词，对每个单词采用相同的线性变换，从而把每个位置的上下文编码到更高维度的表示。FFN是Convection的过程，多头自注意力机制就是是Diffusion的过程。因此我们可以用这种多粒子的对流扩散方程来解释 Transformer结构的含义。我们接下来看看其推导思路。</p>
<ul>
<li>
<p>在流体力学中，单个粒子的运动会用两个部分模拟（此处对应下图标号1）：</p>
<ul>
<li>一个是该粒子本身的运动，通常被称为Convection。</li>
<li>一个是是其他粒子对其作用，通常被称为Diffusion。</li>
</ul>
</li>
<li>
<p>对于一个多粒子动态系统(Multi-Particle Dynamic System)，在流体力学中通常以如下图标号2的方程进行模拟。</p>
</li>
<li>
<p>用Lie-trotter法解上面的常微分方程，得到标号3。</p>
</li>
<li>
<p>多头自注意力机制可以用标号4的公式来表示，该公式可以演变成标号6，对应上面多粒子动态系统里的函数F。</p>
</li>
<li>
<p>FFN可以用标号5的公式来表示，对应多粒子动态系统里的函数G。</p>
</li>
<li>
<p>综合标号5和6，得到Transformer的一层为标号7。</p>
</li>
</ul>
<p>因此，Transformer的处理流程可以看作是：tokens从一个初始位置经过一段时间的处理之后，呈现在高维度空间中的另一位置。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209145003371-1113647121.jpg" alt="" loading="lazy"></p>
<p>论文作者也指出，Lie-Trotter splitting scheme 事实上是一种早已被淘汰掉的数值解法，是一阶的近似方法。在实际中去解常微分方程的时候， 人们用的都是Strang splitting。相较而言，Strang splitting 是一种比较好的数值解，是一种二阶的近似方法。因此，论文作者提出了把 Strang splitting 这种常微分方程的数值解法对应一种神经网络的话，这个网络叫做Macaron结构，具体如下图所示。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209145010868-261722254.jpg" alt="" loading="lazy"></p>
<p>上述方法是从纯粹的数学角度出发，将 Deep Neural Networks 看作一种 Ordinary Differential Equation 的 Numerical solver。论文通过找常微分方程更好的数值解的方法，将其对应回一种更好的网络结构。这种新的 Transformer 结构的性能较之传统有了显著提升。与业界公司通过实验来探索不同，Macaron结构是在“人力”思考的基础上，用数学的方法设计出了一个更好的网络。</p>
<h4 id="流映射视角">流映射视角</h4>
<p>论文"A Mathematical Perspective On Transformer"尝试提供一个从数学角度研究 Transformers 通用且易于理解的框架。DNN 可以看作是从一个<span class="math inline">\(\mathbb R^d\)</span>到另一个<span class="math inline">\(\mathbb R^d\)</span>的流映射（Flow Map），而Transformer可以被认为是在<span class="math inline">\(\mathcal P (R^d)\)</span>上的流映射，即在<span class="math inline">\(\mathbb R^d\)</span> 上的概率测度空间（the space of probability measures）的映射。为了实现这种在度量空间进行转换的流映射，Transformers 建立了一个平均场相互作用的粒子系统（mean-field interacting particle system.）。</p>
<p>论文的模型只关注 Transformer 架构的两个关键组成部分：自注意力机制和layer normalization。.</p>
<ul>
<li>layer normalization有效地将粒子限制在时间变化为轴的单位球体<span class="math inline">\(\mathbb{S}^{d-1}\)</span>的空间内部。</li>
<li>自注意力机制是通过经验度量实现粒子之间的非线性耦合（the particular nonlinear coupling of the particles done through the empirical measure）。或者说，自注意力机制是互相作用的粒子系统中的非线性耦合机制（nonlinear coupling mechanism）。</li>
</ul>
<p>完整的Transformer被表示如下图所示。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209145019923-1558555923.jpg" alt="" loading="lazy"></p>
<p>论文还为自注意机制引入了一个更简单好用的替代模型，一个能量函数的 Wasserstein 梯度流，而能量函数在球面上点的最优配置已经有成熟的研究方法。</p>
<h4 id="差分角度">差分角度</h4>
<p>Transformer结构容易往往会过度关注不相关的上下文，从而倾向于将attention权重分配给这些无关的上下文中。其原因是，随着上下文变长，微小的不相关token的注意力之和可能超过对少数相关token的注意力，从而淹没它们。随着输入长度的增加，经典Transformer可能越来越难以捕捉到关键信息。</p>
<p>论文"Differentical Transformer"的作者称这些无关的上下文为注意力噪音（attention noise）。因此，论文作者为了解决注意力噪音问题，提出了DIFF Transformer。具体来说，「差分注意力」（differential attention）将注意力分数计算为两个单独的softmax 注意力图之间的差异，从而通过减法消除了噪声。这样可以能放大对答案范围的注意力并消除噪音，促使模型关注上下文中的关键信息，从而增强上下文建模的能力。</p>
<h5 id="总体架构">总体架构</h5>
<p>为了方便说明，论文使用了仅解码器（decoder-only）模型作为示例来描述该架构。模型的整体架构和传统Transformer 布局一致，整个模型由L个DIFF Transformer层堆叠而成，每层由一个差分注意力模块和前馈网络模块连接形成。给定一个输入序列 x，模型将输入嵌入打包成 <span class="math inline">\(X^0\)</span>，此后输入会被进一步逐层处理，最终获得输出$ X^L$。</p>
<p>相比于 Transformer，差分 Transformer 的主要差别在于使用差分注意力替换了传统的 softmax 注意力，同时保持整体宏观布局不变。此外，论文也参考 LLaMA 采用了 pre-RMSNorm 和 SwiGLU 这两项改进措施。其中 <span class="math inline">\(W^G\)</span>、<span class="math inline">\(W_1\)</span>、$W_2 $是可学习的矩阵。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209145033005-1549944409.jpg" alt="" loading="lazy"></p>
<h5 id="differential-attention差分注意力">Differential Attention（差分注意力）</h5>
<p>「差分注意力」是指两个softmax函数间的差异来消除注意力噪声。这个想法类似于电气工程中提出的差分放大器，将两个信号之间的差来消除输入的共模噪声。此外，降噪耳机的设计也基于类似的想法。</p>
<p>差分注意力机制具体如下：</p>
<ul>
<li>给定输入 X，首先将它们投射成查询、键和值 <span class="math inline">\(Q_1\)</span>、<span class="math inline">\(Q_2\)</span>、<span class="math inline">\(K_1\)</span>、<span class="math inline">\(K_2\)</span>、<span class="math inline">\(V\)</span>。对应下图标号1。</li>
<li>将query和key 向量分为两组，并计算两个单独的softmax注意力，然后将这两个softmax的差值作为最终注意力分数。对应下图标号2。</li>
</ul>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209145048322-607828230.jpg" alt="" loading="lazy"></p>
<p>可以通过 λ 的大小动态的控制两个注意图之间的权衡程度，从而更好的适用不同的输入和任务要求。</p>
<h5 id="多头">多头</h5>
<p>DIFF Transformer中也可以使用多头注意力机制。令 h 表示注意力头的数量。该方法对各个头使用不同的投影矩阵 <span class="math inline">\(W^Q_i 、W^K_i 、W^V_i ，i ∈ [1, h]\)</span>。标量 λ 在同一层内的头之间共享。然后通过拼接各个头的输出并进行投影获得最终结果。</p>
<p>下图给出了多头差异注意力机制和代码示例。其中使用了 GroupNorm (・) 来强调 LN (・) 独立应用于每个 head。由于差分注意力往往具有更稀疏的模式，因此头之间的统计信息更加多样化。为了改进梯度的统计情况，LN (・) 算子会在连接操作之前对每个头进行归一化。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209145059874-537184250.jpg" alt="" loading="lazy"></p>
<p>下面给出了具体代码。地址：<a href="https://github.com/microsoft/unilm/tree/master/Diff-Transformer" target="_blank" rel="noopener nofollow">https://github.com/microsoft/unilm/tree/master/Diff-Transformer</a></p>
<pre><code class="language-python">
class MultiheadDiffAttn(nn.Module):
    def __init__(
        self,
        args,
        embed_dim,
        depth,
        num_heads,
    ):
        super().__init__()
        self.args = args
        self.embed_dim = embed_dim
        
        # arg num_heads set to half of Transformer's num_heads
        self.num_heads = num_heads
        
        # arg decoder_kv_attention_heads set to half of Transformer's num_kv_heads if use GQA
        # set to same as num_heads if use normal MHA
        self.num_kv_heads = args.decoder_kv_attention_heads if args.decoder_kv_attention_heads is not None else num_heads
        self.n_rep = self.num_heads // self.num_kv_heads
        
        self.head_dim = embed_dim // num_heads // 2
        self.scaling = self.head_dim ** -0.5
        
        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False)
        self.k_proj = nn.Linear(embed_dim, embed_dim // self.n_rep, bias=False)
        self.v_proj = nn.Linear(embed_dim, embed_dim // self.n_rep, bias=False)
        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)

        self.lambda_init = lambda_init_fn(depth)
        self.lambda_q1 = nn.Parameter(torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0,std=0.1))
        self.lambda_k1 = nn.Parameter(torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0,std=0.1))
        self.lambda_q2 = nn.Parameter(torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0,std=0.1))
        self.lambda_k2 = nn.Parameter(torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0,std=0.1))

        self.subln = RMSNorm(2 * self.head_dim, eps=1e-5, elementwise_affine=True)
    
    def forward(
        self,
        x,
        rel_pos,
        attn_mask=None,
    ):
        bsz, tgt_len, embed_dim = x.size()
        src_len = tgt_len

        q = self.q_proj(x)
        k = self.k_proj(x)
        v = self.v_proj(x)

        q = q.view(bsz, tgt_len, 2 * self.num_heads, self.head_dim)
        k = k.view(bsz, src_len, 2 * self.num_kv_heads, self.head_dim)
        v = v.view(bsz, src_len, self.num_kv_heads, 2 * self.head_dim)

        q = apply_rotary_emb(q, *rel_pos, interleaved=True)
        k = apply_rotary_emb(k, *rel_pos, interleaved=True)

        offset = src_len - tgt_len
        q = q.transpose(1, 2)
        k = repeat_kv(k.transpose(1, 2), self.n_rep)
        v = repeat_kv(v.transpose(1, 2), self.n_rep)
        q *= self.scaling
        attn_weights = torch.matmul(q, k.transpose(-1, -2))
        if attn_mask is None:
            attn_mask = torch.triu(
                torch.zeros([tgt_len, src_len])
                .float()
                .fill_(float("-inf"))
                .type_as(attn_weights),
                1 + offset,
            )
        attn_weights = torch.nan_to_num(attn_weights)
        attn_weights += attn_mask   
        attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).type_as(
            attn_weights
        )

        lambda_1 = torch.exp(torch.sum(self.lambda_q1 * self.lambda_k1, dim=-1).float()).type_as(q)
        lambda_2 = torch.exp(torch.sum(self.lambda_q2 * self.lambda_k2, dim=-1).float()).type_as(q)
        lambda_full = lambda_1 - lambda_2 + self.lambda_init
        attn_weights = attn_weights.view(bsz, self.num_heads, 2, tgt_len, src_len)
        attn_weights = attn_weights[:, :, 0] - lambda_full * attn_weights[:, :, 1]
        
        attn = torch.matmul(attn_weights, v)
        attn = self.subln(attn)
        attn = attn * (1 - self.lambda_init)
        attn = attn.transpose(1, 2).reshape(bsz, tgt_len, self.num_heads * 2 * self.head_dim)

        attn = self.out_proj(attn)
        return attn
</code></pre>
<h4 id="图灵完备性质">图灵完备性质</h4>
<p>论文"ASK , AND IT SHALL BE GIVEN: TURING COMPLETE - NESS OF PROMPTING"首次从理论层面证明了大语言模型(LLM)中的prompt机制具有图灵完备性。</p>
<p>图灵完备性（Turing Completeness）是计算理论中的一个核心概念，用来描述某个计算系统的计算能力。如果一个系统具备条件分支、循环或递归能力，并具有理论上的无限存储，那么它可以被称为图灵完备。只要给它足够的时间和资源，这种系统能够模拟任意其他可计算的计算机，执行任何可编程的任务。具备这些特征的系统可以用来模拟任何其他图灵完备的系统。因此，图灵完备的系统之间是等价的，理论上可以用来模拟任何其他图灵完备的系统。图灵机被认为是所有可计算过程的最终抽象，在传统计算理论中，图灵机用于衡量其他系统的计算能力。</p>
<p>当我们说LLM的提示是图灵完备的，意味着我们可以将LLM视为一个通用计算器，只需通过精心设计的prompt，一个固定大小的Transformer模型理论上可以计算任何可计算函数，能够完成任何可以编程的任务。更重要的是，这个固定大小的模型在计算复杂度上几乎可以达到所有不限大小的Transformer模型的理论上限。这为我们使用LLM解决复杂问题提供了一个全新的视角，也为prompt工程提供了坚实的理论基础。</p>
<p>论文在理论和技术层面上的主要贡献是：</p>
<ol>
<li>表达能力：展示了提示的图灵完备性。研究者证明，存在一个固定大小的Transformer Γ，对于任何可计算函数 φ，存在一个相应的有限提示<span class="math inline">\(π_φ\)</span>，使得对于任意输入 x，Transformer Γ 在提示<span class="math inline">\(π_φ\)</span> 的指导下能够计算出 φ(x) 的结果。重要的是，构造的 Transformer Γ 与具体的函数 φ 无关，提示<span class="math inline">\(π_φ\)</span> 与输入 x 也无关，且输入 x 可以是任意长度。</li>
<li>链式思维（CoT）复杂性：研究表明，构造的 Transformer Γ 可以在 <span class="math inline">\(O(t(n))\)</span> 步内计算任何$ TIME_2(t(n)) $类函数，并可以在 <span class="math inline">\(O(t(n) log t(n))\)</span> 步内计算任何$ TIME(t(n)) $类函数，即使是对于长度为 n 的输入。值得注意的是，单个 Transformer 也可以达到几乎与所有 Transformer 类相同的 CoT 复杂性。</li>
<li>精度复杂性：研究还展示了构造的 Transformer Γ 可以在$ O(log(n + t(n)))$ 位精度内计算任何 <span class="math inline">\(TIME(t(n))\)</span> 类函数。这意味着，即使是单个 Transformer 也能够达到与所有 Transformer 类相同的精度复杂性。</li>
</ol>
<p>论文揭示了Prompt的真正潜力：通过合适的设计，我们可以让Transformer模型执行任何复杂的计算任务。对于Prompt工程师来说，提示不再只是给定模型的一段简单文本，而可以将提示看作一种编程语言，通过合适的语法和结构来表达复杂的逻辑和操作。这意味着在设计提示时，我们不仅可以关注如何让模型理解任务，更可以从计算理论的角度出发，去设计能够高效完成计算的提示。只要提示设计得足够巧妙，它就可以模拟任意计算过程，这让Prompt工程具备了更深层次的科学基础。</p>
<h4 id="范畴论">范畴论</h4>
<p>Symbolica首席科学家Paul 在2024年六月份发表了一篇文章想要通过范畴论来统一描述和研究深度学习架构。</p>
<p>范畴学是一种研究数学结构和它们之间关系的数学分支。它关注于对象和态射之间的映射关系，以及这些映射关系之间的组合和复合规则。范畴学提供了一种统一的语言，可以描述和比较不同数学结构之间的共性和相似性，从而使得数学家能够在不同领域之间建立联系和发现共性。在进行类比时，范畴学能够帮助我们发现不同数学领域之间的类似性，找到它们之间的共同模式和结构。通过将问题抽象成范畴论的语言，我们可以将原本复杂的问题简化为更一般性的形式，从而更容易进行类比和推理。范畴学的一些基本概念，如对象、态射、同态和自然变换等，可以帮助我们在不同数学领域之间建立桥梁。这种概念的应用可以使得类比更加灵活和高效，从而促进对问题的深入理解和解决。因此，范畴学是数学中进行类比的一个非常有效的工具，它使得数学家和研究者能够在广泛的数学领域中发现新的见解和联系。同时，范畴学也在其他领域，如计算机科学、物理学和哲学等方面得到了广泛的应用。</p>
<p>如果将深度学习模型视为范畴，则深度学习模型的层可以被视为范畴中的对象，层之间的数据流和变换可以被视为态射。在深度学习中，单子可以用来描述模型必须满足的约束，例如对称性或等变性，代数可以用来描述模型的参数和前向传播。单子代数同态可以用来描述模型层之间的转换，例如从一个层的输出到另一个层的输入。这样使用范畴论来构建和分析深度学习模型，可以帮助实现模型的可信性。比如：</p>
<ul>
<li>范畴论提供了一种清晰的方式来描述模型的组件和它们之间的相互作用，这有助于理解模型的工作原理。</li>
<li>通过单子来定义模型必须满足的约束，如等变性和对称性，确保模型的行为符合预期。</li>
<li>可以对模型的属性进行形式化验证，确保它们满足特定的数学和逻辑规则。</li>
</ul>
<p>范畴视角下的transformer，就是“通过预训练找到每层组合分段线性函数”，并参数化。</p>
<p>大语言模型之所以能够很好地回答问题，部分原因在于其训练数据中包含了各种范畴的信息，并且通过学习这些范畴，模型可以在回答问题时进行类比和推理。在训练大语言模型时，通常会使用大规模的语料库，其中包含了丰富的语言和知识。这些语料涵盖了各种主题、领域和概念，使得模型能够从中学习到大量的范畴和相关信息。当模型接收到一个问题时，它可以尝试从已经学到的范畴中找到类似的类比，然后将问题映射到类似的问题上，进而给出答案。这种类比和推理的过程是通过模型内部的神经网络层次结构和权重参数实现的。</p>
<h3 id="64-物理学角度">6.4 物理学角度</h3>
<p>获得玻尔兹曼奖的物理学家霍菲尔德也曾在一次访谈中提到，“如果你不能用数学的语言去描述大脑，那你将永远不知道大脑是怎么工作的”。而鉴于他自身的习惯，“如果一个问题和我熟知的物理毫无联系，那我将无法取得任何的进展”。所以，在人工智能正在重塑人类社会方方面面的同时，我们有必要去了解物理学的思想如何影响人们对神经网络乃至自我的认知。</p>
<p>数据相当于一种初始化，可以驱动网络连接权重的连续更新以获得一个聪明的自适应的物理模型，而这个更新过程是端对端地优化一个目标函数，优化的过程即执行在高维空间的朗之万动力学。神经网络的奥秘正是在于高维的权重空间，它本质上服从正则系综分布。半严格的物理分析给出了权重空间的分布和数据驱动的权重的对称性破缺。从物理直观出发，人们可以获取非平衡神经动力学的稳态全貌以及隐藏的动力学相变；甚至，人们可以将大语言模型的示例泛化归结为两体自旋模型，依此可以洞察智能的本质。</p>
<h4 id="基本动力学特性">基本动力学特性</h4>
<p>论文"THE ASYMPTOTIC BEHAVIOR OF ATTENTION IN TRANSFORMERS"通过严格的数学分析，揭示了Transformer中注意力机制的基本动力学特性。ASYMPTOTIC （渐近特性）研究的是当某一系统或者函数趋于无穷大或某一特定值时，系统或者函数的性质如何变化。</p>
<p>论文的研究表明，在多种条件下，所有token都会渐近地趋于收敛，收敛行为可能导致模型崩溃，限制输出的多样性。这一发现不仅深化了我们对Transformer模型的理解，也为改进模型设计提供了重要理论指导。未来的研究可以基于此来进一步探索更复杂的模型动力学，并开发更有效的注意力机制变体。</p>
<p>主要定理与证明</p>
<ul>
<li>定理3.2：单头情况下，当注意力矩阵为时不变、正定和对称时，系统动力学表现为黎曼梯度向量场。</li>
<li>定理4.1：当token的初始位置位于椭球某个半球的内部时，系统会收敛到共识平衡点。</li>
<li>定理5.1：在自回归情况下，对于几乎所有初始条件，系统都会收敛到由第一个token决定的共识状态。</li>
<li>定理6.1：在符合一定假设条件下（比如U是对称的），如果所有令牌都从其中一个半球开始，则令牌将收敛到共识均衡点（此外，该均衡是渐近稳定的）。</li>
</ul>
<p>下图给出了连续模型（ continuous model ）的几个特定情况下的结果，其中Q(t)、K(t)和U(t)分别表示查询矩阵、键矩阵和值矩阵。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209145137407-1194271909.jpg" alt="" loading="lazy"></p>
<p>下图展示了定理3.2。在下图左侧，我们可以看到10个token在由随机生成的正定对称矩阵定义的椭球体上的运动。正如预期的那样，所有token都收敛到共识均衡。在这种情况下，动力学是一个梯度向量场。图右侧则显示了相应电势（corresponding potential）的时间演化。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209145145278-364291422.jpg" alt="" loading="lazy"></p>
<p>下图展示了定理4.1。在图的左侧展示了10个token在球体上的运动。我们可以看到所有token开始都并保持在一个半球中，它们最终收敛到共识均衡。其时间演化如图右侧所示。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209145154453-1690910290.jpg" alt="" loading="lazy"></p>
<p>下图展示了定理6.1。在图的左侧，我们可以观察到token收敛到共识平衡点，而在右侧给出了对应的时间演化。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209145203049-831918832.jpg" alt="" loading="lazy"></p>
<h4 id="物理自旋系统的结构">物理自旋系统的结构</h4>
<p>博客"Transformers Are Secretly Collectives of Spin Systems"认为Transformer模块的神经网络架构蓝图可以从经典统计力学中熟悉的物理自旋系统的结构中导出。更具体地说，博客作者认为Transformer模块的正反向传播可映射为矢量自旋模型中的计算磁化。进而可以将Transformer想象成可微自旋系统的集合，其行为可以通过训练来塑造。</p>
<p>训练一个深度transformer模型，相当于通过建立一个可微的关联结构来编排一堆transformer模块，其中一个自旋系统的磁化驱动下一个自旋系统。训练过程中的摆动(数十亿)参数会推动自旋系统集合的级联反应行为，以更好地适应由数据和损失函数指定的集合(元)任务。</p>
<h4 id="受力角度">受力角度</h4>
<p>也有研究人员认为，transformer机制本质上是描述一个运动轨迹，attention是message passing， 其实就是计算受力，MLP可以看作是计算在受力作用下的按照运动方程的运动轨迹，transformer优化的过程，就是通过数据训练来寻求作用力和运动方程从而达到构造满足要求的运动轨迹的过程。</p>
<h2 id="0x07-总结">0x07 总结</h2>
<p>我们首先给出LLaMA的架构，这是Transformer应用的经典案例。在其推理过程中，每 step 内，输入一个 token序列，经过Embedding层将输入token序列变为一个三维张量[b, s, h]，经过一系列计算，最后经logits层将计算结果映射至词表空间，输出张量维度为[b, s, vocab_size]。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209145213655-250682472.jpg" alt="" loading="lazy"></p>
<p>Transformer的处理流程就是token流转的过程：token从从一个初始位置经过一段时间在高维度空间中的另一位置，这是从一个语义空间迁移到了另一个语义空间的过程，或者说，token是常微分方程在不同时间、不同维度的表示。在这个过程中，Attention、FFN和esNet 缺一不可但却各司其职，Attention做信息的提取和聚合，Resnet 提供信息带宽，而真正学到的知识或者信息大多存储在 FFN 中。其具体特点如下：</p>
<ul>
<li>
<p>一个句子进来， 它首先被离散化成一个个单词token的集合，然后 Q，K，V就像是指针一样， 将这些单词实体映射到背后的概念，实现实体的识别和概念的绑定</p>
</li>
<li>
<p>transformer中的encoder与RNN中的encoder作用一样，都是做输入序列各个时间步信息的特征抽取。</p>
<ul>
<li>
<p>Y: &lt;-- LayerNorm(Y + Masked-MultiHead(Y))相当于RNN中各个时间步的 <span class="math inline">\(ℎ_{t−1}\)</span> 和$ y_t $。</p>
<p>m_Y=MultiHead(X, Y)相当于RNN中关联encoder与decoder的注意力context计算。</p>
<p>Y = LayerNorm(Y + m_Y)相当于context与RNN单元各个时间步的 <span class="math inline">\(ℎ_{t−1}\)</span> 和$ y_t $的拼接。</p>
</li>
</ul>
</li>
<li>
<p>而 <span class="math inline">\(Attention(Q,K,V) = softmax( \frac{QK^T}{\sqrt d_k} ) \times V\)</span> 则通过累加和乘积的形式，实现概念和概念的一个全连接图， 它代表了所有可能的命题结构（主谓宾）， 并最终得到新的一组可能的命题结构，</p>
</li>
<li>
<p>接下来通过后面的全连接层（类似一个命题结构的词典），得到新的命题（句子）。</p>
</li>
<li>
<p>通过层数的增加， transformer可以组合得到从简单到复杂逻辑的嵌套结构，也就是实现全文级别的推理。</p>
</li>
</ul>
<h3 id="71-效果">7.1 效果</h3>
<p>Transformer论文从三个维度比较了当时特征提取的主流框架。这三个维度分别为：每一层的计算复杂度、串行操作的复杂度、最大路径长度。</p>
<p><img src="https://img2024.cnblogs.com/blog/1850883/202502/1850883-20250209145224918-350785952.jpg" alt="" loading="lazy"></p>
<p>我们可以从这三个指标分别探讨。</p>
<ul>
<li>首先看序列操作的复杂度。这是自注意力机制的唯一弱点。当序列长度 n 比较大的时候，时间复杂度较高。而大模型时代对长文本的诉求，使这个弱点愈发凸显。目前也有很多方法来解决这个问题。</li>
<li>其次看串行操作的复杂度。自注意力机制的复杂度是O(1)，表示一步就可以完成，并行度最高。RNN 则为 n，因为每一个计算都依赖前面的结果，所以需要 n 步才能完成，也就是无法并行。循环层最大的问题是不能并行训练，序列计算复杂度是O(n)。而自注意力层和卷积一样可以完全并行。</li>
<li>最后看看最大路径长度，其表示数据从某个位置传递到另一个位置的最大长度。注意力本来就是全局查询操作，任意两个位置之间都可以直接联系，可以在O(1)的时间里完成所有元素间信息的传递。它的信息传递速度远胜卷积层和循环层；CNN 是 <span class="math inline">\(log_{k} n\)</span>；而 RNN 最坏情况下，开始位置和结束位置的距离为 n.</li>
</ul>
<h3 id="72-优劣">7.2 优劣</h3>
<p>除了上面分析的优点之外，Transformer还有其他优点，比如：</p>
<ul>
<li>模型可解释性比较高（不同单词之间的相关性有多大）。Self-Attention模型更可解释，Attention结果的分布表明了该模型学习到了一些语法和语义信息。RNN由于其内部复杂的状态更新，往往被认为是一种“黑箱”模型，很难理解内部的决策过程。与此相对，Attention机制提供了一种直观的方式来可视化和理解模型是如何关注序列中不同部分的。通过分析注意力权重，我们可以清楚地看到模型在做出预测时，哪些输入元素起到了关键作用。</li>
</ul>
<ul>
<li>高度适应性：Transformer的架构包含了堆叠的编解码器，这种设计使其不仅在自然语言处理领域，在计算机视觉和语音识别等多个领域也能发挥出色的适应性。</li>
</ul>
<p>Transformer的缺点也同样明显，比如：</p>
<ul>
<li>自注意力机制本身具有二次复杂度，这种复杂度使得该架构在涉及长输入序列或资源受限情况下计算成本高昂且占用内存巨大。</li>
<li>位置编码本身就是一个妥协之举。词向量保存了词语的语言学信息（词性、语义）。然而，位置编码在语义空间中并不具有这种可变换性，它相当于人为设计的一种索引。那么，将这种位置编码与词向量相加，就是不合理的，所以不能很好地表征位置信息。</li>
<li>局部信息的获取不如RNN和CNN强。</li>
<li>参数阈值的牢笼。大模型由于参数量大，往往存在大量的冗余参数，这些参数在训练过程中可能并没有学习到有效的信息，反而增加了模型的复杂性和训练的难度。大量的参数还会导致模型发生过拟合问题。参数过多的另一个副作用就是模型无法学习到更高层级的有效特征：由于存在大量的冗余参数，模型可能无法有效地学习到更高层级的特征。这可能会限制模型的性能，尤其是在处理复杂任务时。大型模型的巨量参数还会导致模型的优化过程更为困难，梯度下降等优化算法在大型模型上可能会遇到局部最优、梯度消失或梯度爆炸等问题。</li>
</ul>
<p>正因为Transformer存在的各种问题，研究者们正在寻找各种方法来进行优化，并提高模型的泛化能力和解释性。比如：</p>
<ul>
<li>很多研究者正在探索知识蒸馏、模型剪枝、参数共享等技术来减少模型的参数量，以降低计算资源的消耗，提高训练效率。</li>
<li>很多研究者在对Transformer架构进行改进：注意力模块稀疏化，在注意力中引入记忆信息，对外部记忆（kv对）的注意力运算，线性随机注意力，在Transformer中引入递归，线性注意力（performer）。</li>
<li>很多非 Transformer 研究都循着“保留 RNN 优势的同时，试图达到 Transformer 性能”的方向去努力。</li>
</ul>
<h2 id="0xff-参考">0xFF 参考</h2>
<p>[<a href="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens" target="_blank" rel="noopener nofollow">interpreting GPT: the logit lens</a>](<a href="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens" target="_blank" rel="noopener nofollow">https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens</a>)   <a href="https://www.lesswrong.com/users/nostalgebraist?from=post_header" target="_blank" rel="noopener nofollow">nostalgebraist</a><br>
<a href="https://transformer-circuits.pub/2021/framework/index.html" target="_blank" rel="noopener nofollow">A Mathematical Framework for Transformer Circuits</a> (Anthropic blog 2021)<br>
<a href="https://arxiv.org/pdf/2312.10794.pdf" target="_blank" rel="noopener nofollow">A Mathematical Perspective On Transformers</a><br>
<a href="https://arxiv.org/pdf/2405.00208" target="_blank" rel="noopener nofollow">A PRIMER ON THE INNER WORKINGS OF TRANSFORMER-BASED LANGUAGE MODELS</a><br>
<a href="https://aclanthology.org/2023.acl-long.893.pdf" target="_blank" rel="noopener nofollow">Analyzing Transformers in Embedding Space</a><br>
<a href="https://arxiv.org/pdf/2411.01992v1" target="_blank" rel="noopener nofollow">ASK, AND IT SHALL BE GIVEN: TURING COMPLETENESS OF PROMPTING</a><br>
<a href="https://arxiv.org/abs/2402.12865" target="_blank" rel="noopener nofollow">Backward Lens: Projecting Language Model Gradients into the Vocabulary Space</a><br>
<a href="https://arxiv.org/abs/2402.12875" target="_blank" rel="noopener nofollow">Chain of Thought Empowers Transformers to Solve Inherently Serial Problems</a><br>
<a href="https://arxiv.org/html/2410.05258v1" target="_blank" rel="noopener nofollow">Differential Transformer</a><br>
](<a href="https://arxiv.org/pdf/2410.05258" target="_blank" rel="noopener nofollow">https://arxiv.org/pdf/2410.05258</a>)<br>
<a href="https://arxiv.org/pdf/2410.05258v1" target="_blank" rel="noopener nofollow">DIFFERENTIAL TRANSFORMER</a><br>
<a href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247694282&amp;idx=2&amp;sn=8491ac70f7a53cd41d2b35b37ac7dbe0&amp;chksm=9773403c63a52323b3a69b79782e512f2ddfbb44a66a41e7af2b2ac50169192f023d9dea0e61&amp;mpshare=1&amp;scene=1&amp;srcid=1124Lvnum8iiy6vqUzOeujCl&amp;sharer_shareinfo=19781616b45faef905bd4137365cdca4&amp;sharer_shareinfo_first=19781616b45faef905bd4137365cdca4#rd" target="_blank" rel="noopener nofollow">EMNLP 2024最佳论文：从反向传播矩阵来理解Transformer的运作机制</a>  <a href="" rel="noopener nofollow">PaperWeekly</a><br>
<a href="https://arxiv.org/pdf/2305.01610" target="_blank" rel="noopener nofollow">Finding Neurons in a Haystack: Case Studies with Sparse</a><br>
<a href="https://www.inderscienceonline.com/doi/abs/10.1504/IJCLM.2011.046439" target="_blank" rel="noopener nofollow">Four types of emergence: a typology of complexity and its implications for a science of management</a><br>
<a href="https://arxiv.org/abs/2405.08944" target="_blank" rel="noopener nofollow">Full Stack Transformer Inference Optimization Season 2: Deploying Long-Context Models</a>)Yao Fu | <a href="https://franxyao.github.io/" target="_blank" rel="noopener nofollow">Website</a> | <a href="https://www.notion.so/b536c3d6912149a395931f1e871370db?pvs=21" target="_blank" rel="noopener nofollow">Blog</a> | <a href="https://twitter.com/Francis_YAO_" target="_blank" rel="noopener nofollow">Twitter / X</a><br>
<a href="https://mp.weixin.qq.com/s?__biz=MzI2MjU4MDYwOA==&amp;mid=2247486218&amp;idx=1&amp;sn=b5f8a4ca808e330202fa5b5142f75c89&amp;scene=21#wechat_redirect" target="_blank" rel="noopener nofollow">GPT4技术原理五：大模型的幻觉，解铃还须系铃人</a>  王庆法 <a href="" rel="noopener nofollow">清熙</a><br>
<a href="https://arxiv.org/html/2412.08835v1" target="_blank" rel="noopener nofollow">Grothendieck Graph Neural Networks Framework: An Algebraic Platform for Crafting Topology-Aware GNNs</a><br>
<a href="https://doi.org/10.18653/v1/p19-1356" target="_blank" rel="noopener nofollow">Jawahar, Ganesh, et al. “What Does BERT Learn about the Structure of Language?” ACL 2019</a><br>
<a href="https://arxiv.org/abs/2404.15758" target="_blank" rel="noopener nofollow">Let's Think Dot by Dot: Hidden Computation in Transformer Language Models</a><br>
<a href="https://mp.weixin.qq.com/s?__biz=MzI2MjU4MDYwOA==&amp;mid=2247487413&amp;idx=1&amp;sn=9e73c053d730257df9f5dcb0fa58d15a&amp;scene=21#wechat_redirect" target="_blank" rel="noopener nofollow">LLM CoT的工作原理</a>  王庆法 <a href="" rel="noopener nofollow">清熙</a><br>
<a href="https://mp.weixin.qq.com/s?__biz=Mzg4MzYxODkzMg==&amp;mid=2247496926&amp;idx=1&amp;sn=6d1797aeb57cb5c0cac969d789f32555&amp;chksm=ce846c8226642a3d461a563fe793715a0b1c9390e20f4c2061c4d540c979b5ae24b8e1d43e9f&amp;mpshare=1&amp;scene=1&amp;srcid=1108cbwBmVp7hgcwYrwtgVSP&amp;sharer_shareinfo=16940ef719fb078f2a0e424d1eb8d51f&amp;sharer_shareinfo_first=16940ef719fb078f2a0e424d1eb8d51f#rd" target="_blank" rel="noopener nofollow">LLM的Prompt竟然是图灵完备的？LLM提示范式的第一个研究 | 重磅</a>  AI修猫Prompt<br>
<a href="https://arxiv.org/abs/2111.11418" target="_blank" rel="noopener nofollow">MetaFormer is Actually What You Need for Vision</a><br>
<a href="https://zhuanlan.zhihu.com/p/106662375" target="_blank" rel="noopener nofollow">More About Attention</a>  <a href="https://www.zhihu.com/people/li-xin-chun-6-51" target="_blank" rel="noopener nofollow">李新春</a><br>
<a href="https://arxiv.org/abs/1806.07366" target="_blank" rel="noopener nofollow">Neural Ordinary Differential Equations</a><br>
<a href="https://arxiv.org/html/2410.01131v1" target="_blank" rel="noopener nofollow">nGPT: Normalized Transformer with Representation Learning on the Hypersphere</a><br>
<a href="https://www.cnblogs.com/huggingface/p/17931611.html" target="_blank">Reformer 模型 - 突破语言建模的极限</a>  <a href="https://www.cnblogs.com/huggingface" target="_blank">Hugging Face 博客</a><br>
<a href="https://arxiv.org/html/2410.01104v2" target="_blank" rel="noopener nofollow">softmax is not enough</a><br>
<a href="https://arxiv.org/pdf/2405.06147v1" target="_blank" rel="noopener nofollow">State-Free Inference of State-Space Models:The Transfer Function Approach</a><br>
<a href="https://arxiv.org/pdf/2412.02682" target="_blank" rel="noopener nofollow">THE ASYMPTOTIC BEHAVIOR OF ATTENTION IN TRANSFORMERS</a><br>
<a href="https://arxiv.org/pdf/2405.07987" target="_blank" rel="noopener nofollow">The Platonic Representation Hypothesis</a><br>
<a href="https://arxiv.org/pdf/2408.04619" target="_blank" rel="noopener nofollow">TRANSFORMER EXPLAINER: Interactive Learning of Text-Generative Models</a><br>
<a href="https://www.armcvai.cn/2024-10-20/transformer-code.html" target="_blank" rel="noopener nofollow">transformer 模型结构详解及实现</a>  zhang<br>
<a href="https://mcbal.github.io/post/transformers-are-secretly-collectives-of-spin-systems/" target="_blank" rel="noopener nofollow">Transformers Are Secretly Collectives of Spin Systems</a> <a href="https://mcbal.github.io/" target="_blank" rel="noopener nofollow">mcbal</a><br>
<a href="https://mp.weixin.qq.com/s?__biz=Mzg5MzUyNTA3Nw==&amp;mid=2247510362&amp;idx=1&amp;sn=b3f4e20f23c5b429e8d345c442bc4af1&amp;chksm=c1e5cea7fa6f9265b518865228332b23247f1efb064d000a689e88399e6a4102b6304aefc32c&amp;mpshare=1&amp;scene=1&amp;srcid=1124sHDHtbmPgODf43qrdnvB&amp;sharer_shareinfo=6c9955628deea4f2c7a42b4ccb5ce932&amp;sharer_shareinfo_first=6c9955628deea4f2c7a42b4ccb5ce932#rd" target="_blank" rel="noopener nofollow">Transformer在生物学上是否合理？MIT团队用神经元和星形胶质细胞来构建</a>  ScienceAI <a href="" rel="noopener nofollow">药物分子设计</a><br>
<a href="https://mp.weixin.qq.com/s?__biz=Mzk0NzY4NDYyOA==&amp;mid=2247484422&amp;idx=1&amp;sn=a25c2d3276af00ef0efcb51d7f9d2a59&amp;chksm=c2d6cc3ac5d806ce0d77612d569f776eb65bfeeb76c64cd013412e497b8ac7451db36015ce82&amp;mpshare=1&amp;scene=1&amp;srcid=1124pmtR95srlO7MCjOPVRzp&amp;sharer_shareinfo=fc0ef35761f3a1404bf70cda25867a18&amp;sharer_shareinfo_first=fc0ef35761f3a1404bf70cda25867a18#rd" target="_blank" rel="noopener nofollow">Transformer模型‌(上篇)</a> OnlyInfo<br>
<a href="https://mp.weixin.qq.com/s?__biz=Mzk0NzY4NDYyOA==&amp;mid=2247484537&amp;idx=1&amp;sn=7a5ae58c4fc2a8094351908ef1bec4cc&amp;chksm=c3725002f405d9146402e9afaf56d833c126dc6a89d0d687d618a5a7a38870b67dc027ee1c25&amp;cur_album_id=3502237268784218115&amp;scene=190#rd" target="_blank" rel="noopener nofollow">Transformer模型‌(下篇) </a> OnlyInfo<br>
<a href="https://mp.weixin.qq.com/s?__biz=MzI2MjU4MDYwOA==&amp;mid=2247484899&amp;idx=1&amp;sn=2a7eff35279cfdea2a7dedf149789572&amp;chksm=ea49b50cdd3e3c1aa9e16c14040b6966c0d19b5e6af972a8f3b620d125890c9b44aa48e47807&amp;cur_album_id=2884802760154791940&amp;scene=189#wechat_redirect" target="_blank" rel="noopener nofollow">Transformer的物理原理</a>  Matthias Bal <a href="" rel="noopener nofollow">清熙</a><br>
<a href="https://arxiv.org/pdf/1906.02762" target="_blank" rel="noopener nofollow">Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View</a><br>
<a href="https://www.omrimallis.com/posts/understanding-how-llm-inference-works-with-llama-cpp/" target="_blank" rel="noopener nofollow">Understanding how LLM inference works with llama.cpp</a>  <a href="https://www.omrimallis.com/" target="_blank" rel="noopener nofollow">omrimallis</a><br>
<a href="https://www.nature.com/articles/s41598-023-29806-3" target="_blank" rel="noopener nofollow">Wavelets based physics informed neural networks to solve non-linear differential equations</a><br>
<a href="https://www.bilibili.com/video/BV1TZ421j7Ke" target="_blank" rel="noopener nofollow">【官方双语】直观解释注意力机制，Transformer的核心 | 【深度学习第6章】</a>  <a href="https://space.bilibili.com/88461692" target="_blank" rel="noopener nofollow">3Blue1Brown</a><br>
<a href="https://mp.weixin.qq.com/s?__biz=MzI2MjU4MDYwOA==&amp;mid=2247486100&amp;idx=1&amp;sn=e3783824675984818eca14f45a5cfc48&amp;chksm=ea49ba7bdd3e336dc1a33a4d59155ddda75229cc8ddf07fe2856a684b2dbf6685eb4a8774def&amp;scene=21#wechat_redirect" target="_blank" rel="noopener nofollow">万字长文介绍为大语言模型建立的“语言、统计和范畴”数学框架</a>  Tai-Danae Bradley 编译：王庆法<br>
<a href="https://blog.csdn.net/weixin_40920183/article/details/127063880" target="_blank" rel="noopener nofollow">大脑里也有个Transformer！和「海马体」机制相同</a>  <a href="https://blog.csdn.net/weixin_40920183" target="_blank" rel="noopener nofollow">人工智能与算法学习</a><br>
<a href="https://zhuanlan.zhihu.com/p/636138307" target="_blank" rel="noopener nofollow">大语言模型背后的神经科学机制</a>  <a href="https://www.zhihu.com/people/lan-hua-cao-9-86" target="_blank" rel="noopener nofollow">雅牧</a><br>
<a href="https://mp.weixin.qq.com/s?__biz=MzkxMTEzNzQ3NA==&amp;mid=2247486759&amp;idx=1&amp;sn=c0904ef42aba3b4fb7fa936216ec7c56&amp;chksm=c057b92f89720f7df8839815162f51e2c51f92d2591473561f7dc1dfe82292927d22d434c86f&amp;mpshare=1&amp;scene=1&amp;srcid=01059yQREjtVYRXs5SbyYNdH&amp;sharer_shareinfo=c3b3dbbe5f3980744c424c946812b341&amp;sharer_shareinfo_first=c3b3dbbe5f3980744c424c946812b341#rd" target="_blank" rel="noopener nofollow">打开黑匣子的神器来了！Transformer Explainer让Transformer模型透明化</a>  小智 <a href="" rel="noopener nofollow">智驻未来</a><br>
<a href="https://zhuanlan.zhihu.com/p/679025334" target="_blank" rel="noopener nofollow">探索AGI系列 | 番外01. （全新视角理解）Transformer和大脑新皮质的一致性</a>  MetaUniTech<br>
<a href="https://zhuanlan.zhihu.com/p/79151996" target="_blank" rel="noopener nofollow">智源论坛 | 王立威：从经验性的积累到理论空白的弥补</a>  <a href="https://www.zhihu.com/org/bei-jing-zhi-yuan-ren-gong-zhi-neng-yan-jiu-yuan" target="_blank" rel="noopener nofollow">北京智源人工智能研究院</a><br>
<a href="https://zhuanlan.zhihu.com/p/719002810" target="_blank" rel="noopener nofollow">浅谈LLM mechanistic interpretability的几个流派（一）</a>  <a href="https://www.zhihu.com/people/time-passenger" target="_blank" rel="noopener nofollow">时间旅客</a><br>
<a href="https://zhuanlan.zhihu.com/p/996110863" target="_blank" rel="noopener nofollow">理解llama.cpp怎么完成大模型推理的</a>  <a href="https://www.zhihu.com/people/hugulas-chen" target="_blank" rel="noopener nofollow">hugulas</a><br>
<a href="https://mp.weixin.qq.com/s?__biz=MzIwNzg3NTM4Nw==&amp;mid=2247513254&amp;idx=1&amp;sn=4dd486fb37429f96ea054176322fb6bc&amp;chksm=966634c7d006ac8fe2d8f608696ac0a8bb914a807c529e4b396737e46161016232f6bcfca988&amp;mpshare=1&amp;scene=1&amp;srcid=0117yvgIpISdr9rSFWhqyaF7&amp;sharer_shareinfo=2e634a12917914ab7b97f771f5bd6b9d&amp;sharer_shareinfo_first=2e634a12917914ab7b97f771f5bd6b9d#rd" target="_blank" rel="noopener nofollow">神经网络理论研究的物理学思想</a>  黄海平 [现代物理知识杂志]<br>
<a href="https://mp.weixin.qq.com/s?__biz=MzI2MjU4MDYwOA==&amp;mid=2247486292&amp;idx=1&amp;sn=0148e951c2fd92a317d012e2bf3b3952&amp;scene=21#wechat_redirect" target="_blank" rel="noopener nofollow">范畴的相变与知识的形成</a><br>
<a href="https://mp.weixin.qq.com/s?__biz=MzAwOTcyNzA0OQ==&amp;mid=2658979556&amp;idx=1&amp;sn=456a392908382dbddec43e63f5f208d6&amp;chksm=81dae58af400afe662cf8761bab81b138904159e4e184df8caf5507a4265f3742ef3a3a1dc41&amp;mpshare=1&amp;scene=1&amp;srcid=1202Bst6s4F6obWld32vWZaZ&amp;sharer_shareinfo=201f4dab410f6c001da167596f2ed59c&amp;sharer_shareinfo_first=201f4dab410f6c001da167596f2ed59c#rd" target="_blank" rel="noopener nofollow">解读小模型——SLM</a>   半吊子全栈工匠 <a href="" rel="noopener nofollow">喔家ArchiSelf</a><br>
<a href="https://zhuanlan.zhihu.com/p/7579023244" target="_blank" rel="noopener nofollow">论文阅读：Differentical Transformer 差分Transformer</a>  Eddie<br>
<a href="https://mp.weixin.qq.com/s?__biz=MzI2MjU4MDYwOA==&amp;mid=2247487287&amp;idx=1&amp;sn=bb0a67efa0d77e3733225ba6bee629ea&amp;scene=21#wechat_redirect" target="_blank" rel="noopener nofollow">降低大模型幻觉的必由之路</a>  <a href="" rel="noopener nofollow">清熙</a><br>
<a href="https://poloclub.github.io/transformer-explainer" target="_blank" rel="noopener nofollow">https://poloclub.github.io/transformer-explainer</a></p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.04644901953125" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-02-15 09:37">2025-02-15 09:37</span>&nbsp;
<a href="https://www.cnblogs.com/rossiXYZ">罗西的思考</a>&nbsp;
阅读(<span id="post_view_count">7</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18706134" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18706134);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18706134', targetLink: 'https://www.cnblogs.com/rossiXYZ/p/18706134', title: '探秘Transformer系列之（2）---总体架构' })">举报</a>
</div>
        