
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/xueweihan/p/18771405" title="发布于 2025-03-14 08:38">
    <span role="heading" aria-level="2">Open-Sora 2.0 重磅开源！</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<blockquote>
<p>潞晨科技正式推出&nbsp;<strong>Open-Sora 2.0</strong> —— 一款全新开源的 SOTA 视频生成模型，仅 20 万美元（224 张 GPU）成功训练商业级 11B 参数视频生成大模型。开发高性能的视频生成模型通常耗资高昂：Meta 的视频模型训练需要 6000 多张 GPU 卡片，投入数百万美元。</p>
<p>在多项关键指标上，它与动辄百万美元训练成本的模型分庭抗礼，全面提升视频生成的可及性与可拓展性。</p>
</blockquote>
<p><img src="https://img2024.cnblogs.com/blog/759200/202503/759200-20250314082500607-1725800830.png" alt="" loading="lazy"></p>
<p>今天，视频生成领域迎来开源革命！Open-Sora 2.0——全新开源的 <strong>SOTA（State-of-the-Art）</strong>视频生成模型正式发布。</p>
<p>仅用 <strong>20 万美元（224 张 GPU）</strong>成功训练出商业级 11B 参数视频生成大模型，性能直追 HunyuanVideo 和 30B 参数的 Step-Video。权威评测 VBench 及用户偏好测试均证实其卓越表现，在多项关键指标上媲美动辄数百万美元训练成本的闭源模型。</p>
<p>此次发布全面开源<strong>模型权重、推理代码及分布式训练全流程</strong>，让高质量视频生成真正触手可及，进一步提升视频生成的可及性与可拓展性。</p>
<p><img src="https://img2024.cnblogs.com/blog/759200/202503/759200-20250314083137351-1020235417.png" alt="" loading="lazy"></p>
<blockquote>
<p>GitHub 地址：<a href="https://github.com/hpcaitech/Open-Sora" target="_blank" rel="noopener nofollow">github.com/hpcaitech/Open-Sora</a></p>
</blockquote>
<h2 id="1-体验与指标双在线">1. 体验与指标双在线</h2>
<h3 id="11-震撼视觉open-sora-20-demo-先行">1.1 震撼视觉：Open-Sora 2.0 Demo 先行</h3>
<p>🎥 <strong>观看宣传片</strong>，体验 Open-Sora 2.0 的强大生成能力</p>
<p><img src="https://img2024.cnblogs.com/blog/759200/202503/759200-20250314090330892-387584980.gif" alt="" loading="lazy"></p>
<p>🎥 <strong>动作幅度可控</strong>：可根据需求设定运动幅度，以更好地展现人物或场景的细腻动作。</p>
<p><img src="https://img2024.cnblogs.com/blog/759200/202503/759200-20250314082954701-1532179661.gif" alt="" loading="lazy"></p>
<p>🎥 <strong>画质与流畅度</strong>：提供 720p 高分辨率和 24 FPS 流畅视频，让最终视频拥有稳定帧率与细节表现。</p>
<p><img src="https://img2024.cnblogs.com/blog/759200/202503/759200-20250314083010918-28755510.gif" alt="" loading="lazy"></p>
<p>🎥 丰富场景切换：从乡村景色到自然风光场景，Open-Sora 2.0 生成的画面细节与过渡平滑度都有出色的表现。</p>
<p><img src="https://img2024.cnblogs.com/blog/759200/202503/759200-20250314083017307-181034968.gif" alt="" loading="lazy"></p>
<h3 id="12-11b-参数规模媲美主流闭源大模型">1.2 11B 参数规模媲美主流闭源大模型</h3>
<p><strong>媲美 HunyuanVideo 和 30B Step-Video：</strong></p>
<p>Open-Sora 2.0 采用 <strong>11B 参数规模</strong>，训练后在 <strong>VBench 和人工偏好（Human Preference）</strong> 评测上都取得与用高昂成本开发的主流闭源大模型同等水平。</p>
<p><strong>用户偏好评测：</strong></p>
<p>在视觉表现、文本一致性和动作表现三个评估维度上，Open Sora 在至少两个指标上超越了开源 SOTA HunyuanVideo，以及商业模型 Runway Gen-3 Alpha 等，以小成本获取了好性能。</p>
<p><img src="https://img2024.cnblogs.com/blog/759200/202503/759200-20250314083259687-984877639.png" alt="" loading="lazy"></p>
<p><strong>VBench 指标表现强势：</strong></p>
<p>根据视频生成权威榜单 VBench 的评测结果，Open-Sora 模型的性能进步显著。从 Open-Sora 1.2 升级到 2.0 版本后，与行业领先的 OpenAI Sora 闭源模型之间的性能差距大幅缩小，从之前的 <strong>4.52%</strong> 缩减至仅 <strong>0.69%</strong>，几乎实现了性能的全面追平。</p>
<p>此外，Open-Sora 2.0 在 VBench 评测中取得的分数已超过腾讯的 HunyuanVideo，以更低的成本实现了更高的性能，为开源视频生成技术树立了全新标杆！</p>
<h2 id="2-实现突破低成本训练与高效能优化">2. 实现突破：低成本训练与高效能优化</h2>
<p>Open Sora自开源以来，凭借其在视频生成领域的高效与优质表现，吸引了众多开发者的关注与参与。然而，随着项目的深入推进，也面临着高质量视频生成成本居高不下的问题。</p>
<p>为解决这些挑战，Open Sora团队展开了一系列卓有成效的技术探索，显著降低了模型训练成本。根据估算，市面上 10B 以上的开源视频模型，动辄需要上百万美元的单次训练成本，而 Open Sora 2.0 将该成本降低了 5-10 倍。</p>
<p><img src="https://img2024.cnblogs.com/blog/759200/202503/759200-20250314083032158-759746332.png" alt="" loading="lazy"></p>
<p>作为开源视频生成领域的领导者，Open-Sora 不仅继续开源了模型代码和权重，更开源了<strong>全流程训练代码</strong>，成功打造了强大的开源生态圈。</p>
<p>据第三方技术平台统计，Open-Sora 的学术论文引用量半年内获得近百引用，在全球开源影响力排名中稳居首位，领先所有开源的 I2V/T2V 视频生成项目，成为全球影响力最大的开源视频生成项目之一。</p>
<p><img src="https://img2024.cnblogs.com/blog/759200/202503/759200-20250314083037931-1256457746.png" alt="" loading="lazy"></p>
<h3 id="21-模型架构">2.1 模型架构</h3>
<p>Open-Sora 2.0 延续 Open-Sora 1.2 的设计思路，继续采用 <strong>3D 自编码器</strong>和 <strong>Flow Matching 训练框架</strong>，并通过<strong>多桶训练</strong>机制，实现对不同视频长度和分辨率的同时训练。</p>
<p>在模型架构上，引入 <strong>3D 全注意力机制</strong>，进一步提升视频生成质量。同时，采用最新的 <strong>MMDiT 架构</strong>，更精准地捕捉<strong>文本信息与视频内容的关系</strong>，并将模型规模从 <strong>1B 扩展至 11B</strong>。此外，借助 <strong>开源图生视频模型 FLUX</strong> 进行初始化，大幅降低训练成本，实现更高效的视频生成优化。</p>
<h3 id="22-高效训练方法和并行方案全开源">2.2 高效训练方法和并行方案全开源</h3>
<p>为了追求极致的成本优化，Open-Sora 2.0 从四个方面着手削减训练开销。</p>
<p>首先，通过<strong>严格的数据筛选</strong>，确保高质量数据输入，从源头提升模型训练效率。采用<strong>多阶段、多层次的筛选机制</strong>，结合多种过滤器，有效提升视频质量，为模型提供更精准、可靠的训练数据。</p>
<p><img src="https://img2024.cnblogs.com/blog/759200/202503/759200-20250314083043524-323097404.png" alt="" loading="lazy"></p>
<p>其次，高分辨率训练的成本远超低分辨率，达到相同数据量时，计算开销可能高达 <strong>40 倍</strong>。以 <strong>256px、5 秒</strong>的视频为例，其 <strong>tokens 数量约 8 千</strong>，而 <strong>768px</strong> 的视频 <strong>tokens 数量接近 8 万</strong>，相差 <strong>10 倍</strong>，再加上<strong>注意力机制的平方级计算复杂度</strong>，高分辨率训练的代价极其昂贵。</p>
<p>因此，Open-Sora 优先将<strong>算力投入到低分辨率训练，以高效学习运动信息</strong>，在降低成本的同时确保模型能够捕捉关键的动态特征。</p>
<p><img src="https://img2024.cnblogs.com/blog/759200/202503/759200-20250314083048912-683612385.png" alt="" loading="lazy"></p>
<p>与此同时，Open-Sora <strong>优先训练图生视频任务</strong>，以加速模型收敛。相比直接训练高分辨率视频，图生视频模型在<strong>提升分辨率时具备更快的收敛速度</strong>，从而进一步降低训练成本。在推理阶段，除了<strong>直接进行文本生视频（T2V）</strong>，还可以结合<strong>开源图像模型</strong>，通过<strong>文本生图再生视频（T2I2V）</strong>，以获得更精细的视觉效果。</p>
<p>最后，Open-Sora 采用<strong>高效的并行训练方案</strong>，结合 <strong>ColossalAI</strong> 和系统级优化，大幅提升计算资源利用率，实现更高效的视频生成训练。为了最大化训练效率，我们引入了一系列关键技术，包括：</p>
<ul>
<li>
<p><strong>高效的序列并行和 ZeroDP</strong>，优化大规模模型的分布式计算效率。</p>
</li>
<li>
<p><strong>细粒度控制的 Gradient Checkpointing</strong>，在降低显存占用的同时保持计算效率。</p>
</li>
<li>
<p><strong>训练自动恢复机制</strong>，确保 99% 以上的有效训练时间，减少计算资源浪费。</p>
</li>
<li>
<p><strong>高效数据加载与内存管理</strong>，优化 I/O，防止训练阻塞，加速训练流程。</p>
</li>
<li>
<p><strong>高效异步模型保存</strong>，减少模型存储对训练流程的干扰，提高 GPU 利用率。</p>
</li>
<li>
<p><strong>算子优化</strong>，针对关键计算模块进行深度优化，加速训练过程。</p>
</li>
</ul>
<p>这些优化措施协同作用，使 Open-Sora 2.0 在<strong>高性能与低成本</strong>之间取得最佳平衡，大大降低了高质量视频生成模型的训练。</p>
<h3 id="23-高压缩比-ae-带来更高速度">2.3 高压缩比 AE 带来更高速度</h3>
<p>在训练完成后，Open-Sora 面向未来，进一步探索<strong>高压缩比视频自编码器</strong>的应用，以大幅降低推理成本。目前，大多数视频模型仍采用 <strong>4×8×8</strong> 的自编码器，导致 <strong>单卡生成 768px、5 秒视频耗时近 30 分钟</strong>。为解决这一瓶颈，Open-Sora 训练了一款<strong>高压缩比（4×32×32）的视频自编码器，将推理时间缩短至单卡 3 分钟以内</strong>，推理速度提升 <strong>10 倍</strong>。</p>
<p><img src="https://img2024.cnblogs.com/blog/759200/202503/759200-20250314083055101-1444381193.png" alt="" loading="lazy"></p>
<p>要实现高压缩比编码器，需要解决两个核心挑战：<strong>如何训练高压缩但仍具备优秀重建效果的自编码器</strong>，以及<strong>如何利用该编码器训练视频生成模型</strong>。</p>
<p>针对前者，Open-Sora 团队在<strong>视频升降采样模块中引入残差连接</strong>，成功训练出一款<strong>重建质量媲美当前开源 SoTA 视频压缩模型</strong>，且具备更<strong>高压缩比</strong>的 VAE，自此奠定了高效推理的基础。</p>
<p><img src="https://img2024.cnblogs.com/blog/759200/202503/759200-20250314083059610-1241633710.png" alt="" loading="lazy"></p>
<p>高压缩自编码器在训练视频生成模型时面临<strong>更高的数据需求和收敛难度</strong>，通常需要<strong>更多训练数据</strong>才能达到理想效果。为解决这一问题，Open-Sora 提出了<strong>基于蒸馏的优化策略</strong>，以提升 <strong>AE（自编码器）特征空间的表达能力</strong>，并利用<strong>已经训练好的高质量模型</strong>作为初始化，减少训练所需的数据量和时间。</p>
<p>此外，Open-Sora 还<strong>重点训练图生视频任务</strong>，利用<strong>图像特征引导视频生成</strong>，进一步提升高压缩自编码器的收敛速度，使其在更短时间内达到一定生成效果。</p>
<p>Open-Sora 认为，高压缩比视频自编码器将成为<strong>未来降低视频生成成本的关键方向</strong>。目前的初步实验结果已展现出<strong>显著的推理加速效果</strong>，希望能进一步激发社区对这一技术的关注与探索，共同推动高效、低成本的视频生成发展。</p>
<h2 id="3-加入-open-sora-20共同推动-ai-视频革命">3. 加入 Open-Sora 2.0，共同推动 AI 视频革命</h2>
<p>🎬 今天，<strong>Open-Sora 2.0 正式开源！</strong></p>
<ul>
<li>
<p>🔗 <strong>GitHub 开源仓库</strong>：<a href="https://github.com/hpcaitech/Open-Sora" target="_blank" rel="noopener nofollow">github.com/hpcaitech/Open-Sora</a></p>
</li>
<li>
<p>📄 技术报告：<a href="https://github.com/hpcaitech/Open-Sora-Demo/blob/main/paper/Open_Sora_2_tech_report.pdf" target="_blank" rel="noopener nofollow">github.com/hpcaitech/Open-Sora-Demo/blob/main/paper/Open_Sora_2_tech_report.pdf</a></p>
</li>
</ul>
<p>💡 欢迎加入 Open-Sora 社区，探索 AI 视频的未来！</p>
<p>Open-Sora 2.0，<strong>未来已来</strong>。让我们用更少的资源、更开放的生态，<strong>创造属于下一代的数字影像世界！</strong> 🎬✨</p>

</div>
<div id="MySignature" role="contentinfo">
    <div>    
    <p style="border-top: #e0e0e0 1px dashed; border-right: #e0e0e0 1px dashed; border-bottom: #e0e0e0 1px dashed; border-left: #e0e0e0 1px dashed; padding-top: 5px; padding-right: 10px; padding-bottom: 10px; padding-left: 150px; background: url(https://images.cnblogs.com/cnblogs_com/xueweihan/859919/o_200924043112qrcode_for_gh_4fb030b35bb4_258.jpg) #e5f1f4 no-repeat 1% 50%; background-size:130px 130px;font-family: 微软雅黑; font-size: 13px" id="PSignature">
    <br>
    作者：<a href="https://github.com/521xueweihan" target="_blank">削微寒</a>

    <br>
    <strong>扫描左侧的二维码可以联系到我</strong>
    <br>

    <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh"><img alt="知识共享许可协议" style="border-width: 0" src="https://licensebuttons.net/l/by-nc-nd/4.0/88x31.png"></a><br>本作品采用<a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh">署名-非商业性使用-禁止演绎 4.0 国际 </a>进行许可。
    </p>
</div>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.47177476575925925" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-03-14 09:08">2025-03-14 08:38</span>&nbsp;
<a href="https://www.cnblogs.com/xueweihan">削微寒</a>&nbsp;
阅读(<span id="post_view_count">631</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18771405" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18771405);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18771405', targetLink: 'https://www.cnblogs.com/xueweihan/p/18771405', title: 'Open-Sora 2.0 重磅开源！' })">举报</a>
</div>
        