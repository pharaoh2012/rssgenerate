
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/DOMLX/p/18827580" title="发布于 2025-04-15 21:31">
    <span role="heading" aria-level="2">pytorch 实战教程之 Feature Pyramid Networks (FPN) 特征金字塔网络实现代码</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body blogpost-body-html">
<p>原文作者：aircraft</p>
<p>原文链接：<a href="https://www.cnblogs.com/DOMLX/p/18827580">pytorch 实战教程之 Feature Pyramid Networks (FPN) 特征金字塔网络实现代码 - aircraft - 博客园</a></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p><span style="font-size: 18px">　　　　　　　</span><span style="font-size: 18px">学习YOLOv5前的准备就是学习DarkNet53网络，FPN特征金字塔网络，PANet结构，（从SPP到SPPF）SPPF空间金字塔池化等。本篇讲FPN特征金字塔网络。。。</span></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<h3><span style="font-size: 14pt">特征金字塔网络（Feature Pyramid Networks, FPN）结构详解</span></h3>
<h4><span style="font-size: 16px"><strong>1. 核心思想​</strong>​</span></h4>
<p><span style="font-size: 16px">FPN 通过结合 ​<strong>​深层语义信息​</strong>​（高层特征）和 ​<strong>​浅层细节信息​</strong>​（低层特征），构建多尺度的特征金字塔，显著提升目标检测模型对不同尺寸目标的检测能力。</span></p>
<h4><span style="font-size: 16px"><strong>​2. 网络结构组成​</strong>​</span></h4>
<p><span style="font-size: 16px">FPN 由以下核心组件构成：</span></p>
<div class="hyc-common-markdown__table-wrapper">
<table>
<thead>
<tr><th><span style="font-size: 16px">组件</span></th><th><span style="font-size: 16px">作用</span></th><th><span style="font-size: 16px">对应代码部分</span></th></tr>
</thead>
<tbody>
<tr>
<td><span style="font-size: 16px">​<strong>​骨干网络​（自底向上C2-C5）</strong>​</span></td>
<td><span style="font-size: 16px">提取多尺度特征（如ResNet）</span></td>
<td><span style="font-size: 16px"><code>layer1</code>-<code>layer4</code></span></td>
</tr>
<tr>
<td><span style="font-size: 16px">​<strong>​自顶向下路径​（P5-P2）</strong>​</span></td>
<td><span style="font-size: 16px">通过上采样传递高层语义信息</span></td>
<td><span style="font-size: 16px"><code>_upsample_add</code></span></td>
</tr>
<tr>
<td><span style="font-size: 16px">​<strong>​横向连接​</strong>​</span></td>
<td><span style="font-size: 16px">将不同层级的特征对齐通道后融合</span></td>
<td><span style="font-size: 16px"><code>latlayer1</code>-<code>latlayer3</code></span></td>
</tr>
<tr>
<td><span style="font-size: 16px">​<strong>​特征平滑层​</strong>​</span></td>
<td><span style="font-size: 16px">消除上采样带来的混叠效应</span></td>
<td><span style="font-size: 16px"><code>smooth1</code>-<code>smooth3</code></span></td>
</tr>
</tbody>
</table>
</div>
<p>&nbsp;</p>
<p><img src="https://img2024.cnblogs.com/blog/1251892/202504/1251892-20250415204446782-756883510.png" alt="" loading="lazy"></p>
<p>&nbsp;</p>
<h4><span style="font-size: 16px"><strong>​3. 详细结构分解​</strong>​</span></h4>
<h5><span style="font-size: 16px">​<strong>​3.1 骨干网络（Bottom-Up Pathway）​</strong></span></h5>
<h5><span style="font-size: 16px">在这个过程中，特征图的分辨率逐渐降低，而语义信息逐渐丰富。每一层特征图都代表了输入图像在不同尺度上的抽象表示​</span></h5>
<ul>
<li><span style="font-size: 16px">​<strong>​作用​</strong>​：逐级提取特征，分辨率递减，语义信息递增</span></li>
<li><span style="font-size: 16px">​<strong>​典型实现​</strong>​：ResNet的四个阶段（C1-C5）</span></li>
<li><span style="font-size: 16px">​<strong>​输出特征图​</strong>​：</span>
<pre><span style="font-size: 16px"><code>C2: [H/4, W/4, 256]  （高分辨率，低层细节）
C3: [H/8, W/8, 512]
C4: [H/16, W/16, 1024]
C5: [H/32, W/32, 2048] （低分辨率，高层语义）</code></span></pre>
</li>
<li><span style="font-size: 16px">​<strong>​代码对应​</strong>​：&nbsp;</span></li>
</ul>
<div class="cnblogs_code">
<pre><span style="font-size: 16px"><span style="color: rgba(0, 0, 0, 1)"> # 构建四个特征提取阶段（stage）
        # stage1: 不进行下采样（stride</span>=<span style="color: rgba(128, 0, 128, 1)">1</span><span style="color: rgba(0, 0, 0, 1)">）
        self.layer1 </span>= self._make_layer(block, <span style="color: rgba(128, 0, 128, 1)">64</span>, num_blocks[<span style="color: rgba(128, 0, 128, 1)">0</span>], stride=<span style="color: rgba(128, 0, 128, 1)">1</span><span style="color: rgba(0, 0, 0, 1)">)
        # stage2: 进行下采样（stride</span>=<span style="color: rgba(128, 0, 128, 1)">2</span><span style="color: rgba(0, 0, 0, 1)">）
        self.layer2 </span>= self._make_layer(block, <span style="color: rgba(128, 0, 128, 1)">128</span>, num_blocks[<span style="color: rgba(128, 0, 128, 1)">1</span>], stride=<span style="color: rgba(128, 0, 128, 1)">2</span><span style="color: rgba(0, 0, 0, 1)">)
        self.layer3 </span>= self._make_layer(block, <span style="color: rgba(128, 0, 128, 1)">256</span>, num_blocks[<span style="color: rgba(128, 0, 128, 1)">2</span>], stride=<span style="color: rgba(128, 0, 128, 1)">2</span><span style="color: rgba(0, 0, 0, 1)">)
        self.layer4 </span>= self._make_layer(block, <span style="color: rgba(128, 0, 128, 1)">512</span>, num_blocks[<span style="color: rgba(128, 0, 128, 1)">3</span>], stride=<span style="color: rgba(128, 0, 128, 1)">2</span>)</span></pre>
</div>
<p>&nbsp;</p>
<p><span style="font-size: 16px"><strong>骨干网络(自底向上，从C2到C5)：</strong></span></p>
<p><span style="font-size: 16px"><strong>　　</strong>C2到C5代表不同的<a class="hl hl-1" href="https://so.csdn.net/so/search?q=ResNet&amp;spm=1001.2101.3001.7020" target="_blank" data-report-click="{&quot;spm&quot;:&quot;1001.2101.3001.7020&quot;,&quot;dest&quot;:&quot;https://so.csdn.net/so/search?q=ResNet&amp;spm=1001.2101.3001.7020&quot;,&quot;extra&quot;:&quot;{\&quot;searchword\&quot;:\&quot;ResNet\&quot;}&quot;}" data-tit="ResNet" data-pretit="resnet" rel="noopener nofollow">ResNet</a>卷积组，这些卷积组包含了多个Bottleneck结构，组内的特征图大小相同，组间大小递减。</span></p>
<p><span style="font-size: 16px">Bottleneck结构（瓶颈块）：包含三个卷积层，能够有效减少参数数量并提升性能。ResNet-18使用基础的块BasicBlock：两个3*3的卷积层，而ResNet-50使用Bottleneck块：一个1*1的卷积层降低通道数目，然后到3*3的卷积层融合特征，再到1*1的卷积层恢复通道数。</span></p>
<p>&nbsp;</p>
<p><span style="font-size: 18px"><strong>&nbsp;</strong></span></p>
<h5><span style="font-size: 18px"><strong>3.2 自顶向下路径（Top-Down Pathway）​</strong>​<strong>（从P5-P2）</strong>：</span></h5>
<p><span style="font-size: 16px">为了解决高层特征图分辨率低、细节信息少的问题，FPN引入了自顶向下的特征融合路径。首先对C5进行1x1卷积降低通道数得到P5，然后依次进行双线性差值上采样后与C2-C4层横向连接过来的数据直接相加，分别得到P4-P2，P4,P3,P2在通过一个3*3的平滑卷积层使得数据融合输出。</span></p>
<p><span style="font-size: 18px"><strong>​流程​</strong>​：</span></p>
<ol>
<li><span style="font-size: 16px">​<strong>​顶层处理​</strong>​：C5 → 1x1卷积 → P5</span></li>
<li><span style="font-size: 16px">​<strong>​逐级上采样​</strong>​：P5 → 上采样 → 与C4融合 → P4 → 上采样 → 与C3融合 → P3 ...</span></li>
</ol>
<div class="cnblogs_code">
<pre><span style="font-size: 16px"><span style="color: rgba(0, 0, 0, 1)">P5 (高层语义)
  ↓ 上采样2x
P4 </span>= P5上采样 +<span style="color: rgba(0, 0, 0, 1)"> C4投影
  ↓ 上采样2x
P3 </span>= P4上采样 +<span style="color: rgba(0, 0, 0, 1)"> C3投影
  ↓ 上采样2x
P2 </span>= P3上采样 + C2投影</span></pre>
</div>
<p><span style="font-size: 16px"><strong>核心操作就是通过双线性上采样后的高层特征与浅层数据直接相加后续融合​</strong>​：&nbsp;</span></p>
<div class="cnblogs_code">
<pre><span style="font-size: 16px"><span style="color: rgba(0, 0, 0, 1)">def _upsample_add(self, x, y):
    _,_,H,W </span>=<span style="color: rgba(0, 0, 0, 1)"> y.size()
    </span><span style="color: rgba(0, 0, 255, 1)">return</span> F.interpolate(x, (H,W), mode=<span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">bilinear</span><span style="color: rgba(128, 0, 0, 1)">'</span>) + y  # 双线性上采样</span></pre>
</div>
<p>&nbsp;</p>
<p>&nbsp;</p>
<h5><span style="font-size: 18px"><strong>3.3 横向连接（Lateral Connections）​：</strong></span></h5>
<p><span style="font-size: 16px">目的是为了将上采样后的高语义特征与浅层的定位细节进行融合，实现多尺度特征融合​​（通过横向连接将浅层细节与深层语义结合），横向连接不仅有助于传递低层特征图的细节信息，还可以增强高层特征图的定位能力。高语义特征经过上采样后，其长宽与对应的浅层特征相同，而通道数固定为256。因此需要对特征C2——C4进行1x1卷积使得其通道数变为256.，然后两者进行逐元素相加得到P4、P3与P2。​</span></p>
<ul>
<li><span style="font-size: 16px">​<strong>​作用​</strong>​：将骨干网络特征与上采样特征对齐通道</span></li>
<li><span style="font-size: 16px">​<strong>​实现方式​</strong>​：1x1卷积（通道压缩/对齐）</span></li>
<li><span style="font-size: 16px">​<strong>​代码对应​</strong>​：&nbsp;</span></li>
</ul>
<div class="cnblogs_code">
<pre><span style="font-size: 16px">self.latlayer1 = nn.Conv2d(<span style="color: rgba(128, 0, 128, 1)">1024</span>, <span style="color: rgba(128, 0, 128, 1)">256</span>, <span style="color: rgba(128, 0, 128, 1)">1</span><span style="color: rgba(0, 0, 0, 1)">)  # C4 (1024通道) → 256通道
self.latlayer2 </span>= nn.Conv2d(<span style="color: rgba(128, 0, 128, 1)">512</span>, <span style="color: rgba(128, 0, 128, 1)">256</span>, <span style="color: rgba(128, 0, 128, 1)">1</span><span style="color: rgba(0, 0, 0, 1)">)   # C3 (512通道) → 256通道
self.latlayer3 </span>= nn.Conv2d(<span style="color: rgba(128, 0, 128, 1)">256</span>, <span style="color: rgba(128, 0, 128, 1)">256</span>, <span style="color: rgba(128, 0, 128, 1)">1</span>)   # C2 (256通道) → 保持256通道</span></pre>
</div>
<p>&nbsp;</p>
<h5><span style="font-size: 16px"><span style="font-size: 18px"><strong>​3.4 特征平滑（Smoothing）​：</strong></span></span></h5>
<p><span style="font-size: 16px">得到相加后的特征后，利用3x3卷积对生成的P2，P3，P4进行融合。目的是消除上采样过程中带来的重叠效应，以生成最终的特征图。​</span></p>
<ul>
<li><span style="font-size: 16px">​<strong>​作用​</strong>​：消除上采样导致的锯齿状伪影</span></li>
<li><span style="font-size: 16px">​<strong>​实现方式​</strong>​：3x3卷积（不改变分辨率）</span></li>
<li><span style="font-size: 16px">​<strong>​代码对应​</strong>​：&nbsp;</span></li>
</ul>
<div class="cnblogs_code">
<pre><span style="font-size: 16px">self.smooth1 = nn.Conv2d(<span style="color: rgba(128, 0, 128, 1)">256</span>, <span style="color: rgba(128, 0, 128, 1)">256</span>, <span style="color: rgba(128, 0, 128, 1)">3</span>, padding=<span style="color: rgba(128, 0, 128, 1)">1</span><span style="color: rgba(0, 0, 0, 1)">)  # P4平滑
self.smooth2 </span>= nn.Conv2d(<span style="color: rgba(128, 0, 128, 1)">256</span>, <span style="color: rgba(128, 0, 128, 1)">256</span>, <span style="color: rgba(128, 0, 128, 1)">3</span>, padding=<span style="color: rgba(128, 0, 128, 1)">1</span><span style="color: rgba(0, 0, 0, 1)">)  # P3平滑
self.smooth3 </span>= nn.Conv2d(<span style="color: rgba(128, 0, 128, 1)">256</span>, <span style="color: rgba(128, 0, 128, 1)">256</span>, <span style="color: rgba(128, 0, 128, 1)">3</span>, padding=<span style="color: rgba(128, 0, 128, 1)">1</span>)  # P2平滑</span></pre>
</div>
<p>&nbsp;</p>
<p>&nbsp;</p>
<h4><span style="font-size: 16px"><span style="font-size: 18px"><strong>4. 输出特征金字塔​</strong></span>​</span></h4>
<div class="hyc-common-markdown__table-wrapper">
<table>
<thead>
<tr><th><span style="font-size: 16px">特征层</span></th><th><span style="font-size: 16px">分辨率（相对于输入）</span></th><th><span style="font-size: 16px">通道数</span></th><th><span style="font-size: 16px">适用目标尺寸</span></th></tr>
</thead>
<tbody>
<tr>
<td><span style="font-size: 16px">P2</span></td>
<td><span style="font-size: 16px">1/4</span></td>
<td><span style="font-size: 16px">256</span></td>
<td><span style="font-size: 16px">小目标（&lt;32x32像素）</span></td>
</tr>
<tr>
<td><span style="font-size: 16px">P3</span></td>
<td><span style="font-size: 16px">1/8</span></td>
<td><span style="font-size: 16px">256</span></td>
<td><span style="font-size: 16px">中等目标（32-96像素）</span></td>
</tr>
<tr>
<td><span style="font-size: 16px">P4</span></td>
<td><span style="font-size: 16px">1/16</span></td>
<td><span style="font-size: 16px">256</span></td>
<td><span style="font-size: 16px">大目标（&gt;96x96像素）</span></td>
</tr>
<tr>
<td><span style="font-size: 16px">P5</span></td>
<td><span style="font-size: 16px">1/32</span></td>
<td><span style="font-size: 16px">256</span></td>
<td><span style="font-size: 16px">极大目标/背景</span></td>
</tr>
</tbody>
</table>
</div>
<p>&nbsp;</p>
<p>&nbsp;</p>
<ul>
<li><span style="font-size: 16px">通过上述步骤，FPN构建了一个特征金字塔（feature pyramid）。这个金字塔包含了从底层到顶层的多个尺度的特征图，每个特征图都融合了不同层次的特征信息。</span></li>
<li><span style="font-size: 16px">特征金字塔的每一层都对应一个特定的尺度范围，使得模型能够同时处理不同大小的目标。</span></li>
</ul>
<h4><span style="font-size: 16px"><span style="font-size: 18px"><strong>5. 设计优势​</strong></span>​</span></h4>
<ol>
<li><span style="font-size: 16px">​<strong>​多尺度预测​</strong>​：每个金字塔层都可独立用于目标检测</span></li>
<li><span style="font-size: 16px">​<strong>​参数共享​</strong>​：所有层级使用相同的检测头（Head）</span></li>
<li><span style="font-size: 16px">​<strong>​计算高效​</strong>​：横向连接仅使用轻量级的1x1卷积</span></li>
<li><span style="font-size: 16px">​<strong>​端到端训练​</strong>​：整个网络可联合优化</span></li>
</ol>
<p>&nbsp;</p>
<h4><span style="font-size: 16px"><span style="font-size: 18px"><strong>6. 典型应用场景​</strong></span>​</span></h4>
<ol>
<li><span style="font-size: 16px">​<strong>​目标检测​</strong>​：Faster R-CNN、Mask R-CNN</span></li>
<li><span style="font-size: 16px">​<strong>​实例分割​</strong>​：Mask预测分支可附加到各金字塔层</span></li>
<li><span style="font-size: 16px">​<strong>​关键点检测​</strong>​：高分辨率特征层（如P2）适合精细定位</span></li>
</ol>
<p>&nbsp;</p>
<p><span style="font-size: 18px">基于pytorch的实现代码（可复制直接运行，注释都打的挺详细了，仔细看即可）：<br></span></p>
<div class="cnblogs_code">
<pre><span style="font-size: 15px"><span style="color: rgba(128, 0, 0, 1)">'''
</span><span style="color: rgba(0, 0, 0, 1)">Feature Pyramid Networks (FPN) 特征金字塔网络实现
论文: 《Feature Pyramid Networks </span><span style="color: rgba(0, 0, 255, 1)">for</span><span style="color: rgba(0, 0, 0, 1)"> Object Detection》
核心思想：通过自顶向下路径和横向连接，构建多尺度特征金字塔
</span><span style="color: rgba(128, 0, 0, 1)">'''
</span><span style="color: rgba(0, 0, 0, 1)">import torch
import torch.nn </span><span style="color: rgba(0, 0, 255, 1)">as</span><span style="color: rgba(0, 0, 0, 1)"> nn
import torch.nn.functional </span><span style="color: rgba(0, 0, 255, 1)">as</span><span style="color: rgba(0, 0, 0, 1)"> F

</span><span style="color: rgba(0, 0, 255, 1)">from</span><span style="color: rgba(0, 0, 0, 1)"> torch.autograd import Variable

</span><span style="color: rgba(0, 0, 255, 1)">class</span><span style="color: rgba(0, 0, 0, 1)"> Bottleneck(nn.Module):
    </span><span style="color: rgba(128, 0, 0, 1)">"""</span><span style="color: rgba(128, 0, 0, 1)">ResNet的瓶颈残差块，通道扩展比例为4</span><span style="color: rgba(128, 0, 0, 1)">"""</span><span style="color: rgba(0, 0, 0, 1)">
    expansion </span>= <span style="color: rgba(128, 0, 128, 1)">4</span>  # 输出通道扩展倍数（最终输出通道数 = planes *<span style="color: rgba(0, 0, 0, 1)"> expansion）

    def __init__(self, in_planes, planes, stride</span>=<span style="color: rgba(128, 0, 128, 1)">1</span><span style="color: rgba(0, 0, 0, 1)">):
        </span><span style="color: rgba(128, 0, 0, 1)">"""
</span><span style="color: rgba(0, 0, 0, 1)">        参数说明：
        in_planes: 输入特征图的通道数
        planes: 中间层的基准通道数（实际输出通道为 planes </span>*<span style="color: rgba(0, 0, 0, 1)"> expansion）
        stride: 第一个卷积层的步长（用于下采样）
        </span><span style="color: rgba(128, 0, 0, 1)">"""
</span><span style="color: rgba(0, 0, 0, 1)">        super(Bottleneck, self).__init__()
        # 第一层：1x1卷积压缩通道（通道数：in_planes </span>-&gt;<span style="color: rgba(0, 0, 0, 1)"> planes）
        self.conv1 </span>= nn.Conv2d(in_planes, planes, kernel_size=<span style="color: rgba(128, 0, 128, 1)">1</span>, bias=<span style="color: rgba(0, 0, 0, 1)">False)
        self.bn1 </span>=<span style="color: rgba(0, 0, 0, 1)"> nn.BatchNorm2d(planes)
        
        # 第二层：3x3卷积处理特征（通道数不变，可能进行下采样）
        self.conv2 </span>= nn.Conv2d(planes, planes, kernel_size=<span style="color: rgba(128, 0, 128, 1)">3</span><span style="color: rgba(0, 0, 0, 1)">, 
                              stride</span>=stride, padding=<span style="color: rgba(128, 0, 128, 1)">1</span>, bias=<span style="color: rgba(0, 0, 0, 1)">False)
        self.bn2 </span>=<span style="color: rgba(0, 0, 0, 1)"> nn.BatchNorm2d(planes)
        
        # 第三层：1x1卷积恢复通道数（通道数：planes </span>-&gt; planes*<span style="color: rgba(0, 0, 0, 1)">expansion）
        self.conv3 </span>= nn.Conv2d(planes, self.expansion*<span style="color: rgba(0, 0, 0, 1)">planes, 
                              kernel_size</span>=<span style="color: rgba(128, 0, 128, 1)">1</span>, bias=<span style="color: rgba(0, 0, 0, 1)">False)
        self.bn3 </span>= nn.BatchNorm2d(self.expansion*<span style="color: rgba(0, 0, 0, 1)">planes)

        # 捷径连接（当输入输出维度不匹配时，使用1x1卷积调整）
        self.shortcut </span>=<span style="color: rgba(0, 0, 0, 1)"> nn.Sequential()
        </span><span style="color: rgba(0, 0, 255, 1)">if</span> stride != <span style="color: rgba(128, 0, 128, 1)">1</span> or in_planes != self.expansion*<span style="color: rgba(0, 0, 0, 1)">planes:
            self.shortcut </span>=<span style="color: rgba(0, 0, 0, 1)"> nn.Sequential(
                nn.Conv2d(in_planes, self.expansion</span>*<span style="color: rgba(0, 0, 0, 1)">planes,
                         kernel_size</span>=<span style="color: rgba(128, 0, 128, 1)">1</span>, stride=stride, bias=<span style="color: rgba(0, 0, 0, 1)">False),
                nn.BatchNorm2d(self.expansion</span>*<span style="color: rgba(0, 0, 0, 1)">planes)
            )

    def forward(self, x):
        # 主路径处理
        </span><span style="color: rgba(0, 0, 255, 1)">out</span> =<span style="color: rgba(0, 0, 0, 1)"> F.relu(self.bn1(self.conv1(x)))  # 压缩通道
        </span><span style="color: rgba(0, 0, 255, 1)">out</span> = F.relu(self.bn2(self.conv2(<span style="color: rgba(0, 0, 255, 1)">out</span><span style="color: rgba(0, 0, 0, 1)">))) # 空间特征处理（可能下采样）
        </span><span style="color: rgba(0, 0, 255, 1)">out</span> = self.bn3(self.conv3(<span style="color: rgba(0, 0, 255, 1)">out</span><span style="color: rgba(0, 0, 0, 1)">))         # 恢复通道数
        
        # 残差连接（如果维度不匹配，通过shortcut调整）
        </span><span style="color: rgba(0, 0, 255, 1)">out</span> +=<span style="color: rgba(0, 0, 0, 1)"> self.shortcut(x)
        </span><span style="color: rgba(0, 0, 255, 1)">out</span> = F.relu(<span style="color: rgba(0, 0, 255, 1)">out</span><span style="color: rgba(0, 0, 0, 1)">)
        </span><span style="color: rgba(0, 0, 255, 1)">return</span> <span style="color: rgba(0, 0, 255, 1)">out</span>


<span style="color: rgba(0, 0, 255, 1)">class</span><span style="color: rgba(0, 0, 0, 1)"> FPN(nn.Module):
    </span><span style="color: rgba(128, 0, 0, 1)">"""</span><span style="color: rgba(128, 0, 0, 1)">特征金字塔网络主结构</span><span style="color: rgba(128, 0, 0, 1)">"""</span><span style="color: rgba(0, 0, 0, 1)">
    def __init__(self, block, num_blocks):
        </span><span style="color: rgba(128, 0, 0, 1)">"""
</span><span style="color: rgba(0, 0, 0, 1)">        参数说明：
        block: 基础构建块类型（本代码中使用Bottleneck）
        num_blocks: 每个stage包含的block数量（列表长度必须为4）
        </span><span style="color: rgba(128, 0, 0, 1)">"""
</span><span style="color: rgba(0, 0, 0, 1)">        super(FPN, self).__init__()
        self.in_planes </span>= <span style="color: rgba(128, 0, 128, 1)">64</span><span style="color: rgba(0, 0, 0, 1)">  # 初始通道数（会在_make_layer中自动更新）
        
        # </span>----------------- 骨干网络初始化 -----------------<span style="color: rgba(0, 0, 0, 1)">
        # 初始卷积层（模仿ResNet的前处理）
        self.conv1 </span>= nn.Conv2d(<span style="color: rgba(128, 0, 128, 1)">3</span>, <span style="color: rgba(128, 0, 128, 1)">64</span>, kernel_size=<span style="color: rgba(128, 0, 128, 1)">7</span>, stride=<span style="color: rgba(128, 0, 128, 1)">2</span>, padding=<span style="color: rgba(128, 0, 128, 1)">3</span>, bias=<span style="color: rgba(0, 0, 0, 1)">False)
        self.bn1 </span>= nn.BatchNorm2d(<span style="color: rgba(128, 0, 128, 1)">64</span><span style="color: rgba(0, 0, 0, 1)">)
        
        # 构建四个特征提取阶段（stage）
        # stage1: 不进行下采样（stride</span>=<span style="color: rgba(128, 0, 128, 1)">1</span><span style="color: rgba(0, 0, 0, 1)">）
        self.layer1 </span>= self._make_layer(block, <span style="color: rgba(128, 0, 128, 1)">64</span>, num_blocks[<span style="color: rgba(128, 0, 128, 1)">0</span>], stride=<span style="color: rgba(128, 0, 128, 1)">1</span><span style="color: rgba(0, 0, 0, 1)">)
        # stage2: 进行下采样（stride</span>=<span style="color: rgba(128, 0, 128, 1)">2</span><span style="color: rgba(0, 0, 0, 1)">）
        self.layer2 </span>= self._make_layer(block, <span style="color: rgba(128, 0, 128, 1)">128</span>, num_blocks[<span style="color: rgba(128, 0, 128, 1)">1</span>], stride=<span style="color: rgba(128, 0, 128, 1)">2</span><span style="color: rgba(0, 0, 0, 1)">)
        self.layer3 </span>= self._make_layer(block, <span style="color: rgba(128, 0, 128, 1)">256</span>, num_blocks[<span style="color: rgba(128, 0, 128, 1)">2</span>], stride=<span style="color: rgba(128, 0, 128, 1)">2</span><span style="color: rgba(0, 0, 0, 1)">)
        self.layer4 </span>= self._make_layer(block, <span style="color: rgba(128, 0, 128, 1)">512</span>, num_blocks[<span style="color: rgba(128, 0, 128, 1)">3</span>], stride=<span style="color: rgba(128, 0, 128, 1)">2</span><span style="color: rgba(0, 0, 0, 1)">)
        
        # </span>----------------- 特征金字塔网络组件 -----------------<span style="color: rgba(0, 0, 0, 1)">
        # 顶层特征处理（将stage4的输出通道压缩到256）
        self.toplayer </span>= nn.Conv2d(<span style="color: rgba(128, 0, 128, 1)">2048</span>, <span style="color: rgba(128, 0, 128, 1)">256</span>, kernel_size=<span style="color: rgba(128, 0, 128, 1)">1</span>, stride=<span style="color: rgba(128, 0, 128, 1)">1</span>, padding=<span style="color: rgba(128, 0, 128, 1)">0</span><span style="color: rgba(0, 0, 0, 1)">)
        
        # 横向连接层（Lateral Connections）
        # 将各stage的输出通道统一为256
        self.latlayer1 </span>= nn.Conv2d(<span style="color: rgba(128, 0, 128, 1)">1024</span>, <span style="color: rgba(128, 0, 128, 1)">256</span>, kernel_size=<span style="color: rgba(128, 0, 128, 1)">1</span>, stride=<span style="color: rgba(128, 0, 128, 1)">1</span>, padding=<span style="color: rgba(128, 0, 128, 1)">0</span><span style="color: rgba(0, 0, 0, 1)">)  # stage3输出通道是1024
        self.latlayer2 </span>= nn.Conv2d(<span style="color: rgba(128, 0, 128, 1)">512</span>, <span style="color: rgba(128, 0, 128, 1)">256</span>, kernel_size=<span style="color: rgba(128, 0, 128, 1)">1</span>, stride=<span style="color: rgba(128, 0, 128, 1)">1</span>, padding=<span style="color: rgba(128, 0, 128, 1)">0</span><span style="color: rgba(0, 0, 0, 1)">)    # stage2输出通道是512
        self.latlayer3 </span>= nn.Conv2d(<span style="color: rgba(128, 0, 128, 1)">256</span>, <span style="color: rgba(128, 0, 128, 1)">256</span>, kernel_size=<span style="color: rgba(128, 0, 128, 1)">1</span>, stride=<span style="color: rgba(128, 0, 128, 1)">1</span>, padding=<span style="color: rgba(128, 0, 128, 1)">0</span><span style="color: rgba(0, 0, 0, 1)">)    # stage1输出通道是256
        
        # 特征平滑层（消除上采样带来的混叠效应）
        self.smooth1 </span>= nn.Conv2d(<span style="color: rgba(128, 0, 128, 1)">256</span>, <span style="color: rgba(128, 0, 128, 1)">256</span>, kernel_size=<span style="color: rgba(128, 0, 128, 1)">3</span>, stride=<span style="color: rgba(128, 0, 128, 1)">1</span>, padding=<span style="color: rgba(128, 0, 128, 1)">1</span><span style="color: rgba(0, 0, 0, 1)">)
        self.smooth2 </span>= nn.Conv2d(<span style="color: rgba(128, 0, 128, 1)">256</span>, <span style="color: rgba(128, 0, 128, 1)">256</span>, kernel_size=<span style="color: rgba(128, 0, 128, 1)">3</span>, stride=<span style="color: rgba(128, 0, 128, 1)">1</span>, padding=<span style="color: rgba(128, 0, 128, 1)">1</span><span style="color: rgba(0, 0, 0, 1)">)
        self.smooth3 </span>= nn.Conv2d(<span style="color: rgba(128, 0, 128, 1)">256</span>, <span style="color: rgba(128, 0, 128, 1)">256</span>, kernel_size=<span style="color: rgba(128, 0, 128, 1)">3</span>, stride=<span style="color: rgba(128, 0, 128, 1)">1</span>, padding=<span style="color: rgba(128, 0, 128, 1)">1</span><span style="color: rgba(0, 0, 0, 1)">)

    def _make_layer(self, block, planes, num_blocks, stride):
        </span><span style="color: rgba(128, 0, 0, 1)">"""
</span><span style="color: rgba(0, 0, 0, 1)">        构建一个特征处理阶段（stage）
        参数：
            block: 块类型（Bottleneck）
            planes: 该stage的基础通道数
            num_blocks: 包含的block数量
            stride: 第一个block的步长
        </span><span style="color: rgba(128, 0, 0, 1)">"""
</span><span style="color: rgba(0, 0, 0, 1)">        # 生成步长列表：第一个block可能下采样，后续保持分辨率
        strides </span>= [stride] + [<span style="color: rgba(128, 0, 128, 1)">1</span>]*(num_blocks-<span style="color: rgba(128, 0, 128, 1)">1</span><span style="color: rgba(0, 0, 0, 1)">)
        layers </span>=<span style="color: rgba(0, 0, 0, 1)"> []
        </span><span style="color: rgba(0, 0, 255, 1)">for</span> stride <span style="color: rgba(0, 0, 255, 1)">in</span><span style="color: rgba(0, 0, 0, 1)"> strides:
            # 逐个添加block，并自动更新输入通道数
            layers.append(block(self.in_planes, planes, stride))
            self.in_planes </span>= planes *<span style="color: rgba(0, 0, 0, 1)"> block.expansion  # 更新为当前block的输出通道数
        </span><span style="color: rgba(0, 0, 255, 1)">return</span> nn.Sequential(*<span style="color: rgba(0, 0, 0, 1)">layers)  # 将多个block打包为顺序模块

    def _upsample_add(self, x, y):
        </span><span style="color: rgba(128, 0, 0, 1)">"""
</span><span style="color: rgba(0, 0, 0, 1)">        特征图上采样并相加（特征融合核心操作）
        参数：
            x: 高层特征（需要上采样）
            y: 低层特征（需要通道对齐）
        </span><span style="color: rgba(128, 0, 0, 1)">"""
</span><span style="color: rgba(0, 0, 0, 1)">        # 获取低层特征y的空间尺寸
        _,_,H,W </span>=<span style="color: rgba(0, 0, 0, 1)"> y.size()
        # 双线性插值上采样（确保尺寸匹配）
        </span><span style="color: rgba(0, 0, 255, 1)">return</span> F.interpolate(x, size=(H,W), mode=<span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">bilinear</span><span style="color: rgba(128, 0, 0, 1)">'</span>, align_corners=False) +<span style="color: rgba(0, 0, 0, 1)"> y

    def forward(self, x):
        # </span>----------------- 自底向上路径（骨干网络） -----------------<span style="color: rgba(0, 0, 0, 1)">
        # Stage </span><span style="color: rgba(128, 0, 128, 1)">0</span>: 初始卷积+<span style="color: rgba(0, 0, 0, 1)">池化
        c1 </span>= F.relu(self.bn1(self.conv1(x)))        # [<span style="color: rgba(128, 0, 128, 1)">1</span>,<span style="color: rgba(128, 0, 128, 1)">64</span>,<span style="color: rgba(128, 0, 128, 1)">300</span>,<span style="color: rgba(128, 0, 128, 1)">450</span><span style="color: rgba(0, 0, 0, 1)">] （假设输入600x900）
        c1 </span>= F.max_pool2d(c1, kernel_size=<span style="color: rgba(128, 0, 128, 1)">3</span>, stride=<span style="color: rgba(128, 0, 128, 1)">2</span>, padding=<span style="color: rgba(128, 0, 128, 1)">1</span>)  # [<span style="color: rgba(128, 0, 128, 1)">1</span>,<span style="color: rgba(128, 0, 128, 1)">64</span>,<span style="color: rgba(128, 0, 128, 1)">150</span>,<span style="color: rgba(128, 0, 128, 1)">225</span><span style="color: rgba(0, 0, 0, 1)">]
        
        # Stage </span><span style="color: rgba(128, 0, 128, 1)">1</span>-<span style="color: rgba(128, 0, 128, 1)">4</span><span style="color: rgba(0, 0, 0, 1)">: 通过四个特征阶段
        c2 </span>= self.layer1(c1)  # [<span style="color: rgba(128, 0, 128, 1)">1</span>,<span style="color: rgba(128, 0, 128, 1)">256</span>,<span style="color: rgba(128, 0, 128, 1)">150</span>,<span style="color: rgba(128, 0, 128, 1)">225</span><span style="color: rgba(0, 0, 0, 1)">] （2个Bottleneck，每个输出256通道）
        c3 </span>= self.layer2(c2)  # [<span style="color: rgba(128, 0, 128, 1)">1</span>,<span style="color: rgba(128, 0, 128, 1)">512</span>,<span style="color: rgba(128, 0, 128, 1)">75</span>,<span style="color: rgba(128, 0, 128, 1)">113</span><span style="color: rgba(0, 0, 0, 1)">]  （下采样，2个Bottleneck）
        c4 </span>= self.layer3(c3)  # [<span style="color: rgba(128, 0, 128, 1)">1</span>,<span style="color: rgba(128, 0, 128, 1)">1024</span>,<span style="color: rgba(128, 0, 128, 1)">38</span>,<span style="color: rgba(128, 0, 128, 1)">57</span><span style="color: rgba(0, 0, 0, 1)">]   （继续下采样）
        c5 </span>= self.layer4(c4)  # [<span style="color: rgba(128, 0, 128, 1)">1</span>,<span style="color: rgba(128, 0, 128, 1)">2048</span>,<span style="color: rgba(128, 0, 128, 1)">19</span>,<span style="color: rgba(128, 0, 128, 1)">29</span><span style="color: rgba(0, 0, 0, 1)">]  （最终高层特征）

        # </span>----------------- 自顶向下路径（特征金字塔构建） -----------------<span style="color: rgba(0, 0, 0, 1)">
        # 顶层特征处理
        p5 </span>= self.toplayer(c5)           # [<span style="color: rgba(128, 0, 128, 1)">1</span>,<span style="color: rgba(128, 0, 128, 1)">256</span>,<span style="color: rgba(128, 0, 128, 1)">19</span>,<span style="color: rgba(128, 0, 128, 1)">29</span>] （<span style="color: rgba(128, 0, 128, 1)">2048</span>-&gt;<span style="color: rgba(0, 0, 0, 1)">256通道）
        
        # 特征融合（自上而下）
        p4 </span>= self._upsample_add(p5, self.latlayer1(c4))  # [<span style="color: rgba(128, 0, 128, 1)">1</span>,<span style="color: rgba(128, 0, 128, 1)">256</span>,<span style="color: rgba(128, 0, 128, 1)">38</span>,<span style="color: rgba(128, 0, 128, 1)">57</span><span style="color: rgba(0, 0, 0, 1)">]
        p3 </span>= self._upsample_add(p4, self.latlayer2(c3))  # [<span style="color: rgba(128, 0, 128, 1)">1</span>,<span style="color: rgba(128, 0, 128, 1)">256</span>,<span style="color: rgba(128, 0, 128, 1)">75</span>,<span style="color: rgba(128, 0, 128, 1)">113</span><span style="color: rgba(0, 0, 0, 1)">]
        p2 </span>= self._upsample_add(p3, self.latlayer3(c2))  # [<span style="color: rgba(128, 0, 128, 1)">1</span>,<span style="color: rgba(128, 0, 128, 1)">256</span>,<span style="color: rgba(128, 0, 128, 1)">150</span>,<span style="color: rgba(128, 0, 128, 1)">225</span><span style="color: rgba(0, 0, 0, 1)">]

        # </span>----------------- 特征平滑处理 -----------------<span style="color: rgba(0, 0, 0, 1)">
        p4 </span>= self.smooth1(p4)  # 保持[<span style="color: rgba(128, 0, 128, 1)">1</span>,<span style="color: rgba(128, 0, 128, 1)">256</span>,<span style="color: rgba(128, 0, 128, 1)">38</span>,<span style="color: rgba(128, 0, 128, 1)">57</span><span style="color: rgba(0, 0, 0, 1)">]
        p3 </span>= self.smooth2(p3)  # 保持[<span style="color: rgba(128, 0, 128, 1)">1</span>,<span style="color: rgba(128, 0, 128, 1)">256</span>,<span style="color: rgba(128, 0, 128, 1)">75</span>,<span style="color: rgba(128, 0, 128, 1)">113</span><span style="color: rgba(0, 0, 0, 1)">]
        p2 </span>= self.smooth3(p2)  # 保持[<span style="color: rgba(128, 0, 128, 1)">1</span>,<span style="color: rgba(128, 0, 128, 1)">256</span>,<span style="color: rgba(128, 0, 128, 1)">150</span>,<span style="color: rgba(128, 0, 128, 1)">225</span><span style="color: rgba(0, 0, 0, 1)">]
        
        </span><span style="color: rgba(0, 0, 255, 1)">return</span><span style="color: rgba(0, 0, 0, 1)"> p2, p3, p4, p5  # 返回多尺度特征（分辨率从高到低排列）

def FPN18():
    </span><span style="color: rgba(128, 0, 0, 1)">"""</span><span style="color: rgba(128, 0, 0, 1)">构建FPN-18结构（类似ResNet-18的配置）</span><span style="color: rgba(128, 0, 0, 1)">"""</span><span style="color: rgba(0, 0, 0, 1)">
    # 参数说明：4个stage分别包含2,</span><span style="color: rgba(128, 0, 128, 1)">2</span>,<span style="color: rgba(128, 0, 128, 1)">2</span><span style="color: rgba(0, 0, 0, 1)">,2个Bottleneck块
    </span><span style="color: rgba(0, 0, 255, 1)">return</span> FPN(Bottleneck, [<span style="color: rgba(128, 0, 128, 1)">2</span>,<span style="color: rgba(128, 0, 128, 1)">2</span>,<span style="color: rgba(128, 0, 128, 1)">2</span>,<span style="color: rgba(128, 0, 128, 1)">2</span><span style="color: rgba(0, 0, 0, 1)">])

def test():
    </span><span style="color: rgba(128, 0, 0, 1)">"""</span><span style="color: rgba(128, 0, 0, 1)">测试函数：验证网络结构</span><span style="color: rgba(128, 0, 0, 1)">"""</span><span style="color: rgba(0, 0, 0, 1)">
    net </span>=<span style="color: rgba(0, 0, 0, 1)"> FPN18()
    # 生成随机输入（1张3通道的600x900图像）
    input_tensor </span>= Variable(torch.randn(<span style="color: rgba(128, 0, 128, 1)">1</span>,<span style="color: rgba(128, 0, 128, 1)">3</span>,<span style="color: rgba(128, 0, 128, 1)">600</span>,<span style="color: rgba(128, 0, 128, 1)">900</span><span style="color: rgba(0, 0, 0, 1)">))
    # 前向传播获取各层特征
    feature_maps </span>=<span style="color: rgba(0, 0, 0, 1)"> net(input_tensor)
    
    # 打印各层输出尺寸
    </span><span style="color: rgba(0, 0, 255, 1)">for</span> i, fm <span style="color: rgba(0, 0, 255, 1)">in</span><span style="color: rgba(0, 0, 0, 1)"> enumerate(feature_maps):
        print(f</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">P{i+2} shape: {fm.size()}</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">)


test()

</span><span style="color: rgba(0, 0, 255, 1)">#if</span> __name__ == "__main__":<span style="color: rgba(0, 0, 0, 1)">
#    test()
    
</span><span style="color: rgba(128, 0, 0, 1)">"""
</span><span style="color: rgba(0, 0, 0, 1)">    预期输出（当输入为600x900时）：
    P2 shape: torch.Size([</span><span style="color: rgba(128, 0, 128, 1)">1</span>, <span style="color: rgba(128, 0, 128, 1)">256</span>, <span style="color: rgba(128, 0, 128, 1)">150</span>, <span style="color: rgba(128, 0, 128, 1)">225</span><span style="color: rgba(0, 0, 0, 1)">])  # 最高分辨率特征
    P3 shape: torch.Size([</span><span style="color: rgba(128, 0, 128, 1)">1</span>, <span style="color: rgba(128, 0, 128, 1)">256</span>, <span style="color: rgba(128, 0, 128, 1)">75</span>, <span style="color: rgba(128, 0, 128, 1)">113</span><span style="color: rgba(0, 0, 0, 1)">])
    P4 shape: torch.Size([</span><span style="color: rgba(128, 0, 128, 1)">1</span>, <span style="color: rgba(128, 0, 128, 1)">256</span>, <span style="color: rgba(128, 0, 128, 1)">38</span>, <span style="color: rgba(128, 0, 128, 1)">57</span><span style="color: rgba(0, 0, 0, 1)">])
    P5 shape: torch.Size([</span><span style="color: rgba(128, 0, 128, 1)">1</span>, <span style="color: rgba(128, 0, 128, 1)">256</span>, <span style="color: rgba(128, 0, 128, 1)">19</span>, <span style="color: rgba(128, 0, 128, 1)">29</span><span style="color: rgba(0, 0, 0, 1)">])    # 最低分辨率特征
    </span>
<span style="color: rgba(128, 0, 0, 1)">"""</span></span></pre>
</div>
<p>&nbsp;</p>
<p><span style="font-size: 18px">&nbsp;</span></p>
<p><span style="font-size: 18px">可能有疑问的代码段详细讲解：</span></p>
<div class="cnblogs_code">
<pre><span style="font-size: 16px"><span style="color: rgba(0, 0, 0, 1)">def _make_layer(self, block, planes, num_blocks, stride):
        </span><span style="color: rgba(128, 0, 0, 1)">"""
</span><span style="color: rgba(0, 0, 0, 1)">        构建一个特征处理阶段（stage）
        参数：
            block: 块类型（Bottleneck）
            planes: 该stage的基础通道数
            num_blocks: 包含的block数量
            stride: 第一个block的步长
        </span><span style="color: rgba(128, 0, 0, 1)">"""
</span><span style="color: rgba(0, 0, 0, 1)">        # 生成步长列表：第一个block可能下采样，后续保持分辨率
        strides </span>= [stride] + [<span style="color: rgba(128, 0, 128, 1)">1</span>]*(num_blocks-<span style="color: rgba(128, 0, 128, 1)">1</span><span style="color: rgba(0, 0, 0, 1)">)
        layers </span>=<span style="color: rgba(0, 0, 0, 1)"> []
        </span><span style="color: rgba(0, 0, 255, 1)">for</span> stride <span style="color: rgba(0, 0, 255, 1)">in</span><span style="color: rgba(0, 0, 0, 1)"> strides:
            # 逐个添加block，并自动更新输入通道数
            layers.append(block(self.in_planes, planes, stride))
            self.in_planes </span>= planes *<span style="color: rgba(0, 0, 0, 1)"> block.expansion  # 更新为当前block的输出通道数
        </span><span style="color: rgba(0, 0, 255, 1)">return</span> nn.Sequential(*layers)  # 将多个block打包为顺序模块</span></pre>
</div>
<p>&nbsp;</p>
<div><span style="font-size: 16px">FPN(Bottleneck, [2,2,2,2])，给FPN网络传瓶颈残差块和&nbsp;[2,2,2,2]每层块的数量</span></div>
<pre><span style="font-size: 16px">block参数这里就指代Bottleneck类，num_blocks就是这里几个块组合，stride步长<br><br></span></pre>
<p><span style="font-size: 16px"><strong>​第一行​</strong>​：<code>strides = [stride] + [1]*(num_blocks-1)</code></span></p>
<ul>
<li><span style="font-size: 16px">这里创建了一个列表<code>strides</code>，由传入的<code>stride</code>参数和一个包含<code>num_blocks-1</code>个1的列表拼接而成。</span></li>
<li><span style="font-size: 16px">例如，如果<code>num_blocks=3</code>且<code>stride=2</code>，则<code>strides</code>为<code>[2, 1, 1]</code>。</span></li>
<li><span style="font-size: 16px">这样做的目的是让第一个block使用给定的<code>stride</code>（可能进行下采样），后续的block使用步长1，保持分辨率不变。</span></li>
</ul>
<p><span style="font-size: 16px">​<strong>​第二行​</strong>​：<code>layers = []</code></span></p>
<ul>
<li><span style="font-size: 16px">初始化一个空列表<code>layers</code>，用于存放该阶段的所有block。</span></li>
</ul>
<p><span style="font-size: 16px">​<strong>​第三行​</strong>​：<code>for stride in strides:</code></span></p>
<ul>
<li><span style="font-size: 16px">遍历之前生成的<code>strides</code>列表中的每一个<code>stride</code>值。</span></li>
</ul>
<p><span style="font-size: 16px">​<strong>​第四行​</strong>​：<code>layers.append(block(self.in_planes, planes, stride))</code></span></p>
<ul>
<li><span style="font-size: 16px">将新创建的block实例添加到<code>layers</code>列表中。</span></li>
<li><span style="font-size: 16px"><code>block</code>是传入的块类型，如Bottleneck。</span></li>
<li><span style="font-size: 16px"><code>self.in_planes</code>是当前输入的通道数，<code>planes</code>是该块的基础通道数，<code>stride</code>是当前步长。</span></li>
<li><span style="font-size: 16px">这一步实例化一个block，并将其添加到层列表中。</span></li>
</ul>
<p><span style="font-size: 16px">​<strong>​第五行​</strong>​：<code>self.in_planes = planes * block.expansion</code></span></p>
<ul>
<li><span style="font-size: 16px">更新<code>self.in_planes</code>为<code>planes</code>乘以<code>block</code>的扩展系数（例如Bottleneck的expansion是4）。</span></li>
<li><span style="font-size: 16px">因为每个block的输出通道数是<code>planes * expansion</code>，所以下一个block的输入通道数需要更新为此值。</span></li>
</ul>
<p><span style="font-size: 16px">​<strong>​第六行​</strong>​：<code>return nn.Sequential(*layers)</code></span></p>
<ul>
<li><span style="font-size: 16px">将<code>layers</code>列表中的block按顺序组合成一个Sequential模块。</span></li>
<li><span style="font-size: 16px"><code>*layers</code>是将列表解包为多个参数，Sequential会按顺序堆叠这些block。</span></li>
</ul>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<pre><span>参考博客：
    https://blog.csdn.net/a8039974/article/details/142288667?spm=1001.2014.3001.5502<br>　　 https://blog.csdn.net/a8039974/article/details/142288667?spm=1001.2014.3001.5502<br></span></pre>
</div>
<div id="MySignature" role="contentinfo">
    转发和使用本文，请注明作者信息和原文地址---本文原作者为aircraft

---大家好我是徐飞机，有没有大佬们的公司招c++开发/图像处理/opengl/opencv/halcon实习的啊，带上我一个呗QAQ。。。hhhhhh  想要免费获取前端，后端，c/c++,matlab，Python，opencv，机器学习，深度学习，安卓，java，等等全套视频教程请关注机器视觉开发公众号，转发集赞28即可百度云获得hhhhhhhh
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.014717270497685186" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-04-16 10:43">2025-04-15 21:31</span>&nbsp;
<a href="https://www.cnblogs.com/DOMLX">aircraft</a>&nbsp;
阅读(<span id="post_view_count">42</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18827580);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18827580', targetLink: 'https://www.cnblogs.com/DOMLX/p/18827580', title: 'pytorch 实战教程之 Feature Pyramid Networks (FPN) 特征金字塔网络实现代码' })">举报</a>
</div>
        