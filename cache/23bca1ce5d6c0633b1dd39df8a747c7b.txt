
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/tshaaa/p/18651129" title="发布于 2025-01-03 23:08">
    <span role="heading" aria-level="2">变分推断(VI)、随机梯度变分推断(SGVI/SGVB)、变分自编码器(VAE)串讲</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                    <div id="cnblogs_post_description" style="display: none">
        <img src="https://img2024.cnblogs.com/blog/2568389/202501/2568389-20250103230742090-366913358.png" alt="变分推断(VI)、随机梯度变分推断(SGVI/SGVB)、变分自编码器(VAE)串讲" class="desc_img">
        主要介绍了变分推断(VI)、随机梯度变分推断(SGVI/SGVB)、变分自编码器(VAE)
    </div>
<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p>参考资料：</p>
<ul>
<li>VI参考：PRML Chapter 10.</li>
<li>SGVI原文：Auto-Encoding Variational Bayes -- Kingma.</li>
<li>VAE参考1：Tutorial on Variational Autoencoders -- CARL DOERSCH.</li>
<li>VAE参考2：<a href="https://deepgenerativemodels.github.io/" target="_blank" rel="noopener nofollow">Stanford University CS236: Deep Generative Models</a>.</li>
</ul>
<h1 id="泛函和变分法">泛函和变分法</h1>
<p>本章主要是了解："变分"这个名称是怎么来的。</p>
<h2 id="函数和泛函">函数和泛函</h2>
<ul>
<li>函数：值到值的映射；</li>
<li>泛函：函数到值的映射。</li>
</ul>
<p>一个典型的泛函 —— 熵的表达式：</p>
<p></p><div class="math display">\[H[p] = \int p(x) \ln p(x) \text{d}x
\]</div><p></p><ul>
<li>函数的导数：输入值发生极小变化时，输出值的变化。</li>
<li>泛函的导数：输入函数发生极小变化时，输出值的变化。</li>
</ul>
<h2 id="泛函的极值">泛函的极值</h2>
<ul>
<li>求泛函的极值：遍历所有可能的函数，来找到最大化或者最小化泛函的那个函数。</li>
<li>变分法：研究<strong>泛函极值</strong>的方法。</li>
<li>在VI中，将隐变量模型的<strong>推断</strong>问题转化成了<strong>求泛函极值</strong>的问题，<strong>所以称它们是"变分"</strong>。</li>
</ul>
<h1 id="变分推断vi">变分推断VI</h1>
<p>本章讨论一种近似推断方法 -- 变分推断。</p>
<h2 id="问题----推断隐变量模型的posterior和evidence">问题 -- 推断隐变量模型的Posterior和Evidence</h2>
<p><strong>符号表示</strong></p>
<ul>
<li><span class="math inline">\(Z\)</span>：隐变量，可以包括模型参数。</li>
<li><span class="math inline">\(X\)</span>：观测数据，显变量。</li>
<li><span class="math inline">\(P(Z)\)</span>：隐变量的先验概率，在这个问题中是已知的。</li>
<li><span class="math inline">\(P(X\mid Z)\)</span>：隐变量的似然概率，在这个问题中是已知的。</li>
<li><span class="math inline">\(P(X,Z)\)</span>：联合概率，可以直接<span class="math inline">\(P(X,Z)=P(X\mid Z)P(Z)\)</span>.</li>
<li><span class="math inline">\(P(Z \mid X)\)</span>：隐变量的后验概率，<em>是需要求出来的</em>。</li>
<li><span class="math inline">\(P(X)\)</span>：Marginal Evidence (或者就称作Evidence)，<em>是需要求出来的</em>。</li>
</ul>
<h2 id="tractable和intractable">Tractable和Intractable</h2>
<p><strong>Tractable</strong><br>
典型例子如隐马尔可夫模型 (HMM)，其主要特点：</p>
<ul>
<li>只有有限个隐状态。</li>
<li>模型的结构简单。</li>
</ul>
<p>所以模型参数已知时，可以直接用基于动态规划的精确推断方法：</p>
<ul>
<li>求<span class="math inline">\(P(X)\)</span>：前向反向算法。</li>
<li>求<span class="math inline">\(P(Z\mid X)\)</span>：维特比算法。</li>
</ul>
<hr>
<p><strong>Intractable</strong><br>
但是很多情况下：</p>
<ul>
<li>涉及对连续随机变量积分。</li>
<li>模型结构很复杂。</li>
</ul>
<p>此时想要精确推断通常intractable. 主要的困难就在于积分：<span class="math inline">\(P(X) = \int P(X,Z)\text{d}Z\)</span>.</p>
<p>因此需要使用近似推断方法。接下来介绍的是一种近似推断方法 —— 变分推断。</p>
<h2 id="elbo的推导">ELBO的推导</h2>
<p>考虑贝叶斯公式，在等式两边加上<span class="math inline">\(\log\)</span>：</p>
<p></p><div class="math display">\[\log P(X) = \log \frac{P(X,Z)}{P(Z|X)} =\log P(X,Z) -\log P(Z\mid X)
\]</div><p></p><p>引入一个由<span class="math inline">\(\phi\)</span>参数化的概率分布<span class="math inline">\(q_\phi(Z)\)</span>:</p>
<p></p><div class="math display">\[\begin{aligned}
\log P(X)   &amp;= \log P(X,Z) -\log P(Z\mid X) \\
			&amp;= \log \frac{P(X,Z)}{q_\phi(Z)} - \log \frac{P(Z\mid X)}{q_\phi(Z)}
\end{aligned}
\]</div><p></p><p>等式两边同时对分布<span class="math inline">\(q_\phi(Z)\)</span>求均值，可以得到：</p>
<p></p><div class="math display">\[\int_Z \log P(X) q_\phi(Z) \text{d} Z = \int_Z q_\phi(Z) \log \frac{P(X,Z)}{q_\phi(Z)}\text{d} Z - \int_Z q_\phi(Z) \log \frac{P(Z\mid X)}{q_\phi(Z)} \text{d}Z
\]</div><p></p><p>由于左边<span class="math inline">\(\log P(X)\)</span>和<span class="math inline">\(Z\)</span>无关，可以直接提出来，又<span class="math inline">\(\int_Z q_\phi(Z) \text{d}Z =1\)</span>，所以有：</p>
<p></p><div class="math display">\[\log P(X) = \int_Z q_\phi(Z) \log \frac{P(X,Z)}{q_\phi(Z)}\text{d} Z - \int_Z q_\phi(Z) \log \frac{P(Z\mid X)}{q_\phi(Z)} \text{d}Z
\]</div><p></p><p>将等式右边的第一项写成期望形式，第二项可以写成KL divergence的形式：</p>
<p></p><div class="math display">\[\begin{aligned}
\log P(X)&amp;= \mathbb E_{Z\sim q_\phi} [\log P(X,Z) - \log q_\phi(Z)] + KL(q_\phi(Z) \mid\mid P(Z\mid X))
\end{aligned}
\]</div><p></p><p>观察这个式子，注意到以下性质：</p>
<ul>
<li>由于<span class="math inline">\(KL\ge 0\)</span>，所以总有<span class="math inline">\(\log P(X) \ge \text{第一项}\)</span>，即第一项是<span class="math inline">\(\log\)</span> Evidence的下界，简称为<span class="math inline">\(ELBO\)</span> (Evidence Lower BOund).</li>
<li>当且仅当<span class="math inline">\(q_\phi(Z) = P(Z\mid X)\)</span>时，<span class="math inline">\(KL=0\)</span>，此时<span class="math inline">\(\log P(X) = ELBO\)</span>.</li>
<li><strong>KEY1</strong>：因此只需要找出<span class="math inline">\(\arg \max_{q_\phi}ELBO\)</span>，就可以使得<span class="math inline">\(ELBO\to \log P(X)\)</span>，同时会有<span class="math inline">\(KL \to 0\)</span>，此时就有<span class="math inline">\(q_\phi(Z) \to P(Z\mid X)\)</span>.</li>
<li><strong>KEY2</strong>：一句话概括变分推断，就是将<em>隐变量模型的推断问题</em>转化成了<em>最优化</em><span class="math inline">\(ELBO\)</span>的问题。</li>
</ul>
<hr>
<p><strong>REMARK1</strong>：把<span class="math inline">\(ELBO\)</span>看作一个泛函<span class="math inline">\(\mathcal L(q_\phi)\)</span>，目标就是<span class="math inline">\(\arg \max_{q_\phi} \mathcal L(q_\phi)\)</span>，即求泛函的极值点 (所以叫变分)。</p>
<p><strong>REMARK2</strong>：变分法天生不是近似方法，因为假如真的能够考虑到所有可能的函数，那就一定可以精确推断到后验概率。但是由于我们<em>通常会对函数的范围做出假定</em>，比如函数是二次型、或者函数满足平均场分解的条件，<em>这导致了最后得出来的总是近似解</em>。</p>
<p><strong>REMARK3</strong>：还有一种基于<em>平均场理论</em>的对<span class="math inline">\(q(z)\)</span>的假定，在此基础上可以利用坐标上升法求最优解，详情可以参考PRML Chapter10.</p>
<h2 id="如果考虑单个样本">如果考虑单个样本</h2>
<ul>
<li><span class="math inline">\(X=\{x^{(1)},...,x^{(n)}\}\)</span>：观测变量。<br>
假定每个样本独立同分布，则<span class="math inline">\(P(X)\)</span>可以拆成连乘：</li>
</ul>
<p></p><div class="math display">\[\log P(X) = \log \prod P(x^{(i)}) = \sum \log P(x^{(i)})
\]</div><p></p><p>引入一个由<span class="math inline">\(\phi^{(i)}\)</span>参数化的分布<span class="math inline">\(q_{\phi^{(i)}}(Z\mid x^{(i)})\)</span>，<span class="math inline">\(\log P(x^{(i)})\)</span>同样可以写成<span class="math inline">\(ELBO+KL\)</span>的形式：</p>
<p></p><div class="math display">\[\begin{aligned}
\log P(x^{(i)}) &amp;= \int_Z q_{\phi^{(i)}}(Z\mid x^{(i)}) \log \frac{P(x^{(i)},Z)}{q_{\phi^{(i)}}(Z\mid x^{(i)})}\text{d} Z - \int_Z q_{\phi^{(i)}}(Z\mid x^{(i)}) \log \frac{P(Z\mid x^{(i)})}{q_{\phi^{(i)}}(Z\mid x^{(i)})} \text{d}Z \\
				&amp;= \mathbb E_{Z\sim q_{\phi^{(i)}}} [\log P(x^{(i)},Z) - \log q_{\phi^{(i)}}(Z\mid x^{(i)})] + KL(q_{\phi^{(i)}}(Z\mid x^{(i)}) \mid\mid P(Z\mid x^{(i)})) \\
				&amp;= ELBO +KL
\end{aligned}
\]</div><p></p><p><strong>NOTE</strong>：<span class="math inline">\(q_\phi\)</span>的参数<span class="math inline">\(\phi\)</span>写作<span class="math inline">\(\phi^{(i)}\)</span>，是因为对于每单个观测变量来说，后验<span class="math inline">\(P(Z\mid x^{(i)})\)</span>是不一样的。之后[[#SGVI]]章节中，为了表示简单，依然省略不写。</p>
<h1 id="sgvi">SGVI</h1>
<p>上一章把隐变量的推断问题转化为了最优化<span class="math inline">\(ELBO\)</span>问题，本章节讲估计<span class="math inline">\(ELBO\)</span>梯度的方法。</p>
<ul>
<li>SGVI即随机梯度(stochastic gradient)变分推断，使用基于梯度的优化方法来最大化<span class="math inline">\(ELBO\)</span>.</li>
<li>求出梯度之后，<span class="math inline">\(q_\phi(z)\)</span>参数更新：<span class="math inline">\(\phi = \phi + \alpha \nabla_\phi ELBO\)</span>.</li>
</ul>
<h2 id="score-function-gradient-estimator">Score function gradient estimator</h2>
<p><span class="math inline">\(ELBO\)</span>对<span class="math inline">\(\phi\)</span>的梯度形如<span class="math inline">\(\nabla_\phi \mathbb E_{z\sim q_\phi(z)}[f(z)]\)</span>，下面先分析这个更一般化的形式的梯度：</p>
<p></p><div class="math display">\[\nabla_\phi \mathbb E_{z\sim q_\phi(z)}[f(z)] = \nabla_\phi \int q_\phi(z) f(z) \text{d}z
\]</div><p></p><p>把梯度符号移入积分号，然后由于<span class="math inline">\(\nabla \log q_\phi=\frac{\nabla q_\phi}{q_\phi}\)</span>，可得：</p>
<p></p><div class="math display">\[\begin{aligned}
\nabla_\phi \mathbb E   &amp;= \int \nabla_\phi q_\phi(z) f(z) \text{d}z \\
						&amp;= \int q_\phi(z) \nabla_\phi \log q_\phi(z) f(z) \text{d}z \\
						&amp;= \mathbb E_{z\sim q_\phi(z)}[f(z) \nabla_\phi \log q_\phi(z)]
\end{aligned}
\]</div><p></p><p>等式右边的期望可以直接使用MC采样来估计：</p>
<p></p><div class="math display">\[\mathbb E_{z\sim q_\phi(z)}[f(z) \nabla_\phi \log q_\phi(z)] \approx \frac{1}{L} \sum_{l=1}^L f(z^{(l)}) \nabla_\phi \log q_\phi(z^{(l)})
\]</div><p></p><ul>
<li><span class="math inline">\(z^{(l)}\sim q_\phi(z)\)</span>.</li>
<li>这个估计方法被称作score function gradient estimator，其中<span class="math inline">\(\nabla_\phi \log q_\phi(z)\)</span>被称作是score，<span class="math inline">\(f(z)\)</span>被称作是cost.</li>
<li>这个estimator存在的问题是：<strong>方差太大</strong>，很多时候都用不了，也不适合直接拿来估计<span class="math inline">\(ELBO\)</span>.</li>
</ul>
<p><strong>NOTE</strong>：我们实际上假定了<span class="math inline">\(q_\phi(z)\)</span>是tractable的，既容易<em>采样</em>又容易<em>计算</em>。</p>
<p><strong>BTW</strong>：强化学习中的<em>REINFORCE算法</em>中，估计<em>策略梯度</em>用的就是这个estimator，所以我们会说REINFORCE是一种方差很大的算法。</p>
<h2 id="reparameterized-trick">Reparameterized trick</h2>
<p>采样过程<span class="math inline">\(z\sim q_\phi(z\mid x)\)</span>，通常可能分解成以下两个步骤：</p>
<ol>
<li>先从一个noise分布中采出<span class="math inline">\(\epsilon \sim p(\epsilon)\)</span>.</li>
<li>然后找出一个由<span class="math inline">\(\phi\)</span>参数化的函数<span class="math inline">\(g_\phi(,)\)</span>，使得<span class="math inline">\(z=g_\phi(\epsilon, x)\)</span>.</li>
</ol>
<p>比如说从正态分布采样<span class="math inline">\(z\sim \mathcal N(\mu, \sigma)\)</span>，就可以拆分成：</p>
<ol>
<li><span class="math inline">\(\epsilon \sim \mathcal N(0,1)\)</span>.</li>
<li><span class="math inline">\(z = \mu + \epsilon \sigma\)</span>.</li>
</ol>
<p>以上就是重参数化技巧。</p>
<h2 id="sgvi-estimator">SGVI estimator</h2>
<p>在使用重参数化技巧之后，可以把期望的表达式写成以下形式：</p>
<p></p><div class="math display">\[ \mathbb E_{z\sim q_\phi(z\mid x^{(i)})}[f(z)]=\mathbb E_{\epsilon \sim p(\epsilon)}[f(g_\phi(\epsilon, x^{(i)}))] \approx \frac{1}{L} \sum_{l=1}^L f(g_\phi(\epsilon^{(l)},x^{(i)}))
\]</div><p></p><p>令<span class="math inline">\(f=ELBO\)</span>，再求梯度，就得到了SGVI estimator：</p>
<p></p><div class="math display">\[\nabla_\phi ELBO=\nabla_\phi \mathcal L(\phi,x^{(i)}) = \nabla_\phi \bigl(\frac{1}{L}\sum_{l=1}^L \log P(x^{(i)},g_\phi(\epsilon^{(l)},x^{(i)})) - \log q_\phi(g_\phi(\epsilon^{(l)},x^{(i)})\mid x^{(i)}) \bigr)
\]</div><p></p><p>其中<span class="math inline">\(\epsilon^{(l)}\sim p(\epsilon)\)</span>.</p>
<p><strong>NOTE</strong>：SGVI estimator通常会比score function gradient estimator有更小的方差。</p>
<h1 id="变分自编码器vae">变分自编码器VAE</h1>
<p>本章讨论VI方法是如何应用到隐变量生成模型的推断和学习中的。</p>
<h2 id="样本的分布">样本的分布</h2>
<p>我们常常希望生成和数据集中的数据相似的样本。比如说给一个人脸数据集，我们希望模型能够通过某种方式学习这批人脸数据的分布，然后能够再通过某种方式生成出人脸来。</p>
<ul>
<li>
<p><strong>对样本的假定</strong>：通常会假定数据集是从一个未知分布<span class="math inline">\(P_{gt}(X)\)</span>采样出来的.</p>
</li>
<li>
<p><strong>理解</strong>：如果一张图片<span class="math inline">\(X\)</span>有人脸的样子，那么<span class="math inline">\(P_{gt}(X)\)</span>就很大；如果一张图片<span class="math inline">\(X\)</span>都是噪声，那么<span class="math inline">\(P_{gt}(X)\)</span>就很小。</p>
</li>
<li>
<p><strong>我们的目的</strong>：找到一个可以采样的模型<span class="math inline">\(P(X)\)</span>，同时<span class="math inline">\(P\)</span>能够尽可能逼近<span class="math inline">\(P_{gt}\)</span>.</p>
</li>
<li>
<p><strong>如何学习模型参数</strong>：假设参数化的模型<span class="math inline">\(P_\theta(X)\)</span>，要学习参数<span class="math inline">\(\theta\)</span>，就是要让模型在已经观测到的数据<span class="math inline">\(X\)</span>上，概率最大，即：<span class="math inline">\(\arg \max_\theta P_\theta(X)\)</span> (Max Likelihood).</p>
</li>
</ul>
<h2 id="隐变量生成模型">隐变量生成模型</h2>
<p>考虑使用隐变量生成模型来逼近<span class="math inline">\(P_{gt}\)</span>.</p>
<p><strong>引例</strong>：以生成数字为例，一种思考方法</p>
<ol>
<li>模型先确定<span class="math inline">\({0,1,...,9}\)</span>范围的一个数字<span class="math inline">\(z\)</span>.</li>
<li>然后再基于<span class="math inline">\(P(X\mid z;\theta)\)</span>从<span class="math inline">\(z\)</span>生成对应数字的图片。</li>
</ol>
<p>这里的<span class="math inline">\(z\)</span>就是隐变量。</p>
<hr>
<p><strong>困难</strong>：确定隐变量，再确定隐变量和观测变量的关系，这个过程实际上可能很复杂：</p>
<ul>
<li>比如模型可能要先确定要生成的数字里有没有圆圈 (0, 8, 9)，再确定圆圈的数量。</li>
<li>除此之外，还可能需要确定数字的倾斜度、字体等更复杂的关系。</li>
</ul>
<hr>
<p>考虑用如下方法来克服：</p>
<ol>
<li>直接假定隐变量服从高斯先验分布<span class="math inline">\(z \sim \mathcal N(0,I)\)</span>.</li>
<li>假定模型也是一个高斯分布<span class="math inline">\(P(X\mid z;\theta)=\mathcal N(X\mid \mu_\theta(z),\sigma_\theta(z))\)</span>，其参数是关于<span class="math inline">\(z\)</span>的函数。</li>
<li>参数<span class="math inline">\((\mu_\theta(z), \sigma_\theta(z))\)</span>都使用<em>神经网络</em>来建模。</li>
</ol>
<p>为什么这样做：</p>
<ul>
<li><strong>KEY1</strong>：简单的高斯分布，经过足够复杂的非线性变换，就可以表示出任意的分布。</li>
<li><strong>KEY2</strong>：由于神经网络可以<strong>建模任意复杂的非线性关系</strong>，所以最后可以表示出<strong>任意我们想要的分布</strong>。</li>
<li>Example：对高斯分布做非线性变换。左边是高斯分布，右边是<span class="math inline">\(g(z) = \frac{z}{10} + \frac{z}{||z||}\)</span>.<br>
<img src="https://img2024.cnblogs.com/blog/2568389/202501/2568389-20250103230425376-712736593.png" alt="image" loading="lazy"></li>
</ul>
<hr>
<p>因此得到我们用来近似真实数据分布<span class="math inline">\(P_{gt}(X)\)</span>的模型<span class="math inline">\(P(X;\theta)\)</span>：</p>
<p></p><div class="math display">\[P(X;\theta) = \int P(X,z;\theta)\text{d}z =\int P(X\mid z;\theta) P(z) \text{d}z =\int \mathcal N(X\mid \mu_\theta,\sigma_\theta) \mathcal N(0,I)\text{d}z
\]</div><p></p><ul>
<li>可以认为这个模型的表示能力是足够的。</li>
<li><span class="math inline">\(z\)</span>对我们来说是不可观测的隐变量，所以需要考虑所有可能<span class="math inline">\(z\)</span> (体现在积分).</li>
<li>是很复杂的模型，推断模型的Marginal Evidence <span class="math inline">\(P(X;\theta) = \int P(X,z;\theta)\text{d}z\)</span>、学习模型参数<span class="math inline">\(\arg \max_\theta P(X;\theta)\)</span>，都很困难。</li>
</ul>
<h2 id="模型的近似推断和学习">模型的近似推断和学习</h2>
<p>考虑使用变分推断的近似方法。</p>
<p>根据[[#变分推断VI]]部分的讨论，可以将模型的<span class="math inline">\(\log\)</span> Evidence写成<span class="math inline">\(ELBO+KL\)</span>的形式：</p>
<p></p><div class="math display">\[\begin{aligned}
\log P(x^{(i)};\theta) &amp;= \mathbb E_{z\sim q_{\phi^{(i)}}} [\log P(x^{(i)},z;\theta) - \log q_{\phi^{(i)}}(z)] + KL(q_{\phi^{(i)}}(z) \mid\mid P(z\mid x^{(i)};\theta)) 
\end{aligned}
\]</div><p></p><p>主要区别：带上了模型参数<span class="math inline">\(\theta\)</span>.</p>
<p><strong>NOTE</strong>：现在最大化<span class="math inline">\(ELBO\)</span>，既需要考虑近似分布<span class="math inline">\(q\)</span>的参数<span class="math inline">\(\phi\)</span>，也需要考虑模型参数<span class="math inline">\(\theta\)</span>.<br>
<strong>NOTE</strong>：此时，最大化<span class="math inline">\(ELBO\)</span>同时具有两个作用：<br>
1. 推断：最小化了<span class="math inline">\(KL\)</span>，使得<span class="math inline">\(q\)</span>逼近<em>真实后验分布</em>；<br>
2. 学习：最大化了Evidence，相当于做了MLE，使得模型<span class="math inline">\(P_\theta\)</span>逼近<em>真实数据分布</em><span class="math inline">\(P_{gt}\)</span>.</p>
<p>再根据[[#SGVI]]部分的讨论，使用SGVI estimator来估计<span class="math inline">\(ELBO\)</span>：</p>
<p></p><div class="math display">\[\nabla \mathcal L(x^{(i)},\phi^{(i)},\theta)=\nabla \bigl(\frac{1}{L}\sum_{l=1}^L \log P(x^{(i)},g_{\phi^{(i)}}(\epsilon^{(l)},x^{(i)});\theta) - \log q_{\phi^{(i)}}(g_{\phi^{(i)}}(\epsilon^{(l)},x^{(i)})) \bigr)
\]</div><p></p><ul>
<li><span class="math inline">\(g_\phi(\epsilon^{(l)},x^{(i)})\)</span>是重参数化后的<span class="math inline">\(z\)</span>.</li>
<li><span class="math inline">\(\epsilon \sim p(\epsilon)\)</span>.</li>
<li>梯度算子<span class="math inline">\(\nabla = \{\nabla_\phi, \nabla_\theta\}\)</span>.</li>
</ul>
<p>讨论至此，可以得到模型的学习过程如下：<br>
<img src="https://img2024.cnblogs.com/blog/2568389/202501/2568389-20250103230353109-185963395.png" alt="image" loading="lazy"></p>
<h2 id="amortized-inference">Amortized Inference</h2>
<ul>
<li>上方学习过程的一个缺点：对于每个<span class="math inline">\(x\)</span>，都要先推断出一个足够好的<span class="math inline">\(q_{\phi^*}\)</span>.</li>
<li><strong>KEY</strong>：近似推断后验概率 = <strong>学习</strong>后验概率的参数 (这个视角是由VI提供的)。</li>
</ul>
<p>Amortized Inference：考虑引入一个参数化的函数<span class="math inline">\(f_\lambda: x^i \to \phi^*\)</span>，它会<strong>学习</strong>如何根据<span class="math inline">\(x^i\)</span>确定一个足够好的<span class="math inline">\(\phi^*\)</span>. 即对于每个<span class="math inline">\(x^i\)</span>，都会得到后验<span class="math inline">\(q(z;f_\lambda(x^i))\)</span>.</p>
<p>如何学习？同样是重参数化+梯度上升。</p>
<p>因此，得到更简化的模型学习过程：<br>
<img src="https://img2024.cnblogs.com/blog/2568389/202501/2568389-20250103230342881-1504597057.png" alt="image" loading="lazy"></p>
<p><strong>NOTE</strong>：直到[[#Amortized Inference]]这一小节截止，才开始有了Encoder-Decoder结构的雏形，即<span class="math inline">\(x^i\to q(z;f_\lambda(x^i)) \to z \to P(X\mid z;\theta) \to \bar x^i\)</span>.</p>
<h2 id="对objective的解析">对Objective的解析</h2>
<p>训练VAE是以最大化<span class="math inline">\(ELBO\)</span>为目标，这一小节从<span class="math inline">\(ELBO\)</span>本身的形式，讨论<span class="math inline">\(ELBO\)</span>的实际意义。</p>
<p>这里推导用的是符号简化版的<span class="math inline">\(ELBO\)</span>表达式：</p>
<p></p><div class="math display">\[\begin{aligned}
ELBO    &amp;= \int q_\phi(Z)\log P(X,Z) - q_\phi(Z)\log q_\phi(Z) \text{d}Z \\
		&amp;= \int q_\phi(Z)\log P(X\mid Z) + q_\phi(Z) \log P(Z) - q_\phi(Z)\log q_\phi(Z) \text{d}Z \\
		&amp;=\int q_\phi(Z)\log P(X\mid Z) + q_\phi(Z) \log \frac{P(Z)}{q_\phi(Z)}\text{d}Z \\
		&amp;= \int q_\phi(Z)\log P(X\mid Z) \text{d}Z + \int q_\phi(Z) \log \frac{P(Z)}{q_\phi(Z)}\text{d}Z \\
		&amp;=\mathbb E_{q_\phi}[\log P(X\mid Z)] - KL(q_\phi(Z)\mid\mid P(Z))
\end{aligned}
\]</div><p></p><p>可以看到<span class="math inline">\(ELBO\)</span>两项的意义：</p>
<ol>
<li><strong>第一项</strong>：可以看作是Loss项，意义是极大似然，表示我们想要生成的<span class="math inline">\(X\)</span>和数据集尽可能相似。</li>
<li><strong>第二项</strong>：可以看作是正则化项，意义是希望后验尽可能接近已知的先验<span class="math inline">\(P(Z)\)</span>，这表明我们想要从<span class="math inline">\(P(Z)\)</span>随机抽一个<span class="math inline">\(z\)</span>，就大概率能生成有意义的结果。</li>
</ol>
<h2 id="vae结构示例">VAE结构示例</h2>
<p>下面是由前馈神经网络实现Encoder、Decoder的VAE的结构示意图。左图是无重参数化技巧的，右图是有重参数化技巧的。<br>
<img src="https://img2024.cnblogs.com/blog/2568389/202501/2568389-20250103230301531-1462095168.png" alt="image" loading="lazy"></p>
<p>几个要点：</p>
<ul>
<li><strong>网络输出</strong>：Decoder从隐变量<span class="math inline">\(z\)</span>重建<span class="math inline">\(\bar X\)</span>；Encoder输出<span class="math inline">\(q_\phi(z\mid X)\)</span>，即从<span class="math inline">\(X\)</span>压缩到隐变量。</li>
<li><strong>重参数化和Autodiff</strong>：重参数化技巧使得autodiff可以将误差从Decoder一直传回到输入。</li>
<li><strong>Objective的选择</strong>：不必严格是<span class="math inline">\(ELBO\)</span>
<ol>
<li>Loss项：要能体现Reconstruction error，根据问题的不同可以灵活选择。</li>
<li>正则项：除了KL散度外，也可以采用其他的散度。</li>
</ol>
</li>
<li><strong>关于生成</strong>：Decoder部分才是我们的隐变量生成模型。随机从<span class="math inline">\(\mathcal N(0,I)\)</span>中抽取<span class="math inline">\(z\)</span>，再输入到Decoder中，就可以生成样本。</li>
</ul>
<h2 id="参考代码">参考代码</h2>
<p>以下是基于CNN的VAE实现。</p>
<p><strong>Encoder</strong></p>
<pre><code class="language-python">class Encoder(nn.Module):
    def __init__(self, num_channels, hidden_size):
        super(Encoder, self).__init__()
        # 输入是(batch_size, num_channels, 28, 28)，就是MNIST数据集的形状
        self.conv1 = nn.Conv2d(num_channels, 32, 4, 2) # (28 - 4) / 2 + 1 = 13
        self.conv2 = nn.Conv2d(32, 64, 4, 2, 1) # (13 - 4 + 1) / 2 + 1 = 6
        self.conv3 = nn.Conv2d(64, 128, 4, 2) # (6 - 4) / 2 + 1 = 2

        self.fc_mu = nn.Linear(128 * 2 * 2, hidden_size)
        self.fc_logstd = nn.Linear(128 * 2 * 2, hidden_size)
    
    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = torch.flatten(x, start_dim=1)
        mu = self.fc_mu(x)
        log_std = self.fc_logstd(x)
        return mu, log_std
</code></pre>
<p><strong>Decoder</strong></p>
<pre><code class="language-python">class Decoder(nn.Module):
    def __init__(self, num_channels, hidden_size):
        super(Decoder, self).__init__()

        self.fc1 = nn.Linear(hidden_size, 512)
        self.deconv1 = nn.ConvTranspose2d(512, 64, 5, 2)
        self.deconv2 = nn.ConvTranspose2d(64, 32, 5, 2)
        self.deconv3 = nn.ConvTranspose2d(32, num_channels, 4, 2)
    
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = x.unsqueeze(-1).unsqueeze(-1) # num -&gt; [[num]]
        x = F.relu(self.deconv1(x))
        x = F.relu(self.deconv2(x))
        reconstruction = F.sigmoid(self.deconv3(x)) # output (0, 1)
        return reconstruction
</code></pre>
<p><strong>VAE</strong></p>
<pre><code class="language-python">class VAE(nn.Module):
    def __init__(self, num_channels, hidden_size):
        super(VAE, self).__init__()
        self.encoder = Encoder(num_channels, hidden_size)
        self.decoder = Decoder(num_channels, hidden_size)

    def forward(self, x):
        mu, log_std = self.encoder(x)
        z = self.reparameterize(mu, log_std)
        reconstruction = self.decoder(z)
        return reconstruction, mu, log_std

    def reparameterize(self, mu, log_std):
        std = log_std.exp()
        eps = torch.randn_like(std)
        return mu + std * eps
</code></pre>
<p><strong>Loss</strong></p>
<pre><code class="language-python">def vae_loss(x, reconstruction, mu, log_std):
	# 图像生成，使用BCE的效果会比较好
&nbsp; &nbsp; rec_loss = F.binary_cross_entropy(reconstruction, x, reduction='sum')
&nbsp; &nbsp; # 这里是化简后的q和N(0, I)的KL散度表达式
&nbsp; &nbsp; kl_loss = -0.5 * torch.sum(1 + 2 * log_std - mu.pow(2) - (2*log_std).exp())
&nbsp; &nbsp; return rec_loss + kl_loss
</code></pre>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.41094379871527775" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-01-03 23:09">2025-01-03 23:08</span>&nbsp;
<a href="https://www.cnblogs.com/tshaaa">伊犁纯流莱</a>&nbsp;
阅读(<span id="post_view_count">29</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18651129" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18651129);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18651129', targetLink: 'https://www.cnblogs.com/tshaaa/p/18651129', title: '变分推断(VI)、随机梯度变分推断(SGVI/SGVB)、变分自编码器(VAE)串讲' })">举报</a>
</div>
        