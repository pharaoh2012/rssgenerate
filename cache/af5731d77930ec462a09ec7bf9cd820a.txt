
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/xiao987334176/p/18922128" title="发布于 2025-06-10 15:36">
    <span role="heading" aria-level="2">SwanLab入门深度学习：Qwen3大模型指令微调</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body blogpost-body-html">
<h1>一、概述</h1>
<p>Qwen3是通义千问团队的开源大语言模型，由阿里云通义实验室研发。以Qwen3作为基座大模型，通过指令微调的方式实现高准确率的文本分类，是学习大语言模型微调的入门任务。</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202506/1341090-20250610143359181-106648973.png" alt="" width="974" height="384"></p>
<p>指令微调是一种通过在由（指令，输出）对组成的数据集上进一步训练LLMs的过程。 其中，指令代表模型的人类指令，输出代表遵循指令的期望输出。 这个过程有助于弥合LLMs的下一个词预测目标与用户让LLMs遵循人类指令的目标之间的差距。</p>
<p>在这个任务中我们会使用Qwen3-1.7B模型在zh_cls_fudan_news数据集上进行指令微调任务，同时使用SwanLab进行监控和可视化。</p>
<p>&nbsp;</p>
<p>实验日志过程：<a href="https://swanlab.cn/@spark_xiao/Qwen3-fintune/runs/9rzt3rv77885ek176nslh" target="_blank" rel="noopener nofollow">https://swanlab.cn/@spark_xiao/Qwen3-fintune/runs/9rzt3rv77885ek176nslh</a></p>
<p>参考代码：<a href="https://github.com/Zeyi-Lin/LLM-Finetune" target="_blank" rel="noopener nofollow">https://github.com/Zeyi-Lin/LLM-Finetune</a></p>
<p>模型：<a href="https://modelscope.cn/models/Qwen/Qwen3-1.7B" target="_blank" rel="noopener nofollow">https://modelscope.cn/models/Qwen/Qwen3-1.7B</a></p>
<p>数据集：<a href="https://www.modelscope.cn/datasets/swift/zh_cls_fudan-news/summary" target="_blank" rel="noopener nofollow">https://www.modelscope.cn/datasets/swift/zh_cls_fudan-news/summary</a></p>
<p>SwanLab：<a href="https://swanlab.cn" target="_blank" rel="noopener nofollow">https://swanlab.cn</a></p>
<h1>二、SwanLab</h1>
<p>SwanLab（https://swanlab.cn）是一个用于AI模型训练过程可视化的工具。SwanLab的主要功能包括：</p>
<p>跟踪模型指标，如损失和准确性等<br>同时支持云端和离线使用，支持远程查看训练过程，比如可以在手机上看远程服务器上跑的训练<br>记录训练超参数，如batch_size和learning_rate等<br>自动记录训练过程中的日志、硬件环境、Python库以及GPU（支持英伟达显卡）、NPU（支持华为昇腾卡）、内存的硬件信息<br>支持团队多人协作，很适合打Kaggle等比赛的队伍</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202506/1341090-20250610144221570-789373986.png" alt="" width="1286" height="567" loading="lazy"></p>
<p>&nbsp;</p>
<p>SwanLab库来自一个中国团队（情感机器），最早的出发点是其开发团队的内部训练需求，后来逐渐开源并且发展成面向公众的产品。SwanLab库在2024年向公众发布。SwanLab刚出现时只有离线版本（对标Tensorboard），后来经过迭代和努力已经有了云端版和各项功能，并且集成了接近30+个深度学习框架，包括PyTorch、HuggingFace Transformers、Keras、XGBoost等等，其中还包括同样是中国团队开发的LLaMA Factory、Modelscope Swift、PaddleYOLO等框架，具有了很全面的功能。</p>
<h2>账号注册</h2>
<p>SwanLab的云端版体验是比较好的（非常推荐），能够支持你在随时随地访问训练过程。</p>
<p>要使用云端版之前需要先注册一下账号：</p>
<p>在电脑或手机浏览器访问SwanLab官网： <a href="https://swanlab.cn" target="_blank" rel="noopener nofollow">https://swanlab.cn</a></p>
<p><strong>点击右上角注册</strong></p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202506/1341090-20250610144448932-453479692.png" alt="" loading="lazy"></p>
<p><strong>填写手机号后，点击「发送短信验证码」按钮</strong></p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202506/1341090-20250610144515871-1191038469.png" alt="" loading="lazy"></p>
<p>&nbsp;</p>
<h4>填写你的信息</h4>
<ul>
<li>用户名称：你的个人昵称，中英文均可</li>
<li>用户ID：你的英文名，可由数字、字母、下划线、中横线组成</li>
<li>邮箱：你的邮箱</li>
<li>机构/院校：你所在的企业、机构或学校</li>
<li>您从哪了解到SwanLab？：（选填项）了解到SwanLab的渠道，比如朋友介绍</li>






</ul>
<p>&nbsp;</p>
<h4>复制API Key</h4>
<p>完成填写后点击「完成」按钮，会进入到下面的页面。然后点击左边的「<strong>设置</strong>」：</p>
<p>在<strong>API Key</strong>这个地方，点击复制按钮，复制你的API Key：</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202506/1341090-20250610144824761-177062976.png" alt="" loading="lazy"></p>
<p>&nbsp;</p>
<h1>三、环境安装</h1>
<p>本案例基于Python 3.13.2，请在您的计算机上安装好Python，并且有一张英伟达显卡（显存要求并不高，大概10GB左右就可以跑）。</p>
<p>在这之前，请确保你的环境内已安装了pytorch以及CUDA：</p>
<p>pytorch以及CUDA安装，请参考文章：<a href="https://www.cnblogs.com/xiao987334176/p/18876317" target="_blank">https://www.cnblogs.com/xiao987334176/p/18876317</a></p>
<p>&nbsp;</p>
<p>我们需要安装以下这几个Python库，一键安装命令：</p>
<div class="cnblogs_code">
<pre>pip <span style="color: rgba(0, 0, 255, 1)">install</span> swanlab modelscope transformers datasets peft pandas accelerate</pre>
</div>
<h2>准备数据集</h2>
<p>本案例使用的是<a href="https://www.modelscope.cn/datasets/swift/zh_cls_fudan-news/summary" target="_blank" rel="noopener nofollow">zh_cls_fudan-news</a>数据集，该数据集主要被用于训练文本分类模型。</p>
<p>zh_cls_fudan-news由几千条数据，每条数据包含text、category、output三列：</p>
<p>text 是训练语料，内容是书籍或新闻的文本内容<br>category 是text的多个备选类型组成的列表<br>output 则是text唯一真实的类型</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202506/1341090-20250610145316360-420938404.png" alt="" width="1505" height="371" loading="lazy"></p>
<p>&nbsp;数据集例子如下：</p>
<div class="cnblogs_code">
<pre><span style="color: rgba(128, 0, 0, 1)">"""
</span><span style="color: rgba(0, 0, 0, 1)">[PROMPT]Text: 第四届全国大企业足球赛复赛结束新华社郑州５月３日电（实习生田兆运）上海大隆机器厂队昨天在洛阳进行的第四届牡丹杯全国大企业足球赛复赛中，以５：４力克成都冶金实验厂队，进入前四名。沪蓉之战，双方势均力敌，９０分钟不分胜负。最后，双方互射点球，沪队才以一球优势取胜。复赛的其它３场比赛，青海山川机床铸造厂队３：０击败东道主洛阳矿山机器厂队，青岛铸造机械厂队３：１战胜石家庄第一印染厂队，武汉肉联厂队１：０险胜天津市第二冶金机械厂队。在今天进行的决定九至十二名的两场比赛中，包钢无缝钢管厂队和河南平顶山矿务局一矿队分别击败河南平顶山锦纶帘子布厂队和江苏盐城无线电总厂队。４日将进行两场半决赛，由青海山川机床铸造厂队和青岛铸造机械厂队分别与武汉肉联厂队和上海大隆机器厂队交锋。本届比赛将于６日结束。（完）
Category: Sports, Politics
Output:[OUTPUT]Sports
</span><span style="color: rgba(128, 0, 0, 1)">"""</span></pre>
</div>
<p>我们的训练任务，便是希望微调后的大模型能够根据Text和Category组成的提示词，预测出正确的Output。</p>
<p>&nbsp;</p>
<p>我们将数据集下载到本地目录下。下载方式是前往<a href="https://www.modelscope.cn/datasets/swift/zh_cls_fudan-news/files" target="_blank" rel="noopener nofollow">zh_cls_fudan-news - 魔搭社区</a>&nbsp;，将<code>train.jsonl</code>和<code>test.jsonl</code>下载到本地根目录下即可：</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202506/1341090-20250610145542632-928687883.png" alt="" width="1375" height="322" loading="lazy"></p>
<h2>加载模型</h2>
<p>这里我们使用modelscope下载Qwen3-1.7B模型（modelscope在国内，所以下载不用担心速度和稳定性问题），然后把它加载到Transformers中进行训练：</p>
<p>train.py</p>
<div class="cnblogs_code">
<pre><span style="color: rgba(0, 0, 0, 1)">import torch
from modelscope import snapshot_download, AutoTokenizer
from transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq

# Transformers加载模型权重
tokenizer </span>= AutoTokenizer.from_pretrained(<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">./Qwen/Qwen3-1.7B/</span><span style="color: rgba(128, 0, 0, 1)">"</span>, use_fast=False, trust_remote_code=<span style="color: rgba(0, 0, 0, 1)">True)
model </span>= AutoModelForCausalLM.from_pretrained(<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">./Qwen/Qwen3-1.7B/</span><span style="color: rgba(128, 0, 0, 1)">"</span>, device_map=<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">auto</span><span style="color: rgba(128, 0, 0, 1)">"</span>, torch_dtype=torch.bfloat16)</pre>
</div>
<p>注意：确保下载的模型路径正确</p>
<p>&nbsp;</p>
<p>运行python代码</p>
<div class="cnblogs_code">
<pre>python train.py</pre>
</div>
<p>输出如下：</p>
<div class="cnblogs_code">
<pre><span style="color: rgba(0, 0, 0, 1)">Loading checkpoint shards: </span><span style="color: rgba(128, 0, 128, 1)">100</span>%|█████████████████████████████████████████████████████████| <span style="color: rgba(128, 0, 128, 1)">2</span>/<span style="color: rgba(128, 0, 128, 1)">2</span> [<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">08</span>&lt;<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>,  <span style="color: rgba(128, 0, 128, 1)">4</span>.01s/<span style="color: rgba(0, 0, 0, 1)">it]
Map: </span><span style="color: rgba(128, 0, 128, 1)">100</span>%|██████████████████████████████████████████████████████████████████| <span style="color: rgba(128, 0, 128, 1)">4000</span>/<span style="color: rgba(128, 0, 128, 1)">4000</span> [<span style="color: rgba(128, 0, 128, 1)">33</span>:<span style="color: rgba(128, 0, 128, 1)">26</span>&lt;<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>,  <span style="color: rgba(128, 0, 128, 1)">1.99</span> examples/s]</pre>
</div>
<p>没有提示报错就可以了</p>
<p>&nbsp;</p>
<h1>四、配置训练可视化工具</h1>
<p>我们使用SwanLab来监控整个训练过程，并评估最终的模型效果。</p>
<p>这里直接使用SwanLab和Transformers的集成来实现</p>
<p>&nbsp;</p>
<p>如果你是第一次使用SwanLab，那么还需要去<a href="https://swanlab.cn/" rel="noopener nofollow">https://swanlab.cn</a>上注册一个账号，在用户设置页面复制你的API Key</p>
<p>登录SwanLab</p>
<div class="cnblogs_code">
<pre>swanlab <span style="color: rgba(0, 0, 255, 1)">login</span></pre>
</div>
<p>输入API Key</p>
<div class="cnblogs_code">
<pre>swanlab: You can <span style="color: rgba(0, 0, 255, 1)">find</span> your API key at: https:<span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)">swanlab.cn/space/~/settings</span>
swanlab: Paste an API key from your profile and hit enter, or press <span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">CTRL + C</span><span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(0, 0, 0, 1)"> to quit
On Windows, use Ctrl </span>+ Shift + V or right-<span style="color: rgba(0, 0, 0, 1)">click to paste the API key:
swanlab: Login successfully. Hi, spark_xiao</span>!</pre>
</div>
<p>提示登录成功</p>
<p>&nbsp;</p>
<h1>五、完整代码</h1>
<p>开始训练时的目录结构：</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202506/1341090-20250610151849091-159038488.png" alt="" loading="lazy"></p>
<p>说明：</p>
<p>Qwen，存放通义千问模型文件</p>
<p>zh_cls_fudan-news，下载的数据集，我这里是下载的所有文件。</p>
<p>train.py，训练代码</p>
<p>&nbsp;</p>
<p>train.py</p>
<div class="cnblogs_code">
<pre><span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> json
</span><span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> pandas as pd
</span><span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> torch
</span><span style="color: rgba(0, 0, 255, 1)">from</span> datasets <span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> Dataset
</span><span style="color: rgba(0, 0, 255, 1)">from</span> modelscope <span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> snapshot_download, AutoTokenizer
</span><span style="color: rgba(0, 0, 255, 1)">from</span> swanlab.integration.huggingface <span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> SwanLabCallback
</span><span style="color: rgba(0, 0, 255, 1)">from</span> peft <span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> LoraConfig, TaskType, get_peft_model
</span><span style="color: rgba(0, 0, 255, 1)">from</span> transformers <span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq
</span><span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> os
</span><span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> swanlab


</span><span style="color: rgba(0, 0, 255, 1)">def</span><span style="color: rgba(0, 0, 0, 1)"> dataset_jsonl_transfer(origin_path, new_path):
    </span><span style="color: rgba(128, 0, 0, 1)">"""</span><span style="color: rgba(128, 0, 0, 1)">
    将原始数据集转换为大模型微调所需数据格式的新数据集
    </span><span style="color: rgba(128, 0, 0, 1)">"""</span><span style="color: rgba(0, 0, 0, 1)">
    messages </span>=<span style="color: rgba(0, 0, 0, 1)"> []

    </span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 读取旧的JSONL文件</span>
    with open(origin_path, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">r</span><span style="color: rgba(128, 0, 0, 1)">"</span>, encoding=<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">utf-8</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">) as file:
        </span><span style="color: rgba(0, 0, 255, 1)">for</span> line <span style="color: rgba(0, 0, 255, 1)">in</span><span style="color: rgba(0, 0, 0, 1)"> file:
            </span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 解析每一行的json数据</span>
            data =<span style="color: rgba(0, 0, 0, 1)"> json.loads(line)
            context </span>= data[<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">text</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">]
            catagory </span>= data[<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">category</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">]
            label </span>= data[<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">output</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">]
            message </span>=<span style="color: rgba(0, 0, 0, 1)"> {
                </span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">instruction</span><span style="color: rgba(128, 0, 0, 1)">"</span>: <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">你是一个文本分类领域的专家，你会接收到一段文本和几个潜在的分类选项，请输出文本内容的正确类型</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">,
                </span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">input</span><span style="color: rgba(128, 0, 0, 1)">"</span>: f<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">文本:{context},类型选型:{catagory}</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">,
                </span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">output</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">: label,
            }
            messages.append(message)

    </span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 保存重构后的JSONL文件</span>
    with open(new_path, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">w</span><span style="color: rgba(128, 0, 0, 1)">"</span>, encoding=<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">utf-8</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">) as file:
        </span><span style="color: rgba(0, 0, 255, 1)">for</span> message <span style="color: rgba(0, 0, 255, 1)">in</span><span style="color: rgba(0, 0, 0, 1)"> messages:
            file.write(json.dumps(message, ensure_ascii</span>=False) + <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">\n</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">)


</span><span style="color: rgba(0, 0, 255, 1)">def</span><span style="color: rgba(0, 0, 0, 1)"> process_func(example):
    </span><span style="color: rgba(128, 0, 0, 1)">"""</span><span style="color: rgba(128, 0, 0, 1)">
    将数据集进行预处理
    </span><span style="color: rgba(128, 0, 0, 1)">"""</span><span style="color: rgba(0, 0, 0, 1)">
    MAX_LENGTH </span>= 384<span style="color: rgba(0, 0, 0, 1)">
    input_ids, attention_mask, labels </span>=<span style="color: rgba(0, 0, 0, 1)"> [], [], []
    instruction </span>=<span style="color: rgba(0, 0, 0, 1)"> tokenizer(
        f</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">&lt;|im_start|&gt;system\n你是一个文本分类领域的专家，你会接收到一段文本和几个潜在的分类选项，请输出文本内容的正确类型&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\n{example['input']}&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">,
        add_special_tokens</span>=<span style="color: rgba(0, 0, 0, 1)">False,
    )
    response </span>= tokenizer(f<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">{example['output']}</span><span style="color: rgba(128, 0, 0, 1)">"</span>, add_special_tokens=<span style="color: rgba(0, 0, 0, 1)">False)
    input_ids </span>= instruction[<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">input_ids</span><span style="color: rgba(128, 0, 0, 1)">"</span>] +<span style="color: rgba(0, 0, 0, 1)"> \
        response[</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">input_ids</span><span style="color: rgba(128, 0, 0, 1)">"</span>] +<span style="color: rgba(0, 0, 0, 1)"> [tokenizer.pad_token_id]
    attention_mask </span>=<span style="color: rgba(0, 0, 0, 1)"> (
        instruction[</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">attention_mask</span><span style="color: rgba(128, 0, 0, 1)">"</span>] + response[<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">attention_mask</span><span style="color: rgba(128, 0, 0, 1)">"</span>] + [1<span style="color: rgba(0, 0, 0, 1)">]
    )
    labels </span>= [-100] * len(instruction[<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">input_ids</span><span style="color: rgba(128, 0, 0, 1)">"</span>]) +<span style="color: rgba(0, 0, 0, 1)"> \
        response[</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">input_ids</span><span style="color: rgba(128, 0, 0, 1)">"</span>] +<span style="color: rgba(0, 0, 0, 1)"> [tokenizer.pad_token_id]
    </span><span style="color: rgba(0, 0, 255, 1)">if</span> len(input_ids) &gt; MAX_LENGTH:  <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 做一个截断</span>
        input_ids =<span style="color: rgba(0, 0, 0, 1)"> input_ids[:MAX_LENGTH]
        attention_mask </span>=<span style="color: rgba(0, 0, 0, 1)"> attention_mask[:MAX_LENGTH]
        labels </span>=<span style="color: rgba(0, 0, 0, 1)"> labels[:MAX_LENGTH]
    </span><span style="color: rgba(0, 0, 255, 1)">return</span> {<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">input_ids</span><span style="color: rgba(128, 0, 0, 1)">"</span>: input_ids, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">attention_mask</span><span style="color: rgba(128, 0, 0, 1)">"</span>: attention_mask, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">labels</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">: labels}


</span><span style="color: rgba(0, 0, 255, 1)">def</span><span style="color: rgba(0, 0, 0, 1)"> predict(messages, model, tokenizer):
    device </span>= <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">cuda</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">
    text </span>=<span style="color: rgba(0, 0, 0, 1)"> tokenizer.apply_chat_template(
        messages,
        tokenize</span>=<span style="color: rgba(0, 0, 0, 1)">False,
        add_generation_prompt</span>=<span style="color: rgba(0, 0, 0, 1)">True
    )
    model_inputs </span>= tokenizer([text], return_tensors=<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">pt</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">).to(device)

    generated_ids </span>=<span style="color: rgba(0, 0, 0, 1)"> model.generate(
        model_inputs.input_ids,
        max_new_tokens</span>=512<span style="color: rgba(0, 0, 0, 1)">
    )
    generated_ids </span>=<span style="color: rgba(0, 0, 0, 1)"> [
        output_ids[len(input_ids):] </span><span style="color: rgba(0, 0, 255, 1)">for</span> input_ids, output_ids <span style="color: rgba(0, 0, 255, 1)">in</span><span style="color: rgba(0, 0, 0, 1)"> zip(model_inputs.input_ids, generated_ids)
    ]

    response </span>=<span style="color: rgba(0, 0, 0, 1)"> tokenizer.batch_decode(
        generated_ids, skip_special_tokens</span>=<span style="color: rgba(0, 0, 0, 1)">True)[0]

    </span><span style="color: rgba(0, 0, 255, 1)">print</span><span style="color: rgba(0, 0, 0, 1)">(response)

    </span><span style="color: rgba(0, 0, 255, 1)">return</span><span style="color: rgba(0, 0, 0, 1)"> response

</span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 在modelscope上下载Qwen模型到本地目录下</span><span style="color: rgba(0, 128, 0, 1)">
#</span><span style="color: rgba(0, 128, 0, 1)"> model_dir = snapshot_download("Qwen/Qwen3-1.7B", cache_dir="./", revision="master")</span>


<span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> Transformers加载模型权重</span>
tokenizer =<span style="color: rgba(0, 0, 0, 1)"> AutoTokenizer.from_pretrained(
    </span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">./Qwen/Qwen3-1.7B/</span><span style="color: rgba(128, 0, 0, 1)">"</span>, use_fast=False, trust_remote_code=<span style="color: rgba(0, 0, 0, 1)">True)
model </span>=<span style="color: rgba(0, 0, 0, 1)"> AutoModelForCausalLM.from_pretrained(
    </span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">./Qwen/Qwen3-1.7B/</span><span style="color: rgba(128, 0, 0, 1)">"</span>, device_map=<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">auto</span><span style="color: rgba(128, 0, 0, 1)">"</span>, torch_dtype=<span style="color: rgba(0, 0, 0, 1)">torch.bfloat16)
model.enable_input_require_grads()  </span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 开启梯度检查点时，要执行该方法</span>

<span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 加载、处理数据集和测试集</span>
train_dataset_path = <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">./zh_cls_fudan-news/train.jsonl</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">
test_dataset_path </span>= <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">./zh_cls_fudan-news/test.jsonl</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">

train_jsonl_new_path </span>= <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">new_train.jsonl</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">
test_jsonl_new_path </span>= <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">new_test.jsonl</span><span style="color: rgba(128, 0, 0, 1)">"</span>

<span style="color: rgba(0, 0, 255, 1)">if</span> <span style="color: rgba(0, 0, 255, 1)">not</span><span style="color: rgba(0, 0, 0, 1)"> os.path.exists(train_jsonl_new_path):
    dataset_jsonl_transfer(train_dataset_path, train_jsonl_new_path)
</span><span style="color: rgba(0, 0, 255, 1)">if</span> <span style="color: rgba(0, 0, 255, 1)">not</span><span style="color: rgba(0, 0, 0, 1)"> os.path.exists(test_jsonl_new_path):
    dataset_jsonl_transfer(test_dataset_path, test_jsonl_new_path)

</span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 得到训练集</span>
train_df = pd.read_json(train_jsonl_new_path, lines=<span style="color: rgba(0, 0, 0, 1)">True)
train_ds </span>=<span style="color: rgba(0, 0, 0, 1)"> Dataset.from_pandas(train_df)
train_dataset </span>=<span style="color: rgba(0, 0, 0, 1)"> train_ds.map(
    process_func, remove_columns</span>=<span style="color: rgba(0, 0, 0, 1)">train_ds.column_names)

config </span>=<span style="color: rgba(0, 0, 0, 1)"> LoraConfig(
    task_type</span>=<span style="color: rgba(0, 0, 0, 1)">TaskType.CAUSAL_LM,
    target_modules</span>=[<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">q_proj</span><span style="color: rgba(128, 0, 0, 1)">"</span>, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">k_proj</span><span style="color: rgba(128, 0, 0, 1)">"</span>, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">v_proj</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">,
                    </span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">o_proj</span><span style="color: rgba(128, 0, 0, 1)">"</span>, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">gate_proj</span><span style="color: rgba(128, 0, 0, 1)">"</span>, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">up_proj</span><span style="color: rgba(128, 0, 0, 1)">"</span>, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">down_proj</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">],
    inference_mode</span>=False,  <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 训练模式</span>
    r=8,  <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> Lora 秩</span>
    lora_alpha=32,  <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> Lora alaph，具体作用参见 Lora 原理</span>
    lora_dropout=0.1,  <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> Dropout 比例</span>
<span style="color: rgba(0, 0, 0, 1)">)

model </span>=<span style="color: rgba(0, 0, 0, 1)"> get_peft_model(model, config)

args </span>=<span style="color: rgba(0, 0, 0, 1)"> TrainingArguments(
    output_dir</span>=<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">./output/Qwen3-zh_cls_fudan-news</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">,
    per_device_train_batch_size</span>=4<span style="color: rgba(0, 0, 0, 1)">,
    gradient_accumulation_steps</span>=4<span style="color: rgba(0, 0, 0, 1)">,
    logging_steps</span>=10<span style="color: rgba(0, 0, 0, 1)">,
    num_train_epochs</span>=2<span style="color: rgba(0, 0, 0, 1)">,
    save_steps</span>=100<span style="color: rgba(0, 0, 0, 1)">,
    learning_rate</span>=1e-4<span style="color: rgba(0, 0, 0, 1)">,
    save_on_each_node</span>=<span style="color: rgba(0, 0, 0, 1)">True,
    gradient_checkpointing</span>=<span style="color: rgba(0, 0, 0, 1)">True,
    report_to</span>=<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">none</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">,
)

swanlab_callback </span>=<span style="color: rgba(0, 0, 0, 1)"> SwanLabCallback(
    project</span>=<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">Qwen3-fintune</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">,
    experiment_name</span>=<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">Qwen3-1.7B</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">,
    description</span>=<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">使用通义千问Qwen3-1.7B模型在zh_cls_fudan-news数据集上微调。</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">,
    config</span>=<span style="color: rgba(0, 0, 0, 1)">{
        </span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">model</span><span style="color: rgba(128, 0, 0, 1)">"</span>: <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">Qwen/Qwen3-1.7B</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">,
        </span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">dataset</span><span style="color: rgba(128, 0, 0, 1)">"</span>: <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">swift/zh_cls_fudan-news</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">,
    }
)

trainer </span>=<span style="color: rgba(0, 0, 0, 1)"> Trainer(
    model</span>=<span style="color: rgba(0, 0, 0, 1)">model,
    args</span>=<span style="color: rgba(0, 0, 0, 1)">args,
    train_dataset</span>=<span style="color: rgba(0, 0, 0, 1)">train_dataset,
    data_collator</span>=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=<span style="color: rgba(0, 0, 0, 1)">True),
    callbacks</span>=<span style="color: rgba(0, 0, 0, 1)">[swanlab_callback],
)

trainer.train()

</span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 用测试集的前10条，测试模型</span>
test_df = pd.read_json(test_jsonl_new_path, lines=True)[:10<span style="color: rgba(0, 0, 0, 1)">]

test_text_list </span>=<span style="color: rgba(0, 0, 0, 1)"> []
</span><span style="color: rgba(0, 0, 255, 1)">for</span> index, row <span style="color: rgba(0, 0, 255, 1)">in</span><span style="color: rgba(0, 0, 0, 1)"> test_df.iterrows():
    instruction </span>= row[<span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">instruction</span><span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(0, 0, 0, 1)">]
    input_value </span>= row[<span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">input</span><span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(0, 0, 0, 1)">]

    messages </span>=<span style="color: rgba(0, 0, 0, 1)"> [
        {</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">role</span><span style="color: rgba(128, 0, 0, 1)">"</span>: <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">system</span><span style="color: rgba(128, 0, 0, 1)">"</span>, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">content</span><span style="color: rgba(128, 0, 0, 1)">"</span>: f<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">{instruction}</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">},
        {</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">role</span><span style="color: rgba(128, 0, 0, 1)">"</span>: <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">user</span><span style="color: rgba(128, 0, 0, 1)">"</span>, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">content</span><span style="color: rgba(128, 0, 0, 1)">"</span>: f<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">{input_value}</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">}
    ]

    response </span>=<span style="color: rgba(0, 0, 0, 1)"> predict(messages, model, tokenizer)
    messages.append({</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">role</span><span style="color: rgba(128, 0, 0, 1)">"</span>: <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">assistant</span><span style="color: rgba(128, 0, 0, 1)">"</span>, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">content</span><span style="color: rgba(128, 0, 0, 1)">"</span>: f<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">{response}</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">})
    result_text </span>= f<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">{messages[0]}\n\n{messages[1]}\n\n{messages[2]}</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">
    test_text_list.append(swanlab.Text(result_text, caption</span>=<span style="color: rgba(0, 0, 0, 1)">response))

swanlab.log({</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">Prediction</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">: test_text_list})
swanlab.finish()</span></pre>
</div>
<p>&nbsp;</p>
<p>执行代码</p>
<div class="cnblogs_code">
<pre>python train.py</pre>
</div>
<p>&nbsp;</p>
<p>看到下面的进度条即代表训练开始：</p>
<div class="cnblogs_code">
<pre>swanlab: Tracking run with swanlab version <span style="color: rgba(128, 0, 128, 1)">0.6</span>.<span style="color: rgba(128, 0, 128, 1)">2</span><span style="color: rgba(0, 0, 0, 1)">
swanlab: Run data will be saved locally </span><span style="color: rgba(0, 0, 255, 1)">in</span> D:\<span style="color: rgba(0, 0, 255, 1)">file</span>\vllm\swanlog\run-20250610_174942-<span style="color: rgba(0, 0, 0, 1)">a3b1799d
swanlab: 👋 Hi spark_xiao, welcome to swanlab</span>!<span style="color: rgba(0, 0, 0, 1)">
swanlab: Syncing run Qwen3</span>-<span style="color: rgba(128, 0, 128, 1)">1</span><span style="color: rgba(0, 0, 0, 1)">.7B to the cloud
swanlab: 🏠 View project at https:</span><span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)">swanlab.cn/@spark_xiao/Qwen3-fintune</span>
swanlab: 🚀 View run at https:<span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)">swanlab.cn/@spark_xiao/Qwen3-fintune/runs/2k85y57dzb3p3ozpvwkow</span>
  <span style="color: rgba(128, 0, 128, 1)">0</span>%|                                                                                          | <span style="color: rgba(128, 0, 128, 1)">0</span>/<span style="color: rgba(128, 0, 128, 1)">500</span> [<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>&lt;?, ?it/s]`<br>use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=<span style="color: rgba(0, 0, 0, 1)">False`.
{</span><span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">loss</span><span style="color: rgba(128, 0, 0, 1)">'</span>: <span style="color: rgba(128, 0, 128, 1)">32.3218</span>, <span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">grad_norm</span><span style="color: rgba(128, 0, 0, 1)">'</span>: <span style="color: rgba(128, 0, 128, 1)">36.5546875</span>, <span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">learning_rate</span><span style="color: rgba(128, 0, 0, 1)">'</span>: <span style="color: rgba(128, 0, 128, 1)">9.82e-05</span>, <span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">epoch</span><span style="color: rgba(128, 0, 0, 1)">'</span>: <span style="color: rgba(128, 0, 128, 1)">0.04</span><span style="color: rgba(0, 0, 0, 1)">}
{</span><span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">loss</span><span style="color: rgba(128, 0, 0, 1)">'</span>: <span style="color: rgba(128, 0, 128, 1)">10.8642</span>, <span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">grad_norm</span><span style="color: rgba(128, 0, 0, 1)">'</span>: <span style="color: rgba(128, 0, 128, 1)">221.5959014892578</span>, <span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">learning_rate</span><span style="color: rgba(128, 0, 0, 1)">'</span>: <span style="color: rgba(128, 0, 128, 1)">9.620000000000001e-05</span>, <span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">epoch</span><span style="color: rgba(128, 0, 0, 1)">'</span>: <span style="color: rgba(128, 0, 128, 1)">0.08</span><span style="color: rgba(0, 0, 0, 1)">}
  </span><span style="color: rgba(128, 0, 128, 1)">5</span>%|████                                                                           | <span style="color: rgba(128, 0, 128, 1)">26</span>/<span style="color: rgba(128, 0, 128, 1)">500</span> [<span style="color: rgba(128, 0, 128, 1)">04</span>:<span style="color: rgba(128, 0, 128, 1)">58</span>&lt;<span style="color: rgba(128, 0, 128, 1)">1</span>:<span style="color: rgba(128, 0, 128, 1)">28</span>:<span style="color: rgba(128, 0, 128, 1)">51</span>, <span style="color: rgba(128, 0, 128, 1)">11</span>.25s/it]</pre>
</div>
<p>&nbsp;</p>
<p>等待1小时50分钟，就完成了，输出</p>
<div class="cnblogs_code">
<pre><span style="color: rgba(128, 0, 128, 1)">100</span>%|██████████████████████████████████████████████████████████████████████████████| <span style="color: rgba(128, 0, 128, 1)">500</span>/<span style="color: rgba(128, 0, 128, 1)">500</span> [<span style="color: rgba(128, 0, 128, 1)">1</span>:<span style="color: rgba(128, 0, 128, 1)">12</span>:<span style="color: rgba(128, 0, 128, 1)">54</span>&lt;<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>,  <span style="color: rgba(128, 0, 128, 1)">8</span>.75s/<span style="color: rgba(0, 0, 0, 1)">it]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input</span><span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">s `attention_mask` to obtain reliable results.</span>
D:\<span style="color: rgba(0, 0, 255, 1)">file</span>\conda\envs\my_unsloth_env\Lib\site-packages\torch\utils\checkpoint.py:<span style="color: rgba(128, 0, 128, 1)">86</span>: UserWarning: None of the inputs have requires_grad=<span style="color: rgba(0, 0, 0, 1)">True. Gradients will be None
  warnings.warn(
Computer
Space
Literature
History
History
Space
Transport
Art
Economy
Art
swanlab: 🏠 View project at https:</span><span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)">swanlab.cn/@spark_xiao/Qwen3-fintune</span>
swanlab: 🚀 View run at https:<span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)">swanlab.cn/@spark_xiao/Qwen3-fintune/runs/9rzt3rv77885ek176nslh</span></pre>
</div>
<p>注意：最后一个输出的url，就可以看到演示结果。</p>
<p>&nbsp;</p>
<h1>六、训练结果演示</h1>
<p>在SwanLab上查看最终的训练结果：</p>
<p>打开链接：https://swanlab.cn/@spark_xiao/Qwen3-fintune/runs/9rzt3rv77885ek176nslh</p>
<p>&nbsp;</p>
<p>可以看到在2个epoch之后，微调后的qwen3的loss降低到了不错的水平——当然对于大模型来说，真正的效果评估还得看主观效果。</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202506/1341090-20250610153338309-855764500.png" alt="" loading="lazy"></p>
<p>可以看到在一些测试样例上，微调后的qwen3能够给出准确的文本类型：</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202506/1341090-20250610153517525-1873628635.png" alt="" loading="lazy"></p>
<p>&nbsp;</p>
<p>至此，你已经完成了qwen3指令微调的训练！</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>本文参考链接：</p>
<p>https://blog.csdn.net/SoulmateY/article/details/139564703</p>
<p>https://blog.csdn.net/qq_45258632/article/details/144971398</p>
<p>&nbsp;</p>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.09722222222222222" data-date-updated="2025-06-10 17:56">2025-06-10 15:36</span>&nbsp;
<a href="https://www.cnblogs.com/xiao987334176">肖祥</a>&nbsp;
阅读(<span id="post_view_count">60</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18922128);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18922128', targetLink: 'https://www.cnblogs.com/xiao987334176/p/18922128', title: 'SwanLab入门深度学习：Qwen3大模型指令微调' })">举报</a>
</div>
        