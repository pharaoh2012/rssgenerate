
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/KubeExplorer/p/18913850" title="发布于 2025-06-06 12:28">
    <span role="heading" aria-level="2">开源 vGPU 方案：HAMi,实现细粒度 GPU 切分</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p><img src="https://img.lixueduan.com/kubernetes/cover/vgpu-hami.png" alt="vgpu-hami.png" loading="lazy"></p>
<p>本文主要分享一个开源的 GPU 虚拟化方案：HAMi，包括如何安装、配置以及使用。<br>
相比于上一篇分享的 TimeSlicing 方案，HAMi 除了 GPU 共享之外还可以实现 GPU core、memory 得限制，保证共享同一 GPU 的各个 Pod 都能拿到足够的资源。</p>

<h2 id="1为什么需要-gpu-共享切分等方案">1.为什么需要 GPU 共享、切分等方案？</h2>
<p>开始之前我们先思考一个问题，<strong>为什么需要 GPU 共享、切分等方案？</strong></p>
<p>或者说是另外一个问题：<strong>明明直接在裸机环境使用，都可以多个进程共享 GPU，怎么到 k8s 环境就不行了</strong>。</p>
<p>推荐阅读前面几篇文章：这两篇分享了如何在各个环境中使用 GPU，在 k8s 环境则推荐使用 NVIDIA 提供的 gpu-operator 快速部署环境。</p>
<p><a href="https://www.lixueduan.com/posts/ai/01-how-to-use-gpu/" target="_blank" rel="noopener nofollow">GPU 环境搭建指南：如何在裸机、Docker、K8s 等环境中使用 GPU</a></p>
<p><a href="https://www.lixueduan.com/posts/ai/02-gpu-operator/" target="_blank" rel="noopener nofollow">GPU 环境搭建指南：使用 GPU Operator 加速 Kubernetes GPU 环境搭建</a></p>
<p>这两篇则分析了 device-plugin 原理以及在 K8s 中创建一个申请 GPU 的 Pod 后的一些列动作，最终该 Pod 是如何使用到 GPU 的。</p>
<p><a href="https://www.lixueduan.com/posts/kubernetes/21-device-plugin/" target="_blank" rel="noopener nofollow">Kubernetes教程(二一)---自定义资源支持：K8s Device Plugin 从原理到实现</a></p>
<p><a href="https://www.lixueduan.com/posts/kubernetes/22-pod-use-gpu-in-k8s-analyze/" target="_blank" rel="noopener nofollow">Kubernetes教程(二二)---在 K8S 中创建 Pod 是如何使用到 GPU 的：device plugin&amp;nvidia-container-toolkit 源码分析</a></p>
<p>看完之后，大家应该就大致明白了。</p>
<h3 id="资源感知">资源感知</h3>
<p>首先在 k8s 中资源是和节点绑定的，对于 GPU 资源，我们使用 NVIDIA 提供的 device-plugin 进行感知，并上报到 kube-apiserver,这样我们就能在 Node 对象上看到对应的资源了。</p>
<p>就像这样：</p>
<pre><code class="language-bash">root@liqivm:~# k describe node gpu01|grep Capacity -A 7
Capacity:
  cpu:                128
  ephemeral-storage:  879000896Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             1056457696Ki
  nvidia.com/gpu:     8
  pods:               110
</code></pre>
<p>可以看到，该节点除了基础的 cpu、memory 之外，还有一个<code>nvidia.com/gpu:     8</code> 信息，表示该节点上有 8 个 GPU。</p>
<h3 id="资源申请">资源申请</h3>
<p>然后我们就可以在创建 Pod 时申请对应的资源了，比如申请一个 GPU：</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: gpu-pod
spec:
  containers:
  - name: gpu-container
    image: nvidia/cuda:11.0-base   # 一个支持 GPU 的镜像
    resources:
      limits:
        nvidia.com/gpu: 1          # 申请 1 个 GPU
    command: ["nvidia-smi"]         # 示例命令，显示 GPU 的信息
  restartPolicy: OnFailure
</code></pre>
<p>apply 该 yaml 之后，kube-scheduler 在调度该 Pod 时就会将其调度到一个拥有足够 GPU 资源的 Node 上。</p>
<p>同时该 Pod 申请的部分资源也会标记为已使用，不会在分配给其他 Pod。</p>
<p>到这里，问题的答案就已经很明显的。</p>
<ul>
<li>1）device-plugin 感知到节点上的物理 GPU 数量，上报到 kube-apiserver</li>
<li>2）kube-scheduler 调度 Pod 时会根据 pod 中的 Request 消耗对应资源</li>
</ul>
<p>即：<strong>Node 上的 GPU 资源被 Pod 申请之后，在 k8s 中就被标记为已消耗了，后续创建的 Pod 会因为资源不够导致无法调度</strong>。</p>
<p>实际上：可能 GPU 性能比较好，可以支持多个 Pod 共同使用，但是因为 k8s 中的调度限制导致多个 Pod 无法正常共享。</p>
<p>因此，我们才需要 GPU 共享、切分等方案。</p>
<p>上一篇文章<a href="https://www.lixueduan.com/posts/kubernetes/25-gpu-share-time-slicing/" target="_blank" rel="noopener nofollow">一文搞懂 GPU 共享方案： NVIDIA Time Slicing</a> 中给大家分享了一个 GPU 共享方案。</p>
<p>可以实现多个 Pod 共享同一个 GPU，但是存在一个问题：Pod 之间并未做任何隔离，每个 Pod 能用到多少 GPU core、memory 都靠竞争，可能会导致部分 Pod 占用大部分资源导致其他 Pod 无法正常使用的情况。</p>
<p>今天给大家分享一个开源的 vGPU 方案 <a href="https://github.com/Project-HAMi/HAMi" target="_blank" rel="noopener nofollow">HAMi</a>。</p>
<blockquote>
<p>ps：NVIDIA 也有自己的 vGPU 方案，但是需要 license</p>
</blockquote>
<h2 id="2-什么是-hami">2. 什么是 HAMi？</h2>
<p>HAMi 全称是：Heterogeneous AI Computing Virtualization Middleware，HAMi 给自己的定位或者希望是做一个异构算力虚拟化平台。</p>
<blockquote>
<p>原 <a href="https://github.com/4paradigm/k8s-vgpu-scheduler" target="_blank" rel="noopener nofollow">第四范式 k8s-vgpu-scheduler</a>, 这次改名 HAMi 同时也将核心的 vCUDA 库 libvgpu.so 也开源了。</p>
</blockquote>
<p>但是现在比较完善的是对 NVIDIA GPU 的 vGPU 方案，因此我们可以简单认为他就是一个 vGPU 方案。</p>
<p>整体架构如下：</p>
<p><img src="https://img.lixueduan.com/kubernetes/vgpu/hami-arch.png" alt="hami-arch.png" loading="lazy"></p>
<p>可以看到组件还是比较多的，设计到 Webhook、Scheduler、Device Plugin、HAMi-Core 等等。</p>
<p>这篇文章只讲使用，因此架构、原理就一笔带过，后续也会有相关文章,欢迎关注~。</p>
<h3 id="feature">Feature</h3>
<p>使用 HAMi 最大的一个功能点就是可以实现 GPU 的细粒度的隔离，可以对 core 和 memory 使用 1% 级别的隔离。</p>
<p>具体如下：</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: gpu-pod
spec:
  containers:
    - name: ubuntu-container
      image: ubuntu:18.04
      command: ["bash", "-c", "sleep 86400"]
      resources:
        limits:
          nvidia.com/gpu: 1 # 请求1个vGPUs
          nvidia.com/gpumem: 3000 # 每个vGPU申请3000m显存 （可选，整数类型）
          nvidia.com/gpucores: 30 # 每个vGPU的算力为30%实际显卡的算力 （可选，整数类型）
</code></pre>
<ul>
<li>nvidia.com/gpu：请求一个 GPU</li>
<li>nvidia.com/gpumem：只申请使用  3000M GPU Memory</li>
<li>nvidia.com/gpucores：申请使用 30% 的 GPU core，也就是该 Pod 只能使用到 30% 的算力</li>
</ul>
<p>相比于上文分享了 TimeSlicing 方案，HAMi 则是实现了 GPU core 和 memory 的隔离。</p>
<blockquote>
<p>在开源方案里面已经算是比较优秀的了。</p>
</blockquote>
<h3 id="design">Design</h3>
<p>HAMi 实现GPU core 和 memory 隔离、限制是使用的 vCUDA 方案，具体设计如下：</p>
<p><img src="https://img.lixueduan.com/kubernetes/vgpu/hami-core-design.png" alt="hami-core-design.png" loading="lazy"></p>
<p>大部分使用 GPU 的应用都是用的 CUDA，HAMi 也是用的 vCUDA 方案，对 NVIDIA 原生的 CUDA 驱动进行重写，然后挂载到 Pod 中进行替换，然后在自己的实现的 CUDA 驱动中对 API 进行拦截，使用资源隔离以及限制的效果。</p>
<p>例如：原生 CUDA 驱动进行内存分配，只有在 GPU 内存真的用完的时候才会提示 CUDA OOM，但是对于 HAMi CUDA 驱动来说，检测到 Pod 中使用的内存超过了 Resource 中的申请量就直接返回 OOM，从而实现资源的一个限制。</p>
<p>然后在执行 nvidia-smi 命令查看 GPU 信息时，也只返回 Pod Resource 中申请的资源，这样在查看时也进行隔离。</p>
<blockquote>
<p>ps：需要对 CUDA 和 NVML 的部分 API 拦截。</p>
</blockquote>
<h2 id="3-hami-部署">3. HAMi 部署</h2>
<p>HAMi 提供了 Helm Chart 安装也是比较简单的。</p>
<h3 id="部署-gpu-operator">部署 GPU Operator</h3>
<p>需要注意的是 HAMi 会依赖 NVIDIA 的那一套，因此推荐先部署 GPU-Operator。</p>
<p>参考这篇文章 --&gt; <a href="https://www.lixueduan.com/posts/ai/02-gpu-operator/" target="_blank" rel="noopener nofollow">GPU 环境搭建指南：使用 GPU Operator 加速 Kubernetes GPU 环境搭建</a></p>
<p>部署好 GPU Operator 之后在部署 HAMi。</p>
<h3 id="部署-hami">部署 HAMi</h3>
<p>首先使用 helm 添加我们的 repo</p>
<pre><code class="language-Plaintext">helm repo add hami-charts https://project-hami.github.io/HAMi/
</code></pre>
<p>随后，使用下列指令获取集群服务端版本</p>
<blockquote>
<p>这里使用的是 v1.27.4 版本</p>
</blockquote>
<pre><code class="language-Plaintext">kubectl version
</code></pre>
<p>在安装过程中须根据集群服务端版本（上一条指令的结果）指定调度器镜像版本，例如集群服务端版本为 v1.27.4，则可以使用如下指令进行安装</p>
<pre><code class="language-Bash">helm install hami hami-charts/hami --set scheduler.kubeScheduler.imageTag=v1.27.4 -n kube-system
</code></pre>
<p>通过kubectl get pods指令看到 <code>vgpu-device-plugin</code> 与 <code>vgpu-scheduler</code> 两个pod 状态为<code>Running</code> 即为安装成功</p>
<pre><code class="language-Bash">root@iZj6c5dnq07p1ic04ei9vwZ:~# kubectl get pods -n kube-system|grep hami
hami-device-plugin-b6mvj                          2/2     Running   0          42s
hami-scheduler-7f5c5ff968-26kjc                   2/2     Running   0          42s
</code></pre>
<h3 id="自定义配置">自定义配置</h3>
<blockquote>
<p>官方文档：<a href="https://github.com/Project-HAMi/HAMi/blob/master/docs/config_cn.md" target="_blank" rel="noopener nofollow">HAMi-config-cn.md</a></p>
</blockquote>
<p>你可以在安装过程中，通过<code>-set</code>来修改以下的客制化参数，例如：</p>
<pre><code class="language-Plaintext">helm install vgpu vgpu-charts/vgpu --set devicePlugin.deviceMemoryScaling=5 ...
</code></pre>
<ul>
<li><code>devicePlugin.deviceSplitCount</code>： 整数类型，预设值是 10。GPU 的分割数，每一张GPU 都不能分配超过其配置数目的任务。若其配置为N的话，每个 GPU 上最多可以同时存在 N 个任务。</li>
<li><code>devicePlugin.deviceMemoryScaling:</code> 浮点数类型，预设值是1。NVIDIA 装置显存使用比例，可以大于1（启用虚拟显存，实验功能）。对于有 M显存大小的 NVIDIA GPU，如果我们配置<code>devicePlugin.deviceMemoryScaling</code>参数为 S ，在部署了我们装置插件的Kubenetes 集群中，这张 GPU 分出的 vGPU 将总共包含 <code>S * M</code> 显存。</li>
<li><code>devicePlugin.migStrategy:</code> 字符串类型，目前支持"none“与“mixed“两种工作方式，前者忽略 MIG 设备，后者使用专门的资源名称指定 MIG 设备，使用详情请参考mix_example.yaml，默认为"none"</li>
<li><code>devicePlugin.disablecorelimit:</code> 字符串类型，"true"为关闭算力限制，"false"为启动算力限制，默认为"false"</li>
<li><code>scheduler.defaultMem:</code> 整数类型，预设值为 5000，表示不配置显存时使用的默认显存大小，单位为 MB</li>
<li><code>scheduler.defaultCores:</code> 整数类型(0-100)，默认为0，表示默认为每个任务预留的百分比算力。若设置为 0，则代表任务可能会被分配到任一满足显存需求的 GPU 中，若设置为100，代表该任务独享整张显卡</li>
<li><code>scheduler.defaultGPUNum:</code> 整数类型，默认为1，如果配置为0，则配置不会生效。当用户在 pod 资源中没有设置 nvidia.com/gpu 这个 key 时，webhook 会检查 nvidia.com/gpumem、resource-mem-percentage、nvidia.com/gpucores 这三个 key 中的任何一个 key 有值，webhook 都会添加 nvidia.com/gpu 键和此默认值到 resources limit中。</li>
<li><code>resourceName:</code> 字符串类型, 申请vgpu个数的资源名, 默认: "nvidia.com/gpu"</li>
<li><code>resourceMem:</code> 字符串类型, 申请vgpu显存大小资源名, 默认: "nvidia.com/gpumem"</li>
<li><code>resourceMemPercentage:</code> 字符串类型，申请vgpu显存比例资源名，默认: "nvidia.com/gpumem-percentage"</li>
<li><code>resourceCores:</code> 字符串类型, 申请vgpu算力资源名, 默认: "nvidia.com/cores"</li>
<li><code>resourcePriority:</code> 字符串类型，表示申请任务的任务优先级，默认: "nvidia.com/priority"</li>
</ul>
<p>除此之外，容器中也有对应配置</p>
<ul>
<li><code>GPU_CORE_UTILIZATION_POLICY:</code> 字符串类型，"default", "force", "disable" 代表容器算力限制策略， "default"为默认，"force"为强制限制算力，一般用于测试算力限制的功能，"disable"为忽略算力限制</li>
<li><code>ACTIVE_OOM_KILLER:</code> 字符串类型，"true", "false" 代表容器是否会因为超用显存而被终止执行，"true"为会，"false"为不会</li>
</ul>
<p>我们只是简单 Demo 就不做任何配置直接部署即可。</p>
<h2 id="4-验证">4. 验证</h2>
<h3 id="查看-node-gpu-资源">查看 Node GPU 资源</h3>
<p>类似于上一篇分享的 TimeSlicing 方案，在安装之后，Node 上可见的 GPU 资源也是增加了。</p>
<p>环境中只有一个物理 GPU，但是 HAMi 默认会扩容 10 倍，理论上现在 Node 上能查看到 1*10 = 10 个 GPU。</p>
<blockquote>
<p>默认参数就是切分为 10 个，可以设置。</p>
</blockquote>
<pre><code class="language-Bash">$ kubectl get node xxx -oyaml|grep capacity -A 7
  capacity:
    cpu: "4"
    ephemeral-storage: 206043828Ki
    hugepages-1Gi: "0"
    hugepages-2Mi: "0"
    memory: 15349120Ki
    nvidia.com/gpu: "10"
    pods: "110"
</code></pre>
<h3 id="验证显存和算力限制">验证显存和算力限制</h3>
<p>使用以下 yaml 来创建 Pod，注意 resources.limit 除了原有的  nvidia.com/gpu 之外还新增了 nvidia.com/gpumem 和 nvidia.com/gpucores，用来指定显存大小和算力大小。</p>
<ul>
<li>nvidia.com/gpu：请求的 vgpu 数量，例如 1</li>
<li>nvidia.com/gpumem ：请求的显存数量，例如 3000M</li>
<li>nvidia.com/gpumem-percentage：显存百分百，例如 50 则是请求 50%显存</li>
<li>nvidia.com/priority: 优先级，0 为高，1 为底，默认为 1。
<ul>
<li>对于高优先级任务，如果它们与其他高优先级任务共享 GPU 节点，则其资源利用率不会受到 <code>resourceCores</code> 的限制。换句话说，如果只有高优先级任务占用 GPU 节点，那么它们可以利用节点上所有可用的资源。</li>
<li>对于低优先级任务，如果它们是唯一占用 GPU 的任务，则其资源利用率也不会受到 <code>resourceCores</code> 的限制。这意味着如果没有其他任务与低优先级任务共享 GPU，那么它们可以利用节点上所有可用的资源。</li>
</ul>
</li>
</ul>
<p>完整 gpu-test.yaml 内容如下：</p>
<pre><code class="language-YAML">apiVersion: v1
kind: Pod
metadata:
  name: gpu-pod
spec:
  containers:
    - name: ubuntu-container
      image: ubuntu:18.04
      command: ["bash", "-c", "sleep 86400"]
      resources:
        limits:
          nvidia.com/gpu: 1 # 请求1个vGPUs
          nvidia.com/gpumem: 3000 # 每个vGPU申请3000m显存 （可选，整数类型）
          nvidia.com/gpucores: 30 # 每个vGPU的算力为30%实际显卡的算力 （可选，整数类型）
</code></pre>
<p>Pod 能够正常启动</p>
<pre><code class="language-Bash">root@iZj6c5dnq07p1ic04ei9vwZ:~# kubectl get po
NAME      READY   STATUS    RESTARTS   AGE
gpu-pod   1/1     Running   0          48s
</code></pre>
<p>进入 Pod执行 nvidia-smi 命令，查看 GPU 信息，可以看到展示的限制就是 Resource 中申请的 3000M。</p>
<pre><code class="language-Bash">root@iZj6c5dnq07p1ic04ei9vwZ:~# kubectl exec -it gpu-pod -- bash
root@gpu-pod:/# nvidia-smi
[HAMI-core Msg(16:139711087368000:libvgpu.c:836)]: Initializing.....
Mon Apr 29 06:22:16 2024
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       On  |   00000000:00:07.0 Off |                    0 |
| N/A   33C    P8             15W /   70W |       0MiB /   3000MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
[HAMI-core Msg(16:139711087368000:multiprocess_memory_limit.c:434)]: Calling exit handler 16
</code></pre>
<p>根据最后的日志就是 HAMi 的 CUDA 驱动打印的。</p>
<pre><code class="language-Bash">[HAMI-core Msg(16:139711087368000:multiprocess_memory_limit.c:434)]: Calling exit handler 16
</code></pre>
<hr>
<p><strong>【Kubernetes 系列】</strong>持续更新中，搜索公众号【<strong>探索云原生</strong>】订阅，阅读更多文章。</p>
<p><img src="https://img.lixueduan.com/about/wechat/search.png" alt="" loading="lazy"></p>
<hr>
<h2 id="5-小结">5. 小结</h2>
<p>本文主要分享了开源 vGPU 方案 HAMi，并通过简单 Demo 进行了验证。</p>
<p><strong>为什么需要 GPU 共享、切分？</strong></p>
<p>在 k8s 中使用默认 device plugin 时，GPU 资源和物理 GPU 是一一对应的，导致一个物理 GPU 被一个 Pod 申请后，其他 Pod 就无法使用了。</p>
<p>为了提高资源利用率，因此我们需要 GPU 共享、切分等方案。</p>
<p><strong>HAMi 大致实现原理</strong></p>
<p>通过替换容器中的 libvgpu.so 库，实现 CUDA API 拦截，最终实现对 GPU core 和 memory 的隔离和限制。</p>
<blockquote>
<p>更加详细的原理分析，可以期待后续文章~</p>
</blockquote>
<p>最后在贴一下相关文章，推荐阅读：</p>
<ul>
<li>
<p><a href="https://www.lixueduan.com/posts/ai/01-how-to-use-gpu/" target="_blank" rel="noopener nofollow">GPU 环境搭建指南：如何在裸机、Docker、K8s 等环境中使用 GPU</a></p>
</li>
<li>
<p><a href="https://www.lixueduan.com/posts/ai/02-gpu-operator/" target="_blank" rel="noopener nofollow">GPU 环境搭建指南：使用 GPU Operator 加速 Kubernetes GPU 环境搭建</a></p>
</li>
<li>
<p><a href="https://www.lixueduan.com/posts/kubernetes/21-device-plugin/" target="_blank" rel="noopener nofollow">Kubernetes教程(二一)---自定义资源支持：K8s Device Plugin 从原理到实现</a></p>
</li>
<li>
<p><a href="https://www.lixueduan.com/posts/kubernetes/22-pod-use-gpu-in-k8s-analyze/" target="_blank" rel="noopener nofollow">Kubernetes教程(二二)---在 K8S 中创建 Pod 是如何使用到 GPU 的：device plugin&amp;nvidia-container-toolkit 源码分析</a></p>
</li>
<li>
<p><a href="https://www.lixueduan.com/posts/kubernetes/25-gpu-share-time-slicing/" target="_blank" rel="noopener nofollow">一文搞懂 GPU 共享方案： NVIDIA Time Slicing</a></p>
</li>
</ul>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.0799917265150463" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-06-06 12:28">2025-06-06 12:28</span>&nbsp;
<a href="https://www.cnblogs.com/KubeExplorer">探索云原生</a>&nbsp;
阅读(<span id="post_view_count">0</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18913850);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18913850', targetLink: 'https://www.cnblogs.com/KubeExplorer/p/18913850', title: '开源 vGPU 方案：HAMi,实现细粒度 GPU 切分' })">举报</a>
</div>
        