
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/lliuhuan/p/18659620" title="发布于 2025-01-08 14:17">
    <span role="heading" aria-level="2">linux 安装 Ollama 框架</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h4 id="概述">概述</h4>
<p>Ollama 是一款旨在简化大语言模型（LLM）本地部署的工具，支持 Windows、Linux 和 MacOS 系统。它提供了一个用户友好的环境，让开发者可以轻松地运行和调优如 Qwen、Llama 等超过1700种的大语言模型。</p>
<p><strong>核心优势</strong></p>
<ul>
<li><strong>多平台兼容性</strong>：无缝适配主流操作系统。</li>
<li><strong>丰富的模型库</strong>：内置了大量预训练模型，直接从官网下载即可使用。</li>
<li><strong>个性化配置</strong>：通过 modelfile 文件调整模型参数，实现对生成内容的精细控制。</li>
<li><strong>自定义模型支持</strong>：允许导入第三方 ggml 格式的模型，或转换其他格式的模型至 ggml。</li>
<li><strong>高性能推理</strong>：优化了多GPU环境下的并行处理能力，提升推理速度。</li>
</ul>
<p>Ollama 的设计降低了大语言模型的使用门槛，特别适合希望快速构建 AI 应用或针对特定任务进行模型优化的开发者。</p>
<p><strong>官方资源</strong></p>
<ul>
<li><strong>下载页面</strong>：<a href="https://ollama.com/download" target="_blank" rel="noopener nofollow">Ollama 下载</a></li>
<li><strong>官方网站</strong>：<a href="https://ollama.com" target="_blank" rel="noopener nofollow">Ollama 官方主页</a></li>
<li><strong>源代码仓库</strong>：<a href="https://github.com/ollama/ollama" target="_blank" rel="noopener nofollow">Ollama GitHub</a></li>
</ul>
<h4 id="快速安装指南">快速安装指南</h4>
<p>对于 Linux 用户，Ollama 提供了一键安装脚本，使得安装过程变得极为简便。以下是详细的安装步骤：</p>
<ol>
<li>
<p><strong>执行一键安装</strong>：</p>
<pre><code class="language-bash">curl -fsSL https://ollama.com/install.sh | sh
</code></pre>
</li>
<li>
<p><strong>启动 Ollama 服务</strong>：</p>
<pre><code class="language-bash">ollama serve
</code></pre>
</li>
</ol>
<p>如果遇到权限问题（例如提示需要 sudo 权限），建议下载离线安装包以避免权限限制。可以从 <a href="https://github.com/ollama/ollama/releases/tag/v0.4.4" target="_blank" rel="noopener nofollow">GitHub 发布页</a> 获取对应系统的安装包，并按照以下步骤操作：</p>
<ol>
<li>
<p><strong>下载离线安装包</strong>：<br>
访问发布页，选择适用于您系统的版本下载。</p>
</li>
<li>
<p><strong>解压文件</strong>：</p>
<pre><code class="language-bash">tar -xzvf ./ollama-linux-amd64.tgz
</code></pre>
</li>
<li>
<p><strong>启动 Ollama 服务</strong>：</p>
<pre><code class="language-bash">./bin/ollama serve &amp;
</code></pre>
</li>
</ol>
<p>为了确保 <code>ollama</code> 命令可以在任何位置被调用，需要将其路径添加到系统的 <code>PATH</code> 环境变量中。这可以通过编辑您的 shell 配置文件（如 <code>.bashrc</code> 或 <code>.zshrc</code>）来完成。</p>
<pre><code class="language-bash">echo 'export PATH=$PATH:/home/ollama/bin' &gt;&gt; ~/.bashrc
echo 'export OLLAMA_KEEP_ALIVE=12h' &gt;&gt; ~/.bashrc
echo 'export OLLAMA_HOST=0.0.0.0:11434' &gt;&gt; ~/.bashrc
echo 'export OLLAMA_MODELS=/data/ollama/models' &gt;&gt; ~/.bashrc
source ~/.bashrc
</code></pre>
<p>现在，您可以直接在终端中使用 <code>ollama</code> 命令来管理和运行模型，例如：</p>
<pre><code class="language-bash">ollama run llama3.1:8b
</code></pre>
<p>以上步骤将帮助您顺利完成 Ollama 的安装与配置，开始探索大语言模型的世界。</p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.0018178005659722223" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-01-08 14:18">2025-01-08 14:17</span>&nbsp;
<a href="https://www.cnblogs.com/lliuhuan">大象。</a>&nbsp;
阅读(<span id="post_view_count">0</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18659620" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18659620);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18659620', targetLink: 'https://www.cnblogs.com/lliuhuan/p/18659620', title: 'linux 安装 Ollama 框架' })">举报</a>
</div>
        