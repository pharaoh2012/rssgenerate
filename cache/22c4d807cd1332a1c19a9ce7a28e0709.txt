
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/IrregularRhythm/p/18882385" title="发布于 2025-05-18 18:32">
    <span role="heading" aria-level="2">稀疏贝叶斯谱估计及EM算法求解</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h1 id="稀疏贝叶斯">稀疏贝叶斯</h1>
<p>稀疏贝叶斯学习(sparse bayes learning，SBL)最早被提出是作为一种机器学习算法[1]。但是在这里我们主要用它来做谱估计，作为求解稀疏重构问题的方法[2]。稀疏重构还有个更好听的名字叫压缩感知，但我既不知道他哪里压缩了也不明白他怎么个感知法，也有人说这是两回事，在此咱们不纠结，就叫他稀疏重构了。</p>
<h2 id="稀疏重构问题">稀疏重构问题</h2>
<p>对于如下问题：</p>
<p></p><div class="math display">\[\begin{equation}
\boldsymbol{t} = \boldsymbol{\Phi}\boldsymbol{w} + \boldsymbol{\epsilon} \tag{1} 
\end{equation}
\]</div><p></p><p>其中<span class="math inline">\(\boldsymbol{\Phi} \in \mathbb{C}^{N\times M}\)</span> 为过完备字典，即<span class="math inline">\(rank(\boldsymbol{\Phi})=N\ and\ M&gt;N\)</span> 。<span class="math inline">\(\boldsymbol{t} \in \mathbb{C}^{N \times 1}\)</span> 为观测信号，<span class="math inline">\(\boldsymbol{w}\in\mathbb{C}^{M \times 1}\)</span>为权重向量，<span class="math inline">\(\boldsymbol{\epsilon}\in\mathbb{C}^{N\times 1}\)</span>为观测噪声，希望求得<span class="math inline">\(\boldsymbol{w}\)</span>满足：</p>
<p></p><div class="math display">\[\begin{equation}
\boldsymbol{w} = \arg\min_{\boldsymbol{w}}||\boldsymbol{t} - \boldsymbol{\Phi} \boldsymbol{w}||_2^2 + \lambda||\boldsymbol{w}||_0 \tag{2} 
\end{equation}
\]</div><p></p><p>其中<span class="math inline">\(||\boldsymbol{w}||_0\)</span> 为0范数，即<span class="math inline">\(\boldsymbol{w}\)</span>中非零元的个数。通俗来讲就是用字典矩阵 <span class="math inline">\(\boldsymbol{\Phi}\)</span> 中最少的向量对<span class="math inline">\(\boldsymbol{t}\)</span>进行表示，所求的<span class="math inline">\(\boldsymbol{w}\)</span>就是求得的权重系数。该问题属于<span class="math inline">\(NP-hard\)</span>问题，无法直接求解，但有许多近似解法，包括LASSO，OMP等，本篇所介绍的SBL也是求解算法之一。</p>
<h2 id="稀疏贝叶斯-1">稀疏贝叶斯</h2>
<p>式(1)中的噪声 <span class="math inline">\(\boldsymbol{\epsilon}\)</span> 通常被认为服从0均值高斯分布，即<span class="math inline">\(\epsilon \sim\mathcal{N}(0,\sigma^2\boldsymbol{I})\)</span>，则<span class="math inline">\(\boldsymbol{t}\)</span>的条件概率密度函数可以写作:</p>
<p></p><div class="math display">\[\begin{equation}
p(\boldsymbol{t}|\boldsymbol{w};\sigma^2)=(2\pi\sigma^2)^{-\frac{N}{2}}\exp(-\frac{1}{2\sigma^2} ||\boldsymbol{t} - \boldsymbol{\Phi} \boldsymbol{w}||_2^2)
\tag{3} 
\end{equation}
\]</div><p></p><p>同时假定<span class="math inline">\(\boldsymbol{w}\)</span>的先验服从高斯分布：</p>
<p></p><div class="math display">\[\begin{equation}
p(\boldsymbol{w};\boldsymbol{\Gamma})=(2\pi)^{-\frac{M}{2}}|\boldsymbol{\Gamma}|^{-\frac{1}{2}}\exp(-\frac{1}{2}\boldsymbol{w}^H{\boldsymbol{\Gamma}^{-1}\boldsymbol{w}}) \tag{4}
\end{equation}
\]</div><p></p><p>其中<span class="math inline">\(\boldsymbol{\Gamma}=diag([\gamma_1,\gamma_2,...,\gamma_M]^T)\)</span>为对角阵，是权重的方差。这里通过式(3)和式(4)可以求得边际概率：</p>
<p></p><div class="math display">\[\begin{align}
p(\boldsymbol{t};\boldsymbol{\Gamma},\boldsymbol{\sigma^2})=&amp;\int p(\boldsymbol{t}|\boldsymbol{w};\sigma^2)p(\boldsymbol{w};\boldsymbol{\Gamma})d\boldsymbol{w}\nonumber \\ 
=&amp;\int(2\pi\sigma^2)^{-\frac{N}{2}}(2\pi)^{-\frac{M}{2}}|\boldsymbol{\Gamma}|^{-\frac{1}{2}}\exp(-\frac{1}{2\sigma^2} ||\boldsymbol{t} - \boldsymbol{\Phi} \boldsymbol{w}||_2^2-\frac{1}{2}\boldsymbol{w}^H{\boldsymbol{\Gamma}^{-1}\boldsymbol{w}})d\boldsymbol{w}
\tag{5}
\end{align}
\]</div><p></p><p>对于上式，先重点看指数里面部分：</p>
<p></p><div class="math display">\[\begin{align}
&amp;-\frac{1}{2\sigma^2} ||\boldsymbol{t} - \boldsymbol{\Phi} \boldsymbol{w}||_2^2-\frac{1}{2}\boldsymbol{w}^H{\boldsymbol{\Gamma}^{-1}\boldsymbol{w}}\nonumber \\
= &amp;-\frac{1}{2}[\sigma^{-2}\boldsymbol{t}^H\boldsymbol{t}-2\sigma^{-2}\boldsymbol{t}^H\boldsymbol{\Phi} \boldsymbol{w}+\boldsymbol{w}^H(\sigma^{-2}\boldsymbol{\Phi}^H\boldsymbol{\Phi}+\boldsymbol{\Gamma}^{-1})\boldsymbol{w}]\nonumber\\
=&amp;-\frac{1}{2}[\sigma^{-2}\boldsymbol{t}^H\boldsymbol{t}-2\sigma^{-2}\boldsymbol{t}^H\boldsymbol{\Phi} \boldsymbol{w}+\boldsymbol{w}^H\boldsymbol{\Sigma_w}^{-1}\boldsymbol{w}]\nonumber \\
=&amp;-\frac{1}{2}(\sigma^{-2}\boldsymbol{\Sigma_w\Phi}^H\boldsymbol{t}-\boldsymbol{w})^H\boldsymbol{\Sigma_w}^{-1}(\sigma^{-2}\boldsymbol{\Sigma_w\Phi}^H\boldsymbol{t}-\boldsymbol{w})-\frac{1}{2}[\boldsymbol{t}^H(\boldsymbol{I-\boldsymbol{\Phi}\boldsymbol{\Sigma_w}\boldsymbol{\Phi}^H})\boldsymbol{t}]

\tag{6}
\end{align}
\]</div><p></p><p>其中<span class="math inline">\(\boldsymbol{\Sigma_w}=(\sigma^{-2}\boldsymbol{\Phi}^H\boldsymbol{\Phi}+\boldsymbol{\Gamma}^{-1})^{-1}\)</span>，并令<span class="math inline">\((\boldsymbol{I-\boldsymbol{\Phi}\boldsymbol{\Sigma_w}\boldsymbol{\Phi}^H})^{-1} = \boldsymbol{\Sigma_t}\)</span>，根据矩阵求逆引理可以推导出<span class="math inline">\(\boldsymbol{\Sigma_t} = \sigma^2\boldsymbol{I}+\boldsymbol{\Phi\Gamma\Phi}^H\)</span>。<br>
然后将上述结果带入(5):</p>
<p></p><div class="math display">\[\begin{align}
&amp;p(\boldsymbol{t};\boldsymbol{\Gamma},\sigma^2)\nonumber \\=&amp;(2\pi\sigma^2)^{-\frac{N}{2}}(2\pi)^{-\frac{M}{2}}|\boldsymbol{\Gamma}|^{-\frac{1}{2}}\exp(-\frac{1}{2}\boldsymbol{t}^H\boldsymbol{\Sigma_t}^{-1}\boldsymbol{t})\nonumber \\
&amp;\ \ \int\exp[-\frac{1}{2}(\sigma^{-2}\boldsymbol{\Sigma_w\Phi}^H\boldsymbol{t}-\boldsymbol{w})^H\boldsymbol{\Sigma_w}^{-1}(\sigma^{-2}\boldsymbol{\Sigma_w\Phi}^H\boldsymbol{t}-\boldsymbol{w})]d\boldsymbol{w}
\tag{7}
\end{align}
\]</div><p></p><p>可以看出指数上凑出了高斯分布的形式，但还差指数外的系数。要凑成完整的高斯分布，注意到<span class="math inline">\(\sigma^{-\frac{N}{2}}|\boldsymbol{\Sigma_w}|^{\frac{1}{2}}|\boldsymbol{\Gamma}|^{-\frac{1}{2}} = |\boldsymbol{\Sigma_t}|^{-\frac{1}{2}}\)</span>，可得：</p>
<p></p><div class="math display">\[\begin{align}
&amp;p(\boldsymbol{t};\boldsymbol{\Gamma},\sigma^2)\nonumber \\
=&amp;(2\pi)^{-\frac{N}{2}}|\boldsymbol{\Sigma_t}|^{-\frac{1}{2}}\exp(-\frac{1}{2}\boldsymbol{t}^H\boldsymbol{\Sigma_t}^{-1}\boldsymbol{t})\nonumber \\
&amp;\ \ \int(2\pi)^{-\frac{M}{2}}|\boldsymbol{\Sigma_w}|^{-\frac{1}{2}}\exp[-\frac{1}{2}(\sigma^{-2}\boldsymbol{\Sigma_w\Phi}^H\boldsymbol{t}-\boldsymbol{w})^H\boldsymbol{\Sigma_w}^{-1}(\sigma^{-2}\boldsymbol{\Sigma_w\Phi}^H\boldsymbol{t}-\boldsymbol{w})]d\boldsymbol{w}
\tag{8}
\end{align}
\]</div><p></p><p>积分内积完结果为<span class="math inline">\(1\)</span>，所以：</p>
<p></p><div class="math display">\[\begin{align}
&amp;p(\boldsymbol{t};\boldsymbol{\Gamma},\sigma^2)=(2\pi)^{-\frac{N}{2}}|\boldsymbol{\Sigma_t}|^{-\frac{1}{2}}\exp(-\frac{1}{2}\boldsymbol{t}^H\boldsymbol{\Sigma_t}^{-1}\boldsymbol{t})
\tag{9}
\end{align}
\]</div><p></p><p>另外，积分内积掉的为<span class="math inline">\(\boldsymbol{w}\)</span>的后验分布：</p>
<p></p><div class="math display">\[\begin{align}
&amp;p(\boldsymbol{w}|\boldsymbol{t};\boldsymbol{\Gamma},\sigma^2)\nonumber\\
=&amp;(2\pi)^{-\frac{M}{2}}|\boldsymbol{\Sigma_w}|^{-\frac{1}{2}}\exp[-\frac{1}{2}(\sigma^{-2}\boldsymbol{\Sigma_w\Phi}^H\boldsymbol{t}-\boldsymbol{w})^H\boldsymbol{\Sigma_w}^{-1}(\sigma^{-2}\boldsymbol{\Sigma_w\Phi}^H\boldsymbol{t}-\boldsymbol{w})]\nonumber\\
=&amp;\mathcal{N}(\boldsymbol{\mu_w},\boldsymbol{\Sigma_w})
\tag{10}
\end{align}
\]</div><p></p><p>其中<span class="math inline">\(\boldsymbol{\mu_w} = \sigma^{-2}\boldsymbol{\Sigma_w\Phi}^H\boldsymbol{t}\)</span>，<span class="math inline">\(\boldsymbol{\Sigma_w}=(\sigma^{-2}\boldsymbol{\Phi}^H\boldsymbol{\Phi}+\boldsymbol{\Gamma}^{-1})^{-1}\)</span>。我们可以选择<span class="math inline">\(\boldsymbol{\mu}\)</span>作为<span class="math inline">\(\boldsymbol{w}\)</span>的求解结果（均值嘛，合情合理），但是算<span class="math inline">\(\boldsymbol{\mu}\)</span>就要知道<span class="math inline">\(\sigma^2\)</span>和<span class="math inline">\(\boldsymbol{\Gamma}\)</span>，这俩可以通过对<span class="math inline">\(p(\boldsymbol{t};\boldsymbol{\Gamma},\sigma^2)\)</span>进行最大似然估计求得，但是很不幸，直接求解似然函数是求不出来的（你可以试试，我没算出来，当然参考文献里也没算出来），而且这大概率不是个凸函数，最优解也算不出来，算个次优解意思意思得了。下面介绍下用EM算法求解这个问题。</p>
<h1 id="em算法">EM算法</h1>
<p>EM算法是怎么回事，什么思想，什么原理网上其他帖子已经讲的很好了[3]，这里直接介绍如何求解上述问题。</p>
<h2 id="理论推导">理论推导</h2>
<p>我们的的核心问题还是要对<span class="math inline">\(p(\boldsymbol{t};\boldsymbol{\Gamma},\sigma^2)\)</span>最大似然求解参数，即：</p>
<p></p><div class="math display">\[\begin{align}
 \sigma^2,\boldsymbol{\Gamma} =&amp; \arg\max_{\sigma^2,\boldsymbol{\Gamma}}\mathcal{L}{(\sigma^2,\boldsymbol{\Gamma})}\nonumber\\
\mathcal{L}{(\sigma^2,\boldsymbol{\Gamma})} 
&amp;= \log[p(\boldsymbol{t};\boldsymbol{\Gamma},\sigma^2)]  
\tag{11}
\end{align}
\]</div><p></p><p>然后引入<span class="math inline">\(\boldsymbol{w}\)</span>，将似然函数写作：</p>
<p></p><div class="math display">\[\begin{align}
\log[p(\boldsymbol{t};\boldsymbol{\Gamma},\sigma^2)]&amp;= \log[\int p(\boldsymbol{t},\boldsymbol{w};\boldsymbol{\Gamma},\sigma^2)d\boldsymbol{w}] \nonumber\\
=&amp;\log[\int \frac{Q(\boldsymbol{w})}{Q(\boldsymbol{w})}p(\boldsymbol{t},\boldsymbol{w};\boldsymbol{\Gamma},\sigma^2)d\boldsymbol{w}] 
\tag{12}
\end{align}
\]</div><p></p><p>上式中<span class="math inline">\(Q(\boldsymbol{w})\)</span>是<span class="math inline">\(\boldsymbol{w}\)</span>的一个函数，显然它可以是任意值不为零的函数，此时我们认为它是<span class="math inline">\(\boldsymbol{w}\)</span>的某个分布函数，因此上式的积分可以写作期望形式：</p>
<p></p><div class="math display">\[\begin{align}
\log[p(\boldsymbol{t};\boldsymbol{\Gamma},\sigma^2)]
=&amp;\log[\mathbf{E}_{\boldsymbol{w}\sim Q(\boldsymbol{w})} \frac{p(\boldsymbol{t},\boldsymbol{w};\boldsymbol{\Gamma},\sigma^2)}{Q(\boldsymbol{w})}] 
\tag{13}
\end{align}
\]</div><p></p><p>对上式进一步操作需要用到Jensen不等式，Jensen不等式网上有详细讲解[4]，大概就是对于一个随机变量<span class="math inline">\(X\)</span> 和一个函数<span class="math inline">\(f(X)\)</span> ，当<span class="math inline">\(f\)</span>的二阶导小于0（上凸）时<span class="math inline">\(f(\mathbf{E}(X)) \geq \mathbf{E}f(X)\)</span>，等号成立的条件是<span class="math inline">\(X = \mathbf{E}(X)\)</span>。则式(13)可写为：</p>
<p></p><div class="math display">\[\begin{align}
\log[p(\boldsymbol{t};\boldsymbol{\Gamma},\sigma^2)]
=&amp;\log[\mathbf{E}_{\boldsymbol{w}\sim Q(\boldsymbol{w})} \frac{p(\boldsymbol{t},\boldsymbol{w};\boldsymbol{\Gamma},\sigma^2)}{Q(\boldsymbol{w})}]\nonumber\\
\geq &amp; \mathbf{E}_{\boldsymbol{w}\sim Q(\boldsymbol{w})} \log[p(\boldsymbol{t},\boldsymbol{w};\boldsymbol{\Gamma},\sigma^2)]-\mathbf{E}_{\boldsymbol{w}\sim Q(\boldsymbol{w})}\log[Q(\boldsymbol{w})]\nonumber\\
=&amp; \mathcal{J}(\sigma^2,\boldsymbol{\Gamma})
\tag{14}
\end{align}
\]</div><p></p><p>这里我们得到了<span class="math inline">\(\mathcal{L}{(\sigma^2,\boldsymbol{\Gamma})}\)</span>的一个下界<span class="math inline">\(\mathcal{J}(\sigma^2,\boldsymbol{\Gamma})\)</span>，理论上可以通过最大化<span class="math inline">\(\mathcal{J}(\sigma^2,\boldsymbol{\Gamma})\)</span>来最大化<span class="math inline">\(\mathcal{L}{(\sigma^2,\boldsymbol{\Gamma})}\)</span>，但是一眼望去<span class="math inline">\(\mathcal{J}(\sigma^2,\boldsymbol{\Gamma})\)</span>更布嚎算，至少<span class="math inline">\(Q(\boldsymbol{w})\)</span>是啥咱还不知道呢！所以得确定下<span class="math inline">\(Q(\boldsymbol{w})\)</span>。</p>
<p>首先可以确定的是，在固定<span class="math inline">\(\sigma^2\)</span>和<span class="math inline">\(\boldsymbol{\Gamma}\)</span>时，根据Jensen不等式取得等号的条件，<span class="math inline">\(\mathcal{J}(\sigma^2,\boldsymbol{\Gamma})\)</span>能取到的最大值就是<span class="math inline">\(\mathcal{L}{(\sigma^2,\boldsymbol{\Gamma})}\)</span>，取得最大值的条件是期望内的数是个常数（与所求期望的随机变量无关）即<span class="math inline">\(\frac{p(\boldsymbol{t},\boldsymbol{w};\boldsymbol{\Gamma},\sigma^2)}{Q(\boldsymbol{w})}\)</span>与<span class="math inline">\(\boldsymbol{w}\)</span>无关。不难看出，满足此条件的<span class="math inline">\(Q(\boldsymbol{w})\)</span>为：</p>
<p></p><div class="math display">\[\begin{align}
Q(\boldsymbol{w}) = p(\boldsymbol{w}|\boldsymbol{t};\boldsymbol{\Gamma},\sigma^2)
\tag{15}
\end{align}
\]</div><p></p><p>但是这样直接把<span class="math inline">\(Q(\boldsymbol{w})\)</span>代回<span class="math inline">\(\mathcal{J}(\sigma^2,\boldsymbol{\Gamma})\)</span>让他等于<span class="math inline">\(\mathcal{L}{(\sigma^2,\boldsymbol{\Gamma})}\)</span>那不是就又绕回去了，所以不能这么干。聪明的人们想到了两步走的方法：<br>
第一步，用当前现有的<span class="math inline">\(\sigma^2_{(k)}\)</span>和<span class="math inline">\(\boldsymbol{\Gamma}_{(k)}\)</span>，得到<span class="math inline">\(Q_{(k)}(\boldsymbol{w}) = p(\boldsymbol{w}|\boldsymbol{t};\boldsymbol{\Gamma}_{(k)},\sigma^2_{(k)})\)</span>，并用<span class="math inline">\(Q_{(k)}(\boldsymbol{w})\)</span>计算出<span class="math inline">\(\mathcal{J}_{(k)}(\sigma^2,\boldsymbol{\Gamma})=\mathbf{E}_{\boldsymbol{w}\sim Q_{(k)}(\boldsymbol{w})} \log[p(\boldsymbol{t},\boldsymbol{w};\boldsymbol{\Gamma},\sigma^2)]-\mathbf{E}_{\boldsymbol{w}\sim Q_{(k)}(\boldsymbol{w})}\log[Q_{(k)}(\boldsymbol{w})]\)</span> ，这就是EM算法中的E步，这一步是从Jensen不等式的层面对<span class="math inline">\(\mathcal{J}(\sigma^2,\boldsymbol{\Gamma})\)</span>进行最大化，使<span class="math inline">\(\mathcal{J}_{(k)}(\sigma^2,\boldsymbol{\Gamma})\)</span>逼近<span class="math inline">\(\mathcal{L}{(\sigma^2,\boldsymbol{\Gamma})}\)</span>。<br>
第二步，固定<span class="math inline">\(Q_{(k)}(\boldsymbol{w})\)</span>不动，通过最大化<span class="math inline">\(\mathcal{J}_{(k)}(\sigma^2,\boldsymbol{\Gamma})\)</span>求<span class="math inline">\(\sigma^2_{(k+1)}\)</span>和<span class="math inline">\(\boldsymbol{\Gamma}_{(k+1)}\)</span>。由于固定<span class="math inline">\(Q_{(k)}(\boldsymbol{w})\)</span>以后<span class="math inline">\(\mathcal{J}_{(k)}(\sigma^2,\boldsymbol{\Gamma})\)</span>中的<span class="math inline">\(\mathbf{E}_{\boldsymbol{w}\sim Q_{(k)}(\boldsymbol{w})}\log[Q_{(k)}(\boldsymbol{w})]\)</span>始终为常数，所以在第一步其实就用不到，直接丢掉。于是乎有：</p>
<p></p><div class="math display">\[\begin{align}
\sigma^2_{(k+1)},\boldsymbol{\Gamma}_{(k+1)} =&amp; \arg\max_{\sigma^2,\boldsymbol{\Gamma}}\mathbf{E}_{\boldsymbol{w}\sim Q_{(k)}(\boldsymbol{w})} \log[p(\boldsymbol{t},\boldsymbol{w};\boldsymbol{\Gamma},\sigma^2)]
\tag{16}
\end{align}
\]</div><p></p><p>这就是EM算法中的M步，这一步是对逼近<span class="math inline">\(\mathcal{L}{(\sigma^2,\boldsymbol{\Gamma})}\)</span>的<span class="math inline">\(\mathcal{J}_{(k)}(\sigma^2,\boldsymbol{\Gamma})\)</span>最大化，让得到的<span class="math inline">\(\sigma^2_{(k+1)},\boldsymbol{\Gamma}_{(k+1)}\)</span>进一步趋近最优解。把M步中算出的<span class="math inline">\(\sigma^2_{(k+1)},\boldsymbol{\Gamma}_{(k+1)}\)</span>再拿回E步，就完成了一次迭代。</p>
<p>目前为止还是理论层面，我们得回到我们的问题，看看<span class="math inline">\(\mathbf{E}_{\boldsymbol{w}\sim Q_{(k)}(\boldsymbol{w})} \log[p(\boldsymbol{t},\boldsymbol{w};\boldsymbol{\Gamma},\sigma^2)]\)</span>以及<span class="math inline">\(\sigma^2_{(k+1)},\boldsymbol{\Gamma}_{(k+1)}\)</span>到底怎么算。</p>
<h2 id="em算法求解sbl">EM算法求解SBL</h2>
<p>先看<span class="math inline">\(\mathbf{E}_{\boldsymbol{w}\sim Q_{(k)}(\boldsymbol{w})} \log[p(\boldsymbol{t},\boldsymbol{w};\boldsymbol{\Gamma},\sigma^2)]\)</span>，其中:</p>
<p></p><div class="math display">\[p(\boldsymbol{t},\boldsymbol{w};\boldsymbol{\Gamma},\sigma^2)=p(\boldsymbol{t};\boldsymbol{\Gamma},\sigma^2)p(\boldsymbol{w}|\boldsymbol{t};\boldsymbol{\Gamma},\sigma^2)
=p(\boldsymbol{t}|\boldsymbol{w};\sigma^2)p(\boldsymbol{w};\boldsymbol{\Gamma})
\tag{17}
\]</div><p></p><p><span class="math inline">\(p(\boldsymbol{t};\boldsymbol{\Gamma},\sigma^2)\)</span>在式(7)，<span class="math inline">\(p(\boldsymbol{w}|\boldsymbol{t};\boldsymbol{\Gamma},\sigma^2)\)</span>在式(10)，<span class="math inline">\(p(\boldsymbol{t}|\boldsymbol{w};\sigma^2)\)</span>在式(3)，<span class="math inline">\(p(\boldsymbol{w};\boldsymbol{\Gamma})\)</span>在式(4)，随便选一对就能算出<span class="math inline">\(p(\boldsymbol{t},\boldsymbol{w};\boldsymbol{\Gamma},\sigma^2)\)</span>，我这里就不算了。<br>
对计算的结果取对数后得到：</p>
<p></p><div class="math display">\[\begin{align}
&amp;\log[p(\boldsymbol{t},\boldsymbol{w};\boldsymbol{\Gamma},\sigma^2)] \nonumber\\
=&amp;-\frac{N}{2}log(\sigma^2)-\frac{1}{2}log|\Gamma|-\frac{1}{2\sigma^2}(\boldsymbol{t}^H\boldsymbol{t}+2\boldsymbol{t}^H\boldsymbol{\Phi w}+\boldsymbol{w}^H\boldsymbol{\Phi}^H\boldsymbol{\Phi}\boldsymbol{w})-\frac{1}{2}\boldsymbol{w}^H\boldsymbol{\Gamma}^{-1}\boldsymbol{w}+C
\tag{18}
\end{align}
\]</div><p></p><p>然后求期望，为了避免式子冗长，后面把没用的常数<span class="math inline">\(C\)</span>扔了，并将<span class="math inline">\(\mathbf{E}_{\boldsymbol{w}\sim Q_{(k)}(\boldsymbol{w})}\)</span>简写为<span class="math inline">\(\mathbf{E}_{(k)}\)</span>：</p>
<p></p><div class="math display">\[\begin{align}
&amp;\mathbf{E}_{(k)}\log[p(\boldsymbol{t},\boldsymbol{w};\boldsymbol{\Gamma},\sigma^2)] \nonumber\\
=&amp;-\frac{N}{2}log(\sigma^2)-\frac{1}{2}log|\boldsymbol{\Gamma}|\nonumber\\
&amp;-\frac{1}{2\sigma^2}(\boldsymbol{t}^H\boldsymbol{t}+2\mathbf{E}_{(k)}[\boldsymbol{t}^H\boldsymbol{\Phi} {w}]+\mathbf{E}_{(k)}[\boldsymbol{w}^H\boldsymbol{\Phi}^H\boldsymbol{\Phi}\boldsymbol{w}])-\frac{1}{2}\mathbf{E}_{(k)}[\boldsymbol{w}^H\boldsymbol{\Gamma}^{-1}\boldsymbol{w}]\nonumber\\
=&amp;\mathcal{J}_{(k)}(\sigma^2,\boldsymbol{\Gamma})
\tag{19}
\end{align}
\]</div><p></p><p>由于此刻<span class="math inline">\(\boldsymbol{w}\sim Q_{(k)}(\boldsymbol{w}),Q_{(k)}(\boldsymbol{w}) = p(\boldsymbol{w}|\boldsymbol{t};\boldsymbol{\Gamma}_{(k)},\sigma^2_{(k)})\)</span>，并结合式(10)，可得出<span class="math inline">\(\mathbf{E}_{(k)}[\boldsymbol{t}^H\boldsymbol{\Phi} {w}]=\boldsymbol{t}^H\boldsymbol{\Phi}{\mu}_{(k)}\)</span>，<span class="math inline">\(\mathbf{E}_{(k)}[\boldsymbol{w}^H\boldsymbol{\Phi}^H\boldsymbol{\Phi}\boldsymbol{w}]=tr[\boldsymbol{\Phi}^H\boldsymbol{\Phi\Sigma_w}^{(k)}]+\boldsymbol{\mu}_{(k)}^H\boldsymbol{\Phi}^H\boldsymbol{\Phi}\boldsymbol{\mu}_{(k)}\)</span>，<span class="math inline">\(\mathbf{E}_{(k)}[\boldsymbol{w}^H\boldsymbol{\Gamma}^{-1}\boldsymbol{w}] = tr[\boldsymbol{\Gamma}^{-1}_{(k)}\boldsymbol{\Sigma_w^{(k)}}]+\boldsymbol{\mu}_{(k)}^H\boldsymbol{\Gamma}^{-1}\boldsymbol{\mu}_{(k)}\)</span>，其中<span class="math inline">\(\boldsymbol{\Sigma_w}^{(k)}=(\sigma_{(k)}^{-2}\boldsymbol{\Phi}^H\boldsymbol{\Phi}+\boldsymbol{\Gamma}_{(k)}^{-1})^{-1}\)</span>，<span class="math inline">\(\boldsymbol{\mu}_{(k)}= \sigma_{(k)}^{-2}\boldsymbol{\Sigma_w^{(k)}}\boldsymbol{\Phi}^H\boldsymbol{t}\)</span>，<span class="math inline">\(tr(\cdot)\)</span>为求矩阵的迹，即主对角线元素的和。对于二次型求期望可以参考matrix cookbook[5]。然后有：</p>
<p></p><div class="math display">\[\begin{align}
&amp;\mathcal{J}_{(k)}(\sigma^2,\boldsymbol{\Gamma})\nonumber\\
=&amp;-\frac{N}{2}log(\sigma^2)-\frac{1}{2}log|\boldsymbol{\Gamma}|\nonumber\\
&amp;-\frac{1}{2\sigma^2}
(\boldsymbol{t}^H\boldsymbol{t}
+\boldsymbol{t}^H\boldsymbol{\Phi}{\mu}_{(k)}
+tr[\boldsymbol{\Phi}^H\boldsymbol{\Phi\Sigma_w}^{(k)}]+\boldsymbol{\mu}_{(k)}^H\boldsymbol{\Phi}^H\boldsymbol{\Phi}\boldsymbol{\mu}_{(k)})\nonumber\\
&amp;-\frac{1}{2}(tr[\boldsymbol{\Gamma}^{-1}_{(k)}\boldsymbol{\Sigma_w^{(k)}}]+\boldsymbol{\mu}_{(k)}^H\boldsymbol{\Gamma}^{-1}\boldsymbol{\mu}_{(k)})
\tag{20}
\end{align}
\]</div><p></p><p>上述部分算是EM算法中的E步，下面看M步，即最大化<span class="math inline">\(\mathcal{J}_{(k)}(\sigma^2,\boldsymbol{\Gamma})\)</span>来求<span class="math inline">\(\sigma^2_{(k+1)}\)</span>和<span class="math inline">\(\boldsymbol{\Gamma}_{(k+1)}\)</span>。最大化的方式就是求导，导函数为0的点即为最大值。当然，严谨的来讲导函数为0的点是否为最大值还需要经过一些验证，这里就不验证了。求导过程比较简单，也不详细讲了，其中涉及到的矩阵求导可以参考matrix cookbook，这里直接给出结果：</p>
<p></p><div class="math display">\[\begin{align}
\frac{\partial\mathcal{J}_{(k)}(\sigma^2,\boldsymbol{\Gamma})}{\partial\gamma_i}=
-\frac{1}{2\gamma_i}+\frac{1}{2\gamma_i^2}\boldsymbol{\Sigma_w}^{(k)}(i,i)
+\frac{1}{2\gamma_i^2}[\boldsymbol{\mu}_{(k)}(i)]^2
\tag{21}
\end{align}
\]</div><p></p><p>其中<span class="math inline">\(\gamma_i\)</span>表示对角阵<span class="math inline">\(\boldsymbol{\Gamma}\)</span>的第<span class="math inline">\(i\)</span>个元素，<span class="math inline">\(\boldsymbol{\Sigma_{w}}^{(k)}(i,i)\)</span>为<span class="math inline">\(\boldsymbol{\Sigma_{w}}^{(k)}\)</span>的主对角线上第<span class="math inline">\(i\)</span>个元素，<span class="math inline">\(\boldsymbol{\mu}_{(k)}(i)\)</span>为<span class="math inline">\(\boldsymbol{\mu}_{(k)}\)</span>的第<span class="math inline">\(i\)</span>个元素。令式(21)为0可以得到：</p>
<p></p><div class="math display">\[\gamma_i^{(k+1)}=\boldsymbol{\Sigma_{w}}^{(k)}(i,i)+[\boldsymbol{\mu}_{(k)}(i)]^2
\tag{22}
\]</div><p></p><p>其中<span class="math inline">\(\gamma_i^{(k+1)}\)</span>为<span class="math inline">\(\boldsymbol{\Gamma}_{(k+1)}\)</span>的第<span class="math inline">\(i\)</span>元素。<span class="math inline">\(\sigma^2_{(k+1)}\)</span>同理：</p>
<p></p><div class="math display">\[\begin{align}
&amp;\frac{\partial\mathcal{J}_{(k)}(\sigma^2,\boldsymbol{\Gamma})}{\partial\sigma^2}=
-\frac{N}{2\sigma^2}+\frac{1}{2(\sigma^2)^2}
(||t-\Phi \boldsymbol{\mu}_{(k)}||^2_2
+tr[\boldsymbol{\Phi\Phi}^h\boldsymbol{\Sigma_w}^{(k)}])=0\nonumber\\
&amp;\sigma^2_{(k+1)}=\frac{||t-\Phi \boldsymbol{\mu}_{(k)}||^2_2
+tr[\boldsymbol{\Phi\Phi}^h\boldsymbol{\Sigma_w}^{(k)}]}{N}
\tag{23}
\end{align}
\]</div><p></p><p>其中的<span class="math inline">\(tr[\boldsymbol{\Phi\Phi}^h\boldsymbol{\Sigma_w}^{(k)}]\)</span>还可以写作<span class="math inline">\(\sigma^2_{(k)}tr[\boldsymbol{I}-\boldsymbol{\Gamma}_{(k)}^{-1}\boldsymbol{\Sigma_w}^{(k)}]\)</span>，读者自证不难）。<br>
至此我们已经完成了全部推导。</p>
<p>回顾式(17)到式(21)不难发现，实际进行EM算法求解的过程中并不需要真正的去求期望，和最大化损失函数，这些步骤已经蕴含在推导中了，而实际要做的只是根据(10)利用<span class="math inline">\(\sigma^2_{(k)}\)</span>和<span class="math inline">\(\boldsymbol{\Gamma}_{(k)}\)</span>算出<span class="math inline">\(\boldsymbol{\mu}_{(k)}\)</span>和<span class="math inline">\(\boldsymbol{\Sigma_{w}}^{(k)}\)</span>，再根据(22)和(23)算出<span class="math inline">\(\sigma^2_{(k+1)}\)</span>和<span class="math inline">\(\boldsymbol{\Gamma}_{(k+1)}\)</span>就行了。鉴于这仍然是两步走，所以第一步是实际中的E步，第二步为M步。</p>
<h1 id="代码及结果">代码及结果</h1>
<p>matlab 代码如下</p>
<pre><code>function [w,ii] = sbl(t,Phi,gpu,sigma,Gamma)
	% t is the received signal
	% Phi is the dictionary
	% gpu means if you want to accelerate your code by gpu
	% sigma is the initial value of sigma, usually set 1
	% Gamma is the initial value of Gamma, usually set eye(M)
	% initial
	
	[N,M] = size(Phi);
	if(strcmp(gpu,'gpu'))
		t = gpuArray(t);
		Phi = gpuArray(Phi);
		Gamma = gpuArray(Gamma);
		matEyeN = gpuArray(eye(N));
		vecOne = gpuArray(ones(M,1));
	else
		matEyeN = eye(N);
		vecOne = ones(M,1);
	end
	
	%save the previous variable
	sigmaS = sigma;
	sigmaP = sigmaS;
	GammaP = Gamma;   
	
	ii = 0;
	iterErrNow = 100;
	
	while((iterErrNow&gt;1e-2)&amp;&amp;(ii&lt;200))
		ii = ii+1;
		% e-step
		Sigmat = sigmaS.*matEyeN + Phi*Gamma*Phi';
		Sigmaw = Gamma-Gamma*Phi'*Sigmat^(-1)*Phi*Gamma;
		mu = sigmaS^(-1).*Sigmaw*(Phi')*t;
		
		% m-step
		Gamma = diag(abs(diag(Sigmaw)+abs(mu).^2));
		sigmaS = abs((t-Phi*mu)'*(t-Phi*mu)+sigmaS*sum(vecOne-...
			diag(Gamma).^(-1).*diag(Sigmaw)))/N;
			
		% stop when variable's changes become small enough
		iterErrNow = norm(sigmaP-sigmaS)+norm(diag(GammaP)-diag(Gamma)); 
		
		sigmaP = sigmaS;
		GammaP = Gamma;
		
		w = mu;
	
	end
end
</code></pre>
<p>实验代码</p>
<pre><code class="language-matlab">% close all

%% parameter
N = 64;
M = 16*N;
fs = 1;
f = 0:1/M:(M-1)/M;
t = 0:N-1;
window = rectwin(N);
Phi = exp(2*pi*1i*t'*f);

%% signal module
% fx = 0.3*rand(1,3);
fx = [0.1,0.2,0.21];
a = 0.3+0.7*rand(3,1);
x = exp(2*pi*1i*t'*fx)*a;
x = x./sqrt(var(x));
SNR = 0;
noise = 2^(-0.5)*(randn(N,1)+1i*randn(N,1));
x = x + 10^(-SNR/20)*noise;
% x = x.*window;
xf = fft(x,M)/N;

fig = figure;
hold on
plot(0:1/M:(M-1)/M,20*log10(abs(xf)))

%% sbl
tic
[wEst,iterNum,sigma] = sbl(x,Phi,'gpu',2,eye(M));
toc
plot(0:1/M:(M-1)/M,20*log10(abs(wEst)))
scatter(fx,20*log10(a))
xlim([0,0.5])
ylim([-60,10])
legend('fft','sbl','truth')
</code></pre>
<p>结果如下：<br>
<img src="https://img2024.cnblogs.com/blog/2737429/202505/2737429-20250518173216868-1210117367.png" alt="" loading="lazy"></p>
<h1 id="参考文献">参考文献</h1>
<p>[1]TIPPING M E. Sparse Bayesian Learning and the Relevance Vector Machine[J]. Journal of Machine Learning Research, 2001, 1(Jun): 211-244<br>
[2]WIPF D P, RAO B D. Sparse Bayesian learning for basis selection[J/OL]. IEEE Transactions on Signal Processing, 2004, 52(8): 2153-2164. DOI:10.1109/TSP.2004.831016<br>
[3]<a href="https://zhuanlan.zhihu.com/p/78311644" target="_blank" rel="noopener nofollow">https://zhuanlan.zhihu.com/p/78311644</a><br>
[4]<a href="https://zhuanlan.zhihu.com/p/39315786" target="_blank" rel="noopener nofollow">https://zhuanlan.zhihu.com/p/39315786</a><br>
[5]<a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf" target="_blank" rel="noopener nofollow">https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf</a></p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.3869175831365741" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-05-18 18:59">2025-05-18 18:32</span>&nbsp;
<a href="https://www.cnblogs.com/IrregularRhythm">节奏不稳</a>&nbsp;
阅读(<span id="post_view_count">30</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18882385);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18882385', targetLink: 'https://www.cnblogs.com/IrregularRhythm/p/18882385', title: '稀疏贝叶斯谱估计及EM算法求解' })">举报</a>
</div>
        