
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/longfurcat/p/18637874" title="发布于 2024-12-29 16:09">
    <span role="heading" aria-level="2">【杂谈】Kafka的日志段为什么不用内存映射？</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body blogpost-body-html">
<h3>什么是内存映射（Memory-Mapped File）？</h3>
<p>内存映射（mmap）是一种将文件内容映射到内存中的技术，应用程序可以像操作内存一样对文件内容进行读写，而不需要显式地进行磁盘 I/O 操作。修改的内容会自动由操作系统同步到磁盘。</p>
<h3>内存映射需要读取磁盘文件吗？</h3>
<p>需要。毕竟，内存中的数据来源于磁盘文件。操作系统会将文件的部分或全部内容加载到内存，供程序访问。</p>
<h3>为什么不直接读取文件？</h3>
<p>直接读取文件，缓存到用户进程内，这样不也可以随意访问吗？相比这种方式，mmap有何优势？</p>
<p>1. <strong>数据拷贝次数少</strong></p>
<p>内存映射相比直接读取文件的一个主要优势是减少了数据拷贝的次数</p>
<ul>
<li><strong>内存映射</strong>：磁盘 =&gt; 内核空间</li>
<li><strong>直接读取</strong>：磁盘 =&gt; 内核空间 =&gt; 用户空间</li>
</ul>
<p>正常情况下，应用程序不能直接访问内核空间中的数据。要访问这些数据，通常需要触发系统调用将数据从内核空间拷贝到用户空间。</p>
<p>而内存映射通过将文件内容直接映射到进程的虚拟地址空间，消除了这种额外的拷贝开销，从而提高了效率。</p>
<p>2. <strong>加载范围与按需加载</strong></p>
<p>直接读取文件时，通常需要将整个文件加载到进程的内存缓存中，这对于大文件来说非常低效。而内存映射则更加高效，操作系统会根据需要<strong>按需加载</strong>文件的部分内容。</p>
<p>对于用户来说，内存映射的效果是可以像操作内存一样访问文件内容，而无需担心数据加载的问题。</p>
<p>3. <strong>自动写回磁盘</strong></p>
<ul>
<li><strong>内存映射</strong>：修改的内容会自动同步到磁盘，操作系统会处理文件内容的写回。</li>
<li><strong>直接读取</strong>：如果是直接读取，文件内容的修改要么全部写回磁盘，要么应用程序需要识别哪些区域发生了变化并单独写回磁盘，这样的管理工作相对繁琐。</li>
</ul>
<h3>Kafka在哪里使用了内存映射？</h3>
<p>从源码中可以看到，Kafka 只在<strong>索引文件</strong>中使用了内存映射（mmap）。内存映射的优势在于它允许<strong>随机访问</strong>，这与索引文件的应用场景非常匹配。</p>
<p>Kafka的索引文件通过二分法查找消息的存储位置，而内存映射的随机访问特性使得这个过程更加高效。</p>
<p><img src="https://img2024.cnblogs.com/blog/1313132/202412/1313132-20241229145201492-1318108098.png" alt="" width="340" height="330" loading="lazy"></p>
<p>但是看源码可以发现，日志段则没有使用文件映射，而是直接使用FileChannel.write(buffer)写出数据。</p>
<p>//kafka 3.9.0部分源码</p>
<p>LogSegment.java</p>
<div class="cnblogs_code">
<pre><em><span style="color: rgba(0, 0, 0, 1)">package<span>&nbsp;org.apache.kafka.storage.internals.log</span></span></em></pre>
<pre><em id="__mceDel"><span style="color: rgba(0, 0, 0, 1)">...

</span><span style="color: rgba(0, 0, 255, 1)">public</span> <span style="color: rgba(0, 0, 255, 1)">class</span> LogSegment <span style="color: rgba(0, 0, 255, 1)">implements</span><span style="color: rgba(0, 0, 0, 1)"> Closeable {
    ...
    
    </span><span style="color: rgba(0, 0, 255, 1)">private</span> <span style="color: rgba(0, 0, 255, 1)">final</span><span style="color: rgba(0, 0, 0, 1)"> FileRecords log;
    ...

    </span><span style="color: rgba(0, 128, 0, 1)">/**</span><span style="color: rgba(0, 128, 0, 1)">
     * Append the given messages starting with the given offset. Add
     * an entry to the index if needed.
     *
     * It is assumed this method is being called from within a lock, it is not thread-safe otherwise.
     *
     * </span><span style="color: rgba(128, 128, 128, 1)">@param</span><span style="color: rgba(0, 128, 0, 1)"> largestOffset The last offset in the message set
     * </span><span style="color: rgba(128, 128, 128, 1)">@param</span><span style="color: rgba(0, 128, 0, 1)"> largestTimestampMs The largest timestamp in the message set.
     * </span><span style="color: rgba(128, 128, 128, 1)">@param</span><span style="color: rgba(0, 128, 0, 1)"> shallowOffsetOfMaxTimestamp The last offset of earliest batch with max timestamp in the messages to append.
     * </span><span style="color: rgba(128, 128, 128, 1)">@param</span><span style="color: rgba(0, 128, 0, 1)"> records The log entries to append.
     * </span><span style="color: rgba(128, 128, 128, 1)">@throws</span><span style="color: rgba(0, 128, 0, 1)"> LogSegmentOffsetOverflowException if the largest offset causes index offset overflow
     </span><span style="color: rgba(0, 128, 0, 1)">*/</span>
    <span style="color: rgba(0, 0, 255, 1)">public</span> <span style="color: rgba(0, 0, 255, 1)">void</span> append(<span style="color: rgba(0, 0, 255, 1)">long</span><span style="color: rgba(0, 0, 0, 1)"> largestOffset,
                       </span><span style="color: rgba(0, 0, 255, 1)">long</span><span style="color: rgba(0, 0, 0, 1)"> largestTimestampMs,
                       </span><span style="color: rgba(0, 0, 255, 1)">long</span><span style="color: rgba(0, 0, 0, 1)"> shallowOffsetOfMaxTimestamp,
                       MemoryRecords records) </span><span style="color: rgba(0, 0, 255, 1)">throws</span><span style="color: rgba(0, 0, 0, 1)"> IOException {
        </span><span style="color: rgba(0, 0, 255, 1)">if</span> (records.sizeInBytes() &gt; 0<span style="color: rgba(0, 0, 0, 1)">) {
            LOGGER.trace(</span>"Inserting {} bytes at end offset {} at position {} with largest timestamp {} at offset {}"<span style="color: rgba(0, 0, 0, 1)">,
                records.sizeInBytes(), largestOffset, log.sizeInBytes(), largestTimestampMs, shallowOffsetOfMaxTimestamp);
            </span><span style="color: rgba(0, 0, 255, 1)">int</span> physicalPosition =<span style="color: rgba(0, 0, 0, 1)"> log.sizeInBytes();
            </span><span style="color: rgba(0, 0, 255, 1)">if</span> (physicalPosition == 0<span style="color: rgba(0, 0, 0, 1)">)
                rollingBasedTimestamp </span>=<span style="color: rgba(0, 0, 0, 1)"> OptionalLong.of(largestTimestampMs);

            ensureOffsetInRange(largestOffset);

            </span><span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)"> append the messages</span>
            <span style="color: rgba(0, 0, 255, 1)">long</span> appendedBytes =<span style="color: rgba(0, 0, 0, 1)"> log.append(records);
            LOGGER.trace(</span>"Appended {} to {} at end offset {}"<span style="color: rgba(0, 0, 0, 1)">, appendedBytes, log.file(), largestOffset);
            </span><span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)"> Update the in memory max timestamp and corresponding offset.</span>
            <span style="color: rgba(0, 0, 255, 1)">if</span> (largestTimestampMs &gt;<span style="color: rgba(0, 0, 0, 1)"> maxTimestampSoFar()) {
                maxTimestampAndOffsetSoFar </span>= <span style="color: rgba(0, 0, 255, 1)">new</span><span style="color: rgba(0, 0, 0, 1)"> TimestampOffset(largestTimestampMs, shallowOffsetOfMaxTimestamp);
            }
            </span><span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)"> append an entry to the index (if needed)<br></span>            // 稀疏索引，有一定的间隔。可以减少索引量
            <span style="color: rgba(0, 0, 255, 1)">if</span> (bytesSinceLastIndexEntry &gt;<span style="color: rgba(0, 0, 0, 1)"> indexIntervalBytes) {
                offsetIndex().append(largestOffset, physicalPosition);
                timeIndex().maybeAppend(maxTimestampSoFar(), shallowOffsetOfMaxTimestampSoFar());
                bytesSinceLastIndexEntry </span>= 0<span style="color: rgba(0, 0, 0, 1)">;
            }
            bytesSinceLastIndexEntry </span>+=<span style="color: rgba(0, 0, 0, 1)"> records.sizeInBytes();
        }
    }

   ...
}</span></em></pre>
</div>
<p>FileRecords.java</p>
<div class="cnblogs_code">
<pre><span style="color: rgba(0, 0, 255, 1)">package</span><span style="color: rgba(0, 0, 0, 1)"> org.apache.kafka.common.record;

...

</span><span style="color: rgba(0, 0, 255, 1)">public</span> <span style="color: rgba(0, 0, 255, 1)">class</span> FileRecords <span style="color: rgba(0, 0, 255, 1)">extends</span> AbstractRecords <span style="color: rgba(0, 0, 255, 1)">implements</span><span style="color: rgba(0, 0, 0, 1)"> Closeable {
     ...  
     </span><span style="color: rgba(0, 0, 255, 1)">private</span> <span style="color: rgba(0, 0, 255, 1)">final</span><span style="color: rgba(0, 0, 0, 1)"> FileChannel channel;
     ....

    </span><span style="color: rgba(0, 0, 255, 1)">public</span> <span style="color: rgba(0, 0, 255, 1)">int</span> append(MemoryRecords records) <span style="color: rgba(0, 0, 255, 1)">throws</span><span style="color: rgba(0, 0, 0, 1)"> IOException {
        </span><span style="color: rgba(0, 0, 255, 1)">if</span> (records.sizeInBytes() &gt; Integer.MAX_VALUE -<span style="color: rgba(0, 0, 0, 1)"> size.get())
            </span><span style="color: rgba(0, 0, 255, 1)">throw</span> <span style="color: rgba(0, 0, 255, 1)">new</span> IllegalArgumentException("Append of size " + records.sizeInBytes() +
                    " bytes is too large for segment with current file position at " +<span style="color: rgba(0, 0, 0, 1)"> size.get());

        </span><span style="color: rgba(0, 0, 255, 1)">int</span> written =<span style="color: rgba(0, 0, 0, 1)"> records.writeFullyTo(channel);
        size.getAndAdd(written);
        </span><span style="color: rgba(0, 0, 255, 1)">return</span><span style="color: rgba(0, 0, 0, 1)"> written;
    }
  
    ...


}</span></pre>
</div>
<p>MemoryRecords.java</p>
<div class="cnblogs_code">
<pre><span style="color: rgba(0, 0, 255, 1)">package</span><span style="color: rgba(0, 0, 0, 1)"> org.apache.kafka.common.record;
....

</span><span style="color: rgba(0, 0, 255, 1)">public</span> <span style="color: rgba(0, 0, 255, 1)">class</span> MemoryRecords <span style="color: rgba(0, 0, 255, 1)">extends</span><span style="color: rgba(0, 0, 0, 1)"> AbstractRecords {
      ...
      </span><span style="color: rgba(0, 0, 255, 1)">private</span> <span style="color: rgba(0, 0, 255, 1)">final</span><span style="color: rgba(0, 0, 0, 1)"> ByteBuffer buffer;
      ...

          </span><span style="color: rgba(0, 128, 0, 1)">/**</span><span style="color: rgba(0, 128, 0, 1)">
     * Write all records to the given channel (including partial records).
     * </span><span style="color: rgba(128, 128, 128, 1)">@param</span><span style="color: rgba(0, 128, 0, 1)"> channel The channel to write to
     * </span><span style="color: rgba(128, 128, 128, 1)">@return</span><span style="color: rgba(0, 128, 0, 1)"> The number of bytes written
     * </span><span style="color: rgba(128, 128, 128, 1)">@throws</span><span style="color: rgba(0, 128, 0, 1)"> IOException For any IO errors writing to the channel
     </span><span style="color: rgba(0, 128, 0, 1)">*/</span>
    <span style="color: rgba(0, 0, 255, 1)">public</span> <span style="color: rgba(0, 0, 255, 1)">int</span> writeFullyTo(GatheringByteChannel channel) <span style="color: rgba(0, 0, 255, 1)">throws</span><span style="color: rgba(0, 0, 0, 1)"> IOException {
        buffer.mark();
        </span><span style="color: rgba(0, 0, 255, 1)">int</span> written = 0<span style="color: rgba(0, 0, 0, 1)">;
        </span><span style="color: rgba(0, 0, 255, 1)">while</span> (written &lt;<span style="color: rgba(0, 0, 0, 1)"> sizeInBytes())
            written </span>+=<span style="color: rgba(0, 0, 0, 1)"> channel.write(buffer);
        buffer.reset();
        </span><span style="color: rgba(0, 0, 255, 1)">return</span><span style="color: rgba(0, 0, 0, 1)"> written;
    }

    ....
}</span></pre>
</div>
<p>&nbsp;</p>
<h3>为什么日志段不使用内存映射？</h3>
<p>按理说，直接读写内存不是更快吗？日志段为什么不使用内存映射。</p>
<p><strong>1. 内存消耗过大</strong></p>
<p>Kafka 每个主题和分区都有多个日志段文件。如果将所有日志段文件都映射到内存中，将消耗大量的内存资源。尤其是在日志数据量非常大的情况下，这种做法会极大增加内存的负担，可能会在内存受限的环境中不可行。</p>
<p><strong>2. 顺序读写已足够高效</strong></p>
<p><strong>连续区域：</strong>Kafka 的写入和读取操作通常涉及批量消息，这些消息在磁盘上是按顺序存储的。由于数据在物理存储上是连续的，操作系统可以通过一次磁盘寻道就定位到所需的区域，从而减少寻道时间和开销。</p>
<p><strong>页缓存（Page Cache）：</strong>操系统的页缓存机制（Page Cache）能够将频繁访问的文件内容缓存到内存中。操作系统也会预读取一部分文件后续内容到缓存中，提高缓存命中的概率，避免频繁从磁盘加载数据。</p>
<p><strong>零拷贝（sendfile）</strong>：Kafka 的日志文件主要由远端消费者触发读取。由于日志在写入文件的时候都已经处理好了，而且读取也是顺序进行的，故Kafka Broker无需进行额外处理，数据可以直接从磁盘通过 <code>sendfile()</code> 系统调用发送到客户端，从内核直接拷贝到 socket 缓冲区，而不需要先载入到用户空间内存中。</p>
<p><img src="https://img2024.cnblogs.com/blog/1313132/202412/1313132-20241229151945200-154219036.png" alt="" width="527" height="273" loading="lazy"></p>
<p>&nbsp;</p>
<h3>总结</h3>
<p>内存映射技术通过将文件内容映射到内存，有效避免了多次拷贝和高昂的 I/O 成本，非常适合需要随机访问的场景。然而，对于 Kafka 的日志段文件，顺序写入和读取已经足够高效，因此 Kafka 选择不使用内存映射，而是依赖操作系统的页缓存来提高性能。通过这种设计，Kafka 在内存消耗和 I/O 性能之间实现了良好的平衡。</p>
<h3>参考内容</h3>
<p>https://stackoverflow.com/questions/2100584/difference-between-sequential-write-and-random-write</p>
<p>https://storedbits.com/sequential-vs-random-data/</p>
<p>https://www.mail-archive.com/users@kafka.apache.org/msg30260.html</p>
<p>https://lists.freebsd.org/pipermail/freebsd-questions/2004-June/050371.html</p>
<p>&nbsp;</p>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.006254544469907407" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2024-12-29 16:09">2024-12-29 16:09</span>&nbsp;
<a href="https://www.cnblogs.com/longfurcat">猫毛·波拿巴</a>&nbsp;
阅读(<span id="post_view_count">0</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18637874" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18637874);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18637874', targetLink: 'https://www.cnblogs.com/longfurcat/p/18637874', title: '【杂谈】Kafka的日志段为什么不用内存映射？' })">举报</a>
</div>
        