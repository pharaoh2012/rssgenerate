
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/moonout/p/18980763" title="发布于 2025-07-18 23:32">
    <span role="heading" aria-level="2">Skill Discovery | METRA：让策略探索 state 的紧凑 embedding space</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                    <div id="cnblogs_post_description" style="display: none">
        
        为 state space 训练一个紧凑的 embedding space，使得 embedding 间的距离与 temporal distance 相匹配，然后让 policy 尽可能覆盖 embedding space。
    </div>
<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<br>
<ul>
<li>论文标题：METRA: Scalable Unsupervised RL with Metric-Aware Abstraction</li>
<li>ICLR 2024 Oral。</li>
<li>arxiv：<a href="https://arxiv.org/abs/2310.08887" target="_blank" rel="noopener nofollow">https://arxiv.org/abs/2310.08887</a></li>
<li>pdf：<a href="https://arxiv.org/pdf/2310.08887" target="_blank" rel="noopener nofollow">https://arxiv.org/pdf/2310.08887</a></li>
<li>html：<a href="https://arxiv.org/html/2310.08887" target="_blank" rel="noopener nofollow">https://arxiv.org/html/2310.08887</a></li>
<li>website：<a href="https://seohong.me/projects/metra/" target="_blank" rel="noopener nofollow">https://seohong.me/projects/metra/</a></li>
<li>GitHub：<a href="https://github.com/seohongpark/METRA" target="_blank" rel="noopener nofollow">https://github.com/seohongpark/METRA</a></li>
</ul>
<p>主要内容：</p>
<ul>
<li>metra 关注 RL 的子领域 skill discovery，是目前 skill discovery 领域的 sota 方法之一。</li>
<li>skill discovery 希望以无监督的方式（没有 reward 信号）训练一系列策略 <span class="math inline">\(\pi(a|s,z)\)</span> ，它们 condition on 一个高维向量 z，z 被称为 skill。总的来说，我们希望这一系列策略，可以彼此有很大的区分度，但又覆盖整个状态空间。具体的，这些策略满足两个特征：
<ul>
<li>1 可区分性：不同 skill z 生成的轨迹可以被区分，即，我们可以训练 / 得到一个 <span class="math inline">\(q(z|\tau)\)</span>，来分辨一个轨迹是哪个 skill 生成的。</li>
<li>2 状态覆盖：这些策略应该尽可能地覆盖状态空间。</li>
</ul>
</li>
<li>skill discovery 的通常做法：最大化 s 和 z 之间的互信息，<span class="math inline">\(I(s|z) = I(z|s) = H(s) - H(s|z) = H(z) - H(z|s)\)</span> 。</li>
<li>metra 的核心思想：使用 temporal distance，为 state space 训练一个 embedding space，然后让 policy 在 embedding space 里跑的尽可能远。
<ul>
<li>训 embedding space 的动机：传统 skill discovery 的互信息方法，认为 ① 一个 humanoid 抬左脚 / 抬右脚 ② humanoid 往左跑 / 往右跑 是没有区别的，认为这两个行为都是可分的。</li>
<li>然而，人类会觉得 ② 比 ① 区分度更大，这其实因为，抬左脚 / 抬右脚 是很容易互相达到的两个状态（temporal distance 小），而 humanoid 在地图左边 / 右边，这两个状态不容易相互达到（temporal distance 大）。</li>
<li>metra 发现，当我们跑一些复杂的环境，其中 state space 大一些，比如 state 维度高一些，传统 skill discovery 方法就很难覆盖很多 state 了。这是因为它们的很多 skill，都用来覆盖细枝末节、人类认为不重要的 state（temporal distance 小）。</li>
<li>因此，metra 认为，如果我们可以为 state space <span class="math inline">\(s\)</span> 训练一个紧凑的 embedding space <span class="math inline">\(\phi(s)\)</span>，使得 <span class="math inline">\(\phi(s_1),\phi(s_2)\)</span> 之间的距离，与 <span class="math inline">\(s_1,s_2\)</span> 之间的 temporal distance 相匹配；然后让 policy 尽可能覆盖 embedding space，便可以学到更有价值的 skill。</li>
</ul>
</li>
</ul>
<hr>
<p></p><div class="toc"><div class="toc-container-header">目录</div><ul><li><a href="#1-metra-的故事" rel="noopener nofollow">1 metra 的故事</a><ul><li><a href="#11-动机现有方法的-gapmetra-如何解决" rel="noopener nofollow">1.1 动机：现有方法的 Gap，METRA 如何解决</a></li><li><a href="#12-方法metra-为什么合理和精妙" rel="noopener nofollow">1.2 方法：METRA 为什么合理和精妙</a></li><li><a href="#13-总结一下这个故事" rel="noopener nofollow">1.3 总结一下这个故事</a></li></ul></li><li><a href="#2-metra-的理论--方法" rel="noopener nofollow">2 metra 的理论 &amp; 方法</a></li><li><a href="#3-metra-的实验--实验结果" rel="noopener nofollow">3 metra 的实验 &amp; 实验结果</a><ul><li><a href="#31-实验环境" rel="noopener nofollow">3.1 实验环境</a></li><li><a href="#32-baseline" rel="noopener nofollow">3.2 baseline</a></li><li><a href="#33-评估指标" rel="noopener nofollow">3.3 评估指标</a></li><li><a href="#34-实验结果" rel="noopener nofollow">3.4 实验结果</a></li></ul></li></ul></div><p></p>
<hr>
<h2 id="1-metra-的故事">1 metra 的故事</h2>
<p>（deepseek 总结的，感觉比我写得好）</p>
<p>这篇论文讲的是<strong>如何在没有奖励信号的情况下，让 agent 自己摸索出各种有用的动作技能</strong>。这很重要，因为如果机器人能自己学会很多基础技能（比如走路、转身、拿东西），以后学具体任务（比如送快递）就会快得多。下面，分析它的动机和方法：</p>
<h3 id="11-动机现有方法的-gapmetra-如何解决">1.1 动机：现有方法的 Gap，METRA 如何解决</h3>
<ul>
<li>
<p>现有方法的 Gap：</p>
<ul>
<li><strong>方法一：瞎逛型（Pure Exploration）</strong>：让机器人尽量去没去过的地方。问题：在复杂环境（比如有很多关节的人形机器人）里，状态多到逛不完，效率太低。就像让你蒙着眼探索整个城市，可能一直绕圈子。</li>
<li><strong>方法二：技能区分型（Mutual Info Skill）</strong>：让机器人学不同的技能，保证每个技能去的地方不一样。问题：它只关心技能“不一样”，不关心“多有用”。机器人可能学会“轻微抖腿”和“使劲抖腿”就算两种技能，但根本没移动！相当于厨师只会切不同形状的土豆丝，但不会炒菜。原文：“然而，它们存在一个共同局限，即它们往往最终发现的是简单、静态的行为，且状态覆盖范围有限”。</li>
<li><strong>核心问题</strong>：这两种方法在简单环境还行，但在<strong>高维复杂环境（如像素输入的四足机器人、人形机器人）都搞不定</strong>，学不到真正有用的移动技能。</li>
</ul>
</li>
<li>
<p>METRA 的动机，METRA 如何解决 Gap：</p>
<ul>
<li><strong>关键 Insight</strong>：不用费劲覆盖整个状态空间（太庞大），只需覆盖一个<strong>紧凑的“核心”空间（Latent Space Z）</strong>，这个空间能<strong>代表环境中真正重要的变化方向</strong>。</li>
<li><strong>如何连接“核心”空间和真实世界？</strong>：用<strong>时间距离（Temporal Distance）</strong> 当尺子。时间距离 = 两点间最快需要多少步到达。这很合理：在像素世界里，两个看起来不同的画面（比如机器人位置变化1米），如果一步就能到，那它们在这个“核心”空间里就该离得很近；如果需要跑 10 秒才能到，就该离得远。</li>
<li><strong>METRA 的目标</strong>：学一个映射函数 <span class="math inline">\(\phi(s)\)</span> 把状态 <span class="math inline">\(s\)</span>（比如像素图）映射到“核心”空间 <span class="math inline">\(Z\)</span>（比如2维点），并满足：<span class="math inline">\(||\phi(s1) - \phi(s2)|| &lt;= 时间距离(s1, s2)\)</span>。同时，让机器人学会在这个空间里<strong>朝各个方向移动</strong>。</li>
<li><strong>解决 Gap</strong>：既避开了“瞎逛”的低效，又避免了“技能区分”学无用技能的问题。专注于学习在<strong>最重要的维度（时间距离反映的）</strong> 上做<strong>长距离移动</strong>的技能。</li>
</ul>
</li>
</ul>
<h3 id="12-方法metra-为什么合理和精妙">1.2 方法：METRA 为什么合理和精妙</h3>
<ul>
<li><strong>方法的核心：</strong>
<ul>
<li><strong>两个一起学</strong>：
<ol>
<li>映射函数 <span class="math inline">\(\phi(s)\)</span>：把状态（如像素图）映射到低维空间 <span class="math inline">\(Z\)</span>，并遵守 <span class="math inline">\(||\phi(s) - \phi(s')|| &lt;= 1\)</span>（如果 <span class="math inline">\(s\)</span> 和 <span class="math inline">\(s'\)</span> 是相邻状态）。这保证了 <span class="math inline">\(Z\)</span> 空间的距离能反映时间距离。</li>
<li>技能策略 <span class="math inline">\(π(a|s, z)\)</span>：目标是让机器人执行技能 <span class="math inline">\(z\)</span> 时，在 <span class="math inline">\(Z\)</span> 空间里尽量沿着 <span class="math inline">\(z\)</span> 指定的方向移动。</li>
</ol>
</li>
<li><strong>奖励函数</strong>：<span class="math inline">\(r(s, z, s') = (\phi(s') - \phi(s))^T z\)</span>。
<ul>
<li>解释：如果机器人成功让状态在 <span class="math inline">\(Z\)</span> 空间移动了 <span class="math inline">\(Δ\phi = \phi(s') - \phi(s)\)</span>，并且这个移动方向 <span class="math inline">\(Δ\phi\)</span> 和它指定的方向 <span class="math inline">\(z\)</span> 很一致（点积大），就给它高奖励。</li>
<li><strong>Intuition</strong>：这等于告诉机器人：“你选一个方向 <span class="math inline">\(z\)</span>，然后努力让状态在 <span class="math inline">\(Z\)</span> 空间里沿着这个方向走得越远越好”。因为 <span class="math inline">\(Z\)</span> 空间被约束得能反映真实的时间距离，在这里走得远，相当于在真实世界完成了有意义的移动（比如从房间一头走到另一头）。</li>
</ul>
</li>
</ul>
</li>
<li><strong>为什么合理 &amp; 精妙？</strong>
<ol>
<li><strong>时间距离是普适的“好尺子”</strong>：它只依赖于环境本身的动态（几步能到），不依赖于状态的具体表示（比如像素值），完美适用于高维像素输入环境。</li>
<li><strong>约束 <span class="math inline">\(||Δ\phi|| &lt;= 1\)</span> 是安全的保证</strong>：它确保 <span class="math inline">\(Z\)</span> 空间不会扭曲现实。你在 <span class="math inline">\(Z\)</span> 空间移动 1 单位，真实世界至少需要 1 步，学到的技能反映真实的可达性。</li>
<li><strong>最大化点积驱动多样性探索</strong>：为了最大化奖励，机器人会倾向于：
<ul>
<li>学那些能在 <span class="math inline">\(Z\)</span> 空间产生<strong>大位移 <span class="math inline">\(Δ\phi\)</span></strong> 的技能（移动得远）。</li>
<li>让不同 <span class="math inline">\(z\)</span> 指向 <span class="math inline">\(Z\)</span> 空间中<strong>不同的方向</strong>（覆盖不同方向）。</li>
<li>这恰好让机器人学会了在<strong>最重要的维度</strong>上做<strong>长距离移动</strong>的各种技能。</li>
</ul>
</li>
<li><strong>自洽的闭环</strong>：<span class="math inline">\(\phi\)</span> 约束 <span class="math inline">\(Z\)</span> 空间的结构，<span class="math inline">\(π\)</span> 利用这个结构学习移动技能，技能收集到的数据又反过来优化 <span class="math inline">\(\phi\)</span>。两者相互促进。</li>
</ol>
</li>
</ul>
<h3 id="13-总结一下这个故事">1.3 总结一下这个故事</h3>
<ul>
<li><strong>动机故事</strong>：以前的无监督 RL 在复杂环境（尤其是像素）不是逛得太低效（瞎逛），就是学些没用的花拳绣腿（互信息技能）。根本原因是它们没抓住重点（环境的关键变化维度）。</li>
<li><strong>解决方法故事</strong>：我们提出 METRA，用<strong>时间距离</strong>这把万能钥匙，构建一个紧凑的“核心地图”（<span class="math inline">\(Z\)</span> 空间）。机器人只需学习在这个地图上<strong>朝各个方向快速移动</strong>的技能。这样既能高效探索，又能学到真正有意义（反映长距离移动）的技能。这把钥匙（时间距离）让我们的方法天然适应像素环境，约束保证了地图不骗人，奖励机制驱动了有效探索。</li>
<li><strong>结果</strong>：METRA 首次在像素输入的四足机器人（Quadruped）和人形机器人（Humanoid）上学会了多样的移动技能，并且在多个任务上超越了之前的 SOTA 方法。</li>
</ul>
<h2 id="2-metra-的理论--方法">2 metra 的理论 &amp; 方法</h2>
<p>metra 所做的第一件事，就是把 skill discovery 里经典的互信息目标，换成了 Wasserstein dependency measure（WDM）。关于 WDM 的讲解，可以看 <a href="https://www.cnblogs.com/moonout/p/18965260#wasserstein-dependency-measure-for-representation-learning" target="_blank">本站博客</a>，关于把互信息换成 WDM 动机，<a href="https://www.cnblogs.com/moonout/p/18965260#wasserstein-dependency-measure-for-representation-learning" target="_blank">本站博客</a> 里也有写。总之，我们的互信息 <span class="math inline">\(I(s,z)\)</span> 现在变成了这样：<span class="math inline">\(I_W(s,z)\)</span>。</p>
<p>根据 <a href="https://www.cnblogs.com/moonout/p/18965260#wasserstein-dependency-measure-for-representation-learning" target="_blank">本站博客</a> ，可以这样计算 <span class="math inline">\(I_W(s,z)\)</span> ：</p>
<p></p><div class="math display">\[I_W(S;Z) = \sup_{\|f\|_{\mathcal L}\le 1}\,
\Bigl\{\,
  \mathbb E_{p(s,z)}\bigl[f(s,z)\bigr]
  -
  \mathbb E_{p(s)\,p(z)}\bigl[f(s,z)\bigr]
\Bigr\} \tag{1}
\]</div><p></p><p>这里的 <span class="math inline">\(f(s,z)\)</span> 相当于 WDM 里所讲的相似性函数，它应该满足 1-Lipschitz 条件 <span class="math inline">\(\|f\|_{\mathcal L}\le 1\)</span>，即，<span class="math inline">\(|f(s_1,z_1) - f(s_2,z_2) | \le \|(s_1,z_1) - (s_2,z_2)\|_d\)</span> ，其中 <span class="math inline">\(\|\cdot \|_d\)</span> 是任选的 可以度量向量之间距离的 metric，下文 metra 把它选成了 temporal distance。</p>
<p>metra 声称，(1) 式的形式已经可以使用 WDM 的方法来训，同时，我们可以训一个策略 <span class="math inline">\(\pi(s|s,z)\)</span>，它的 reward 是 <span class="math inline">\(r(s,z) = f(s,z) -(1/N)\sum_{i=1}^N f(s,z_i)\)</span> 。然而，这个形式还可以再简化。</p>
<p>metra 将 <span class="math inline">\(f(s,z)\)</span> 设为 <span class="math inline">\(\phi(s)^Tz\)</span> 的形式，把 (1) 式转换为以下形式：</p>
<p></p><div class="math display">\[I_W(S;Z) \approx \sup_{\|\phi\|_{\mathcal L}\le 1}\,
\Bigl\{\,
  \mathbb E_{p(s,z)}\bigl[\phi(s)^Tz\bigr]
  -
  \mathbb E_{p(s)}\big[\phi(s)\big]^T \mathbb E_{p(z)}\big[z\big]
\Bigr\} \tag{2}
\]</div><p></p><p>这里是约等于，因为其实 LHS RHS 并不完全相等。metra 声称，<span class="math inline">\(\|\phi\|_{\mathcal L}\le 1\)</span>（即 <span class="math inline">\(\|\phi(s_1) - \phi(s_2) \|_2 \le \|s_1 - s_2\|_d\)</span>）并不等价于 <span class="math inline">\(\|f\|_{\mathcal L}\le 1\)</span>，但反正 <span class="math inline">\(\|\phi\|_{\mathcal L}\le 1\)</span> 更好算。并且，<span class="math inline">\(\|f\|_{\mathcal L}\)</span> 的 upper bound 可以用 <span class="math inline">\(\|\phi\|_{\mathcal L}\)</span> 等一堆东西表示出来，所以 metra 认为这样做是完全可以的。我不懂数学，不过感觉这个形式挺好看的。</p>
<p>（metra 原文是这样写的：</p>
<p></p><div class="math display">\[I_W(S;Z) \approx \sup_{\|\phi\|_{\mathcal L}\le 1 ,\, \|\psi\|_{\mathcal L}\le 1}\,
\Bigl\{\,
  \mathbb E_{p(s,z)}\bigl[\phi(s)^T \psi(z)\bigr]
  -
  \mathbb E_{p(s)}\big[\phi(s)\big]^T \mathbb E_{p(z)}\big[\psi(z)\big]
\Bigr\} \tag{3}
\]</div><p></p><p>然后在后文，把 <span class="math inline">\(\psi(z)\)</span> 设为 <span class="math inline">\(z\)</span>。）</p>
<p>接下来，metra 继续变换 (2) 式。metra 让 <span class="math inline">\(I_W(S;Z)\)</span> 只考虑一个 trajectory 里的最后一个状态，即 <span class="math inline">\(I_W(S_T;Z)\)</span>。然后，metra 进行一个裂项相消，得到：</p>
<p></p><div class="math display">\[I_W(S_T;Z) \approx \sup_{\|\phi\|_L \leq 1} \mathbb{E}_{p(\tau, z)}[\phi(S_T)^\top z] - \mathbb{E}_{p(\tau)}[\phi(S_T)]^\top \mathbb{E}_{p(z)}[z] \\
= \sup_{\phi} \sum_{t=0}^{T-1} \Big( \mathbb{E}_{p(\tau, z)}[(\phi(s_{t+1}) - \phi(s_t))^\top z] - \mathbb{E}_{p(\tau)}[\phi(s_{t+1}) - \phi(s_t)]^\top \mathbb{E}_{p(z)}[z] \Big) \tag{4}
\]</div><p></p><p>这样的话，还应该剩一个 <span class="math inline">\(\mathbb{E}_{p(\tau, z)}[\phi(s_0)^\top z] - \mathbb{E}_{p(\tau)}[\phi(s_{0})]^\top \mathbb{E}_{p(z)}[z]\)</span> 才对。不过， <span class="math inline">\(s_0, z\)</span> 是相互独立的，这一项就 = 0 了。</p>
<p>我们把 (4) 式整理一下，就变成了：</p>
<p></p><div class="math display">\[I_W(S_T;Z) \approx \sup_{\|\phi\|_L \leq 1} \mathbb{E}_{p(\tau, z)} \bigg[\sum_{t=0}^{T-1} \big[\phi(s_{t+1}) - \phi(s_{t})\big]^T (z-\bar z) \bigg] \tag{5}
\]</div><p></p><p>然后，我们假设 skill 的平均值 <span class="math inline">\(\bar z=0\)</span>，这很容易做到，因为在训练过程中，z 的值 其实是我们采样得到的。</p>
<p>现在，Lipschitz 条件 <span class="math inline">\(\|\phi\|_L \leq 1\)</span> 还不知道该怎么处理。这里的关键在于，我们要去优化 <span class="math inline">\(\|\phi(s_1) - \phi(s_2) \|_2 \le \|s_1 - s_2\|_d\)</span> 的目标，但是 metric d 还不知道该选什么。</p>
<p>metra 选择 metric d 为 temporal distance（时序距离，可参考 <a href="https://www.cnblogs.com/moonout/p/18812958#-%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0-temporal-distance" target="_blank">本站博客</a> ）。temporal distance 的大致含义，是我们去训练一个 state embedding <span class="math inline">\(\phi(s)\)</span> ，使得容易相互到达的 state 的 embedding 离得近。具体的，</p>
<p></p><div class="math display">\[\|\phi(s_{t+1}) - \phi(s_t) \|_2 \le 1 \tag{6}
\]</div><p></p><p>我们发现，这跟 Lipschitz 条件的形式很相像，就直接用 <span class="math inline">\(\|\phi(s_{t+1}) - \phi(s_t) \|_2 \le 1\)</span> 来作为 (5) 式的约束了。现在，(5) 式变成了这样：</p>
<p></p><div class="math display">\[\sup_{\pi, \phi} \quad \mathbb{E}_{p(\tau, z)} \left[ \sum_{t=0}^{T-1} (\phi(s_{t+1}) - \phi(s_t))^\top z \right] \quad \text{s.t.} \quad \|\phi(s) - \phi(s')\|_2 \leq 1 \tag{7}
\]</div><p></p><p>具体的，上式可以被拆为三部分，进行跟 WDM 差不多的对偶 learning：</p>
<ul>
<li>训练 state embedding <span class="math inline">\(\phi(s)\)</span> ：<span class="math inline">\(\max \mathbb{E}_{(s,z,s') \sim \mathcal{D}} [(\phi(s') - \phi(s))^\top z + \lambda \cdot \min(\varepsilon, 1 - \|\phi(s) - \phi(s')\|_2^2)]\)</span> ；</li>
<li>更新 Lagrange 乘子 <span class="math inline">\(\lambda\)</span> ：<span class="math inline">\(\min \mathbb{E}_{(s,z,s') \sim \mathcal{D}} [\lambda \cdot \min(\varepsilon, 1 - \|\phi(s) - \phi(s')\|_2^2)]\)</span> ；</li>
<li>更新 policy <span class="math inline">\(\pi(a|s,z)\)</span> ：reward <span class="math inline">\(r(s, z, s') = (\phi(s') - \phi(s))^\top z\)</span>，使用 SAC 算法。</li>
<li>从 reward 的角度来看，突然感觉 <a href="https://www.cnblogs.com/moonout/p/18812958" target="_blank">ETD</a> 跟 metra 真像（</li>
</ul>
<h2 id="3-metra-的实验--实验结果">3 metra 的实验 &amp; 实验结果</h2>
<h3 id="31-实验环境">3.1 实验环境</h3>
<ol>
<li>state-based 环境：
<ul>
<li>Ant（29 维状态）：四足机器人运动。</li>
<li>HalfCheetah（18 维状态）：仿猎豹机器人奔跑。</li>
</ul>
</li>
<li>pixel-based 环境：
<ul>
<li>Quadruped &amp; Humanoid（64×64×3 图像）：四足 / 人形机器人运动。</li>
<li>Kitchen（64×64×3 图像）：机器人完成 6 项厨房子任务，烧水 / 微波炉 / 开关灯等。</li>
</ul>
</li>
</ol>
<h3 id="32-baseline">3.2 baseline</h3>
<ol>
<li><strong>skill discovery 方法</strong>：</li>
</ol>
<ul>
<li>LSD：最大化状态间欧氏距离，还没读过，貌似是 metra 作者之前的工作。</li>
<li>DIAYN：互信息最大化区分技能，是 Eysenbach 的工作，skill discovery 最早的工作之一。</li>
<li>DADS：学习技能动力学模型，还没读过。</li>
<li>CIC：对比学习 + 探索奖励混合，还没读过。</li>
<li>APS：使用 successor feature 的 skill discovery 工作，有点忘记具体内容了。</li>
</ul>
<ol start="2">
<li><strong>探索方法</strong>：
<ul>
<li>ICM / RND / APT：基于状态新颖性的内在奖励。</li>
<li>LBS：基于贝叶斯惊喜的探索，没读过。</li>
<li>Plan2Explore：动态模型不确定性最大化，没读过。</li>
</ul>
</li>
<li><strong>目标达成方法</strong>：
<ul>
<li>LEXA：联合训练探索策略 + 目标条件策略，没读过。</li>
</ul>
</li>
</ol>
<h3 id="33-评估指标">3.3 评估指标</h3>
<ol>
<li>策略状态覆盖率：评估当前策略在 <strong>状态空间</strong> 的覆盖范围。并不考察整个 state space 的覆盖率，而是 state space 的一个子集，如 Ant / Humanoid 记录 x-y 平面被访问的 1×1 方格数。</li>
<li>任务覆盖率：对于 Kitchen 环境，统计 <strong>6 项子任务</strong> 完成数量。</li>
<li>下游任务性能：使用 <strong>分层控制</strong>，冻结 low-level 的 skill-conditioned policy，训练一个 high-level 策略，优化具体的任务奖励。</li>
<li>零样本目标达成：用学习到的技能 <strong>直接执行</strong> 目标条件任务，如 Humanoid 中计算与目标的欧氏距离。</li>
</ol>
<h3 id="34-实验结果">3.4 实验结果</h3>
<ol>
<li>技能多样性（定性的）：
<ul>
<li>metra 是 <strong>首个在像素 Quadruped / Humanoid 中发现运动技能</strong> 的方法。LSD / DIAYN 等仅在状态环境有效，像素环境完全失败。</li>
<li>（metra 把 pixel-based 环境的地面染成了五彩的颜色，往每个方向跑，都有不同的颜色。我一开始以为，这是为了可视化好看，后来意识到，只有这样，state embedding <span class="math inline">\(\phi(s)\)</span> 才能分辨出来 往不同方向跑的小蜘蛛，彼此之间有区别。）</li>
</ul>
</li>
<li>状态覆盖率（以下都是定量的）：
<ul>
<li>metra 在 Ant 覆盖率超 2000，比第二名 LSD（~500）高4倍（图5）</li>
<li>在 Humanoid 覆盖率 75，远超探索方法（&lt;25）（图8）</li>
</ul>
</li>
<li>下游任务性能：
<ul>
<li>metra 在5个任务中4项排名第一，例如：
<ul>
<li>Quadruped Goal 任务得分 6.0，LSD 仅 2.5。</li>
<li>HalfCheetah Hurdle 跳跃任务得分 7.5，DADS 仅 5.0。</li>
</ul>
</li>
</ul>
</li>
<li>零样本目标达成：
<ul>
<li>metra  在 Ant 目标距离减少 40 单位，LEXA 仅 20。</li>
<li>Kitchen 任务完成数 4/6，LEXA 为 3/6。</li>
</ul>
</li>
</ol>
<br>
<br>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0" data-date-updated="2025-07-18 23:32">2025-07-18 23:32</span>&nbsp;
<a href="https://www.cnblogs.com/moonout">MoonOut</a>&nbsp;
阅读(<span id="post_view_count">1</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18980763);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18980763', targetLink: 'https://www.cnblogs.com/moonout/p/18980763', title: 'Skill Discovery | METRA：让策略探索 state 的紧凑 embedding space' })">举报</a>
</div>
        