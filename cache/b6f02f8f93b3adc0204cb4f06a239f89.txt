
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/carpell/p/18839936" title="发布于 2025-04-22 09:22">
    <span role="heading" aria-level="2">最详细最易懂的【YOLOX原理篇】</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p></p><div class="toc"><div class="toc-container-header">目录</div><ul><li><a href="#前言" rel="noopener nofollow">前言</a></li><li><a href="#简介" rel="noopener nofollow">简介</a></li><li><a href="#详细解读" rel="noopener nofollow">详细解读</a><ul><li><a href="#mosaic-and-mixup" rel="noopener nofollow">Mosaic and Mixup</a><ul><li><a href="#mixup" rel="noopener nofollow">Mixup</a></li><li><a href="#mosaic" rel="noopener nofollow">Mosaic</a></li></ul></li><li><a href="#decoupled-head" rel="noopener nofollow">Decoupled Head</a></li><li><a href="#anchor-free" rel="noopener nofollow">anchor free</a></li><li><a href="#simota" rel="noopener nofollow">SimOTA</a><ul><li><a href="#in_boxes-和-in_center" rel="noopener nofollow">in_boxes 和 in_center</a></li><li><a href="#计算cost矩阵" rel="noopener nofollow">计算cost矩阵</a></li><li><a href="#dynamic_k_matching算法" rel="noopener nofollow">dynamic_k_matching算法</a></li></ul></li></ul></li><li><a href="#网络架构" rel="noopener nofollow">网络架构</a></li><li><a href="#参考资料" rel="noopener nofollow">参考资料</a></li></ul></div><p></p>
<h1 id="前言">前言</h1>
<blockquote>
<p>提出时间：2021年<br>
作者单位：旷视科技<br>
旷视官方代码：<a href="https://github.com/Megvii-BaseDetection/YOLOX" target="_blank" rel="noopener nofollow">https://github.com/Megvii-BaseDetection/YOLOX</a><br>
论文下载地址：<a href="https://arxiv.org/abs/2107.08430" target="_blank" rel="noopener nofollow">https://arxiv.org/abs/2107.08430</a><br>
论文题目：《YOLOX: Exceeding YOLO Series in 2021》</p>
</blockquote>
<p>本文带大家走进YOLOX，常用于深度学习中的目标检测领域，其隶属于YOLO系列。YOLO系列以其快速、准确的目标检测能力而闻名，而YOLOX在此基础上进行了多方面的改进和优化，旨在提供一个更灵活、可扩展且性能更强的检测框架。本文将会带大家来看看YOLOX当时所使用的黑科技，详细解读YOLOX的原理，当然后续也会给大家带来相关的实际应用的，敬请期待！</p>
<p><img src="https://gitee.com/fouen/image/raw/master/image/20250420152413226.png" alt="image-20250420152413154" loading="lazy"></p>
<h1 id="简介">简介</h1>
<p>看YOLOX当时发表的论文标题，特别的张狂，号称超越当时所有的YOLO系列模型。</p>
<p>首先YOLOX其是基于YOLOv5的基础上进行设计的，像YOLOv5一样设计了一个统一的基础网络结构，使得模型可以在训练和推理阶段共享相同的网络架构，这简化了模型的部署流程并提高了效率。并且通过模块化设计，使得我们可以更换或者替换不同的组件比如 Backbone（主干网络）、Neck（颈部网络）以及Head（头部网络），以适应不同任务需求或进行算法创新。</p>
<p>YOLOX核心贡献突破：</p>
<ul>
<li><strong>Decoupled Head</strong>：YOLOX引入了解耦头的设计通过设计不同的分支来进行分类任务和回归任务。</li>
<li><strong>anchor free：</strong>不在使用以往YOLO系列所使用的基于anchor的策略，能更好的减少模型参数同时能够带来较强的鲁棒性。</li>
<li><strong>Mosaic Data Augmentation：</strong>采用马赛克数据增强技术，随机分割组合图片，增加数据的多样性，提升模型的泛化能力。</li>
<li><strong>SimOTA (Similarity-based One-Stage Object Detection Assignment)</strong>：解决传统目标检测方法中手工设置阈值的问题，通过动态分配正负样本，提高训练效率和检测精度。其核心思想是根据预测框与真实框之间的相似度来分配样本，从而优化模型性能。这部分比较难，后续讲解的时候比篇幅会有点长。</li>
</ul>
<h1 id="详细解读">详细解读</h1>
<h2 id="mosaic-and-mixup">Mosaic and Mixup</h2>
<h3 id="mixup">Mixup</h3>
<p>Mixup是MIT和FAIR在ICLR 2018上发表的文章中提到的一种数据增强算法。在介绍mixup之前，我们首先简单了解两个概念：<strong>经验风险最小化（Empirical risk minimization，ERM）</strong>和<strong>邻域风险最小化（Vicinal Risk Minimization，VRM）</strong>。</p>
<p><strong>经验风险最小化（ERM）</strong>：“经验风险最小化”是目前大多数网络优化都遵循的一个原则，即使用已知的经验数据（训练样本）训练得到的学习器的误差或风险，也叫作“经验误差”或“训练误差”。相对的，在新样本（未知样本）上的误差称为“泛化误差”，显然，我们希望学习器的“泛化误差”越小越好。然而，通常我们事先并不知道新样本是什么样的，实际能做的是努力使经验误差越小越好。但是，过分的减小经验误差，通常会在未知样本上产生很差的结果，也就是我们常说的“过拟合”。</p>
<p>为了提高模型泛化性（模型在验证集的表现能力），通常可以通过使用大规模训练数据来提高，但是实际上，获取有标签的大规模数据需要耗费巨大的人工成本，甚至有些情况下根本无法获取数据。解决这个问题的一个有效途径是“<strong>邻域风险最小化</strong>”，即通过先验知识构造训练样本的邻域值。一般的做法就是传统的数据增强方法，比如加噪、翻转、缩放等，但是这种做法很依赖于特定的数据集和人类的先验知识。</p>
<p>其原理很简单：</p>
<p><img src="https://gitee.com/fouen/image/raw/master/image/20250420144428806.png" alt="image-20250420144428763" loading="lazy"></p>
<p>其中， ($x_i$ ,$y_i$ ) ($x_i$, $y_i$ ) 和 ($x_j$, $y_j$) ($x_j$, $y_j$)是从原始训练数据中随机选取的两个样本， λ ∈ [ 0 , 1 ] 。 λ 是mixup的超参数，控制两个样本插值的强度，当 λ → 0 或 λ → 1 时，则退化到了ERM的情况。增强的效果图如下，其实就是将一张图片和另一张图片进行融合处理。</p>
<p><img src="https://gitee.com/fouen/image/raw/master/image/20250420144859401.png" alt="image-20250420144859355" loading="lazy"></p>
<h3 id="mosaic">Mosaic</h3>
<p>Mosaic技术的基本思想是将多张图像拼接成一张图像，从而在一张图像中包含更多的目标，增加目标的数量和多样性。其步骤就是首先随机选取四张图像，将每张图像缩放到相同的大小，然后通过拼接成一张大图像，形成类似于马赛克的效果。</p>
<p><img src="https://gitee.com/fouen/image/raw/master/image/20250420145357085.png" alt="image-20250420145356986" loading="lazy"></p>
<p><strong>最重要的来了，Mosaic和Mixup需要再训练结束前的15个epoch关掉。因为Mosaic+Mixup生成的训练图片，脱离自然图片的真实分布的，而且Mosaic大量的crop操作会带来很多不准确的标注框。提前关闭Mosaic和Mixup，能够使检测器避开不准确标注框的影响，在自然图片的数据分布下完成最终的收敛。</strong></p>
<h2 id="decoupled-head">Decoupled Head</h2>
<p>在YOLO v3-v5版本中，所使用的head头同时预测输出目标位置和类别，每个特征图的预测张量为$N × N × [ 3 ∗ ( 4 + 1 + c l a s s e s _ n u m ) ] $，其中$N$是特征图的大小。这种被称为耦合头（Coupled head），它的设计思路简单，仅需要几个全连接层或者卷积层即可。这样虽然简单但是会影响到模型的性能，为什么，因为这样的话分类和回归任务是共用head的参数进行预测的，但是分类和回归任务是存在差异的，但是其共用同一组参数来预测不同的任务，势必会损耗性能。</p>
<p>所以在YOLOX中使用的解耦合头（Decoupled head），使用不同的分支分别预测目标位置和类别信息。首先先使用一个$1×1$的卷积减少通道维数，然后并行分类和回归两个分支，每个分支堆叠两个$3×3$的卷积。分类分支用来预测类别信息；回归分支用来输出bbox的位置信息。在回归分支中，添加了IoU 分支。通过将两个不同的任务进行解耦，分开预测，从而提升模型的性能。</p>
<p><img src="https://gitee.com/fouen/image/raw/master/image/20250420134245434.png" alt="image-20250420134245372" loading="lazy"></p>
<p>同时实验也是证明了其能够更快收敛，并且提升模型的检测精度（能带来4.2%AP提升），</p>
<p><img src="https://gitee.com/fouen/image/raw/master/image/20250420134457705.png" alt="image-20250420134457659" loading="lazy"></p>
<h2 id="anchor-free">anchor free</h2>
<p>在YOLO v1-v5版本中，都是使用的是基于anchor的方法进行检测的。什么意思呢？就是为了能够取得更好的性能，在训练之前，通常都会通过聚类分析训练数据集的方式确定anchor box的大小，确定的anchor box一般仅仅适用于该数据集，通用性不强。而且Anchor的存在增加了检测头的复杂度以及生成结果的数量。所以在使用其进行训练前，我们都会提前使用聚类算法来分析其anchor box的大小，再进行训练。</p>
<p>而YOLOX采用了anchor-free（无锚点）的检测方法，这是一种不同于传统YOLO系列及其他一些目标检测模型中使用固定anchor boxes的设计。在anchor-free方法中，模型直接预测目标物体的中心点及其宽高，而不是基于预设的anchor boxes进行偏移和尺寸调整。这一改变简化了模型结构，降低了超参数调优的复杂性，并有可能提升模型的灵活性和准确性。</p>
<p><strong>实现机制</strong>：在anchor-based的方法中我们是通过预测相对于预设的anchor的偏移量的，但是在anchor-free中，我们直接预测其中心点(x,y)，然后预测其宽和高(w,h)，通过这样的方式避免了anchor-based的方法所需要来预设anchor的弊端。</p>
<p>并且通过anchor-free的方法能有效的减少参数量。举个例子来说，通过前面的backbone以及fpn结果，所提取出来的进入head检测头的特征图的大小分别为52×52、26 × 26 和13 × 13(三个尺度的特征图大小)，使用anchor-free的方法进行预测的话，我们只需要预测1 × 3549 × 85 个结果，其中3549表示一共有3549个像素，而我们只需要对每个像素预测1个结果即可，85表示80个类别，4个bbox参数(中心点和宽高)，1表示IoU。</p>
<p>但是对于anchor-based来说就是3 × 3549 × 85 对每个像素预测三个结果，其预测结果是anchor-free的3倍。</p>
<p>通过下图我们可以明显的看出二者的不同。</p>
<p><img src="https://gitee.com/fouen/image/raw/master/image/20250420143450882.png" alt="image-20250420143450805" loading="lazy"></p>
<h2 id="simota">SimOTA</h2>
<p>在这3549个预测框中，其中有2704个预测框所对应的锚框的大小为8 × 8; 有676个预测框所对应的锚框的大小为16 × 16;有169个预测框所对应的锚框的大小为32 × 32。这3549个预测框中，只有少部分是正样本，绝大多数是负样本，那么哪些是正样本？如何将正样本预测框挑选出来呢？思路就是将这3549个预测框和图片上所有的gt框(ground truth)进行关联，从而挑选出正样本，这种关联方式，被称为标签分配。</p>
<p>首先通过提取落在gt bboxes 一定范围内的所有候选框。然后就是SimOTA求解。标签分配问题可以转换为标准的OTA问题，但Sinkhorn-Knopp算法需要多次迭代才能求得最优解，官方发现该算法会导致25%的额外训练时间，因此采用简化版的OTA方法，SimOTA，求解近似最优解。</p>
<p>接下来详细讲解什么是SimOTA。</p>
<h3 id="in_boxes-和-in_center">in_boxes 和 in_center</h3>
<p>首先我们需要先来了解两个概念，什么是<strong>in_boxes (anchor point在ground truth中)</strong>和 <strong>in_center(anchor point在ground truth的展开域中)</strong>。如下图所示，红色的点为anchor point，绿色的框为gt，蓝色的框为gt展开域。其中落入gt中的anchor即为in_boxes，落入蓝色框中的anchor point即为in_center。</p>
<p><img src="https://gitee.com/fouen/image/raw/master/image/20250420202235061.png" alt="image-20250420202234980" loading="lazy"></p>
<p>这里为方便理解我们以尺度20×20为例。在20×20的feature map上总共有400个anchor，每一个anchor都会有一个输出(num_class+4+1)。如下图所示，这张图片中总共有三个gt。</p>
<p><img src="https://gitee.com/fouen/image/raw/master/image/20250420202708530.png" alt="image-20250420202708461" loading="lazy"></p>
<p>根据刚刚我们所说的in_boxes，我们可以挑选出anchor point落在gt中的网格，同理也挑选出anchor point落在gt展开域中的网格，通过取两者的并集，我们可以得到一个长度为400的Bool型的tensor--fg_mask(<strong>foreground mask，前景信息)</strong>，<strong>形如[False, False, False, True, True, ... ,False, False]</strong>，其中有95个True，如下图所示。</p>
<p><img src="https://gitee.com/fouen/image/raw/master/image/20250420203002093.png" alt="image-20250420203002023" loading="lazy"></p>
<p>然后我们同样的按照刚才的方式，只不过这回我们取两者的交集，如下图。这样我们可以得到个shape为3×95的Bool类型的tensor--<strong>is_in_boxes_and_centers</strong>。其中is_in_boxes_and_centers[i]代表第i个gt的in_boxes &amp; in_centers。这里可能有些人有点小疑问，为什么需要每个gt都对应着一个，不能直接一个表示不同gt的in_boxes &amp; in_centers吗？这个图上是可以的，但是实际我们在检测中，难免会出现部分重叠的情况，那么true所表示的到底是在还是不在或者到底是表示的在哪个呢？所有我们需要每个gt对应一个才可以。</p>
<p><img src="https://gitee.com/fouen/image/raw/master/image/20250420203143423.png" alt="image-20250420203143349" loading="lazy"></p>
<p>到此为止，先梳理一下，目前以COCO(80类)中某张图片为输入，得到20*20的feature map，图片中有3个gt，现在我们有以下信息：</p>
<ol>
<li>网络输出的类别信息，400*80的Tensor——pred_cls；</li>
<li>网络输出的置信度信息，400*1的Tensor——pred_obj；</li>
<li>网络输出的box回归信息，400*4的Tensor——pred_box；</li>
<li>20×20 feature map上得到的前景信息，长度为400的Bool类型Tensor——fg_mask，其中为True的为前景，其余False部分为背景，有95个True</li>
<li>20×20 feature map上得到的各个gt的前景信息，3×95的Bool类型Tensor——is_in_boxes_and_centers</li>
</ol>
<p>这边看下面的图结合理解：</p>
<p><img src="https://gitee.com/fouen/image/raw/master/image/20250420203906486.png" alt="image-20250420203906407" loading="lazy"></p>
<p>现在我们需要根据前景信息fg_mask对网络输出进行初步的筛选(网路输出20*20的feature map上400个点位，每个点位都有类别信息、置信度信息、回归信息，这些信息大部分都是没有用的)</p>
<h3 id="计算cost矩阵">计算cost矩阵</h3>
<p>首先来计算iou loss损失：</p>
<pre><code class="language-python"># 计算3个gt与95个网络输出框(pred_box)的iou
pair_wise_ious = bbox_iou(gt_bboxes, box_pred) # 得到一个3*95的iou矩阵
# 计算iou loss损失
pair_wise_ious_loss = -torch.log(pair_wise_ious + 1e-8)
# 3*95的iou loss矩阵iou矩阵如图
</code></pre>
<p><img src="https://gitee.com/fouen/image/raw/master/image/20250420204358880.png" alt="image-20250420204358836" loading="lazy"></p>
<p>然后我们来计算cls loss损失：</p>
<pre><code class="language-python"># 先对3个gt的cls信息进行one hot编码
gt_cls = F.one_hot(gt_class,80) # 3*95*80
# 得到类别信息
cls_preds_ = cls_pred * obj_pred # 3*95*80
# 计算cls loss
pair_wise_cls_loss = F.binary_cross_entropy(cls_preds_ , gt_cls) # 3*95
</code></pre>
<p>然后我们计算SimOTA关键的cost矩阵：</p>
<pre><code class="language-python">cost = (pair_wise_cls_loss + 3.0 * pair_wise_ious_loss + 100000 * (~is_in_boxes_and_centers))
</code></pre>
<ul>
<li>
<p>pair_wise_cls_loss就是每个样本与每个GT之间的分类损失</p>
</li>
<li>
<p>pair_wise_ious_loss是每个样本与每个GT之间的回归损失</p>
</li>
<li>
<p>is_in_boxes_and_center代表那些落入in_boxes 和 in_center交集内的样本，即上图中橙色勾对应的样本，然后这里进行了取反~表示不在in_boxes 和 in_center交集内的样本。接着又乘以100000.0，也就是说对于in_boxes 和 in_center交集外的样本cost加上了一个非常大的数，这样在最小化cost过程中会优先选择in_boxes 和 in_center交集内的样本。</p>
</li>
</ul>
<p>简而言之，我们根据fg_mask得到400个anchor point里面有95个是初步认定的正样本，我们根据它们与gt box的iou、类别损失、前景背景信息得到一个3*95的cost矩阵，表示每一个gt与每一个正样本之间的成本cost(成本cost越小优先级越高)。cost矩阵如图所示。</p>
<p><img src="https://gitee.com/fouen/image/raw/master/image/20250420204557884.png" alt="image-20250420204557825" loading="lazy"></p>
<h3 id="dynamic_k_matching算法">dynamic_k_matching算法</h3>
<ul>
<li>
<p>首先我们来看，cost矩阵shape为3*95，我们取n_candidate_k = min(10, 95) = 10；</p>
</li>
<li>
<p>然后我们在在iou矩阵中，根据n_candidate_k=10，取对于每一个gt，取其与95个pred box的所有iou里最大的10个并且求和，得到dynamic_k示意图如下</p>
</li>
</ul>
<p><img src="https://gitee.com/fouen/image/raw/master/image/20250420204752498.png" alt="image-20250420204752440" loading="lazy"></p>
<ul>
<li>然后我们在根据dynamic_k，从cost矩阵中，GT0那一行找出4个最小的值，GT1那一行找出3个最小的值，GT2那一行找出2个最小的值，示意图如下；</li>
</ul>
<p><img src="https://gitee.com/fouen/image/raw/master/image/20250420204816930.png" alt="image-20250420204816878" loading="lazy"></p>
<p>上图中，A3与GT0匹配成功，A91与GT2匹配成功，A92与GT2匹配成功，A4与GT1匹配成功(为什么不是GT0? 每一个anchor只能与一个GT匹配成功，如果出现A4与GT0和GT1同时被选中的情况，取cost最小的)，A5与GT0匹配成功，A6与GT0匹配成功。</p>
<ul>
<li>最后我们再根据匹配成功的信息，得到matching matrix (shape与iou矩阵和cost矩阵相同)，如下：</li>
</ul>
<p><img src="https://gitee.com/fouen/image/raw/master/image/20250420204954125.png" alt="image-20250420204954066" loading="lazy"></p>
<p>matching matrix中就是最终的样本分配的结果，从图中可以看见的部分可以得知，最终正样本为A3、A4、A5、A6、...... 、A91、A92。</p>
<h1 id="网络架构">网络架构</h1>
<p><img src="https://gitee.com/fouen/image/raw/master/image/20250420152047382.png" alt="image-20250420152047335" loading="lazy"></p>
<p>从图中我们就可以看出YOLOX的标准的模块化结构设计了，总共可分为三部分backbone(用于提取出高级语义特征)，neck(双通道的FPN融合多尺度的特征)，head(用于对特征图进行检测)。其中各个的部分都是可以进行更换的，比如backbone部分，想的话也可以换成别的网络，都是十分便利的。</p>
<h1 id="参考资料">参考资料</h1>
<p><a href="https://blog.csdn.net/qq_51605551/article/details/140655760" target="_blank" rel="noopener nofollow">35_YOLOX网络详解-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/qq_38253797/article/details/116668074?ops_request_misc=%7B%22request%5Fid%22%3A%22e66e8366eeb28a8abb18a3d3936ad31a%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&amp;request_id=e66e8366eeb28a8abb18a3d3936ad31a&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-116668074-null-null.142%5Ev102%5Epc_search_result_base8&amp;utm_term=mosic%E5%92%8Cmixup&amp;spm=1018.2226.3001.4187" target="_blank" rel="noopener nofollow">【YOLO v4 相关理论】Data augmentation: MixUp、Random Erasing、CutOut、CutMix、Mosic_yolo mixup-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/weixin_42111770/article/details/131401473?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=yolox&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-7-131401473.142%5Ev102%5Epc_search_result_base8&amp;spm=1018.2226.3001.4187" target="_blank" rel="noopener nofollow">【YOLO系列】YOLOX（含代码解析）-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/qq_43799400/article/details/123958495?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=yolox&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-4-123958495.142%5Ev102%5Epc_search_result_base8&amp;spm=1018.2226.3001.4187" target="_blank" rel="noopener nofollow">【目标检测】YOLOX ，YOLO系列的集大成者_yolox模型-CSDN博客</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/609370771" target="_blank" rel="noopener nofollow">YOLOX——SimOTA图文详解 - 知乎</a></p>
<p><a href="https://blog.csdn.net/qq_37541097/article/details/125132817" target="_blank" rel="noopener nofollow">YOLOX：2021年超越YOLOv5的先进检测技术-CSDN博客</a></p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.7082429502210648" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-04-22 09:22">2025-04-22 09:22</span>&nbsp;
<a href="https://www.cnblogs.com/carpell">carpell</a>&nbsp;
阅读(<span id="post_view_count">133</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18839936);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18839936', targetLink: 'https://www.cnblogs.com/carpell/p/18839936', title: '最详细最易懂的【YOLOX原理篇】' })">举报</a>
</div>
        