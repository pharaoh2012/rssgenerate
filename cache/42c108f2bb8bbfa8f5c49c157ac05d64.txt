
	<h1 class="postTitle"><a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/yexiaochai/p/18717840" title="发布于 2025-02-16 10:27">
    <span role="heading" aria-level="2">聊聊DeepSeek的MLA和GRPO</span>
    

</a>
</h1>
	<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<blockquote>
<p>关注公众号<strong>回复1</strong></p>
<p>获取<strong>一线、总监、高管《管理秘籍》</strong></p>
</blockquote>
<p>结合近期对DeepSeek的探索，对之前文章的内容做一些更新，主要是两个方面：<strong>训练过程的优化策略GRPO，以及架构侧的一个创新MLA</strong>。</p>
<h2 id="mla">MLA</h2>
<p><strong>MLA，被认为是实现DeepSeek高效推理和低成本训练的核心技术。</strong></p>
<p>MHA（多头注意力）通过多个注意力头并行工作捕捉序列特征，但面临高计算成本和显存占用；</p>
<p>MLA（多头潜在注意力）则通过低秩压缩优化键值矩阵，降低显存占用并提高推理效率。</p>
<p><strong>我知道各位读不懂，这种偏底层机制，对应用层来说也没必要懂，但其策略原理是需要学习的</strong>，所以这里用产品语言进行介绍。</p>
<p><strong>苹果</strong>作为一种普通的水果，我们对它的认知通常包括几个最重要的特征：</p>
<ol>
<li><strong>颜色：</strong>红色、绿色或黄色</li>
<li><strong>口感：</strong>甜的或酸的</li>
<li><strong>形状：</strong>圆形</li>
<li><strong>用途：</strong>可以食用或做成果汁</li>
</ol>
<p>这些是我们最常提到的苹果的基本特征，它们构成了<strong>我们对苹果的核心认知。</strong></p>
<h3 id="mha传统的全面分析">MHA：传统的全面分析</h3>
<p>MHA就是我们在分析苹果时，全面地考虑所有可能的特征，不仅仅是颜色、口感和形状，还可能包括苹果的<strong>品种、产地、种子类型等详细信息。</strong></p>
<p>在这个过程中，<strong>每个特征都会被单独处理</strong>。比如，有一个“颜色头”专注于苹果的颜色，另一个“口感头”专注于苹果的味道，等等。这些“头”并行工作，从不同角度抓取信息。</p>
<p>但这里问题也就出现了：<strong>虽然这个方法非常全面，可以捕捉到所有细节，但它也非常耗费资源。</strong></p>
<p>就像我们在吃苹果时，不仅仅关心它的味道和形状，可能还会考虑苹果的种子、品种等，结果就会浪费很多时间和精力。</p>
<p><strong>对于大规模的计算任务来说，这样的处理方式会导致计算成本和内存占用过高。</strong></p>
<h3 id="mla精简且高效的分析">MLA：精简且高效的分析</h3>
<p>MLA则不一样，他会更接近于人类的思维：对苹果的分析更加高效、精简。我们还是关注最核心的特征（比如颜色、口感和形状），但不再去详细分析那些不常用的特征（比如种子类型、品种等）。</p>
<p>MLA通过“低秩压缩”来优化这个过程，相当于精简和压缩信息，使得我们只存储和计算最重要的部分，而忽略那些次要信息。</p>
<p>这样，当我们吃苹果时，只关心最关键的体验，比如味道和颜色，而不需要过多关注种子或产地等。</p>
<p>这种方式让推理变得更快、更高效，而且显存占用更少，就像我们吃苹果时，如果只关心味道和形状，而不是去研究每个细节，体验感更好，且效率大大提高。</p>
<h3 id="信息分级">信息分级</h3>
<p>可以认为MLA是<strong>更加贴近人类思考和基本认知方式的机制。</strong></p>
<p>我们大脑在处理信息时，通常会<strong>把信息按照重要性进行分级</strong>，这样做是为了在处理大量信息时，能够高效地抓住最核心的部分。</p>
<p>例如，当我们认知“苹果”时，我们首先关注的是它的颜色、口感、形状这些关键信息，而不会去细究种子类型、产地等不那么关键的细节。</p>
<p>MLA机制的核心思想与此类似：<strong>它通过信息分级的方式，优先保留最关键的特征，忽略不那么重要的细节，从而提高推理效率。</strong></p>
<p>信息分级是通过<strong>对输入数据进行分析和训练来自动完成的。</strong></p>
<p>MLA机制通过算法和模型设计，识别出哪些特征是在当前任务中最重要的，哪些是次要的，并将它们分为不同的层级：</p>
<ol>
<li>第一层级是<strong>最重要的特征</strong>，比如苹果的颜色、口感、形状；这些是最常被提及、最常用于分类和判断的信息。</li>
<li>第二层级是<strong>次要信息</strong>，比如苹果的产地、种子类型等，这些信息可能在某些特殊情况下有用，但并不会常常被用到。</li>
<li>第三层级则是<strong>最不重要的信息</strong>，例如苹果的历史背景，这些信息在大多数情况下并不会影响我们的决策。</li>
</ol>
<p>通过这种信息分级，MLA能够让模型只关注最关键的部分，避免对不必要的细节进行计算，从而<strong>减少计算资源和内存占用。</strong></p>
<p>所以，<strong>实现MLA的难度非常高。</strong>这不仅仅是一个算法层面的挑战，它还涉及到如何有效地在训练过程中动态地进行信息分级，</p>
<h3 id="mla的局限性">MLA的局限性</h3>
<p>虽然MLA在推理和训练中的效率提升是显而易见的，但它也有一定的局限性。</p>
<p>因为MLA的核心是通过信息分级进行优化，<strong>所以它只关注最关键的特征，忽略了很多细节。</strong></p>
<p>这就像是你在学习一篇文章时，<strong>只背诵了关键章节，</strong>而没有记住整篇文章的所有细节。</p>
<p>相比之下，传统的<strong>MHA（多头注意力）就像是全文背诵。</strong>MHA会处理所有细节，不遗漏任何信息，虽然这样计算开销大，但它能确保更全面的推理，避免遗漏任何可能重要的信息。</p>
<p>因此，<strong>MLA的上限可能会比MHA低，</strong>因为它的目标是高效处理最关键的部分，但它可能会错过一些细节，导致在某些复杂任务或细节要求较高的场景中，性能不如MHA。</p>
<h2 id="grpo">GRPO</h2>
<p>DeepSeek-R1-Zero使用GRPO来进行基础推理能力的训练，而DeepSeek-R1则通过冷启动、面向推理的强化学习等手段，进一步强化了推理能力，并对模型的生成结果进行筛选和微调，确保模型输出更符合人类偏好。</p>
<p>所以，模型训练有很多路径，但每条路径的最终验证都需要白花花的银子，<strong>所以理论上的最优路径很可能胎死腹中。</strong></p>
<p>一般的公司也确实玩不起这一套，他们依赖的最优技术路径一直是基于成本最优作为考量。</p>
<p>DeepSeek-R1的成功就是其中一条路径的验证，现在甚至成了新的<strong>模型训练范式</strong>，过程中有两点需要注意：</p>
<ol>
<li>跳过微调环节，直接启用强化学习（GRPO框架）；</li>
<li>通过模型蒸馏技术直接将推理模式也转移到小模型中，这会让小模型的推理能力直线上升；</li>
</ol>
<h3 id="grpo-1">GRPO</h3>
<p>这里重点介绍下GRPO：一种强化学习算法，它专注于通过组内相对策略优化来降低训练成本，尤其是在模型推理任务中，GRPO能够显著提升模型的学习效率和稳定性。</p>
<p>在DeepSeek-R1中，通过在“组”内部估算基线来优化策略。具体来说，GRPO通过对多个策略进行比较，来决定如何在没有明确标签数据的情况下优化推理策略：</p>
<ol>
<li><strong>准确性奖励：</strong>评估模型在数学题目、代码编译等任务中的准确性。通过正向奖励鼓励模型生成正确的答案。</li>
<li><strong>格式奖励：</strong>引导模型在生成推理过程时，将思考过程放置在标签内（如&lt;think&gt;标签），促使模型更清晰地展示推理步骤。</li>
<li><strong>自进化奖励：</strong>GRPO使得DeepSeek-R1能够自主学习和调整推理策略，不仅提高了准确率，还提升了模型在复杂推理任务中的适应性。</li>
</ol>
<h3 id="数学题">数学题</h3>
<p>为了帮助各位更好的理解GRPO，依旧使用产品语言进行介绍：</p>
<p><strong>预训练就像是学习整本数学教材，掌握所有的数学知识和基本解题方法。</strong></p>
<p>在这个阶段，我们并不专注于解决具体的问题，而是广泛地吸收各种数学概念、公式和定理，确保我们对所有可能用到的知识都有一个全面的理解。这一过程为后续的实际应用奠定了基础。</p>
<p>微调就像是做试卷，<strong>通过不断做题来应用已学的知识，熟悉不同题型并提高解题速度和准确性。</strong></p>
<p>在微调阶段，我们不再只是学习基本概念，而是开始针对特定的题型进行训练，进一步熟悉如何运用已有的知识来解决具体的问题。</p>
<p><strong>GRPO与前面的两者不同，它不仅关注解题的最终结果（即正确与否），更重要的是它强调解题的过程。</strong></p>
<p>GRPO的核心目标是在做题的过程中不断反思，调整解题步骤，通过反馈优化整个解题策略，从而在下一道题上能表现得更加高效和准确：</p>
<p><strong>反馈和自我优化：</strong>就像做数学题时，我们不仅关注答案是否正确，更重要的是要反思自己的解题过程。</p>
<p>例如，如果我们发现某个步骤过于繁琐或者耗时过长，我们会在下一次做类似的题目时尝试改进方法。</p>
<p>通过不断的反馈和优化，我们逐渐找到了更简洁、高效的解题方式。</p>
<p><strong>在模型训练中：</strong>GRPO在强化学习中扮演着类似的角色。</p>
<p>它通过策略优化来提升模型的推理能力，但不仅仅是通过对最终答案的正确与否进行奖励，GRPO还关注模型如何得出答案的过程。</p>
<p>在这个过程中，模型通过不断<strong>“尝试-反馈”</strong>来改进推理步骤，并通过群体内的比较和优化，找到最优的推理路径。这就像是你在做数学题时，通过不断调整解题步骤，确保不仅最终的答案正确，而且解题过程也高效且清晰。</p>
<p>假设你在做一道数学题，传统的强化学习可能会直接给你一个奖励，告诉你答案是否正确，而GRPO则会根据你在解题过程中所使用的步骤、思考方式等提供反馈。</p>
<p>比如，如果你的某一步骤不够高效，GRPO会提醒你如何改进这个步骤，从而在下次做类似的题目时，能够更迅速地找到解决方案。</p>
<h2 id="结语">结语</h2>
<p>MLA和GRPO是DeepSeek两项核心技术创新，他们是实现高效推理与低成本训练的重要组成，为AI领域的技术进步提供了新的思路。</p>
<p>MLA通过低秩压缩和信息分级机制，显著降低了计算和内存消耗，同时保持了高效的推理能力，克服了传统MHA在大规模任务中的资源瓶颈。</p>
<p>而GRPO则通过强化学习中的策略优化，专注于推理过程的自我迭代与优化，不仅提升了模型的学习效率，还增强了其在复杂任务中的适应性和稳定性。</p>
<p>尽管MLA和GRPO在提升效率和降低成本方面表现出色，我们也需要认识到它们的局限性。</p>
<p>例如，MLA在信息分级过程中可能会忽略一些细节，导致在需要全面上下文信息的任务中表现不如MHA；</p>
<p>而GRPO跳过微调环节直接启用强化学习，虽然提升了训练效率，但在某些特定任务上可能需要额外的调整以确保稳定性。</p>
<p>未来，如何在高效推理与全面性能之间进一步权衡，将是DeepSeek及其他AI模型持续优化的关键方向。</p>
<p><img src="https://files.mdnice.com/user/25507/dfbdbae0-7236-421c-bbb0-badae3db3d76.png" alt="" loading="lazy"></p>

</div>
<div id="MySignature" role="contentinfo">
    <img id="view_img" src="https://img2022.cnblogs.com/blog/294743/202202/294743-20220216140902628-1163053035.png" width="80%" alt="" border="0">

</div>
<div class="clear"></div>

	<div class="postDesc">posted on 
<span id="post-date" data-last-update-days="0.4906064037662037" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-02-16 10:27">2025-02-16 10:27</span>&nbsp;
<a href="https://www.cnblogs.com/yexiaochai">叶小钗</a>&nbsp;
阅读(<span id="post_view_count">65</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18717840" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18717840);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18717840', targetLink: 'https://www.cnblogs.com/yexiaochai/p/18717840', title: '聊聊DeepSeek的MLA和GRPO' })">举报</a>
</div>
