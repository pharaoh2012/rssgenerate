
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/jinjiangongzuoshi/p/18826921" title="å‘å¸ƒäº 2025-04-15 16:02">
    <span role="heading" aria-level="2">å­¦ä¼šè¿™4ä¸ªçˆ¬è™«ç¥å™¨ï¼Œä¸‰åˆ†é’Ÿå°±èƒ½æå®šæ•°æ®é‡‡é›†ï¼</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p>åœ¨ä¿¡æ¯çˆ†ç‚¸çš„æ—¶ä»£ï¼Œæ•°æ®å°±æ˜¯è´¢å¯Œã€‚æ— è®ºæ˜¯å¸‚åœºè°ƒç ”ã€ç«å“åˆ†æï¼Œè¿˜æ˜¯ä¸ªäººå…´è¶£ç ”ç©¶ï¼Œå¿«é€Ÿä¸”å‡†ç¡®åœ°è·å–æ‰€éœ€æ•°æ®è‡³å…³é‡è¦ã€‚ä»Šå¤©ï¼Œå°±ä¸ºå¤§å®¶æ­ç§˜ 4 ä¸ªåŠŸèƒ½å®ç”¨ã€å¼ºå¤§çš„çˆ¬è™«ç¥å™¨ï¼Œæœ‰é€‚åˆé›¶ä»£ç æ— ç¼–ç åŸºç¡€çš„ï¼Œä¹Ÿæœ‰éœ€é€šè¿‡ç¼–ç¨‹è¿›è¡Œæ·±åº¦å®šåˆ¶çš„ï¼Œè®©ä½ è½»æ¾å®ç°ä¸‰åˆ†é’Ÿæå®šæ•°æ®é‡‡é›†ï¼</p>
<h2 id="1ç¥å™¨ä¸€å…«çˆªé±¼é‡‡é›†å™¨">1ã€ç¥å™¨ä¸€ï¼šå…«çˆªé±¼é‡‡é›†å™¨</h2>
<p>é¦–å…ˆç™»åœºçš„æ˜¯å…«çˆªé±¼é‡‡é›†å™¨ï¼Œå ªç§°ç®€å•æ˜“ç”¨çš„å…¨èƒ½é€‰æ‰‹ï¼Œå³ä½¿ä½ æ˜¯ç¼–ç¨‹å°ç™½ï¼Œä¹Ÿèƒ½è¿…é€Ÿä¸Šæ‰‹ã€‚</p>
<p>è¿™æ¬¾è½¯ä»¶ä»¥å…¶ç›´è§‚çš„å›¾å½¢åŒ–ç•Œé¢ã€å¯è§†åŒ–çš„æµç¨‹è®¾è®¡å’Œå¼ºå¤§çš„è‡ªå®šä¹‰åŠŸèƒ½è‘—ç§°ã€‚æ— éœ€ç¼–ç¨‹åŸºç¡€ï¼Œåªéœ€ç‚¹ç‚¹é¼ æ ‡ï¼Œè®¾ç½®å‡ ä¸ªè§„åˆ™ï¼Œå°±èƒ½è½»æ¾æŠ“å–ç½‘é¡µä¸Šçš„å„ç±»æ•°æ®ã€‚æ— è®ºæ˜¯ç”µå•†å•†å“ä¿¡æ¯ã€ç¤¾äº¤åª’ä½“å¸–å­è¿˜æ˜¯æ–°é—»ç½‘ç«™çš„æ–‡ç« å†…å®¹æˆ–æ˜¯åŠ¨æ€åŠ è½½çš„æ•°æ®ï¼Œå®ƒéƒ½èƒ½è½»æ¾åº”å¯¹ï¼Œæœ‰å…è´¹ç‰ˆä¹Ÿæœ‰æ”¶è´¹ç‰ˆã€‚æ ¹æ®éœ€æ±‚é€‰æ‹©å³å¯ã€‚</p>
<p><img src="https://files.mdnice.com/user/3808/1470f1aa-29f7-4474-9e21-e0f40d834583.png" alt="" loading="lazy"></p>
<p><strong>å®˜ç½‘ï¼š</strong> <code>https://www.bazhuayu.com/</code></p>
<p><strong>ä¸Šæ‰‹éš¾åº¦ï¼š</strong> ğŸŒŸ</p>
<p><strong>é€‚ç”¨åœºæ™¯ï¼š</strong> ç”µå•†ä»·æ ¼ç›‘æ§ã€æ–°é—»èšåˆã€ç¤¾äº¤åª’ä½“æ•°æ®æŠ“å–ç­‰ã€‚</p>
<h2 id="2ç¥å™¨äºŒweb-scraper">2ã€ç¥å™¨äºŒï¼šWeb Scraper</h2>
<p><code>Web Scraper</code> æ˜¯ä¸€æ¬¾åŸºäºæµè§ˆå™¨çš„é›¶ä»£ç çˆ¬è™«å·¥å…·ï¼Œæ”¯æŒåŠ¨æ€é¡µé¢æŠ“å–å’Œæ™ºèƒ½å…ƒç´ å®šä½ã€‚ä¸“é—¨ç”¨äºæ•°æ®é‡‡é›†ï¼Œåœ¨æµè§ˆå™¨ä¸Šç›´æ¥æŠ“ç½‘é¡µï¼Œé€šè¿‡æ¨¡æ‹Ÿäººç±»æµè§ˆè¡Œä¸ºå®ç°ç½‘é¡µæ•°æ®è‡ªåŠ¨åŒ–é‡‡é›†ã€‚å…¶æ ¸å¿ƒåŠŸèƒ½åŒ…æ‹¬æ™ºèƒ½å…ƒç´ é€‰æ‹©å™¨ã€åŠ¨æ€é¡µé¢è§£æå’Œå¤šå±‚çº§æ•°æ®æŠ“å–ï¼Œæ”¯æŒæ–‡æœ¬ã€å›¾ç‰‡ã€é“¾æ¥ç­‰å¤šç§æ•°æ®ç±»å‹ã€‚<br>
<img src="https://files.mdnice.com/user/3808/20458a18-189e-4cb0-b5d7-0d4db48e73a5.png" alt="" loading="lazy"></p>
<p><code>Web Scraper</code>æ’ä»¶æ”¯æŒç¿»é¡µã€ç™»å½•è®¤è¯å’Œç®€å•æ•°æ®æ¸…æ´—ï¼Œè€Œä¸”æ”¯æŒå¤šç§æ•°æ®ç±»å‹é‡‡é›†ï¼Œå¹¶å¯å°†é‡‡é›†åˆ°çš„æ•°æ®å¯¼å‡ºä¸ºExcelã€CSVç­‰å¤šç§æ ¼å¼ã€‚</p>
<p><strong>å®˜ç½‘ï¼š</strong> <code>https://webscraper.io/</code></p>
<p><strong>ä¸Šæ‰‹éš¾åº¦ï¼š</strong> ğŸŒŸ</p>
<h2 id="3ç¥å™¨ä¸‰scrapy">3ã€ç¥å™¨ä¸‰ï¼šScrapy</h2>
<p><code>Scrapy</code> æ˜¯ä¸€æ¬¾åŸºäº Python çš„å¼€æºçˆ¬è™«æ¡†æ¶ï¼Œé€‚åˆæœ‰ä¸€å®šç¼–ç¨‹åŸºç¡€çš„ä¸“ä¸šå¼€å‘è€…ã€‚å®ƒå…·æœ‰é«˜åº¦çš„çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ï¼Œå¼€å‘è€…å¯ä»¥æ ¹æ®é¡¹ç›®éœ€æ±‚ï¼Œè‡ªç”±å®šåˆ¶çˆ¬è™«åŠŸèƒ½ã€‚ä¸”Scrapyä»¥å…¶é«˜æ•ˆçš„å¼‚æ­¥è¯·æ±‚ã€å¼ºå¤§çš„æ‰©å±•æ€§å’Œä¸°å¯Œçš„ä¸­é—´ä»¶è€Œé—»åã€‚å¯¹äºæœ‰ä¸€å®šç¼–ç¨‹åŸºç¡€çš„æœ‹å‹æ¥è¯´ï¼ŒScrapyæ˜¯æ‰“é€ å®šåˆ¶åŒ–çˆ¬è™«çš„ä¸äºŒä¹‹é€‰ã€‚</p>
<p><strong>å®‰è£…ï¼š</strong></p>
<pre><code>pip install scrapy
</code></pre>
<p><strong>ä¸Šæ‰‹éš¾åº¦ï¼š</strong> ğŸŒŸğŸŒŸğŸŒŸ</p>
<p><strong>é€‚ç”¨åœºæ™¯ï¼š</strong> å¤§è§„æ¨¡ç½‘ç«™çˆ¬å–ã€æ•°æ®æ¸…æ´—ä¸å­˜å‚¨ã€å¤æ‚é€»è¾‘å¤„ç†ã€è‡ªç”±å®šåˆ¶çˆ¬è™«åŠŸèƒ½ã€‚</p>
<p><strong>ç¤ºä¾‹ï¼š</strong> ä¸‹é¢ä»¥Scrapyçˆ¬å–è±†ç“£ç”µå½±ä¸ºä¾‹ï¼š</p>
<p>1ã€é¦–å…ˆï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„Scrapyé¡¹ç›®ï¼š</p>
<pre><code class="language-bash">scrapy startproject douban_movie
cd douban_movie
</code></pre>
<ol start="2">
<li>æ–°å»ºitems.pyï¼Œå®šä¹‰æˆ‘ä»¬è¦æŠ“å–çš„æ•°æ®ç»“æ„ï¼š</li>
</ol>
<pre><code class="language-python">import scrapy

class DoubanMovieItem(scrapy.Item):
    # ç”µå½±æ’å
    ranking = scrapy.Field()
    # ç”µå½±åç§°
    title = scrapy.Field()
    # ç”µå½±è¯„åˆ†
    score = scrapy.Field()
    # è¯„è®ºäººæ•°
    comment_num = scrapy.Field()
    # ç”µå½±ç®€ä»‹
    quote = scrapy.Field()
    # ç”µå½±è¯¦æƒ…é¡µé“¾æ¥
    detail_url = scrapy.Field()
    # ç”µå½±å°é¢å›¾ç‰‡é“¾æ¥
    cover_url = scrapy.Field()
</code></pre>
<ol start="3">
<li>åˆ›å»ºSpiderï¼Œåœ¨spidersç›®å½•ä¸‹åˆ›å»ºdouban_spider.pyï¼š</li>
</ol>
<pre><code class="language-python">import scrapy
from douban_movie.items import DoubanMovieItem
from scrapy.http import Request

class DoubanSpider(scrapy.Spider):
    name = "douban"
    allowed_domains = ["movie.douban.com"]
    start_urls = ["https://movie.douban.com/top250"]
    
    # è®¾ç½®è‡ªå®šä¹‰è¯·æ±‚å¤´
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'
    }
    
    def start_requests(self):
        for url in self.start_urls:
            yield Request(url, headers=self.headers, callback=self.parse)
    
    def parse(self, response):
        item = DoubanMovieItem()
        movies = response.xpath('//ol[@class="grid_view"]/li')
        
        for movie in movies:
            item['ranking'] = movie.xpath(
                './/div[@class="pic"]/em/text()').extract()[0]
            item['title'] = movie.xpath(
                './/div[@class="hd"]/a/span[1]/text()').extract()[0]
            item['score'] = movie.xpath(
                './/div[@class="star"]/span[@class="rating_num"]/text()').extract()[0]
            item['comment_num'] = movie.xpath(
                './/div[@class="star"]/span[4]/text()').re(r'(\d+)')[0]
            item['quote'] = movie.xpath(
                './/p[@class="quote"]/span/text()').extract()[0] if movie.xpath('.//p[@class="quote"]/span/text()') else ''
            item['detail_url'] = movie.xpath(
                './/div[@class="hd"]/a/@href').extract()[0]
            item['cover_url'] = movie.xpath(
                './/div[@class="pic"]/a/img/@src').extract()[0]
            
            yield item
        
        # å¤„ç†ä¸‹ä¸€é¡µ
        next_url = response.xpath('//span[@class="next"]/a/@href').extract()
        if next_url:
            next_url = 'https://movie.douban.com/top250' + next_url[0]
            yield Request(next_url, headers=self.headers, callback=self.parse)
</code></pre>
<ol start="4">
<li>åœ¨settings.pyä¸­æ·»åŠ ä»¥ä¸‹é…ç½®ï¼š</li>
</ol>
<pre><code class="language-python"># éµå®ˆrobots.txtè§„åˆ™
ROBOTSTXT_OBEY = False

# è®¾ç½®ä¸‹è½½å»¶è¿Ÿ
DOWNLOAD_DELAY = 2

# å¯ç”¨Pipeline
ITEM_PIPELINES = {
    'douban_movie.pipelines.DoubanMoviePipeline': 300,
}

# è®¾ç½®è¯·æ±‚å¤´
DEFAULT_REQUEST_HEADERS = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'en',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'
}
</code></pre>
<ol start="5">
<li>åˆ›å»ºPipelineï¼Œåˆ›å»ºæ•°æ®å¤„ç†ç®¡é“ï¼š</li>
</ol>
<pre><code class="language-python">import json
import pymongo
from scrapy.exceptions import DropItem

class DoubanMoviePipeline(object):
    def __init__(self):
        # å¯é€‰ï¼šä¿å­˜åˆ°JSONæ–‡ä»¶
        self.file = open('douban_movie.json', 'w', encoding='utf-8')
        
        # å¯é€‰ï¼šè¿æ¥MongoDB
        # self.client = pymongo.MongoClient('localhost', 27017)
        # self.db = self.client['douban']
        # self.collection = self.db['movies']
    
    def process_item(self, item, spider):
        # æ£€æŸ¥å¿…è¦å­—æ®µæ˜¯å¦å­˜åœ¨
        if not all(item.get(field) for field in ['title', 'score', 'detail_url']):
            raise DropItem("Missing required fields in %s" % item)
        
        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        line = json.dumps(dict(item), ensure_ascii=False) + "\n"
        self.file.write(line)
        
        # ä¿å­˜åˆ°MongoDB
        # self.collection.insert_one(dict(item))
        
        return item
    
    def close_spider(self, spider):
        self.file.close()
        # self.client.close()
</code></pre>
<ol start="6">
<li>è¿è¡Œä»¥ä¸‹å‘½ä»¤å¯åŠ¨çˆ¬è™«ï¼š</li>
</ol>
<pre><code class="language-bash">scrapy crawl douban -o movies.csv
æˆ–è€…å°†ç»“æœä¿å­˜ä¸ºJSONæ ¼å¼ï¼š
scrapy crawl douban -o movies.json
</code></pre>
<p>å¯ä»¥çœ‹å‡ºï¼ŒScrapyè™½çµæ´»ï¼Œä½†ä½¿ç”¨èµ·æ¥è¿˜æ˜¯æœ‰ç‚¹éš¾åº¦çš„ï¼Œå¦‚æœæ²¡äº›ç¼–ç åŸºç¡€çš„åŒå­¦ï¼Œä¸å¤ªå¥½é©¾é©­ã€‚</p>
<h2 id="4ç¥å™¨å››beautiful-soup">4ã€ç¥å™¨å››ï¼šBeautiful Soup</h2>
<p><code>Beautiful Soup</code> ä¹Ÿæ˜¯ä¸€ä¸ª Python åº“ï¼Œä¸“æ³¨äºä»HTML å’Œ XML æ–‡ä»¶ä¸­æå–æ•°æ®ã€‚ç›¸æ¯”Scrapyå®ƒç®€å•æ˜“ç”¨ï¼Œèƒ½å¤Ÿå¿«é€Ÿæå–ç½‘é¡µä¸­çš„ç‰¹å®šä¿¡æ¯ï¼Œæ˜¯ç½‘é¡µè§£æçš„å¾—åŠ›åŠ©æ‰‹ã€‚</p>
<p><strong>ä¸Šæ‰‹éš¾åº¦ï¼š</strong> ğŸŒŸğŸŒŸ</p>
<p><strong>é€‚ç”¨åœºæ™¯ï¼š</strong> å°è§„æ¨¡æ•°æ®æŠ“å–ã€ç½‘é¡µå†…å®¹æå–ã€æ•°æ®æ¸…æ´—ã€‚</p>
<p><strong>ä½¿ç”¨ç¤ºä¾‹ï¼š</strong></p>
<p>1ã€å®‰è£… Beautiful Soup</p>
<pre><code>pip install beautifulsoup4
</code></pre>
<p>å¦‚æœéœ€è¦ä½¿ç”¨å…¶ä»–è§£æå™¨ï¼Œè¿˜éœ€è¦å®‰è£…ï¼š</p>
<pre><code>pip install lxml  # æ¨èä½¿ç”¨ï¼Œé€Ÿåº¦å¿«
pip install html5lib  # å®¹é”™æ€§å¥½
</code></pre>
<p>2ã€è§£æ HTML æ–‡æ¡£</p>
<pre><code class="language-python">from bs4 import BeautifulSoup
import requests

# è·å–ç½‘é¡µå†…å®¹
url = "https://example.com"
response = requests.get(url)
html_content = response.text

# åˆ›å»º BeautifulSoup å¯¹è±¡
soup = BeautifulSoup(html_content, 'lxml')  # ä½¿ç”¨ lxml è§£æå™¨

# è·å–ç¬¬ä¸€ä¸ª &lt;title&gt; æ ‡ç­¾
title_tag = soup.title
print(title_tag)          # &lt;title&gt;Example Domain&lt;/title&gt;
print(title_tag.string)   # Example Domain

# è·å–ç¬¬ä¸€ä¸ª &lt;p&gt; æ ‡ç­¾
first_p = soup.p
print(first_p.get_text())  # è·å–æ ‡ç­¾å†…çš„æ–‡æœ¬å†…å®¹
</code></pre>
<h2 id="æœ€å">æœ€å</h2>
<p>é€‰æ‹©ä¸€æ¬¾æœ€é€‚åˆä½ çš„è½¯ä»¶ï¼ŒåŠ¨æ‰‹å®è·µï¼Œè®©æ•°æ®æˆä¸ºä½ æ¢ç´¢ä¸–ç•Œã€åˆ›é€ ä»·å€¼çš„å¼ºå¤§æ­¦å™¨ã€‚<br>
å½“ç„¶ï¼Œè®°å¾—ï¼Œåˆæ³•åˆè§„æ˜¯ä½¿ç”¨çˆ¬è™«çš„å‰æï¼Œå°Šé‡ç½‘ç«™çš„ä½¿ç”¨æ¡æ¬¾ï¼Œä¿æŠ¤æ•°æ®éšç§ï¼Œå¾ˆé‡è¦ï¼</p>

</div>
<div id="MySignature" role="contentinfo">
    æŠ€æœ¯æ”¹å˜ä¸–ç•Œï¼
         --ç‹‚è¯—ç»å‰‘
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.01407643306712963" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-04-15 16:02">2025-04-15 16:02</span>&nbsp;
<a href="https://www.cnblogs.com/jinjiangongzuoshi">ç‹‚å¸ˆ</a>&nbsp;
é˜…è¯»(<span id="post_view_count">0</span>)&nbsp;
è¯„è®º(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18826921);return false;">æ”¶è—</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18826921', targetLink: 'https://www.cnblogs.com/jinjiangongzuoshi/p/18826921', title: 'å­¦ä¼šè¿™4ä¸ªçˆ¬è™«ç¥å™¨ï¼Œä¸‰åˆ†é’Ÿå°±èƒ½æå®šæ•°æ®é‡‡é›†ï¼' })">ä¸¾æŠ¥</a>
</div>
        