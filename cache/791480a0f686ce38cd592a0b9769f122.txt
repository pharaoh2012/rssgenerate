
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/qhhfRA/p/18994130" title="发布于 2025-07-20 16:40">
    <span role="heading" aria-level="2">AI 发展 &amp;&amp; MCP</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                    <div id="cnblogs_post_description" style="display: none">
        
        AI发展——计算机视觉、ChatGPT、Sora、DeepSeek、生成式AI。什么是MCP，Prompt、LLM、Function Call、Agent、MCP是什么，各自区别；MCP如何工作，MCP架构、MCP Server工作原理，Cursor如何使用MCP，自定义MCP Server
    </div>
<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p><strong>文章目录</strong></p>
<p>一、AI 的发展历程</p>
<p>二、关键技术演进</p>
<ul>
<li>2.1 架构演进</li>
<li>2.2 训练方法演进</li>
<li>2.3 应用领域扩展</li>
</ul>
<p>三、重要里程碑产品/工具</p>
<ul>
<li>3.1 开发框架</li>
<li>3.2 AI 应用</li>
</ul>
<p>四、MCP</p>
<ul>
<li>
<p>4.1 什么是MCP（Model Context Protocol）</p>
<ul>
<li>4.1.1 定义</li>
<li>4.1.2 为什么MCP是一个突破</li>
</ul>
</li>
<li>
<p>4.2 Prompt/LLM/FunctionCall/Agent/MCP区别（为什么是 MCP）</p>
<ul>
<li>4.2.1 具体示例</li>
<li>4.2.2 思考</li>
</ul>
</li>
<li>
<p>4.3 MCP 如何工作</p>
<ul>
<li>4.3.1 MCP 架构</li>
<li>4.3.2 MCP Server 的工作原理</li>
</ul>
</li>
<li>
<p>4.4 如何使用 MCP</p>
<ul>
<li>Cursor MCP 使用示例（自定义mcp server）</li>
</ul>
</li>
<li>
<p>MCP 的一些资源</p>
</li>
</ul>
<h1 id="一ai-的发展历程">一、AI 的发展历程</h1>
<ul>
<li>
<p>早期探索阶段（1950-1969）</p>
<ul>
<li>
<p>1950：图灵测试（Turing Test）提出</p>
</li>
<li>
<p>1956：达特茅斯会议，"人工智能"术语诞生</p>
</li>
<li>
<p>1958：感知器（Perceptron）发明，最早的人工神经网络</p>
</li>
<li>
<p>1964：ELIZA 问世，首个聊天机器人</p>
</li>
<li>
<p>1965：专家系统开始发展</p>
</li>
</ul>
</li>
<li>
<p>第一次低谷（1970-1979）</p>
<ul>
<li>
<p>AI 研究经费削减</p>
</li>
<li>
<p>计算能力限制</p>
</li>
<li>
<p>理论基础不足</p>
</li>
<li>
<p>实际应用困难</p>
</li>
</ul>
</li>
<li>
<p>专家系统繁荣（1980-1987）</p>
<ul>
<li>
<p>1980s：专家系统广泛应用</p>
<ul>
<li>XCON：Digital Equipment 公司的配置系统</li>
<li>MYCIN：医疗诊断系统</li>
</ul>
</li>
<li>
<p>1982：Hopfield 网络提出</p>
</li>
<li>
<p>1986：反向传播算法（Backpropagation）发展</p>
</li>
</ul>
</li>
<li>
<p>第二次低谷（1988-1993）</p>
<ul>
<li>
<p>专家系统维护成本高</p>
</li>
<li>
<p>适应性差</p>
</li>
<li>
<p>通用性不足</p>
</li>
</ul>
</li>
<li>
<p>现代AI起步（1994-2010）</p>
<ul>
<li>
<p>1997：IBM Deep Blue 战胜国际象棋冠军卡斯帕罗夫</p>
</li>
<li>
<p>1998：支持向量机（Support Vector Machine，SVM）理论成熟</p>
</li>
<li>
<p>2006：深度学习概念提出</p>
<ul>
<li>深度信念网络</li>
<li>预训练技术</li>
</ul>
</li>
<li>
<p>2010：ImageNet 大规模视觉识别挑战赛开始</p>
</li>
</ul>
</li>
<li>
<p>深度学习爆发（2011-2018）</p>
<ul>
<li>
<p>2011：IBM Watson 在智力问答节目获胜</p>
</li>
<li>
<p>2012：AlexNet 在 ImageNet 比赛中取得突破性进展</p>
</li>
<li>
<p>2014：</p>
<ul>
<li>GAN（生成对抗网络，Generative Adversarial Network）提出</li>
<li>DeepMind 被 Google 收购</li>
</ul>
</li>
<li>
<p>2015：</p>
<ul>
<li>ResNet 残差网络提出</li>
<li>谷歌开源 TensorFlow 框架</li>
</ul>
</li>
<li>
<p>2016：</p>
<ul>
<li>AlphaGo 战胜李世石</li>
<li>OpenAI 成立</li>
</ul>
</li>
<li>
<p>2017：</p>
<ul>
<li>Transformer 架构提出</li>
<li>AlphaGo Zero 发布</li>
</ul>
</li>
<li>
<p>2018：BERT（Bidirectional Encoder Representation from Transformers） 模型发布</p>
</li>
</ul>
</li>
<li>
<p><strong>大语言模型时代（2019-2022）</strong></p>
<ul>
<li>
<p>2019：</p>
<ul>
<li>GPT-2 发布</li>
<li>BERT 得到广泛应用</li>
</ul>
</li>
<li>
<p>2020：</p>
<ul>
<li>GPT-3 发布</li>
<li>DALL-E 首次亮相</li>
</ul>
</li>
<li>
<p>2021：</p>
<ul>
<li>Codex（GitHub Copilot 背后的模型）发布</li>
<li>PaLM 模型发布</li>
</ul>
</li>
<li>
<p>2022：</p>
<ul>
<li><strong>ChatGPT</strong> <strong>发布</strong></li>
<li>Stable Diffusion 发布</li>
<li>DALL-E 2 发布</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>AI 大爆发（2023-至今）</strong></p>
<ul>
<li>
<p>2023（AI大模型竞争年 GPT系列、Claude系列1.0-2.1）：</p>
<ul>
<li>GPT-4 发布</li>
<li>Claude 系列模型发布</li>
<li>Midjourney V5</li>
<li>Stable Diffusion XL</li>
<li>Google发布Gemini</li>
<li>DALL-E 3</li>
<li>Code Llama</li>
<li>Llama 2</li>
</ul>
</li>
<li>
<p>2024（AI应用落地年）：</p>
<ul>
<li>Claude 3</li>
<li>Gemini 1.5</li>
<li>GPT-4 Turbo</li>
<li><strong>Sora</strong>（OpenAI 视频生成模型）</li>
</ul>
</li>
<li>
<p>2025</p>
<ul>
<li><strong>Deepseek</strong></li>
<li>GPT-4</li>
<li>Claude 3.5，Claude 4</li>
<li>AI Agent</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="二关键技术演进">二、关键技术演进</h1>
<h2 id="21-架构演进">2.1 架构演进</h2>
<ul>
<li>感知器 → 多层感知器</li>
<li>CNN → RNN → LSTM</li>
<li>Transformer</li>
<li>Diffusion Models</li>
</ul>
<h2 id="22-训练方法演进">2.2 训练方法演进</h2>
<ul>
<li>监督学习</li>
<li>无监督学习</li>
<li>强化学习</li>
<li>半监督学习</li>
<li>自监督学习</li>
<li>提示学习（Prompt Learning）</li>
</ul>
<h2 id="23-应用领域扩展">2.3 应用领域扩展</h2>
<ul>
<li>计算机视觉：
<ul>
<li>图像分类</li>
<li>目标检测</li>
<li>图像生成</li>
<li>视频生成</li>
</ul>
</li>
<li>自然语言处理：
<ul>
<li>机器翻译</li>
<li>文本生成</li>
<li>问答系统</li>
<li>对话系统</li>
</ul>
</li>
<li>语音技术：
<ul>
<li>语音识别</li>
<li>语音合成</li>
<li>声音克隆</li>
</ul>
</li>
<li>多模态：
<ul>
<li>图文理解</li>
<li>跨模态生成</li>
<li>3D 生成</li>
</ul>
</li>
</ul>
<h1 id="三重要里程碑产品工具">三、重要里程碑产品/工具</h1>
<h2 id="31-开发框架">3.1 开发框架</h2>
<ul>
<li>TensorFlow</li>
<li>PyTorch</li>
<li>JAX</li>
<li>Keras</li>
<li>Hugging Face Transformers</li>
</ul>
<h2 id="32-ai-应用">3.2 AI 应用</h2>
<ul>
<li>语言模型：
<ul>
<li>ChatGPT</li>
<li>Claude</li>
<li>Gemini</li>
<li>Llama</li>
</ul>
</li>
<li>图像生成：
<ul>
<li>DALL-E</li>
<li>Midjourney</li>
<li>Stable Diffusion</li>
</ul>
</li>
<li>代码生成：
<ul>
<li>GitHub Copilot</li>
<li>Amazon CodeWhisperer</li>
<li>Cursor</li>
</ul>
</li>
</ul>
<h1 id="四mcp">四、MCP</h1>
<p>在2024.11月之前，AI 技术虽然日新月异，但是 AI 应用层的开发并没有多少新的东西，大体还是Prompt、RAG（Retrieval Augmented Generation）、Agent。</p>
<p>但是自从去年 11 月底 Claude(Anthropic) 主导发布了 MCP(Model Context Protocol 模型上下文协议) 后，AI 应用层的开发算是进入了新的时代。</p>
<p><strong>MCP逐渐被接受，是因为MCP是开放标准。在AI项目开发中可以发现，集成AI模型复杂</strong>。MCP的优势：一是开放标准利于服务商开发API，二是避免重复造轮子，可利用现有MCP服务增强Agent。</p>
<p>查看全球热点趋势的网站 <a href="https://trends.google.com/trends/" target="_blank" rel="noopener nofollow">Google Trends</a><br>
<img alt="4-1" loading="lazy" data-src="https://img2024.cnblogs.com/blog/3047087/202507/3047087-20250720163710882-1791818113.png" class="lazyload"></p>
<h2 id="41-什么是mcpmodel-context-protocol">4.1 什么是MCP（Model Context Protocol）</h2>
<h3 id="411-定义">4.1.1 定义</h3>
<p>MCP（Model Context Protocol，模型上下文协议） ，2024年11月底，由 Anthropic（Claude的母公司） 推出的一种开放标准，是一种为大语言模型（LLM）和AI Agent生态设计的<strong>上下文管理与交互协议</strong>，帮助 AI 模型更好地理解上下文。它的核心目标是<strong>标准化模型与外部世界（如插件、工具、数据库、其他模型等）之间的上下文信息交换和调用方式</strong>。定义了模型如何获取上下文、调用外部函数、管理会话状态、处理多轮对话等机制。</p>
<p>简单来说，<strong>MCP</strong> 是一个 <strong>AI 大模型的标准化工具箱</strong>。大模型可以通过这些工具与 <strong>外界互动</strong>，<strong>获取信息</strong>，并 <strong>完成具体任务</strong>。</p>
<p><strong>为什么需要MCP？</strong></p>
<ul>
<li><strong>标准化交互</strong>：不同AI模型、插件、工具之间的接口和数据格式不统一，MCP提供统一协议，降低集成和扩展成本。</li>
<li><strong>丰富上下文</strong>：模型可以获取更丰富的上下文信息（如用户历史、环境变量、外部知识），提升智能和个性化能力。</li>
<li><strong>安全与可控</strong>：通过协议层控制模型能访问哪些功能、数据，提升安全性和可控性。</li>
<li><strong>多模型/多Agent协作</strong>：支持多个模型/Agent之间的协作与分工，适合复杂任务和场景。</li>
</ul>
<p><strong>主要内容包括：</strong></p>
<ul>
<li>上下文管理：定义如何组织、传递和更新模型的上下文信息（如历史对话、外部知识、用户状态等）。</li>
<li>多模型/多Agent协作：支持多个模型或Agent之间的信息共享与协作。</li>
<li>插件/工具调用：标准化模型调用外部工具、API、数据库等的接口协议。</li>
<li>可扩展性：为未来更多AI能力的集成和生态建设打下基础。</li>
</ul>
<p>MCP 服务器可以提供三种主要类型的功能：</p>
<ul>
<li>资源：客户端可以读取的类似文件的数据（例如 API 响应或文件内容）</li>
<li>工具：可由 LLM 调用的函数（经用户批准）</li>
<li>提示：预先编写的模板，帮助用户完成特定任务</li>
</ul>
<p>价值：MCP是AI时代的USB协议</p>
<p>举个例子，在过去，为了让大模型等 AI 应用使用数据，要么复制粘贴，要么上传知识库，非常局限。</p>
<p>即使是最强大模型也会受到数据隔离的限制，形成信息孤岛，要做出更强的大模型，每个新数据源都需要自己重新定制实现，使真正互联的系统难以扩展，存在很多的局限性。</p>
<p>现在，MCP 可以直接在 AI 与数据（包括本地数据和互联网数据）之间架起一座桥梁，通过 MCP 服务器和 MCP 客户端，大家只要都遵循这套协议，就能实现“万物互联”。有了MCP，可以和数据和文件系统、开发工具、Web 和浏览器自动化、生产力和通信、各种社区生态能力全部集成，实现强大的协作工作能力，它的价值远不可估量。</p>
<h3 id="412-为什么mcp是一个突破">4.1.2 为什么MCP是一个突破</h3>
<p>我们知道过去一年时间，AI 模型的发展非常迅速，从 GPT 4 到 Claude Sonnet 3.5 到 Deepseek R1，推理和幻觉都进步的非常明显。</p>
<p>新的 AI 应用也很多，但我们都能感受到的一点是，目前市场上的 AI 应用基本都是全新的服务，和我们原来常用的服务和系统并没有集成，换句话说，AI 模型和我们已有系统集成发展的很缓慢。</p>
<p>例如我们目前还不能<strong>同时</strong>通过某个 AI 应用来做到联网搜索、发送邮件、发布自己的博客等等，这些功能单个实现都不是很难，但是如果要全部集成到一个系统里面，就会变得遥不可及。</p>
<p>如果你还没有具体的感受，我们可以思考一下日常开发中，想象一下在 IDE 中，我们可以通过 IDE 的 AI 来完成下面这些工作。</p>
<ul>
<li>询问 AI 来查询本地数据库已有的数据来辅助开发</li>
<li>询问 AI 搜索 Github Issue 来判断某问题是不是已知的bug</li>
<li>通过 AI 将某个 PR 的意见发送给同事的即时通讯软件(例如 Slack)来 Code Review</li>
<li>通过 AI 查询甚至修改当前 AWS、Azure 的配置来完成部署</li>
</ul>
<p>以上谈到的这些功能通过 MCP 目前正在变为现实，大家可以关注 <a href="https://docs.cursor.com/context/model-context-protocol" target="_blank" rel="noopener nofollow">Cursor MCP</a> 和 <a href="https://www.youtube.com/watch?v=Y_kaQmhGmZk" target="_blank" rel="noopener nofollow">Windsurf MCP</a> 获取更多的信息。可以试试用 Cursor MCP + <a href="https://browsertools.agentdesk.ai/installation" target="_blank" rel="noopener nofollow">browsertools</a> 插件来体验一下在 Cursor 中自动获取 Chrome dev tools console log 的能力。</p>
<p>为什么 AI 集成已有服务的进展这么缓慢？这里面有很多的原因，一方面是企业级的数据很敏感，大多数企业都要很长的时间和流程来动。另一个方面是技术方面，我们缺少一个开放的、通用的、有共识的协议标准。</p>
<p>MCP 就是 Claude(Anthropic) 主导发布的一个开放的、通用的、有共识的协议标准，如果我们对 AI 模型熟悉，想必对 Anthropic 这个公司不会陌生，2025.5.22他们发布了 Claude Sonnet 4、Claude Opus 4 的模型（更精准地执行指令、且在“记忆”能力上实现提升，在需要持续专注且涉及数千步骤的长期任务中保持稳定性能），到目前为止应该还是最强的编程 AI 模型。在一项对比不同大型语言模型软件工程任务表现的基准测试中，Anthropic的两款模型击败了OpenAI的最新模型，而谷歌的最佳模型则表现落后。</p>
<p><a href="https://www.anthropic.com/" target="_blank" rel="noopener nofollow">https://www.anthropic.com/</a></p>
<p><img alt="4-2" loading="lazy" data-src="https://img2024.cnblogs.com/blog/3047087/202507/3047087-20250720163735733-1954581735.png" class="lazyload"></p>
<blockquote>
<p>这里还是要多提一句，这个协议的发布最好机会应该是属于 OpenAI 的，如果 OpenAI 刚发布 GPT 时就推动协议，相信大家都不会拒绝，但是 OpenAI 变成了 CloseAI，只发布了一个封闭的 GPTs，这种需要主导和共识的标准协议一般很难社区自发形成，一般由行业巨头来主导。</p>
</blockquote>
<p>Claude 发布了 MCP 后，官方的 Claude Desktop 就开放了 MCP 功能，并且推动了开源组织 <a href="https://github.com/modelcontextprotocol" target="_blank" rel="noopener nofollow">Model Context Protocol</a>，由不同的公司和社区进行参与，例如下面就列举了一些由不同组织发布 MCP 服务器的例子。</p>
<p><strong>MCP 官方集成教学</strong></p>
<ul>
<li><a href="https://github.com/modelcontextprotocol/servers/blob/main/src/git" target="_blank" rel="noopener nofollow">Git</a> - Git 读取、操作、搜索。</li>
<li><a href="https://github.com/modelcontextprotocol/servers/blob/main/src/github" target="_blank" rel="noopener nofollow">GitHub</a> - Repo 管理、文件操作和 GitHub API 集成。</li>
<li><a href="https://github.com/modelcontextprotocol/servers/blob/main/src/google-maps" target="_blank" rel="noopener nofollow">Google Maps</a> - 集成 Google Map 获取位置信息。</li>
<li><a href="https://github.com/modelcontextprotocol/servers/blob/main/src/postgres" target="_blank" rel="noopener nofollow">PostgreSQL</a> - 只读数据库查询。</li>
<li><a href="https://github.com/modelcontextprotocol/servers/blob/main/src/slack" target="_blank" rel="noopener nofollow">Slack</a> - Slack 消息发送和查询。</li>
</ul>
<p><strong>第三方平台官方支持 MCP 的例子</strong></p>
<p>由第三方平台构建的 MCP 服务器。</p>
<ul>
<li><a href="https://github.com/grafana/mcp-grafana" target="_blank" rel="noopener nofollow">Grafana</a> - 在 Grafana 中搜索查询数据。</li>
<li><a href="https://github.com/JetBrains/mcp-jetbrains" target="_blank" rel="noopener nofollow">JetBrains</a> – JetBrains IDEs。</li>
<li><a href="https://github.com/stripe/agent-toolkit" target="_blank" rel="noopener nofollow">Stripe</a> - 与Stripe API交互。</li>
</ul>
<p><strong>社区 MCP 服务器</strong></p>
<p>下面是一些由开源社区开发和维护的 MCP 服务器。</p>
<ul>
<li><a href="https://github.com/rishikavikondala/mcp-server-aws" target="_blank" rel="noopener nofollow">AWS</a> - 用 LLM 操作 AWS 资源。</li>
<li><a href="https://github.com/sooperset/mcp-atlassian" target="_blank" rel="noopener nofollow">Atlassian</a> - 与 Confluence 和 Jira 进行交互，包括搜索/查询 Confluence 空间/页面，访问 Jira Issue 和项目。</li>
<li><a href="https://github.com/v-3/google-calendar" target="_blank" rel="noopener nofollow">Google Calendar</a> - 与 Google 日历集成，日程安排，查找时间，并添加/删除事件。</li>
<li><a href="https://github.com/Flux159/mcp-server-kubernetes" target="_blank" rel="noopener nofollow">Kubernetes</a> - 连接到 Kubernetes 集群并管理 pods、deployments 和 services。</li>
<li><a href="https://github.com/EnesCinr/twitter-mcp" target="_blank" rel="noopener nofollow">X (Twitter)</a> - 与 Twitter API 交互。发布推文并通过查询搜索推文。</li>
<li><a href="https://github.com/ZubeidHendricks/youtube-mcp-server" target="_blank" rel="noopener nofollow">YouTube</a> - 与 YouTube API 集成，视频管理、短视频创作等。</li>
</ul>
<h2 id="42-promptllmfunctioncallagentmcp区别为什么是-mcp">4.2 Prompt/LLM/FunctionCall/Agent/MCP区别（为什么是 MCP）</h2>
<p>看到这里你可能有一个问题，在 23 年 OpenAI 发布 GPT function calling 的时候，不是也是可以实现类似的功能吗？目前热门的 AI Agent，不就是用来集成不同的服务吗？为什么又出现了 MCP。</p>
<p>function call、AI Agent、MCP 这三者之间有什么区别？</p>
<p>网上有个说法：Agent是智能体，MCP是AI时代的USB协议。</p>
<p><strong>概念简述</strong></p>
<blockquote>
<ul>
<li>Prompt：提出的问题或想说的话。SYSTEM PROMPT 描述AI的角色、性格、背景知识、语气等</li>
<li>LLM：大语言模型（如GPT-4），能理解和生成自然语言文本</li>
<li>Function Call：LLM通过结构化方式调用外部函数/工具的机制（单一API调用）</li>
<li>AI Agent：具备自主决策、任务分解、调用工具能力的智能体，通常基于LLM，利用Function Calling和MCP来分析和执行任务，实现特定目标（<strong>复杂任务自动化</strong>）</li>
<li>MCP：Model Context Protocol，标准化模型与外部世界（工具、Agent等）交互的协议，使大模型与API无缝交互（场景：多Agent协作、插件生态）</li>
</ul>
</blockquote>
<p><strong>区别与联系</strong></p>
<blockquote>
<p>1.<strong>LLM（大语言模型）</strong></p>
<ul>
<li>本质：<strong>核心的AI能力提供者，负责理解和生成文本</strong>。</li>
<li>局限：本身不具备调用外部工具、记忆复杂上下文、主动决策等能力。</li>
</ul>
<p>2.<strong>Function Call（函数调用）</strong></p>
<ul>
<li>本质：<strong>让LLM能以结构化方式调用外部函数/API</strong>的机制。充当 AI 模型与外部系统之间的桥梁，不同的模型有不同的 Function Calling 实现，代码集成的方式也不一样。由不同的 AI 模型平台来定义和实现（通过代码给 LLM 提供一组 functions，并且提供清晰的函数描述、函数输入和输出，LLM 就可以根据清晰的结构化数据进行推理、执行函数）</li>
<li>局限：通常是一次性、无复杂上下文管理，功能有限。处理不好多轮对话和复杂需求，适合边界清晰、描述明确的任务。如果需要处理很多的任务，那么 Function Calling 的代码比较难维护</li>
</ul>
<p>3.<strong>AI Agent（智能体）</strong></p>
<ul>
<li>本质：智能系统，自主运行以实现特定目标。<strong>在LLM基础上，能自主决策、任务分解、调用多种工具/函数的</strong>。（传统的 AI 聊天仅提供建议或者需要手动执行任务，AI Agent 则可以分析具体情况、做出决策、并自行采取行动）
<ul>
<li>AI Agent 可以利用 MCP 提供的功能描述来理解更多的上下文，并在各种平台/服务自动执行任务</li>
<li>Agent概念的核心：让AI真正实现<strong>自主完成任务</strong>   想+做</li>
</ul>
</li>
<li>联系：通常用LLM作为“思考大脑”，通过Function Call等方式与外部世界交互。</li>
<li>优势：能处理复杂、多步任务，维护长期目标和上下文。</li>
</ul>
<p>4.<strong>MCP（Model Context Protocol）</strong></p>
<ul>
<li>本质：一个标准<strong>协议，定义模型与外部世界（工具、Agent、环境等）如何结构化交互、共享上下文、管理会话</strong>，如同电子设备的 Type C 协议(可以充电也可以传输数据)。    旨在替换碎片化的 Agent 代码集成，建立通用标准，服务商可以基于协议推出自己服务的AI能力、更强大的AI应用；开发者也无需重复造轮子，通过开源项目可以建立强大的 AI Agent 生态</li>
<li>联系：
<ul>
<li>MCP可以规范Function Call的格式和流程。</li>
<li>MCP为AI Agent提供统一的上下文、工具发现、权限管理等能力。</li>
<li>LLM通过MCP可以更好地与外部世界协作，成为Agent的一部分。</li>
</ul>
</li>
<li>优点：可以在不同的应用/服务之间保持上下文，从而增强整体自主执行任务的能力</li>
</ul>
</blockquote>
<p><strong>MCP 与 Function Call 的区别</strong></p>
<ul>
<li>MCP（Model Context Protocol），模型上下文协议</li>
<li>Function Calling，函数调用</li>
</ul>
<p>这两种技术都旨在增强 AI 模型与外部数据的交互能力，但 MCP 不止可以增强 AI 模型，还可以是其他的应用系统。</p>
<p>（<strong>MCP 协议</strong>与 <strong>Function Calling</strong>相似，<strong>MCP最大优点</strong>是整合之前各大模型不同的<strong>Function Call标准</strong>，形成一个<strong>统一的标准协议</strong>）</p>
<p><img alt="4-3" loading="lazy" data-src="https://img2024.cnblogs.com/blog/3047087/202507/3047087-20250720163800970-686066256.png" class="lazyload"><br>
<img alt="4-4" loading="lazy" data-src="https://img2024.cnblogs.com/blog/3047087/202507/3047087-20250720163817154-1020426708.png" class="lazyload"><br>
<img alt="4-5" loading="lazy" data-src="https://img2024.cnblogs.com/blog/3047087/202507/3047087-20250720163827459-493242918.png" class="lazyload"></p>
<h3 id="421-具体示例">4.2.1 具体示例</h3>
<p><strong>1.LLM 只做问答</strong></p>
<p>用户问：“北京天气如何？”</p>
<ul>
<li>LLM直接生成文本：“北京今天晴，气温25度。”</li>
</ul>
<p><strong>2.Function Call</strong></p>
<p>用户问：“北京天气如何？”</p>
<ul>
<li>LLM识别到需要调用天气API，生成结构化调用</li>
</ul>
<p><img alt="4-6" loading="lazy" data-src="https://img2024.cnblogs.com/blog/3047087/202507/3047087-20250720163844162-1735647457.png" class="lazyload"></p>
<ul>
<li>外部系统执行API，返回结果，LLM再生成回答。</li>
</ul>
<p><strong>3.AI Agent</strong></p>
<p>用户问：“帮我订明天去北京的机票，并告诉我天气。”</p>
<ul>
<li>Agent分解任务：
<ul>
<li>查询明天北京天气（调用getWeather）</li>
<li>查询航班（调用searchFlight）</li>
<li>预订机票（调用bookFlight）</li>
</ul>
</li>
<li>Agent维护任务状态，自动串联多步操作，最终给出完整答复。</li>
</ul>
<p><strong>4.MCP</strong></p>
<p>同样的任务，MCP提供：</p>
<ul>
<li>统一的上下文（如用户历史、可用API、权限等）</li>
<li>结构化的多轮交互协议</li>
<li>支持多Agent/多工具协作</li>
</ul>
<p>MCP交互示例：</p>
<pre><code class="language-JSON">{
    "function_call": {
      "name": "getWeather",
      "arguments": {"city": "Beijing"}
    }
}
</code></pre>
<ul>
<li>MCP让LLM/Agent能动态发现可用工具、获取丰富上下文、自动完成多步任务。</li>
</ul>
<h3 id="422-思考">4.2.2 思考</h3>
<p>为什么 Claude 推出 MCP 后会被广泛接受呢？开发AI项目过程中，将 AI 模型集成现有的系统或者第三方系统比较麻烦。虽然市面上有一些框架支持 Agent 开发，例如 <a href="https://www.langchain.com/" target="_blank" rel="noopener nofollow">LangChain Tools</a>, <a href="https://docs.llamaindex.ai/en/stable/" target="_blank" rel="noopener nofollow">LlamaIndex</a> 或者是 <a href="https://sdk.vercel.ai/docs/introduction" target="_blank" rel="noopener nofollow">Vercel AI SDK</a>。</p>
<p>LangChain 和 LlamaIndex 虽然都是开源项目，但是整体发展还是挺混乱的，首先是代码的抽象层次太高了，想要推广的都是让开发人员几行代码就完成某某 AI 功能，这在 Demo 阶段是挺好用的，但是在实际开发中，只要业务一旦开始复杂，糟糕的代码设计带来了非常糟糕的编程体验。还有就是这几个项目都太想商业化了，忽略了整体生态的建设。</p>
<p>还有一个就是 Vercel AI SDK，尽管 Vercel AI SDK 代码抽象的比较好，但是也只是对于前端 UI 结合和部分 AI 功能的封装还不错，最大的问题是和 Nextjs 绑定太深了，对其它的框架和语言支持度不够。</p>
<p>所以 Claude 推动 MCP 可以说是一个很好的时机，首先是 Claude Sonnet 3.5 在开发人员心中有较高的地位，而 MCP 又是一个开放的标准，所以很多公司和社区都愿意参与进来，希望 Claude 能够一直保持一个良好的开放生态。</p>
<p>MCP 对于社区生态的好处主要是下面两点：</p>
<ul>
<li>开放标准给服务商，服务商可以针对 MCP 开放自己的 API 和部分能力。</li>
<li>不需要重复造轮子，开发者可以用已有的开源 MCP 服务来增强自己的 Agent。</li>
</ul>
<h2 id="43-mcp-如何工作">4.3 MCP 如何工作</h2>
<h3 id="431-mcp-架构">4.3.1 MCP 架构</h3>
<p><a href="https://modelcontextprotocol.io/introduction" target="_blank" rel="noopener nofollow">官方给出的MCP架构图</a></p>
<p><img alt="4-7" loading="lazy" data-src="https://img2024.cnblogs.com/blog/3047087/202507/3047087-20250720163905233-1070960211.png" class="lazyload"></p>
<p>MCP 遵循客户端-服务器架构（client-server），总共分为了下面五个部分：</p>
<ul>
<li><strong>MCP Hosts（MCP 主机）</strong>: Hosts 是指 LLM 启动连接的应用程序，像 Cursor, Claude Desktop、Cline 这样的应用程序。</li>
<li>MCP Clients（MCP 客户端）: 客户端是用来在 Hosts 应用程序内维护与 Server 之间 1:1 连接。</li>
<li>MCP Servers（MCP 服务器）: 通过标准化的协议，为 Client 端提供上下文、工具和提示。</li>
<li>Local Data Sources（本地资源）: 本地的文件、数据库和 API，可供 MCP Server 安全访问。</li>
<li>Remote Services（远程资源）: 外部的文件、数据库和 API。</li>
</ul>
<p>整个 MCP 协议核心的在于 Server，因为 Host 和 Client 相信熟悉计算机网络的都不会陌生，非常好理解，但是 Server 如何理解呢？</p>
<p>看看 Cursor 的 AI Agent 发展过程，我们会发现整个 AI 自动化的过程发展会是<strong>从 Chat 到 Composer 再进化到完整的 AI Agent</strong>。</p>
<p>AI Chat 只是提供建议，如何将 AI 的 response 转化为行为和最终的结果，全部依靠人类，例如手动复制粘贴，或者进行某些修改。</p>
<p>AI Composer 是可以自动修改代码，但是需要人类参与和确认，并且无法做到除了修改代码之外的其它操作。</p>
<p>AI Agent 是一个完全的自动化程序，未来完全可以做到自动读取 Figma 的图片，自动生产代码，自动读取日志，自动调试代码，自动 push 代码到 GitHub。</p>
<p>而 MCP Server 就是为了实现 AI Agent 的自动化而存在的，它是一个中间层，告诉 AI Agent 目前存在哪些服务，哪些 API，哪些数据源，AI Agent 可以根据 Server 提供的信息来决定是否调用某个服务，然后通过 Function Calling 来执行函数。</p>
<p><strong>MCP Client</strong></p>
<p>MCP client 充当 LLM 和 MCP server 之间的桥梁，MCP client 的工作流程如下：</p>
<ul>
<li>MCP client 首先从 MCP server 获取可用的工具列表。</li>
<li>将用户的查询连同工具描述通过 function calling 一起发送给 LLM。</li>
<li>LLM 决定是否需要使用工具以及使用哪些工具。</li>
<li>如果需要使用工具，MCP client 会通过 MCP server 执行相应的工具调用。</li>
<li>工具调用的结果会被发送回 LLM。</li>
<li>LLM 基于所有信息生成自然语言响应。</li>
<li>最后将响应展示给用户。</li>
</ul>
<p>Claude Desktop 和Cursor都支持了MCP Server接入能力，它们就是作为 MCP client来连接某个MCP Server感知和实现调用。</p>
<p><strong>MCP Server</strong></p>
<p>MCP server 是 MCP 架构中的关键组件，它可以提供 3 种主要类型的功能：</p>
<ul>
<li>资源（Resources）：类似文件的数据，可以被客户端读取，如 API 响应或文件内容。</li>
<li>工具（Tools）：可以被 LLM 调用的函数（需要用户批准）。</li>
<li>提示（Prompts）：预先编写的模板，帮助用户完成特定任务。</li>
</ul>
<p>这些功能使 MCP server 能够为 AI 应用提供丰富的上下文信息和操作能力，从而增强 LLM 的实用性和灵活性。</p>
<p>你可以在 MCP Servers Repository 和 Awesome MCP Servers 这两个 repo 中找到许多由社区实现的 MCP server。使用 TypeScript 编写的 MCP server 可以通过 npx 命令来运行，使用 Python 编写的 MCP server 可以通过 uvx 命令来运行。</p>
<h3 id="432-mcp-server-的工作原理">4.3.2 MCP Server 的工作原理</h3>
<p>我们先看一个简单的例子，假设我们想让 AI Agent 完成自动搜索 GitHub Repository，接着搜索 Issue，然后再判断是否是一个已知的 bug，最后决定是否需要提交一个新的 Issue 的功能。</p>
<p>那么我们就需要创建一个 Github MCP Server，这个 Server 需要提供查找 Repository、搜索 Issues 和创建 Issue 三种能力。</p>
<p>该后台服务进程 核心功能：将一系列GitHub操作封装成工具Tools，并提供给一个外部系统（很可能是一个大型语言模型或AI智能体）来调用。</p>
<p>我们直接来看看代码：</p>
<pre><code class="language-TypeScript">// 服务器初始化，创建一个服务器实例
const server = new Server(
  {
    name: "github-mcp-server",
    version: VERSION,
  },
  {
    capabilities: {
      tools: {},
    },
  }
);

// 菜单——注册可用的工具列表。
// setRequestHandler 为服务器注册一个请求处理器，ListToolsRequestSchema列出所有可用工具
// inputSchema: zodToJsonSchema(...): 每个工具的“使用说明书”。定义调用这个工具需要提供哪些参数，以及这些参数的格式和类型。zod库来做格式校验
server.setRequestHandler(ListToolsRequestSchema, async () =&gt; {
  return {
    tools: [
      {
        name: "search_repositories",
        description: "Search for GitHub repositories",
        inputSchema: zodToJsonSchema(repository.SearchRepositoriesSchema),
      },
      {
        name: "create_issue",
        description: "Create a new issue in a GitHub repository",
        inputSchema: zodToJsonSchema(issues.CreateIssueSchema),
      },
      {
        name: "search_issues",
        description: "Search for issues and pull requests across GitHub repositories",
        inputSchema: zodToJsonSchema(search.SearchIssuesSchema),
      }
    ],
  };
});

// 处理工具调用请求
// CallToolRequestSchema 处理器负责处理“调用某个具体工具”的请求，当外部系统说“帮我执行create_issue”操作时，它会被触发

server.setRequestHandler(CallToolRequestSchema, async (request) =&gt; {
  try {
    if (!request.params.arguments) {
      throw new Error("Arguments are required");
    }

    switch (request.params.name) {
      case "search_repositories": {
        // 使用zod验证传入的参数 是否符合SearchRepositoriesSchema定义的规范
        const args = repository.SearchRepositoriesSchema.parse(request.params.arguments);
        // 校验通过，调用searchRepositories 实际的业务逻辑函数
        const results = await repository.searchRepositories(
          args.query,
          args.page,
          args.perPage
        );
        return {
          content: [{ type: "text", text: JSON.stringify(results, null, 2) }],
        };
      }

      case "create_issue": {
        const args = issues.CreateIssueSchema.parse(request.params.arguments);
        const { owner, repo, ...options } = args;
        const issue = await issues.createIssue(owner, repo, options);
        return {
          content: [{ type: "text", text: JSON.stringify(issue, null, 2) }],
        };
      }

      case "search_issues": {
        const args = search.SearchIssuesSchema.parse(request.params.arguments);
        const results = await search.searchIssues(args);
        return {
          content: [{ type: "text", text: JSON.stringify(results, null, 2) }],
        };
      }

      default:
        throw new Error(`Unknown tool: ${request.params.name}`);
    }
  } catch (error) {}
});

async function runServer() {
  // StdioServerTransport 是理解这个服务运行方式的关键。它没有使用常见的HTTP（网络端口），而使用stdio (Standard Input/Output，标准输入/输出) 作为通信方式
  // 这意味着：这个服务作为一个独立的命令行进程运行。它的父进程（比如AI模型的主程序）通过向它的标准输入（stdin）写入请求数据来和它通信，并通过读取它的标准输出（stdout）来接收响应。这是一种高效的进程间通信方式
  const transport = new StdioServerTransport();
  // 将服务器逻辑和通信方式连接起来，让服务器正式开始监听来自stdio的请求
  await server.connect(transport);
  console.error("GitHub MCP Server running on stdio");
}

runServer().catch((error) =&gt; {
  console.error("Fatal error in main():", error);
  process.exit(1);
});
</code></pre>
<p>上面的代码中，我们通过 <code>server.setRequestHandler</code> 来告诉 Client 端我们提供了哪些能力，通过 <code>description</code> 字段来描述这个能力的作用，通过 <code>inputSchema</code> 来描述完成这个能力需要的输入参数。</p>
<p><strong>一个设计良好、模块化的AI工具集服务，将复杂的GitHub操作简化为AI可以直接理解和调用的标准化工具。核心功能：将一系列GitHub操作封装成工具Tools，并提供给一个外部系统（很可能是一个大型语言模型或AI智能体）来调用。</strong></p>
<ol>
<li>启动：作为一个命令行工具启动，通过标准输入/输出与外界通信。</li>
<li>广播能力：当被问及时，它会返回一个包含 search_repositories、create_issue 和 search_issues 三个工具的列表，并附上详细的参数说明。</li>
<li>执行任务：当收到调用具体工具的请求时，它会：
<ol>
<li>验证传入的参数是否合法。</li>
<li>调用相应的函数去执行真正的 GitHub API 操作。</li>
<li>将操作结果返回给请求方。</li>
</ol>
</li>
</ol>
<p>我们再来看看具体的实现代码：</p>
<pre><code class="language-TypeScript">// 使用zod库定义调用GitHub API时所需的参数结构
// 一个可复用的基础搜索选项
export const SearchOptions = z.object({
  q: z.string(),                             // 搜索查询，必须是字符串
  order: z.enum(["asc", "desc"]).optional(), // 排序方式，可选，且值必须是 'asc' 或 'desc'
  page: z.number().min(1).optional(),        // 页码，可选，必须是数字且最小为 1
  per_page: z.number().min(1).max(100).optional(), // 每页数量，可选，必须是数字，范围在 1-100
});

// 扩展基础选项，用于搜索 Issue（Issue有自己独特的排序标准，如按评论数排序）
export const SearchIssuesOptions = SearchOptions.extend({
  sort: z.enum([  // 排序字段
    "comments",
    ...
  ]).optional(),
});

// 异步函数，搜索用户
// typeof SearchUsersSchema 获取zod schema对象；z.infer&lt;&gt;  从schema中自动推断出TypeScript类型；保证类型安全
export async function searchUsers(params: z.infer&lt;typeof SearchUsersSchema&gt;) {
  return githubRequest(buildUrl("https://api.github.com/search/users", params));
}

// 专门为搜索仓库SearchRepositories函数定义的Schema。describe为字段添加描述信息，可以被一些自动化工具（如文档生成器或AI）读取
export const SearchRepositoriesSchema = z.object({
  query: z.string().describe("Search query (see GitHub search syntax)"),
  page: z.number().optional().describe("Page number for pagination (default: 1)"),
  perPage: z.number().optional().describe("Number of results per page (default: 30, max: 100)"),
});

// 搜索仓库的函数
export async function searchRepositories(query: string,
  page: number = 1,
  perPage: number = 30) {
  const url = new URL("https://api.github.com/search/repositories");
  url.searchParams.append("q", query);
  url.searchParams.append("page", page.toString());
  url.searchParams.append("per_page", perPage.toString());

  const response = await githubRequest(url.toString());
  // 使用Zod schema（GitHubSearchResponseSchema），验证返回的数据格式是否符合预期
  return GitHubSearchResponseSchema.parse(response);
}
</code></pre>
<p>这段代码主要由两部分组成：</p>
<ol>
<li>Zod Schemas：定义数据结构的“蓝图”或“规则书”。</li>
<li>API 调用函数：实际执行与 GitHub API 通信的“执行者”。</li>
</ol>
<p>可以很清晰的看到，我们最终实现是通过了 <code>https://api.github.com</code> 的 API 来实现和 Github 交互的，我们通过 <code>githubRequest</code> 函数来调用 GitHub 的 API，最后返回结果。</p>
<p>在调用 Github 官方的 API 之前，MCP 的主要工作是描述 Server 提供了哪些能力(给 LLM 提供)，需要哪些参数(参数具体的功能是什么)，最后返回的结果是什么。</p>
<p>所以 MCP Server 并不是一个新颖的、高深的东西，它只是一个具有共识的协议。</p>
<p>如果我们想要实现一个更强大的 AI Agent，例如我们想让 AI Agent 自动的根据本地错误日志，自动搜索相关的 GitHub Repository，然后搜索 Issue，最后将结果发送到 Slack。</p>
<p>那么我们可能需要创建三个不同的 MCP Server，一个是 Local Log Server，用来查询本地日志；一个是 GitHub Server，用来搜索 Issue；还有一个是 Slack Server，用来发送消息。</p>
<p>AI Agent 在用户输入 <code>我需要查询本地错误日志，将相关的 Issue 发送到 Slack</code> 指令后，自行判断需要调用哪些 MCP Server，并决定调用顺序，最终根据不同 MCP Server 的返回结果来决定是否需要调用下一个 Server，以此来完成整个任务。</p>
<h2 id="44-如何使用-mcp">4.4 如何使用 MCP</h2>
<p>如果你还没有尝试过如何使用 MCP 的话，我们可以考虑用 Cursor(此处以Cursor为例)，Claude Desktop 或者 Cline 来体验一下。</p>
<p>当然，我们并不需要自己开发 MCP Servers，MCP 的好处就是通用、标准，所以开发者并不需要重复造轮子（但是学习可以重复造轮子）。</p>
<p>首先推荐的是官方组织的一些 Server：<a href="https://github.com/modelcontextprotocol/servers" target="_blank" rel="noopener nofollow">官方的 MCP Server 列表</a>。</p>
<p>目前社区的 MCP Server 还是比较混乱，有很多缺少教程和文档，很多的代码功能也有问题，我们可以自行尝试一下 <a href="https://cursor.directory/" target="_blank" rel="noopener nofollow">Cursor Directory</a> 的一些例子，具体的配置和实战 可以参考官方文档。</p>
<p>MCP Github地址：<a href="https://github.com/modelcontextprotocol" target="_blank" rel="noopener nofollow">https://github.com/modelcontextprotocol</a></p>
<ul>
<li>官方MCP服务：<a href="https://github.com/modelcontextprotocol/servers" target="_blank" rel="noopener nofollow">https://github.com/modelcontextprotocol/servers</a></li>
<li>第三方和个人开发的MCP服务</li>
<li>MCP 工具的聚合网站：<a href="https://smithery.ai/" target="_blank" rel="noopener nofollow">smithery.ai</a></li>
</ul>
<p>Smithery平台上的MCP工具与github上的MCP工具对比：</p>
<ul>
<li>托管方式：
<ul>
<li>Smithery提供两种MCP服务器，①远程，由Smithery在其基础设施上运行，用户通过网络访问；②本地，用户通过Smithery的CLI工具将MCP服务器安装并运行在本地环境中。</li>
<li>Github：主要提供MCP服务器源码，开发者自行下载配置并在本地服务器运行</li>
</ul>
</li>
<li>安装与管理
<ul>
<li>Smithery提供统一的界面和CLI工具</li>
<li>Github：需要手动克隆，安装依赖项</li>
</ul>
</li>
</ul>
<h3 id="cursor-mcp-使用示例自定义mcp-server">Cursor MCP 使用示例（自定义mcp server）</h3>
<p>使用模型上下文协议 (MCP) 插件系统将外部工具和数据源连接到 Cursor</p>
<p>MCP 是一个开放协议，它规范了应用程序向 LLM 提供上下文和工具的方式。<strong>MCP 可以理解为 Cursor 的插件系统</strong>，它允许您通过标准化接口将 Agent 连接到各种数据源和工具，从而扩展 Agent 的功能。</p>
<p>用途：MCP 允许您将 Cursor 连接到外部系统和数据源。这意味着您可以将 Cursor 与现有的工具和基础架构集成，而无需在代码本身之外告知 Cursor 项目的结构。</p>
<p>Cursor工具中集成mcp server功能对开发增加效率非常明显，配置入口在：File—&gt;Preferences—&gt;Cursor Settings—&gt;MCP—&gt;MCP Servers—&gt;Add new global MCP server</p>
<p><img alt="4-8" loading="lazy" data-src="https://img2024.cnblogs.com/blog/3047087/202507/3047087-20250720163940940-134565887.png" class="lazyload"><br>
<img alt="4-9" loading="lazy" data-src="https://img2024.cnblogs.com/blog/3047087/202507/3047087-20250720163950242-1243212095.png" class="lazyload"></p>
<p><strong>手动配置</strong></p>
<p>MCP 配置文件采用 JSON 格式，结构如下：</p>
<pre><code class="language-JSON">// This example demonstrated an MCP server using the stdio format
// Cursor automatically runs this process for you
// This uses a Python server, ran with `python`
{
  "mcpServers": {
    "wj-server-name": {
      "command": "python",
      "args": ["server.py"],
      "env": {
        "API_KEY": "xxx"
      }
    }
  }
}
</code></pre>
<p>基于MCP（Model Context Protocol）协议的天气查询插件服务，可以通过 MCP 框架与外部系统集成，实现“查询指定城市实时天气”的功能。</p>
<pre><code class="language-Python">#!/usr/bin/env python3

import asyncio     # Python的标准库，用于编写异步代码，适合处理网络请求等I/O密集型任务
import os
from typing import Any, Dict
import aiohttp     # 第三方库，用于执行异步的HTTP请求
from mcp.server import Server   # MCP框架相关，用于定义服务、工具和数据类型
from mcp.server.stdio import stdio_server
from mcp.types import Tool, TextContent

# 和风天气API设置
API_KEY = os.getenv('WEATHER_API_KEY')  # 需要在mcp.json里配置
API_URL = "https://mq564tupt3.re.qweatherapi.com/v7/weather/now"

# 服务初始化。创建一个MCP服务实例，服务名为weather-mcp-server
server = Server("weather-mcp-server")

# 异步请求和风天气API，查询指定城市的实时天气
async def query_weather(city: str) -&gt; Dict[str, Any]:
    """查询天气API"""
    if not API_KEY:
        return {"error": "WEATHER_API_KEY环境变量未设置"}
    params = {
        "location": city,
        "key": API_KEY
    }
    try:
        # 使用aiohttp的标准模式，创建一个客户端会话，并以aiohttp异步方式发送GET请求
        async with aiohttp.ClientSession() as session:
            async with session.get(API_URL, params=params) as response:
                if response.status == 200:
                    return await response.json()
                else:
                    return {"error": f"API调用失败: {await response.text()}"}
    except Exception as e:
        return {"error": f"请求失败: {str(e)}"}

# 工具列表说明
@server.list_tools()     # 装饰器Decorator，将list_tools函数注册到server实例上，作为“列出所有工具”请求的处理器
async def list_tools() -&gt; list[Tool]:
    return [
        Tool(
            name="weather_query",
            description="查询指定城市的实时天气",
            # 输入参数为city
            inputSchema={
                "type": "object",
                "properties": {
                    "city": {
                        "type": "string",
                        "description": "要查询的城市名称（如beijing/shanghai/shenzhen）"
                    }
                },
                "required": ["city"]
            }
        )
    ]

# 工具调用实现。根据工具名和参数，执行相应的功能
@server.call_tool()
async def call_tool(name: str, arguments: Dict[str, Any]) -&gt; list[TextContent]:
    # 如果调用weather_query工具，提取城市名、调用API查询天气、返回结果格式化为文本内容返回
    if name == "weather_query":
        city = arguments.get("city", "")
        result = await query_weather(city)
        if "error" in result:
            return [TextContent(type="text", text=f"Error: {result['error']}")]
        if result.get("code") != "200":
            return [TextContent(type="text", text=f"API返回错误: {result.get('code')}")]
        now = result.get("now", {})
        weather_text = (
            f"{city} 当前天气：{now.get('text', '未知')}\n"
            f"温度：{now.get('temp', '未知')}℃\n"
            f"风向：{now.get('windDir', '未知')}\n"
            f"风速：{now.get('windSpeed', '未知')} km/h"
        )
        return [TextContent(type="text", text=weather_text)]
    raise ValueError(f"Unknown tool: {name}")

async def main():
    # stdio_server表示服务器的通信方式是标准输入/输出（stdio），而不是通过网络端口
    async with stdio_server() as (read_stream, write_stream):
        # 启动服务器的主事件循环。服务器会开始从read_stream（标准输入）监听请求，并将响应写入write_stream（标准输出）
        await server.run(
            read_stream,
            write_stream,
            server.create_initialization_options()
        )

if __name__ == "__main__":
    asyncio.run(main())
</code></pre>
<p>MCP插件服务：通过和风天气API查询实时天气。对外暴露一个工具weather_query，输入城市名、返回天气信息</p>
<p>使用Python和asyncio，构建一个后台服务进程，将将 Hugging Face 的 BLOOM 模型封装成一个标准化的“工具”，供外部系统（如 AI 大模型）调用。</p>
<pre><code class="language-Python"># Hugging Face API settings
API_TOKEN = os.getenv('API_KEY')
API_URL = "https://api-inference.huggingface.co/models/bigscience/bloom"

# Create MCP server
server = Server("huggingface-mcp-server")

async def query_huggingface(payload: Dict[str, Any]) -&gt; Dict[str, Any]:
    """Query Hugging Face API"""
    if not API_TOKEN:
        return {"error": "API_KEY environment variable not set"}
    
    headers = {"Authorization": f"Bearer {API_TOKEN}"}
    
    try:
        # 使用aiohttp的标准模式，创建一个客户端会话，并以异步方式发送一个POST请求，将payload作为JSON数据发送
        async with aiohttp.ClientSession() as session:
            async with session.post(API_URL, headers=headers, json=payload) as response:
                if response.status == 200:
                    result = await response.json()
                    return result
                else:
                    error_text = await response.text()
                    return {"error": f"API call failed: {error_text}"}
    except Exception as e:
        return {"error": f"Request failed: {str(e)}"}

@server.list_tools()   # 装饰器Decorator，将list_tools函数注册到server实例上，作为“列出所有工具”请求的处理器
async def list_tools() -&gt; list[Tool]:
    """List available tools"""
    return [
        Tool(
            name="huggingface_generate",
            description="Generate text using Hugging Face BLOOM model",
            inputSchema={   # 工具的使用说明书，使用JSON Schema格式定义。告诉调用方需要提供哪些参数(prompt是必须的)，参数的类型(string,integer,number)，以及默认值
           
                "type": "object",
                "properties": {
                    "prompt": {
                        "type": "string",
                        "description": "The text prompt to generate from"
                    },
                    "max_length": {
                        "type": "integer",
                        "description": "Maximum length of generated text",
                        "default": 100
                    },
                    "temperature": {
                        "type": "number",
                        "description": "Temperature for generation",
                        "default": 0.7
                    }
                },
                "required": ["prompt"]
            }
        )
    ]

@server.call_tool()    # 装饰器Decorator，将call_tool函数注册为“执行具体工具”请求的处理器
async def call_tool(name: str, arguments: Dict[str, Any]) -&gt; list[TextContent]:
    """Handle tool calls"""
    if name == "huggingface_generate":
        prompt = arguments.get("prompt", "")
        max_length = arguments.get("max_length", 100)
        temperature = arguments.get("temperature", 0.7)
        
        # Call Hugging Face API
        hf_payload = {
            "inputs": prompt,
            "parameters": {
                "max_length": max_length,
                "temperature": temperature,
                "return_full_text": False
            }
        }
        
        # 调用query_huggingface函数来完成实际的 API 请求
        result = await query_huggingface(hf_payload)
        
        if "error" in result:
            return [TextContent(type="text", text=f"Error: {result['error']}")]
        else:
            generated_text = result[0].get('generated_text', '') if result else ''
            return [TextContent(type="text", text=generated_text)]
    
    raise ValueError(f"Unknown tool: {name}")

async def main():
    """Main entry point"""
    # stdio_server表示服务器的通信方式是标准输入/输出（stdio），而不是通过网络端口
    async with stdio_server() as (read_stream, write_stream):
        # 启动服务器的主事件循环。服务器会开始从read_stream（标准输入）监听请求，并将响应写入write_stream（标准输出）
        await server.run(
            read_stream,
            write_stream,
            server.create_initialization_options()
        )

if __name__ == "__main__":  # Python 的标准入口点，确保只有当这个文件被直接执行时，asyncio.run(main()) 才会运行，从而启动整个服务
    asyncio.run(main())
</code></pre>
<p>这是一个非常典型的 AI Agent 工具化 实践，将一个外部服务（Hugging Face API）包装成了一个可以让 AI 模型方便、安全调用的标准化工具。</p>
<p>完成 server.py、requirements.txt，并在Cursor的config.json添加MCP服务器配置后，可以在Cursor中使用这个自定义模型，操作步骤如下：</p>
<p><strong>步骤 1: 启动你的本地服务器</strong></p>
<p>运行 server.py 文件。这个服务器会作为一个“中间人”，接收来自 Cursor 的请求，请求和风天气的API/转发给 Hugging Face API。</p>
<p>重要提示: 在你使用自定义模型的整个过程中，这个终端窗口必须保持打开状态。关闭它就会中断服务器，Cursor 将无法连接。</p>
<p><strong>步骤 2: 在 Cursor 中选择并使用你的模型、开始聊天</strong></p>
<p>选择 wj-server-name 后，你就可以像平常一样使用 Cursor 的聊天功能了。</p>
<p>两种方式：</p>
<ol>
<li>Cursor编辑器 选中/激活weather-server，找到weather-server相关的工具。
<ol>
<li>选择weather_query工具，输入参数（城市名），工具会自动调用MCP Server，返回天气信息并显示在编辑器侧边栏或弹窗中</li>
</ol>
</li>
<li>在Cursor的Chat，选择weather-server 作为当前MCP工具，输入<code>weather_query city=beijing</code></li>
</ol>
<p>所有这些请求现在都会发送到你的本地 server.py，再由它调用 和风天气 的API（可以用其他各种模型等）进行处理，最后将结果返回到 Cursor 界面。</p>
<h2 id="mcp-的一些资源">MCP 的一些资源</h2>
<p><strong>MCP 官方资源</strong></p>
<ul>
<li><a href="https://github.com/modelcontextprotocol" target="_blank" rel="noopener nofollow">官方的开源组织 Model Context Protocol</a>。</li>
<li><a href="https://guangzhengli.com/blog/zh/%5Bmodelcontextprotocol%5D(https://modelcontextprotocol.io/introduction)" target="_blank" rel="noopener nofollow">官方的文档 modelcontextprotocol</a>。</li>
<li><a href="https://github.com/modelcontextprotocol/servers" target="_blank" rel="noopener nofollow">官方的 MCP Server 列表</a></li>
<li><a href="https://www.anthropic.com/news/model-context-protocol" target="_blank" rel="noopener nofollow">Claude Blog</a></li>
</ul>
<p><strong>社区的 MCP Server 的列表</strong></p>
<ul>
<li><a href="https://mcp.so/" target="_blank" rel="noopener nofollow">MCP.so</a></li>
<li><a href="https://cursor.directory/" target="_blank" rel="noopener nofollow">Cursor Directory</a></li>
<li><a href="https://www.pulsemcp.com/" target="_blank" rel="noopener nofollow">Pulsemcp</a></li>
<li><a href="https://glama.ai/mcp/servers" target="_blank" rel="noopener nofollow">Glama MCP Servers</a></li>
</ul>
<p><strong>Agent</strong></p>
<p>2D数字人项目：<a href="https://github.com/wan-h/awesome-digital-human-live2d" target="_blank" rel="noopener nofollow">https://github.com/wan-h/awesome-digital-human-live2d</a></p>
<p><strong>参考：</strong></p>
<p><a href="https://zhuanlan.zhihu.com/p/375549477" target="_blank" rel="noopener nofollow">一文概览人工智能(AI)发展历程</a></p>
<p><a href="https://guangzhengli.com/blog/zh/model-context-protocol" target="_blank" rel="noopener nofollow">MCP 终极指南</a></p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0" data-date-updated="2025-07-20 16:41">2025-07-20 16:40</span>&nbsp;
<a href="https://www.cnblogs.com/qhhfRA">BJRA</a>&nbsp;
阅读(<span id="post_view_count">72</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18994130);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18994130', targetLink: 'https://www.cnblogs.com/qhhfRA/p/18994130', title: 'AI 发展 &amp;amp;&amp;amp; MCP' })">举报</a>
</div>
        