
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/sunstrikes/p/18668809" title="发布于 2025-01-13 16:14">
    <span role="heading" aria-level="2">[megatron代码阅读] 1. 初始化和组网</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p>以<code>pretrain_gpt.py</code>为例, 看megatron的整体逻辑. 本章主要包括megatron初始化相关逻辑, 核心函数为<code>initialize_megatron</code>, <code>setup_model_and_optimizer</code>两个</p>
<h2 id="initialize_megatron">initialize_megatron</h2>
<h3 id="parse_args">parse_args</h3>
<p>从argparse中直接读取超参数配置. 如学习率, 正则化等. 从环境变量中获取rank等</p>
<h3 id="load_args_from_checkpoint">load_args_from_checkpoint</h3>
<ul>
<li>
<p>优先从未被持久化的ckpt加载, 并且只加载rank0的args</p>
</li>
<li>
<p>_load_non_persistent_base_checkpoint</p>
<ul>
<li>
<p>find_checkpoint_rank_0</p>
<p>在不知道是否使用pp/ep策略的情况下, 尝试拼装出rank0 ckpt的名称, 如果存在就能定位到实际的存放目录</p>
</li>
<li>
<p>verify_checkpoint_and_load_strategy</p>
<p>根据是zarr还是 torch_dist选择不同的加载策略</p>
</li>
<li>
<p>TorchCommonLoadStrategy-&gt;torch.load()</p>
</li>
</ul>
</li>
<li>
<p>如果没有非持久化的, 加载远端ckpt</p>
</li>
<li>
<p>从ckpt里的args替换掉之前解析的部分args, 比如tp/pp/vp等超参数</p>
</li>
</ul>
<h3 id="校验yamlargs-全局变量设置">校验yaml/args, 全局变量设置</h3>
<h3 id="_initialize_distributed">_initialize_distributed</h3>
<p>pytorch里的get_world_size 返回的是gpu总卡数</p>
<p>初始化torch.distributed</p>
<h4 id="mpuinitialize_model_parallel-并行设置核心函数">mpu.initialize_model_parallel (并行设置,核心函数)</h4>
<p><code>RankGenerator</code>:</p>
<ol>
<li>在每块GPU上启动一个进程（process），每个进程<strong>独立执行</strong>自己所维护的那部分模型的计算，实现并行训练</li>
<li>存储tp/pp/dp/ep/cp 各种并行度配置大小. 并且能够从 tp-dp str格式的并行配置里获取 tp/dp对应的mask和并行度大小设置.</li>
<li><code>get_ranks</code>: 根据parallel_size和mask, 计算各种并行策略拆分后的rank group.</li>
</ol>
<blockquote>
<p>[!NOTE]</p>
<p>举例: 假定有2个8卡机器，node1: rank 0-7，node2: rank 8-15 tp-pp-dp: [2,4,2]</p>
<ul>
<li>_TENSOR_MODEL_PARALLEL_GROUP ：[g0, g1], [g2, g3], [g4, g5], [g6, g7], [g8, g9], [g10, g11], [g12, g13], [g14, g15]。</li>
<li>_PIPELINE_MODEL_PARALLEL_GROUP ： [g0, g4, g8, g12], [g1, g5, g9, g13], [g2, g6, g10, g14], [g3, g7, g11, g15]。</li>
<li>_MODEL_PARALLEL_GROUP ：tp-pp = 2 * 4 = 8 [0, 1, 4, 5, 8, 9, 12, 13]，[2, 3, 6, 7, 10, 11, 14, 15]</li>
<li>_DATA_PARALLEL_GROUP ：[g0, g2], [g1, g3], [g4, g6], [g5, g7], [g8, g10], [g9, g11], [g12, g14], [g13, g15]。</li>
</ul>
</blockquote>
<img src="https://img2023.cnblogs.com/blog/1439743/202501/1439743-20250109211056415-1710420763.png" alt="分隔样例" style="zoom: 33%">
<p>注意在PP内输入层和输出层共享一个word_embedding，PP组中的第一个和最后一个rank需要通讯，保证word_embedding完全一致</p>
<p>group全局变量赋值: 每个并行模式有一个分组全局变量.通过 generator_wrapper生成, 自己的进程rank如果在group内, 初始化对应的nccl/gloo <a href="https://pytorch.org/docs/stable/distributed.html#torch.distributed.new_group" target="_blank" rel="noopener nofollow">torch.distributed.new_group</a></p>
<p>GlobalMemoryBuffer: 保存每个已经分配出的tensor, 避免显存重分配.</p>
<h2 id="setup_model_and_optimizer">setup_model_and_optimizer</h2>
<p>主要逻辑是配置模型组网和优化器.</p>
<h4 id="model_provider-torch-gpt组网">model_provider: torch gpt组网</h4>
<p><code>megatron/core/transformer</code>, transformer组网核心逻辑, 基于torch.nn.Module, 将涉及到的子模型结构进行了抽象. 通过subModule的方式嵌入自定义module, 便于代码复用</p>
<p>例如</p>
<pre><code class="language-python">self_attention=ModuleSpec(
    module=SelfAttention,
    params={"attn_mask_type": attn_mask_type},
    submodules=SelfAttentionSubmodules(
        linear_qkv=ColumnParallelLinear,
        core_attention=DotProductAttention,
        linear_proj=RowParallelLinear,
        q_layernorm=IdentityOp,
        k_layernorm=IdentityOp,
    ),
)
</code></pre>
<p>在<code>attention.py</code>里读到之前moduleSpec中的对应linear_qkv的实现, 即TP列并行的Linear实现. 加上TransformerConfig, 就能定义出最终的网络逻辑. TP相关逻辑在后续专门看的时候再细写.</p>
<pre><code class="language-python">self.linear_qkv = build_module(
    submodules.linear_qkv,
    self.config.hidden_size,
    self.query_projection_size + 2 * self.kv_projection_size,
    config=self.config,
    init_method=self.config.init_method,
    gather_output=False,
    bias=self.config.add_bias_linear or self.config.add_qkv_bias,
    skip_bias_add=False,
    is_expert=False,
    tp_comm_buffer_name='qkv',
)
</code></pre>
<p>torch里实现module时, 主要关注<code>__init__()</code>和<code>forward()</code>, bp通过自动微分生成.</p>
<h5 id="配置">配置</h5>
<p>配置类 ModelParallelConfig, TransformerConfig</p>
<p>ModelParallelConfig: 主要包括 模型并行/PP/通信overlap相关优化开关/cpuOffload 等相关配置</p>
<p>TransformerConfig: 主要包括 模型结构/MOE/算子fusion加速/激活重计算/Context并行 等配置</p>
<h4 id="modelsgptgpt_modelpy">models/gpt/gpt_model.py</h4>
<h5 id="preprocess">preprocess</h5>
<p>分为word_emb和pos_emb两部分. 输出为 word_emb(b,s,h) + pos_emb(s,h) + tokentype_emb(b,s,h)(需要转置适配)</p>
<p>注意在embedding最后要进行dropout处理, 应该是为了减少模型过拟合的风险</p>
<h6 id="wordembeddings">WordEmbeddings</h6>
<p><code>tensor_parallel.VocabParallelEmbedding</code></p>
<p>vocab_size表示词表维度, 例如分词预处理后保留能查到的几千个常用单词. 将vocab_size个embed均分存储到global_world_size张卡上, embedding lookup时从对应的存储卡上拉取. 这里把非自身rank的emb通过[start_idx, end_idx)的mask操作置0, 然后通过reduce就能获取完整的词表.</p>
<p>如果配置开了序列并行, reduce操作会变为reduceScatter操作, lookup之后直接分配好sp的输入.</p>
<h6 id="rope旋转位置编码">RoPE(旋转位置编码)</h6>
<p>位置编码需要满足几个性质: 1. 不能满足交换律, 第m个token与第n个token的位置关系，和第n个token与第m个token的位置关系一定要有区分度。 2.需要有远程衰减性</p>
<img src="https://img2023.cnblogs.com/blog/1439743/202501/1439743-20250109211052803-753712782.png" alt="image-20250108114351771" style="zoom: 50%">
<p>为了便于加速计算, 可以等价优化为下面这种向量乘法的形式:</p>
<img src="https://img2023.cnblogs.com/blog/1439743/202501/1439743-20250109211050727-478264396.png" alt="image-20250108114806801" style="zoom: 50%">
<img src="https://img2023.cnblogs.com/blog/1439743/202501/1439743-20250109211048866-53440003.png" alt="image-20250108114336830" style="zoom: 50%">
<h6 id="tokentype_embedding">tokentype_embedding</h6>
<p>类型嵌入层，用于区分输入中不同类型的token, 例如，在BERT中用于区分两个句子，而在某些GPT变种或特定任务中可能用于区分不同类型的输入数据，如对话中的提问和回答.</p>
<h5 id="transformer">transformer</h5>
<p>self.decoder就是上面通过ModuleSpec获得的module, 可以根据配置选择普通的selfAttention, 还是MLA.</p>
<ol>
<li>MLA原理: 在模型能力不变基础上，通过KV低秩压缩, 使得推理的KVcache显存占用和计算效率上对比MHA性能有明显提升.</li>
</ol>
<img src="https://img2023.cnblogs.com/blog/1439743/202501/1439743-20250109211043913-1425120685.png" alt="image-20250107171551928" style="zoom: 50%">
<h5 id="postprocess">postprocess</h5>
<h6 id="1output_layer--loss">1.output_layer &amp; loss</h6>
<p>训练时output可以并行, 这里是个TP列并行的方式, 训练方式如下例子:</p>
<pre><code class="language-python">&lt;s&gt;
&lt;s&gt; i
&lt;s&gt; i love 
&lt;s&gt; i love maching
&lt;s&gt; i love maching learning &lt;eos/&gt;
</code></pre>
<p>训练阶段将这个矩阵直接输入到decoder，分别得到 5个输出 <span class="math inline">\(O_i, i\in [1,2,3,4,5]\)</span>， 理想的输出应该是[i, love, maching, learning, <eos>] ,然后 比较<span class="math inline">\(O_i\)</span>和理想输出的交叉熵，得到loss. 而且这五个序列可以放在一个batch内并行计算.</eos></p>
<h4 id="optimizer">optimizer</h4>
<h5 id="_get_param_groups_and_buffers">_get_param_groups_and_buffers</h5>
<p>从多个model_chunks中遍历所有的param向量, 对其中某些param进行特殊的处理</p>
<ul>
<li>decoupled_lr是为input/output layer单独设置的lr</li>
<li><code>no_weight_decay_cond</code>: 配置参数是否应该执行权重衰减。</li>
<li>scale_lr_cond: 对某些指定层的参数进行学习率缩放, 匹配到对应的param_map后执行.</li>
</ul>
<h5 id="_get_megatron_optimizer_based_on_param_groups">_get_megatron_optimizer_based_on_param_groups</h5>
<p>主要逻辑是混合精度optimizer的设置(MixedPrecisionOptimizer), TODO: 细看<a href="https://github.com/NVIDIA/apex/blob/7b73b12361068a10b0f44844534613f252a5ea75/apex/optimizers/fused_adam.py#L16" target="_blank" rel="noopener nofollow">Apex.FusedAdam</a>, 和<code>torch.adamW</code>的区别在哪里</p>
<h6 id="梯度缩放-dynamicgradscaler">梯度缩放: DynamicGradScaler</h6>
<p>混合精度训练的时候, 用于动态调整梯度缩放比例，以处理梯度爆炸或消失问题.</p>
<p>主要逻辑是有一个初始化scale值, 当连续<code>hysteresis</code>次迭代中出现NaN，<em>torch.max(scale * backoff_factor, min_scale)</em> 用来减小scale<span class="math inline">\(backoff\_factor \in (0, 1)\)</span>.</p>
<p>当连续growth_interval次没出现NaN, 按照_scale * growth_factor_, 放大scale, <span class="math inline">\(growth\_factor &gt; 1\)</span></p>
<h6 id="distributedoptimizer">DistributedOptimizer</h6>
<p>接口继承自<a href="https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" target="_blank" rel="noopener nofollow">torch.optimizer</a>, 核心逻辑在<code>step(self)</code>, 有3个类: <code>FP32Optimizer, ChainedOptimizer, MixedPrecisionOptimizer</code></p>
<p><strong>FP32Optimizer</strong>: fp32训练使用到的, 主要功能是配置了clip_grad后进行normalization, norm分两种, 一种是取max_grad, 一种是l2范数, 通过all_reduce拿到total_norm, 最后用这个值分别对每个param tensor进行scale. 在scale之后就调用的是torch.optimizer.step进行正常的Adam更新.</p>
<p><strong>MixedPrecisionOptimizer</strong>: 混合精度训练使用</p>
<ul>
<li>prepare_grads: 先从param.grad copy到 param.main_grad, 这一步同时做了fp16-&gt;fp32的转换, 然后检查所有的grad, 先unscale, 再看是否存在NaN. 注意只有fp16需要, bf16不需要.</li>
<li>clip_grad_norm: 与FP32Optimizer一样的方法scale grad.</li>
<li>step_with_ready_grads: optimizer.step后, 再把fp32的main_param copy回用于下一轮bp的fp16 param里面.</li>
</ul>
<p><strong>ChainedOptimizer</strong>: 用于moe场景, 每个分块子模型配置不同的optimizer时使用. 多个optimizer之间串行执行.</p>
<p>下一节看megatron的模型保存&amp;加载, 并行训练相关代码.</p>
<h2 id="参考链接">参考链接</h2>
<p><a href="https://zhuanlan.zhihu.com/p/359502624" target="_blank" rel="noopener nofollow">ROPE位置编码博客</a>, <a href="https://arxiv.org/pdf/2104.09864" target="_blank" rel="noopener nofollow">论文</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/696671064" target="_blank" rel="noopener nofollow">MLA原理博客</a></p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.08504728558333334" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-01-13 16:15">2025-01-13 16:14</span>&nbsp;
<a href="https://www.cnblogs.com/sunstrikes">SunStriKE</a>&nbsp;
阅读(<span id="post_view_count">1</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18668809" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18668809);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18668809', targetLink: 'https://www.cnblogs.com/sunstrikes/p/18668809', title: '[megatron代码阅读] 1. 初始化和组网' })">举报</a>
</div>
        