
		<h1 class="postTitle">
			<a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/bigdata1024/p/18704285" title="发布于 2025-02-08 14:47">
    <span role="heading" aria-level="2">Huggingface使用</span>
    

</a>

		</h1>
		<div class="clear"></div>
		<div class="postBody">
			<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p></p><div class="toc"><div class="toc-container-header">目录</div><ul><li><a href="#1-transformer模型" rel="noopener nofollow">1. Transformer模型</a><ul><li><a href="#11-核心组件" rel="noopener nofollow">1.1 核心组件</a></li><li><a href="#12-模型结构" rel="noopener nofollow">1.2 模型结构</a></li><li><a href="#13-transformer-使用" rel="noopener nofollow">1.3 Transformer 使用</a><ul><li><a href="#131-使用-hugging-face-transformers-库" rel="noopener nofollow">1.3.1 使用 Hugging Face Transformers 库</a></li><li><a href="#132-自定义-transformer-模型" rel="noopener nofollow">1.3.2 自定义 Transformer 模型</a></li><li><a href="#133-transformer-的-demo" rel="noopener nofollow">1.3.3 Transformer 的 Demo</a><ul><li><a href="#1331-安装依赖" rel="noopener nofollow">1.3.3.1 安装依赖</a></li><li><a href="#1332-代码实现" rel="noopener nofollow">1.3.3.2 代码实现</a></li><li><a href="#1333--输出示例" rel="noopener nofollow">1.3.3.3  输出示例</a></li></ul></li></ul></li></ul></li><li><a href="#2-huggingface" rel="noopener nofollow">2. Huggingface</a><ul><li><a href="#21-huggingface-的具体介绍" rel="noopener nofollow">2.1 Huggingface 的具体介绍</a></li><li><a href="#22-huggingface-的-models" rel="noopener nofollow">2.2 Huggingface 的 Models</a></li><li><a href="#23-模型的使用" rel="noopener nofollow">2.3 模型的使用</a><ul><li><a href="#231-使用方法-1" rel="noopener nofollow">2.3.1 使用方法-1</a></li><li><a href="#232--使用方法-2" rel="noopener nofollow">2.3.2  使用方法-2</a></li></ul></li><li><a href="#24--huggingface的datasets" rel="noopener nofollow">2.4  Huggingface的Datasets</a><ul><li><a href="#241-导入数据集的方法" rel="noopener nofollow">2.4.1 导入数据集的方法</a></li><li><a href="#242--有了数据后训练模型方法" rel="noopener nofollow">2.4.2  有了数据后训练模型方法</a></li></ul></li><li><a href="#25--huggingface的spaces" rel="noopener nofollow">2.5  Huggingface的Spaces</a></li></ul></li></ul></div><p></p>
<h2 id="1-transformer模型">1. Transformer模型</h2>
<p><strong>Transformer</strong> 是一种基于自注意力机制（Self-Attention）的深度学习模型，最初由 Vaswani 等人在 2017 年的论文《Attention is All You Need》中提出。Transformer 模型在自然语言处理（NLP）任务中表现出色，逐渐取代了传统的循环神经网络（RNN）和卷积神经网络（CNN）模型，成为 NLP 领域的主流架构。</p>
<p><img src="https://img2024.cnblogs.com/blog/682547/202502/682547-20250208142028813-343225409.png" alt="image" loading="lazy"></p>
<h3 id="11-核心组件">1.1 核心组件</h3>
<p><strong>自注意力机制（Self-Attention）:</strong></p>
<p>自注意力机制允许模型在处理输入序列时，关注序列中的不同部分，从而捕捉序列内部的依赖关系。</p>
<p>通过计算每个词与其他词的相关性，模型可以动态地调整每个词的表示。</p>
<p><strong>多头注意力（Multi-Head Attention）:</strong></p>
<p>多头注意力机制通过并行计算多个自注意力头，捕捉不同子空间的信息，增强模型的表达能力。</p>
<p><strong>位置编码（Positional Encoding）:</strong></p>
<p>由于 Transformer 模型没有显式的序列信息（如 RNN 中的时间步），位置编码被引入以提供序列中每个词的位置信息。</p>
<p><strong>前馈神经网络（Feed-Forward Neural Network）:</strong></p>
<p>每个 Transformer 层包含一个前馈神经网络，用于进一步处理自注意力机制的输出。</p>
<p>残差连接和层归一化（Residual Connection and Layer Normalization）:</p>
<p>残差连接有助于缓解梯度消失问题，层归一化则用于稳定训练过程。</p>
<h3 id="12-模型结构">1.2 模型结构</h3>
<p>Transformer 模型通常由编码器（Encoder）和解码器（Decoder）两部分组成：</p>
<p>编码器：由多个相同的层堆叠而成，每层包含一个多头自注意力机制和一个前馈神经网络。</p>
<p>解码器：同样由多个相同的层堆叠而成，每层包含一个多头自注意力机制、一个编码器-解码器注意力机制和一个前馈神经网络。</p>
<h3 id="13-transformer-使用">1.3 Transformer 使用</h3>
<p>Transformer 模型广泛应用于各种 NLP 任务，如机器翻译、文本生成、文本分类、问答系统等。以下是使用 Transformer 模型的基本步骤：</p>
<h4 id="131-使用-hugging-face-transformers-库">1.3.1 使用 Hugging Face Transformers 库</h4>
<p>首先，确保安装了必要的深度学习框架，如 PyTorch 或 TensorFlow。此外，可以使用 Hugging Face 的 transformers 库，它提供了预训练的 Transformer 模型和简单的接口。</p>
<pre><code>from transformers import pipeline

# 加载预训练的文本生成模型（如 GPT-2）
generator = pipeline("text-generation", model="gpt2")

# 生成文本
prompt = "Once upon a time"
output = generator(prompt, max_length=50, num_return_sequences=1)

print(output[0]['generated_text'])
</code></pre>
<h4 id="132-自定义-transformer-模型">1.3.2 自定义 Transformer 模型</h4>
<p>如果需要从头实现 Transformer，可以参考以下步骤：<br>
示例：使用 PyTorch 实现 Transformer</p>
<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F

class Transformer(nn.Module):
    def __init__(self, input_dim, model_dim, num_heads, num_layers, output_dim):
        super(Transformer, self).__init__()
        self.embedding = nn.Embedding(input_dim, model_dim)
        self.positional_encoding = nn.Parameter(torch.zeros(1, 1000, model_dim))  # 假设最大序列长度为 1000
        self.encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads)
        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)
        self.fc = nn.Linear(model_dim, output_dim)

    def forward(self, src):
        src = self.embedding(src) + self.positional_encoding[:, :src.size(1), :]
        output = self.transformer_encoder(src)
        output = self.fc(output.mean(dim=1))  # 取序列的平均值作为输出
        return output

# 示例用法
input_dim = 10000  # 词汇表大小
model_dim = 512    # 模型维度
num_heads = 8      # 注意力头数
num_layers = 6     # 编码器层数
output_dim = 10    # 输出类别数

model = Transformer(input_dim, model_dim, num_heads, num_layers, output_dim)
src = torch.randint(0, input_dim, (32, 100))  # 输入序列 (batch_size, seq_len)
output = model(src)
print(output.shape)  # 输出形状: (batch_size, output_dim)
</code></pre>
<h4 id="133-transformer-的-demo">1.3.3 Transformer 的 Demo</h4>
<p>以下是一个简单的文本分类任务的 Demo，使用 Hugging Face 的预训练模型。</p>
<h5 id="1331-安装依赖">1.3.3.1 安装依赖</h5>
<pre><code>pip install transformers torch
</code></pre>
<h5 id="1332-代码实现">1.3.3.2 代码实现</h5>
<pre><code>from transformers import pipeline

# 加载预训练的文本分类模型
classifier = pipeline("text-classification", model="distilbert-base-uncased-finetuned-sst-2-english")

# 输入文本
text = "I love using transformers, it's so easy and powerful!"

# 进行分类
result = classifier(text)
print(result)
</code></pre>
<h5 id="1333--输出示例">1.3.3.3  输出示例</h5>
<pre><code>[{'label': 'POSITIVE', 'score': 0.9998}]
</code></pre>
<h2 id="2-huggingface">2. Huggingface</h2>
<p>Huggingface 既是网站名也是其公司名，随着 transformer 浪潮，Huggingface 逐步收纳了众多最前沿的模型和数据集等有趣的工<br>
作，与 transformers 库结合，可以快速使用学习这些模型。目前提到 NLP 必然绕不开 Huggingface。</p>
<h3 id="21-huggingface-的具体介绍">2.1 Huggingface 的具体介绍</h3>
<p>进入 Huggingface 网站,如下图所示<br>
<img src="https://img2024.cnblogs.com/blog/682547/202502/682547-20250208142552341-2014217342.png" alt="image" loading="lazy"></p>
<p>其主要包含：<br>
Models（模型），包括各种处理 CV 和 NLP 等任务的模型，上面模型都是可以免费获得<br>
Datasets（数据集），包括很多数据集<br>
Spaces（分享空间），包括社区空间下最新的一些有意思的分享，可以理解为 huggingface 朋友圈<br>
Docs（文档，各种模型算法文档），包括各种模型算法等说明使用文档<br>
Solutions（解决方案，体验等），包括 others</p>
<h3 id="22-huggingface-的-models">2.2 Huggingface 的 Models</h3>
<p>点开 Models。可以看到下图的任务<br>
<img src="https://img2024.cnblogs.com/blog/682547/202502/682547-20250208142616223-737215713.png" alt="image" loading="lazy"></p>
<p>其中，主要包括计算机视觉、自然语言处理、语音处理、多模态、表格处理、强化学习。</p>
<p><strong>展开介绍：</strong></p>
<p><strong>Computer Vision（计算机视觉任务）</strong>：包括 lmage Classification（图像分类），lmage Segmentation（图像分割）、zero-Shot lmage Classification（零样本图像分类）、lmage-to-Image（图像到图像的任务）、Unconditional lmage Generation（无条件图像生成）、Object Detection(目标检测)、Video Classification（视频分类）、Depth Estimation(深度估计，估计拍摄者距离图像各处的距离)</p>
<p><strong>Natural Language Processing（自然语言处理）</strong>：包括 Translation（机器翻译）、Fill-Mask(填充掩码，预测句子中被遮掩的词)、Token Classification（词分类）、Sentence Similarity（句子相似度）、Question Answering（问答系统），Summarization（总结，缩句）、Zero-Shot Classification (零样本分类)、Text Classification（文本分类）、Text2Tex（t 文本到文本的生成）、Text Generation<br>
（文本生成）、Conversational（聊天）、Table Question Answer（表问答，1.预测表格中被遮掩单词 2.数字推理，判断句子是否被表格数据支持）</p>
<p><strong>Audio（语音）</strong>：Automatic Speech Recognition（语音识别）、Audio Classification（语音分类）、Text-to-Speech（文本到语音的生成）、Audio-to-Audio（语音到语音的生成）、Voice Activity Detection（声音检测、检测识别出需要的声音部分）</p>
<p><strong>Multimodal（多模态）</strong>：Feature Extraction（特征提取）、Text-to-Image（文本到图像）、Visual Question Answering（视觉问答）、Image2Text（图像到文本）、Document Question Answering（文档问答）</p>
<p><strong>Tabular（表格）</strong>：Tabular Classification（表分类）、Tabular Regression（表回归）</p>
<p><strong>Reinforcement Learning（强化学习）</strong>：Reinforcement Learning（强化学习）、Robotics（机器人）</p>
<h3 id="23-模型的使用">2.3 模型的使用</h3>
<p>一般来说，页面上会给出模型的介绍。例如，我们打开其中一个 fill-mask 任务下下载最多的模型 bert-base-uncased<br>
<img src="https://img2024.cnblogs.com/blog/682547/202502/682547-20250208144035743-697048616.png" alt="image" loading="lazy"></p>
<p>可以看到模型描述：<br>
<img src="https://img2024.cnblogs.com/blog/682547/202502/682547-20250208144059960-626331135.png" alt="image" loading="lazy"></p>
<h4 id="231-使用方法-1">2.3.1 使用方法-1</h4>
<p>需要提前安装 transformers 库，可以直接 pip install transformers 安装。还有 Pytorch 或 TensorFlow 库，读者自行下载。</p>
<p>下载完后可以使用 pipeline 直接简单的使用这些模型。第一次执行时 pipeline 会加载模型，模型会自动下载到本地，可以直接用。</p>
<p>第一个参数是任务类型，第二个是具体模型名字</p>
<pre><code>from transformers import pipeline

unmasker = pipeline('fill-mask', model='bert-base-uncased')

unmasker("Hello I'm a [MASK] model.")
</code></pre>
<p>运行结果：</p>
<pre><code>[
    {
        "score": 0.10731087625026703,
        "token": 4827,
        "token_str": "fashion",
        "sequence": "hello i ' m a fashion model."
    },
    {
        "score": 0.08774463832378387,
        "token": 2535,
        "token_str": "role",
        "sequence": "hello i ' m a role model."
    },
    {
        "score": 0.053383927792310715,
        "token": 2047,
        "token_str": "new",
        "sequence": "hello i ' m a new model."
    },
    {
        "score": 0.046672236174345016,
        "token": 3565,
        "token_str": "super",
        "sequence": "hello i ' m a super model."
    },
    {
        "score": 0.027095887809991837,
        "token": 2986,
        "token_str": "fine",
        "sequence": "hello i ' m a fine model."
    }
]

</code></pre>
<p>模型下载在这个地方:</p>
<p>C:\Users\用户\.cache\huggingface\hub</p>
<p>不同模型使用方法略有区别，直接通过页面学习或文档学习最好<br>
<img src="https://img2024.cnblogs.com/blog/682547/202502/682547-20250208144143279-2093376612.png" alt="image" loading="lazy"></p>
<p><strong>可以自定义加载输入分词器:使用 AutoTokenizer</strong></p>
<pre><code>from transformers import AutoTokenizer 
#下面这种方式可以自动加载 bert-base-uncased 中使用的分词器
tokenizer=AutoTokenizer.from_pretrained("bert-base-uncased")
</code></pre>
<p><strong>可以自定义加载模型结构:使用 AutoModel ， 不包括输入分词器和输出部分！！！</strong></p>
<pre><code>from transformers import AutoModel
#下面这种方式可以自动加载 bert-base-uncased 中使用的模型，没有最后的全连接输出层和 softmax
model=AutoModel.from_pretrained("bert-base-uncased")
</code></pre>
<p><strong>可以自定义加载模型和输出部分:使用 AutoModelForSequenceClassification 等</strong></p>
<pre><code>from transformers import AutoModelForSequenceClassification
#下面这种方式可以自动加载 bert-base-uncased 中使用的模型（包括了输出部分），有最后的全连接输出层
model=AutoModel.AutoModelForSequenceClassification("bert-base-uncased")
</code></pre>
<p><strong>模型保存</strong></p>
<pre><code>model.save_pretrained("./")#保持到当前目录
</code></pre>
<p><strong>一个简单的流程例子：</strong></p>
<p>代码接收一个句子列表，对其进行分词，将其传递给一个预训练的情感分析模型，然后处理输出以获得每个类别的预测概率。最后，将结果打印到控制台</p>
<pre><code>input=['The first sentence!','The second sentence!']

from transformers import AutoTokenizer

#从 Transformers 库中导入 AutoTokenizer 类，用于对输入句子进行分词。
#分词是将文本转换为数值标记的过程，这些标记可以被模型理解。from_pretrained 方法加载一个预训练的分词器
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

#使用 tokenizer 对象对输入句子进行分词。padding=True 参数确保所有句子都被填充到相同的长度，
#truncation=True 截断过长的句子，return_tensors='pt' 返回 PyTorch 张量
input = tokenizer(input, padding=True, truncation=True, return_tensors='pt')

from transformers import AutoModelForSequenceClassification
# 模型加载：使用 AutoModelForSequenceClassification 类加载一个预训练的序列分类模型。
# from_pretrained 方法加载模型
model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

print(model)

# 模型推理
output = model(**input)

print(output.logits.shape)

import torch

predictions = torch.nn.functional.softmax(output.logits, dim=-1)

# 打印预测概率
print(predictions)

# ID 到标签的映射
print(model.config.id2label)
</code></pre>
<h4 id="232--使用方法-2">2.3.2  使用方法-2</h4>
<p><img src="https://img2024.cnblogs.com/blog/682547/202502/682547-20250208144356312-1005121862.png" alt="image" loading="lazy"></p>
<p>下面以 ChatGLM2-6B 为例（见上图），先在 github 上 git 下 ChatGLM2-6B 除模型外的相关文件</p>
<pre><code>git clone git@github.com:THUDM/ChatGLM2-6B.git

cd ChatGLM2-6B-main
</code></pre>
<p>安装好相关依赖</p>
<pre><code>pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple
</code></pre>
<p>类似刚才的方法一直接执行下面代码，会在网上自动下载模型文件</p>
<pre><code>&gt;&gt;&gt; from transformers import AutoTokenizer, AutoModel
&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True)
&gt;&gt;&gt; model = AutoModel.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True, device='cuda')
&gt;&gt;&gt; model = model.eval()
&gt;&gt;&gt; response, history = model.chat(tokenizer, "你好", history=[])
&gt;&gt;&gt; print(response)
你好👋!我是人工智能助手 ChatGLM2-6B,很高兴见到你,欢迎问我任何问题。
&gt;&gt;&gt; response, history = model.chat(tokenizer, "晚上睡不着应该怎么办", history=history)
&gt;&gt;&gt; print(response)
晚上睡不着可能会让你感到焦虑或不舒服,但以下是一些可以帮助你入睡的方法:

1. 制定规律的睡眠时间表:保持规律的睡眠时间表可以帮助你建立健康的睡眠习惯,使你更容易入睡。尽量在每天的相同时间上床,并在同一时间起床。
2. 创造一个舒适的睡眠环境:确保睡眠环境舒适,安静,黑暗且温度适宜。可以使用舒适的床上用品,并保持房间通风。
3. 放松身心:在睡前做些放松的活动,例如泡个热水澡,听些轻柔的音乐,阅读一些有趣的书籍等,有助于缓解紧张和焦虑,使你更容易入睡。
4. 避免饮用含有咖啡因的饮料:咖啡因是一种刺激性物质,会影响你的睡眠质量。尽量避免在睡前饮用含有咖啡因的饮料,例如咖啡,茶和可乐。
5. 避免在床上做与睡眠无关的事情:在床上做些与睡眠无关的事情,例如看电影,玩游戏或工作等,可能会干扰你的睡眠。
6. 尝试呼吸技巧:深呼吸是一种放松技巧,可以帮助你缓解紧张和焦虑,使你更容易入睡。试着慢慢吸气,保持几秒钟,然后缓慢呼气。

如果这些方法无法帮助你入睡,你可以考虑咨询医生或睡眠专家,寻求进一步的建议。
</code></pre>
<p>也可以方法二，找到 huggingface 上 ChatGLM2-6B 模型地址，直接 git</p>
<pre><code>git clone https://huggingface.co/THUDM/chatglm2-6b
</code></pre>
<p>然后打开刚才的 ChatGLM2-6B 里的 web_demo.py，修改里面的模型和 AutoTokenizer 目录，为刚才 git 模型的目录，例如我在ChatGLM2-6B 里新建了一个 model，在 model 目录下 git 模型的，所以我的目录修改为下图</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained("model/chatglm2-6b", trust_remote_code=True)
model = AutoModel.from_pretrained("model/chatglm2-6b", trust_remote_code=True).cuda()
</code></pre>
<p>最后，在终端直接执行下面代码</p>
<pre><code>python web_demo.py
</code></pre>
<p>点击启动后的链接，即可使用 web版本的ChatGLM2-6B</p>
<h3 id="24--huggingface的datasets">2.4  Huggingface的Datasets</h3>
<p>可以看到有如下任务的数据集。读者可自行打开学习</p>
<p><img src="https://img2024.cnblogs.com/blog/682547/202502/682547-20250208144438996-1503040558.png" alt="image" loading="lazy"></p>
<p>例如，我们打开 Text Classification 任务的 glue 数据集,可以看到下图，里面会有数据集的介绍、相关信息和下载方式，读者自行查看。<br>
<img src="https://img2024.cnblogs.com/blog/682547/202502/682547-20250208144449012-902800212.png" alt="image" loading="lazy"></p>
<h4 id="241-导入数据集的方法">2.4.1 导入数据集的方法</h4>
<p>提前 <code>pip install datasets</code></p>
<pre><code>from datasets import load_dataset

datasets = load_dataset('glue', 'mrpc')  # 加载glue数据集
print(datasets)  # 打印数据集

print(datasets['train'][0])  # 打印第一个样本
</code></pre>
<h4 id="242--有了数据后训练模型方法">2.4.2  有了数据后训练模型方法</h4>
<p>下面给出 bert-base-uncased 的例子，实现对两个句子的相似度计算</p>
<pre><code># 导入tokenizer

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# input = tokenizer("The first sentence!", "The second sentence!")
#
# print(tokenizer.convert_ids_to_tokens(input['input_ids']))


# 实际使用 tokenizer 的方法，得到 tokenizer_data
def tokenize_function(examples):
    return tokenizer(examples['sentence1'], examples['sentence2'], truncation=True)


from datasets import load_dataset
datasets = load_dataset("glue", "mrpc")
tokenizer_data = datasets.map(tokenize_function, batched=True)
print(tokenizer_data)

# 训练参数
from transformers import TrainingArguments

# https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments
training_args = TrainingArguments("test_trainer")
print(training_args)#看下默认值

# 导入模型
from transformers import AutoModelForSequenceClassification
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

#导入数据处理的一个东西 DataCollatorWithPadding，变成一个一个 batch
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

#导入训练器，进行训练,API : https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Traine

from transformers import Trainer
trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenizer_data["train"],
    eval_dataset=tokenizer_data["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)

trainer.train()
</code></pre>
<h3 id="25--huggingface的spaces">2.5  Huggingface的Spaces</h3>
<p>点开如下图所示。里面有些近些天有趣的东西火热的 apps<br>
<img src="https://img2024.cnblogs.com/blog/682547/202502/682547-20250208144530402-881224256.png" alt="image" loading="lazy"></p>
<p>比如下面的一个统一的多模态理解和生成模型<br>
<img src="https://img2024.cnblogs.com/blog/682547/202502/682547-20250208144537707-1862109150.png" alt="image" loading="lazy"></p>

</div>
<div id="MySignature" role="contentinfo">
    <div>
<h3><span style="font: 400 16px Simsun; color: #0000C6"> 微信公众号</span></h3>
<h3><img style="width: 400px; margin-left: 2px;" src="https://images.cnblogs.com/cnblogs_com/bigdata1024/2014130/o_221015130328_%E6%89%AB%E7%A0%81_%E6%90%9C%E7%B4%A2%E8%81%94%E5%90%88%E4%BC%A0%E6%92%AD%E6%A0%B7%E5%BC%8F-%E6%A0%87%E5%87%86%E8%89%B2%E7%89%88.png"></h3>
</div>
<div>
<h3><span style="font: 400 16px Simsun; color: #0000C6">作者：<a href="https://www.cnblogs.com/bigdata1024/" target="_blank">chaplinthink</a></span> 
<a style="color: green" href="https://www.cnblogs.com/bigdata1024/p/16795143.html"> ===&gt; [欢迎赞赏作者， 您的赞赏，是我前进的动力🙂]</a>
</h3>
<h3><span style="font: 400 16px Simsun; color: #0000C6">出处：<a href="https://www.cnblogs.com/bigdata1024/p/18704285" target="_blank">https://www.cnblogs.com/bigdata1024/p/18704285</a></span></h3>
<h3><span style="font: 400 16px Simsun; color: #0000C6">本文以学习、研究和分享为主，如需转载，请联系本人，标明作者和出处，非商业用途!</span></h3>
</div>
</div>
<div class="clear"></div>

		</div>
		<div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.05327655332175926" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-02-08 15:00">2025-02-08 14:47</span>&nbsp;
<a href="https://www.cnblogs.com/bigdata1024">chaplinthink</a>&nbsp;
阅读(<span id="post_view_count">19</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18704285" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18704285);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18704285', targetLink: 'https://www.cnblogs.com/bigdata1024/p/18704285', title: 'Huggingface使用' })">举报</a>
</div>
	