
		<h1 class="postTitle">
			<a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/bigdata1024/p/18704285" title="å‘å¸ƒäº 2025-02-08 14:47">
    <span role="heading" aria-level="2">Huggingfaceä½¿ç”¨</span>
    

</a>

		</h1>
		<div class="clear"></div>
		<div class="postBody">
			<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p></p><div class="toc"><div class="toc-container-header">ç›®å½•</div><ul><li><a href="#1-transformeræ¨¡å‹" rel="noopener nofollow">1. Transformeræ¨¡å‹</a><ul><li><a href="#11-æ ¸å¿ƒç»„ä»¶" rel="noopener nofollow">1.1 æ ¸å¿ƒç»„ä»¶</a></li><li><a href="#12-æ¨¡å‹ç»“æ„" rel="noopener nofollow">1.2 æ¨¡å‹ç»“æ„</a></li><li><a href="#13-transformer-ä½¿ç”¨" rel="noopener nofollow">1.3 Transformer ä½¿ç”¨</a><ul><li><a href="#131-ä½¿ç”¨-hugging-face-transformers-åº“" rel="noopener nofollow">1.3.1 ä½¿ç”¨ Hugging Face Transformers åº“</a></li><li><a href="#132-è‡ªå®šä¹‰-transformer-æ¨¡å‹" rel="noopener nofollow">1.3.2 è‡ªå®šä¹‰ Transformer æ¨¡å‹</a></li><li><a href="#133-transformer-çš„-demo" rel="noopener nofollow">1.3.3 Transformer çš„ Demo</a><ul><li><a href="#1331-å®‰è£…ä¾èµ–" rel="noopener nofollow">1.3.3.1 å®‰è£…ä¾èµ–</a></li><li><a href="#1332-ä»£ç å®ç°" rel="noopener nofollow">1.3.3.2 ä»£ç å®ç°</a></li><li><a href="#1333--è¾“å‡ºç¤ºä¾‹" rel="noopener nofollow">1.3.3.3  è¾“å‡ºç¤ºä¾‹</a></li></ul></li></ul></li></ul></li><li><a href="#2-huggingface" rel="noopener nofollow">2. Huggingface</a><ul><li><a href="#21-huggingface-çš„å…·ä½“ä»‹ç»" rel="noopener nofollow">2.1 Huggingface çš„å…·ä½“ä»‹ç»</a></li><li><a href="#22-huggingface-çš„-models" rel="noopener nofollow">2.2 Huggingface çš„ Models</a></li><li><a href="#23-æ¨¡å‹çš„ä½¿ç”¨" rel="noopener nofollow">2.3 æ¨¡å‹çš„ä½¿ç”¨</a><ul><li><a href="#231-ä½¿ç”¨æ–¹æ³•-1" rel="noopener nofollow">2.3.1 ä½¿ç”¨æ–¹æ³•-1</a></li><li><a href="#232--ä½¿ç”¨æ–¹æ³•-2" rel="noopener nofollow">2.3.2  ä½¿ç”¨æ–¹æ³•-2</a></li></ul></li><li><a href="#24--huggingfaceçš„datasets" rel="noopener nofollow">2.4  Huggingfaceçš„Datasets</a><ul><li><a href="#241-å¯¼å…¥æ•°æ®é›†çš„æ–¹æ³•" rel="noopener nofollow">2.4.1 å¯¼å…¥æ•°æ®é›†çš„æ–¹æ³•</a></li><li><a href="#242--æœ‰äº†æ•°æ®åè®­ç»ƒæ¨¡å‹æ–¹æ³•" rel="noopener nofollow">2.4.2  æœ‰äº†æ•°æ®åè®­ç»ƒæ¨¡å‹æ–¹æ³•</a></li></ul></li><li><a href="#25--huggingfaceçš„spaces" rel="noopener nofollow">2.5  Huggingfaceçš„Spaces</a></li></ul></li></ul></div><p></p>
<h2 id="1-transformeræ¨¡å‹">1. Transformeræ¨¡å‹</h2>
<p><strong>Transformer</strong> æ˜¯ä¸€ç§åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œæœ€åˆç”± Vaswani ç­‰äººåœ¨ 2017 å¹´çš„è®ºæ–‡ã€ŠAttention is All You Needã€‹ä¸­æå‡ºã€‚Transformer æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œé€æ¸å–ä»£äº†ä¼ ç»Ÿçš„å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰å’Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¨¡å‹ï¼Œæˆä¸º NLP é¢†åŸŸçš„ä¸»æµæ¶æ„ã€‚</p>
<p><img src="https://img2024.cnblogs.com/blog/682547/202502/682547-20250208142028813-343225409.png" alt="image" loading="lazy"></p>
<h3 id="11-æ ¸å¿ƒç»„ä»¶">1.1 æ ¸å¿ƒç»„ä»¶</h3>
<p><strong>è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰:</strong></p>
<p>è‡ªæ³¨æ„åŠ›æœºåˆ¶å…è®¸æ¨¡å‹åœ¨å¤„ç†è¾“å…¥åºåˆ—æ—¶ï¼Œå…³æ³¨åºåˆ—ä¸­çš„ä¸åŒéƒ¨åˆ†ï¼Œä»è€Œæ•æ‰åºåˆ—å†…éƒ¨çš„ä¾èµ–å…³ç³»ã€‚</p>
<p>é€šè¿‡è®¡ç®—æ¯ä¸ªè¯ä¸å…¶ä»–è¯çš„ç›¸å…³æ€§ï¼Œæ¨¡å‹å¯ä»¥åŠ¨æ€åœ°è°ƒæ•´æ¯ä¸ªè¯çš„è¡¨ç¤ºã€‚</p>
<p><strong>å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰:</strong></p>
<p>å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶é€šè¿‡å¹¶è¡Œè®¡ç®—å¤šä¸ªè‡ªæ³¨æ„åŠ›å¤´ï¼Œæ•æ‰ä¸åŒå­ç©ºé—´çš„ä¿¡æ¯ï¼Œå¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚</p>
<p><strong>ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰:</strong></p>
<p>ç”±äº Transformer æ¨¡å‹æ²¡æœ‰æ˜¾å¼çš„åºåˆ—ä¿¡æ¯ï¼ˆå¦‚ RNN ä¸­çš„æ—¶é—´æ­¥ï¼‰ï¼Œä½ç½®ç¼–ç è¢«å¼•å…¥ä»¥æä¾›åºåˆ—ä¸­æ¯ä¸ªè¯çš„ä½ç½®ä¿¡æ¯ã€‚</p>
<p><strong>å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeed-Forward Neural Networkï¼‰:</strong></p>
<p>æ¯ä¸ª Transformer å±‚åŒ…å«ä¸€ä¸ªå‰é¦ˆç¥ç»ç½‘ç»œï¼Œç”¨äºè¿›ä¸€æ­¥å¤„ç†è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„è¾“å‡ºã€‚</p>
<p>æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–ï¼ˆResidual Connection and Layer Normalizationï¼‰:</p>
<p>æ®‹å·®è¿æ¥æœ‰åŠ©äºç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œå±‚å½’ä¸€åŒ–åˆ™ç”¨äºç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚</p>
<h3 id="12-æ¨¡å‹ç»“æ„">1.2 æ¨¡å‹ç»“æ„</h3>
<p>Transformer æ¨¡å‹é€šå¸¸ç”±ç¼–ç å™¨ï¼ˆEncoderï¼‰å’Œè§£ç å™¨ï¼ˆDecoderï¼‰ä¸¤éƒ¨åˆ†ç»„æˆï¼š</p>
<p>ç¼–ç å™¨ï¼šç”±å¤šä¸ªç›¸åŒçš„å±‚å †å è€Œæˆï¼Œæ¯å±‚åŒ…å«ä¸€ä¸ªå¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œä¸€ä¸ªå‰é¦ˆç¥ç»ç½‘ç»œã€‚</p>
<p>è§£ç å™¨ï¼šåŒæ ·ç”±å¤šä¸ªç›¸åŒçš„å±‚å †å è€Œæˆï¼Œæ¯å±‚åŒ…å«ä¸€ä¸ªå¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€ä¸€ä¸ªç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›æœºåˆ¶å’Œä¸€ä¸ªå‰é¦ˆç¥ç»ç½‘ç»œã€‚</p>
<h3 id="13-transformer-ä½¿ç”¨">1.3 Transformer ä½¿ç”¨</h3>
<p>Transformer æ¨¡å‹å¹¿æ³›åº”ç”¨äºå„ç§ NLP ä»»åŠ¡ï¼Œå¦‚æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆã€æ–‡æœ¬åˆ†ç±»ã€é—®ç­”ç³»ç»Ÿç­‰ã€‚ä»¥ä¸‹æ˜¯ä½¿ç”¨ Transformer æ¨¡å‹çš„åŸºæœ¬æ­¥éª¤ï¼š</p>
<h4 id="131-ä½¿ç”¨-hugging-face-transformers-åº“">1.3.1 ä½¿ç”¨ Hugging Face Transformers åº“</h4>
<p>é¦–å…ˆï¼Œç¡®ä¿å®‰è£…äº†å¿…è¦çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå¦‚ PyTorch æˆ– TensorFlowã€‚æ­¤å¤–ï¼Œå¯ä»¥ä½¿ç”¨ Hugging Face çš„ transformers åº“ï¼Œå®ƒæä¾›äº†é¢„è®­ç»ƒçš„ Transformer æ¨¡å‹å’Œç®€å•çš„æ¥å£ã€‚</p>
<pre><code>from transformers import pipeline

# åŠ è½½é¢„è®­ç»ƒçš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚ GPT-2ï¼‰
generator = pipeline("text-generation", model="gpt2")

# ç”Ÿæˆæ–‡æœ¬
prompt = "Once upon a time"
output = generator(prompt, max_length=50, num_return_sequences=1)

print(output[0]['generated_text'])
</code></pre>
<h4 id="132-è‡ªå®šä¹‰-transformer-æ¨¡å‹">1.3.2 è‡ªå®šä¹‰ Transformer æ¨¡å‹</h4>
<p>å¦‚æœéœ€è¦ä»å¤´å®ç° Transformerï¼Œå¯ä»¥å‚è€ƒä»¥ä¸‹æ­¥éª¤ï¼š<br>
ç¤ºä¾‹ï¼šä½¿ç”¨ PyTorch å®ç° Transformer</p>
<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F

class Transformer(nn.Module):
    def __init__(self, input_dim, model_dim, num_heads, num_layers, output_dim):
        super(Transformer, self).__init__()
        self.embedding = nn.Embedding(input_dim, model_dim)
        self.positional_encoding = nn.Parameter(torch.zeros(1, 1000, model_dim))  # å‡è®¾æœ€å¤§åºåˆ—é•¿åº¦ä¸º 1000
        self.encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads)
        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)
        self.fc = nn.Linear(model_dim, output_dim)

    def forward(self, src):
        src = self.embedding(src) + self.positional_encoding[:, :src.size(1), :]
        output = self.transformer_encoder(src)
        output = self.fc(output.mean(dim=1))  # å–åºåˆ—çš„å¹³å‡å€¼ä½œä¸ºè¾“å‡º
        return output

# ç¤ºä¾‹ç”¨æ³•
input_dim = 10000  # è¯æ±‡è¡¨å¤§å°
model_dim = 512    # æ¨¡å‹ç»´åº¦
num_heads = 8      # æ³¨æ„åŠ›å¤´æ•°
num_layers = 6     # ç¼–ç å™¨å±‚æ•°
output_dim = 10    # è¾“å‡ºç±»åˆ«æ•°

model = Transformer(input_dim, model_dim, num_heads, num_layers, output_dim)
src = torch.randint(0, input_dim, (32, 100))  # è¾“å…¥åºåˆ— (batch_size, seq_len)
output = model(src)
print(output.shape)  # è¾“å‡ºå½¢çŠ¶: (batch_size, output_dim)
</code></pre>
<h4 id="133-transformer-çš„-demo">1.3.3 Transformer çš„ Demo</h4>
<p>ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„æ–‡æœ¬åˆ†ç±»ä»»åŠ¡çš„ Demoï¼Œä½¿ç”¨ Hugging Face çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚</p>
<h5 id="1331-å®‰è£…ä¾èµ–">1.3.3.1 å®‰è£…ä¾èµ–</h5>
<pre><code>pip install transformers torch
</code></pre>
<h5 id="1332-ä»£ç å®ç°">1.3.3.2 ä»£ç å®ç°</h5>
<pre><code>from transformers import pipeline

# åŠ è½½é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ†ç±»æ¨¡å‹
classifier = pipeline("text-classification", model="distilbert-base-uncased-finetuned-sst-2-english")

# è¾“å…¥æ–‡æœ¬
text = "I love using transformers, it's so easy and powerful!"

# è¿›è¡Œåˆ†ç±»
result = classifier(text)
print(result)
</code></pre>
<h5 id="1333--è¾“å‡ºç¤ºä¾‹">1.3.3.3  è¾“å‡ºç¤ºä¾‹</h5>
<pre><code>[{'label': 'POSITIVE', 'score': 0.9998}]
</code></pre>
<h2 id="2-huggingface">2. Huggingface</h2>
<p>Huggingface æ—¢æ˜¯ç½‘ç«™åä¹Ÿæ˜¯å…¶å…¬å¸åï¼Œéšç€ transformer æµªæ½®ï¼ŒHuggingface é€æ­¥æ”¶çº³äº†ä¼—å¤šæœ€å‰æ²¿çš„æ¨¡å‹å’Œæ•°æ®é›†ç­‰æœ‰è¶£çš„å·¥<br>
ä½œï¼Œä¸ transformers åº“ç»“åˆï¼Œå¯ä»¥å¿«é€Ÿä½¿ç”¨å­¦ä¹ è¿™äº›æ¨¡å‹ã€‚ç›®å‰æåˆ° NLP å¿…ç„¶ç»•ä¸å¼€ Huggingfaceã€‚</p>
<h3 id="21-huggingface-çš„å…·ä½“ä»‹ç»">2.1 Huggingface çš„å…·ä½“ä»‹ç»</h3>
<p>è¿›å…¥ Huggingface ç½‘ç«™,å¦‚ä¸‹å›¾æ‰€ç¤º<br>
<img src="https://img2024.cnblogs.com/blog/682547/202502/682547-20250208142552341-2014217342.png" alt="image" loading="lazy"></p>
<p>å…¶ä¸»è¦åŒ…å«ï¼š<br>
Modelsï¼ˆæ¨¡å‹ï¼‰ï¼ŒåŒ…æ‹¬å„ç§å¤„ç† CV å’Œ NLP ç­‰ä»»åŠ¡çš„æ¨¡å‹ï¼Œä¸Šé¢æ¨¡å‹éƒ½æ˜¯å¯ä»¥å…è´¹è·å¾—<br>
Datasetsï¼ˆæ•°æ®é›†ï¼‰ï¼ŒåŒ…æ‹¬å¾ˆå¤šæ•°æ®é›†<br>
Spacesï¼ˆåˆ†äº«ç©ºé—´ï¼‰ï¼ŒåŒ…æ‹¬ç¤¾åŒºç©ºé—´ä¸‹æœ€æ–°çš„ä¸€äº›æœ‰æ„æ€çš„åˆ†äº«ï¼Œå¯ä»¥ç†è§£ä¸º huggingface æœ‹å‹åœˆ<br>
Docsï¼ˆæ–‡æ¡£ï¼Œå„ç§æ¨¡å‹ç®—æ³•æ–‡æ¡£ï¼‰ï¼ŒåŒ…æ‹¬å„ç§æ¨¡å‹ç®—æ³•ç­‰è¯´æ˜ä½¿ç”¨æ–‡æ¡£<br>
Solutionsï¼ˆè§£å†³æ–¹æ¡ˆï¼Œä½“éªŒç­‰ï¼‰ï¼ŒåŒ…æ‹¬ others</p>
<h3 id="22-huggingface-çš„-models">2.2 Huggingface çš„ Models</h3>
<p>ç‚¹å¼€ Modelsã€‚å¯ä»¥çœ‹åˆ°ä¸‹å›¾çš„ä»»åŠ¡<br>
<img src="https://img2024.cnblogs.com/blog/682547/202502/682547-20250208142616223-737215713.png" alt="image" loading="lazy"></p>
<p>å…¶ä¸­ï¼Œä¸»è¦åŒ…æ‹¬è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è¯­éŸ³å¤„ç†ã€å¤šæ¨¡æ€ã€è¡¨æ ¼å¤„ç†ã€å¼ºåŒ–å­¦ä¹ ã€‚</p>
<p><strong>å±•å¼€ä»‹ç»ï¼š</strong></p>
<p><strong>Computer Visionï¼ˆè®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼‰</strong>ï¼šåŒ…æ‹¬ lmage Classificationï¼ˆå›¾åƒåˆ†ç±»ï¼‰ï¼Œlmage Segmentationï¼ˆå›¾åƒåˆ†å‰²ï¼‰ã€zero-Shot lmage Classificationï¼ˆé›¶æ ·æœ¬å›¾åƒåˆ†ç±»ï¼‰ã€lmage-to-Imageï¼ˆå›¾åƒåˆ°å›¾åƒçš„ä»»åŠ¡ï¼‰ã€Unconditional lmage Generationï¼ˆæ— æ¡ä»¶å›¾åƒç”Ÿæˆï¼‰ã€Object Detection(ç›®æ ‡æ£€æµ‹)ã€Video Classificationï¼ˆè§†é¢‘åˆ†ç±»ï¼‰ã€Depth Estimation(æ·±åº¦ä¼°è®¡ï¼Œä¼°è®¡æ‹æ‘„è€…è·ç¦»å›¾åƒå„å¤„çš„è·ç¦»)</p>
<p><strong>Natural Language Processingï¼ˆè‡ªç„¶è¯­è¨€å¤„ç†ï¼‰</strong>ï¼šåŒ…æ‹¬ Translationï¼ˆæœºå™¨ç¿»è¯‘ï¼‰ã€Fill-Mask(å¡«å……æ©ç ï¼Œé¢„æµ‹å¥å­ä¸­è¢«é®æ©çš„è¯)ã€Token Classificationï¼ˆè¯åˆ†ç±»ï¼‰ã€Sentence Similarityï¼ˆå¥å­ç›¸ä¼¼åº¦ï¼‰ã€Question Answeringï¼ˆé—®ç­”ç³»ç»Ÿï¼‰ï¼ŒSummarizationï¼ˆæ€»ç»“ï¼Œç¼©å¥ï¼‰ã€Zero-Shot Classification (é›¶æ ·æœ¬åˆ†ç±»)ã€Text Classificationï¼ˆæ–‡æœ¬åˆ†ç±»ï¼‰ã€Text2Texï¼ˆt æ–‡æœ¬åˆ°æ–‡æœ¬çš„ç”Ÿæˆï¼‰ã€Text Generation<br>
ï¼ˆæ–‡æœ¬ç”Ÿæˆï¼‰ã€Conversationalï¼ˆèŠå¤©ï¼‰ã€Table Question Answerï¼ˆè¡¨é—®ç­”ï¼Œ1.é¢„æµ‹è¡¨æ ¼ä¸­è¢«é®æ©å•è¯ 2.æ•°å­—æ¨ç†ï¼Œåˆ¤æ–­å¥å­æ˜¯å¦è¢«è¡¨æ ¼æ•°æ®æ”¯æŒï¼‰</p>
<p><strong>Audioï¼ˆè¯­éŸ³ï¼‰</strong>ï¼šAutomatic Speech Recognitionï¼ˆè¯­éŸ³è¯†åˆ«ï¼‰ã€Audio Classificationï¼ˆè¯­éŸ³åˆ†ç±»ï¼‰ã€Text-to-Speechï¼ˆæ–‡æœ¬åˆ°è¯­éŸ³çš„ç”Ÿæˆï¼‰ã€Audio-to-Audioï¼ˆè¯­éŸ³åˆ°è¯­éŸ³çš„ç”Ÿæˆï¼‰ã€Voice Activity Detectionï¼ˆå£°éŸ³æ£€æµ‹ã€æ£€æµ‹è¯†åˆ«å‡ºéœ€è¦çš„å£°éŸ³éƒ¨åˆ†ï¼‰</p>
<p><strong>Multimodalï¼ˆå¤šæ¨¡æ€ï¼‰</strong>ï¼šFeature Extractionï¼ˆç‰¹å¾æå–ï¼‰ã€Text-to-Imageï¼ˆæ–‡æœ¬åˆ°å›¾åƒï¼‰ã€Visual Question Answeringï¼ˆè§†è§‰é—®ç­”ï¼‰ã€Image2Textï¼ˆå›¾åƒåˆ°æ–‡æœ¬ï¼‰ã€Document Question Answeringï¼ˆæ–‡æ¡£é—®ç­”ï¼‰</p>
<p><strong>Tabularï¼ˆè¡¨æ ¼ï¼‰</strong>ï¼šTabular Classificationï¼ˆè¡¨åˆ†ç±»ï¼‰ã€Tabular Regressionï¼ˆè¡¨å›å½’ï¼‰</p>
<p><strong>Reinforcement Learningï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰</strong>ï¼šReinforcement Learningï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰ã€Roboticsï¼ˆæœºå™¨äººï¼‰</p>
<h3 id="23-æ¨¡å‹çš„ä½¿ç”¨">2.3 æ¨¡å‹çš„ä½¿ç”¨</h3>
<p>ä¸€èˆ¬æ¥è¯´ï¼Œé¡µé¢ä¸Šä¼šç»™å‡ºæ¨¡å‹çš„ä»‹ç»ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬æ‰“å¼€å…¶ä¸­ä¸€ä¸ª fill-mask ä»»åŠ¡ä¸‹ä¸‹è½½æœ€å¤šçš„æ¨¡å‹ bert-base-uncased<br>
<img src="https://img2024.cnblogs.com/blog/682547/202502/682547-20250208144035743-697048616.png" alt="image" loading="lazy"></p>
<p>å¯ä»¥çœ‹åˆ°æ¨¡å‹æè¿°ï¼š<br>
<img src="https://img2024.cnblogs.com/blog/682547/202502/682547-20250208144059960-626331135.png" alt="image" loading="lazy"></p>
<h4 id="231-ä½¿ç”¨æ–¹æ³•-1">2.3.1 ä½¿ç”¨æ–¹æ³•-1</h4>
<p>éœ€è¦æå‰å®‰è£… transformers åº“ï¼Œå¯ä»¥ç›´æ¥ pip install transformers å®‰è£…ã€‚è¿˜æœ‰ Pytorch æˆ– TensorFlow åº“ï¼Œè¯»è€…è‡ªè¡Œä¸‹è½½ã€‚</p>
<p>ä¸‹è½½å®Œåå¯ä»¥ä½¿ç”¨ pipeline ç›´æ¥ç®€å•çš„ä½¿ç”¨è¿™äº›æ¨¡å‹ã€‚ç¬¬ä¸€æ¬¡æ‰§è¡Œæ—¶ pipeline ä¼šåŠ è½½æ¨¡å‹ï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨ä¸‹è½½åˆ°æœ¬åœ°ï¼Œå¯ä»¥ç›´æ¥ç”¨ã€‚</p>
<p>ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯ä»»åŠ¡ç±»å‹ï¼Œç¬¬äºŒä¸ªæ˜¯å…·ä½“æ¨¡å‹åå­—</p>
<pre><code>from transformers import pipeline

unmasker = pipeline('fill-mask', model='bert-base-uncased')

unmasker("Hello I'm a [MASK] model.")
</code></pre>
<p>è¿è¡Œç»“æœï¼š</p>
<pre><code>[
    {
        "score": 0.10731087625026703,
        "token": 4827,
        "token_str": "fashion",
        "sequence": "hello i ' m a fashion model."
    },
    {
        "score": 0.08774463832378387,
        "token": 2535,
        "token_str": "role",
        "sequence": "hello i ' m a role model."
    },
    {
        "score": 0.053383927792310715,
        "token": 2047,
        "token_str": "new",
        "sequence": "hello i ' m a new model."
    },
    {
        "score": 0.046672236174345016,
        "token": 3565,
        "token_str": "super",
        "sequence": "hello i ' m a super model."
    },
    {
        "score": 0.027095887809991837,
        "token": 2986,
        "token_str": "fine",
        "sequence": "hello i ' m a fine model."
    }
]

</code></pre>
<p>æ¨¡å‹ä¸‹è½½åœ¨è¿™ä¸ªåœ°æ–¹:</p>
<p>C:\Users\ç”¨æˆ·\.cache\huggingface\hub</p>
<p>ä¸åŒæ¨¡å‹ä½¿ç”¨æ–¹æ³•ç•¥æœ‰åŒºåˆ«ï¼Œç›´æ¥é€šè¿‡é¡µé¢å­¦ä¹ æˆ–æ–‡æ¡£å­¦ä¹ æœ€å¥½<br>
<img src="https://img2024.cnblogs.com/blog/682547/202502/682547-20250208144143279-2093376612.png" alt="image" loading="lazy"></p>
<p><strong>å¯ä»¥è‡ªå®šä¹‰åŠ è½½è¾“å…¥åˆ†è¯å™¨:ä½¿ç”¨ AutoTokenizer</strong></p>
<pre><code>from transformers import AutoTokenizer 
#ä¸‹é¢è¿™ç§æ–¹å¼å¯ä»¥è‡ªåŠ¨åŠ è½½ bert-base-uncased ä¸­ä½¿ç”¨çš„åˆ†è¯å™¨
tokenizer=AutoTokenizer.from_pretrained("bert-base-uncased")
</code></pre>
<p><strong>å¯ä»¥è‡ªå®šä¹‰åŠ è½½æ¨¡å‹ç»“æ„:ä½¿ç”¨ AutoModel ï¼Œ ä¸åŒ…æ‹¬è¾“å…¥åˆ†è¯å™¨å’Œè¾“å‡ºéƒ¨åˆ†ï¼ï¼ï¼</strong></p>
<pre><code>from transformers import AutoModel
#ä¸‹é¢è¿™ç§æ–¹å¼å¯ä»¥è‡ªåŠ¨åŠ è½½ bert-base-uncased ä¸­ä½¿ç”¨çš„æ¨¡å‹ï¼Œæ²¡æœ‰æœ€åçš„å…¨è¿æ¥è¾“å‡ºå±‚å’Œ softmax
model=AutoModel.from_pretrained("bert-base-uncased")
</code></pre>
<p><strong>å¯ä»¥è‡ªå®šä¹‰åŠ è½½æ¨¡å‹å’Œè¾“å‡ºéƒ¨åˆ†:ä½¿ç”¨ AutoModelForSequenceClassification ç­‰</strong></p>
<pre><code>from transformers import AutoModelForSequenceClassification
#ä¸‹é¢è¿™ç§æ–¹å¼å¯ä»¥è‡ªåŠ¨åŠ è½½ bert-base-uncased ä¸­ä½¿ç”¨çš„æ¨¡å‹ï¼ˆåŒ…æ‹¬äº†è¾“å‡ºéƒ¨åˆ†ï¼‰ï¼Œæœ‰æœ€åçš„å…¨è¿æ¥è¾“å‡ºå±‚
model=AutoModel.AutoModelForSequenceClassification("bert-base-uncased")
</code></pre>
<p><strong>æ¨¡å‹ä¿å­˜</strong></p>
<pre><code>model.save_pretrained("./")#ä¿æŒåˆ°å½“å‰ç›®å½•
</code></pre>
<p><strong>ä¸€ä¸ªç®€å•çš„æµç¨‹ä¾‹å­ï¼š</strong></p>
<p>ä»£ç æ¥æ”¶ä¸€ä¸ªå¥å­åˆ—è¡¨ï¼Œå¯¹å…¶è¿›è¡Œåˆ†è¯ï¼Œå°†å…¶ä¼ é€’ç»™ä¸€ä¸ªé¢„è®­ç»ƒçš„æƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼Œç„¶åå¤„ç†è¾“å‡ºä»¥è·å¾—æ¯ä¸ªç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡ã€‚æœ€åï¼Œå°†ç»“æœæ‰“å°åˆ°æ§åˆ¶å°</p>
<pre><code>input=['The first sentence!','The second sentence!']

from transformers import AutoTokenizer

#ä» Transformers åº“ä¸­å¯¼å…¥ AutoTokenizer ç±»ï¼Œç”¨äºå¯¹è¾“å…¥å¥å­è¿›è¡Œåˆ†è¯ã€‚
#åˆ†è¯æ˜¯å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å€¼æ ‡è®°çš„è¿‡ç¨‹ï¼Œè¿™äº›æ ‡è®°å¯ä»¥è¢«æ¨¡å‹ç†è§£ã€‚from_pretrained æ–¹æ³•åŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒçš„åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

#ä½¿ç”¨ tokenizer å¯¹è±¡å¯¹è¾“å…¥å¥å­è¿›è¡Œåˆ†è¯ã€‚padding=True å‚æ•°ç¡®ä¿æ‰€æœ‰å¥å­éƒ½è¢«å¡«å……åˆ°ç›¸åŒçš„é•¿åº¦ï¼Œ
#truncation=True æˆªæ–­è¿‡é•¿çš„å¥å­ï¼Œreturn_tensors='pt' è¿”å› PyTorch å¼ é‡
input = tokenizer(input, padding=True, truncation=True, return_tensors='pt')

from transformers import AutoModelForSequenceClassification
# æ¨¡å‹åŠ è½½ï¼šä½¿ç”¨ AutoModelForSequenceClassification ç±»åŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒçš„åºåˆ—åˆ†ç±»æ¨¡å‹ã€‚
# from_pretrained æ–¹æ³•åŠ è½½æ¨¡å‹
model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

print(model)

# æ¨¡å‹æ¨ç†
output = model(**input)

print(output.logits.shape)

import torch

predictions = torch.nn.functional.softmax(output.logits, dim=-1)

# æ‰“å°é¢„æµ‹æ¦‚ç‡
print(predictions)

# ID åˆ°æ ‡ç­¾çš„æ˜ å°„
print(model.config.id2label)
</code></pre>
<h4 id="232--ä½¿ç”¨æ–¹æ³•-2">2.3.2  ä½¿ç”¨æ–¹æ³•-2</h4>
<p><img src="https://img2024.cnblogs.com/blog/682547/202502/682547-20250208144356312-1005121862.png" alt="image" loading="lazy"></p>
<p>ä¸‹é¢ä»¥ ChatGLM2-6B ä¸ºä¾‹ï¼ˆè§ä¸Šå›¾ï¼‰ï¼Œå…ˆåœ¨ github ä¸Š git ä¸‹ ChatGLM2-6B é™¤æ¨¡å‹å¤–çš„ç›¸å…³æ–‡ä»¶</p>
<pre><code>git clone git@github.com:THUDM/ChatGLM2-6B.git

cd ChatGLM2-6B-main
</code></pre>
<p>å®‰è£…å¥½ç›¸å…³ä¾èµ–</p>
<pre><code>pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple
</code></pre>
<p>ç±»ä¼¼åˆšæ‰çš„æ–¹æ³•ä¸€ç›´æ¥æ‰§è¡Œä¸‹é¢ä»£ç ï¼Œä¼šåœ¨ç½‘ä¸Šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶</p>
<pre><code>&gt;&gt;&gt; from transformers import AutoTokenizer, AutoModel
&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True)
&gt;&gt;&gt; model = AutoModel.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True, device='cuda')
&gt;&gt;&gt; model = model.eval()
&gt;&gt;&gt; response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
&gt;&gt;&gt; print(response)
ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
&gt;&gt;&gt; response, history = model.chat(tokenizer, "æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ", history=history)
&gt;&gt;&gt; print(response)
æ™šä¸Šç¡ä¸ç€å¯èƒ½ä¼šè®©ä½ æ„Ÿåˆ°ç„¦è™‘æˆ–ä¸èˆ’æœ,ä½†ä»¥ä¸‹æ˜¯ä¸€äº›å¯ä»¥å¸®åŠ©ä½ å…¥ç¡çš„æ–¹æ³•:

1. åˆ¶å®šè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨:ä¿æŒè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨å¯ä»¥å¸®åŠ©ä½ å»ºç«‹å¥åº·çš„ç¡çœ ä¹ æƒ¯,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚å°½é‡åœ¨æ¯å¤©çš„ç›¸åŒæ—¶é—´ä¸ŠåºŠ,å¹¶åœ¨åŒä¸€æ—¶é—´èµ·åºŠã€‚
2. åˆ›é€ ä¸€ä¸ªèˆ’é€‚çš„ç¡çœ ç¯å¢ƒ:ç¡®ä¿ç¡çœ ç¯å¢ƒèˆ’é€‚,å®‰é™,é»‘æš—ä¸”æ¸©åº¦é€‚å®œã€‚å¯ä»¥ä½¿ç”¨èˆ’é€‚çš„åºŠä¸Šç”¨å“,å¹¶ä¿æŒæˆ¿é—´é€šé£ã€‚
3. æ”¾æ¾èº«å¿ƒ:åœ¨ç¡å‰åšäº›æ”¾æ¾çš„æ´»åŠ¨,ä¾‹å¦‚æ³¡ä¸ªçƒ­æ°´æ¾¡,å¬äº›è½»æŸ”çš„éŸ³ä¹,é˜…è¯»ä¸€äº›æœ‰è¶£çš„ä¹¦ç±ç­‰,æœ‰åŠ©äºç¼“è§£ç´§å¼ å’Œç„¦è™‘,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚
4. é¿å…é¥®ç”¨å«æœ‰å’–å•¡å› çš„é¥®æ–™:å’–å•¡å› æ˜¯ä¸€ç§åˆºæ¿€æ€§ç‰©è´¨,ä¼šå½±å“ä½ çš„ç¡çœ è´¨é‡ã€‚å°½é‡é¿å…åœ¨ç¡å‰é¥®ç”¨å«æœ‰å’–å•¡å› çš„é¥®æ–™,ä¾‹å¦‚å’–å•¡,èŒ¶å’Œå¯ä¹ã€‚
5. é¿å…åœ¨åºŠä¸Šåšä¸ç¡çœ æ— å…³çš„äº‹æƒ…:åœ¨åºŠä¸Šåšäº›ä¸ç¡çœ æ— å…³çš„äº‹æƒ…,ä¾‹å¦‚çœ‹ç”µå½±,ç©æ¸¸æˆæˆ–å·¥ä½œç­‰,å¯èƒ½ä¼šå¹²æ‰°ä½ çš„ç¡çœ ã€‚
6. å°è¯•å‘¼å¸æŠ€å·§:æ·±å‘¼å¸æ˜¯ä¸€ç§æ”¾æ¾æŠ€å·§,å¯ä»¥å¸®åŠ©ä½ ç¼“è§£ç´§å¼ å’Œç„¦è™‘,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚è¯•ç€æ…¢æ…¢å¸æ°”,ä¿æŒå‡ ç§’é’Ÿ,ç„¶åç¼“æ…¢å‘¼æ°”ã€‚

å¦‚æœè¿™äº›æ–¹æ³•æ— æ³•å¸®åŠ©ä½ å…¥ç¡,ä½ å¯ä»¥è€ƒè™‘å’¨è¯¢åŒ»ç”Ÿæˆ–ç¡çœ ä¸“å®¶,å¯»æ±‚è¿›ä¸€æ­¥çš„å»ºè®®ã€‚
</code></pre>
<p>ä¹Ÿå¯ä»¥æ–¹æ³•äºŒï¼Œæ‰¾åˆ° huggingface ä¸Š ChatGLM2-6B æ¨¡å‹åœ°å€ï¼Œç›´æ¥ git</p>
<pre><code>git clone https://huggingface.co/THUDM/chatglm2-6b
</code></pre>
<p>ç„¶åæ‰“å¼€åˆšæ‰çš„ ChatGLM2-6B é‡Œçš„ web_demo.pyï¼Œä¿®æ”¹é‡Œé¢çš„æ¨¡å‹å’Œ AutoTokenizer ç›®å½•ï¼Œä¸ºåˆšæ‰ git æ¨¡å‹çš„ç›®å½•ï¼Œä¾‹å¦‚æˆ‘åœ¨ChatGLM2-6B é‡Œæ–°å»ºäº†ä¸€ä¸ª modelï¼Œåœ¨ model ç›®å½•ä¸‹ git æ¨¡å‹çš„ï¼Œæ‰€ä»¥æˆ‘çš„ç›®å½•ä¿®æ”¹ä¸ºä¸‹å›¾</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained("model/chatglm2-6b", trust_remote_code=True)
model = AutoModel.from_pretrained("model/chatglm2-6b", trust_remote_code=True).cuda()
</code></pre>
<p>æœ€åï¼Œåœ¨ç»ˆç«¯ç›´æ¥æ‰§è¡Œä¸‹é¢ä»£ç </p>
<pre><code>python web_demo.py
</code></pre>
<p>ç‚¹å‡»å¯åŠ¨åçš„é“¾æ¥ï¼Œå³å¯ä½¿ç”¨ webç‰ˆæœ¬çš„ChatGLM2-6B</p>
<h3 id="24--huggingfaceçš„datasets">2.4  Huggingfaceçš„Datasets</h3>
<p>å¯ä»¥çœ‹åˆ°æœ‰å¦‚ä¸‹ä»»åŠ¡çš„æ•°æ®é›†ã€‚è¯»è€…å¯è‡ªè¡Œæ‰“å¼€å­¦ä¹ </p>
<p><img src="https://img2024.cnblogs.com/blog/682547/202502/682547-20250208144438996-1503040558.png" alt="image" loading="lazy"></p>
<p>ä¾‹å¦‚ï¼Œæˆ‘ä»¬æ‰“å¼€ Text Classification ä»»åŠ¡çš„ glue æ•°æ®é›†,å¯ä»¥çœ‹åˆ°ä¸‹å›¾ï¼Œé‡Œé¢ä¼šæœ‰æ•°æ®é›†çš„ä»‹ç»ã€ç›¸å…³ä¿¡æ¯å’Œä¸‹è½½æ–¹å¼ï¼Œè¯»è€…è‡ªè¡ŒæŸ¥çœ‹ã€‚<br>
<img src="https://img2024.cnblogs.com/blog/682547/202502/682547-20250208144449012-902800212.png" alt="image" loading="lazy"></p>
<h4 id="241-å¯¼å…¥æ•°æ®é›†çš„æ–¹æ³•">2.4.1 å¯¼å…¥æ•°æ®é›†çš„æ–¹æ³•</h4>
<p>æå‰ <code>pip install datasets</code></p>
<pre><code>from datasets import load_dataset

datasets = load_dataset('glue', 'mrpc')  # åŠ è½½glueæ•°æ®é›†
print(datasets)  # æ‰“å°æ•°æ®é›†

print(datasets['train'][0])  # æ‰“å°ç¬¬ä¸€ä¸ªæ ·æœ¬
</code></pre>
<h4 id="242--æœ‰äº†æ•°æ®åè®­ç»ƒæ¨¡å‹æ–¹æ³•">2.4.2  æœ‰äº†æ•°æ®åè®­ç»ƒæ¨¡å‹æ–¹æ³•</h4>
<p>ä¸‹é¢ç»™å‡º bert-base-uncased çš„ä¾‹å­ï¼Œå®ç°å¯¹ä¸¤ä¸ªå¥å­çš„ç›¸ä¼¼åº¦è®¡ç®—</p>
<pre><code># å¯¼å…¥tokenizer

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# input = tokenizer("The first sentence!", "The second sentence!")
#
# print(tokenizer.convert_ids_to_tokens(input['input_ids']))


# å®é™…ä½¿ç”¨ tokenizer çš„æ–¹æ³•ï¼Œå¾—åˆ° tokenizer_data
def tokenize_function(examples):
    return tokenizer(examples['sentence1'], examples['sentence2'], truncation=True)


from datasets import load_dataset
datasets = load_dataset("glue", "mrpc")
tokenizer_data = datasets.map(tokenize_function, batched=True)
print(tokenizer_data)

# è®­ç»ƒå‚æ•°
from transformers import TrainingArguments

# https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments
training_args = TrainingArguments("test_trainer")
print(training_args)#çœ‹ä¸‹é»˜è®¤å€¼

# å¯¼å…¥æ¨¡å‹
from transformers import AutoModelForSequenceClassification
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

#å¯¼å…¥æ•°æ®å¤„ç†çš„ä¸€ä¸ªä¸œè¥¿ DataCollatorWithPaddingï¼Œå˜æˆä¸€ä¸ªä¸€ä¸ª batch
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

#å¯¼å…¥è®­ç»ƒå™¨ï¼Œè¿›è¡Œè®­ç»ƒ,API : https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Traine

from transformers import Trainer
trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenizer_data["train"],
    eval_dataset=tokenizer_data["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)

trainer.train()
</code></pre>
<h3 id="25--huggingfaceçš„spaces">2.5  Huggingfaceçš„Spaces</h3>
<p>ç‚¹å¼€å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚é‡Œé¢æœ‰äº›è¿‘äº›å¤©æœ‰è¶£çš„ä¸œè¥¿ç«çƒ­çš„ apps<br>
<img src="https://img2024.cnblogs.com/blog/682547/202502/682547-20250208144530402-881224256.png" alt="image" loading="lazy"></p>
<p>æ¯”å¦‚ä¸‹é¢çš„ä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæ¨¡å‹<br>
<img src="https://img2024.cnblogs.com/blog/682547/202502/682547-20250208144537707-1862109150.png" alt="image" loading="lazy"></p>

</div>
<div id="MySignature" role="contentinfo">
    <div>
<h3><span style="font: 400 16px Simsun; color: #0000C6"> å¾®ä¿¡å…¬ä¼—å·</span></h3>
<h3><img style="width: 400px; margin-left: 2px;" src="https://images.cnblogs.com/cnblogs_com/bigdata1024/2014130/o_221015130328_%E6%89%AB%E7%A0%81_%E6%90%9C%E7%B4%A2%E8%81%94%E5%90%88%E4%BC%A0%E6%92%AD%E6%A0%B7%E5%BC%8F-%E6%A0%87%E5%87%86%E8%89%B2%E7%89%88.png"></h3>
</div>
<div>
<h3><span style="font: 400 16px Simsun; color: #0000C6">ä½œè€…ï¼š<a href="https://www.cnblogs.com/bigdata1024/" target="_blank">chaplinthink</a></span> 
<a style="color: green" href="https://www.cnblogs.com/bigdata1024/p/16795143.html"> ===&gt; [æ¬¢è¿èµèµä½œè€…ï¼Œ æ‚¨çš„èµèµï¼Œæ˜¯æˆ‘å‰è¿›çš„åŠ¨åŠ›ğŸ™‚]</a>
</h3>
<h3><span style="font: 400 16px Simsun; color: #0000C6">å‡ºå¤„ï¼š<a href="https://www.cnblogs.com/bigdata1024/p/18704285" target="_blank">https://www.cnblogs.com/bigdata1024/p/18704285</a></span></h3>
<h3><span style="font: 400 16px Simsun; color: #0000C6">æœ¬æ–‡ä»¥å­¦ä¹ ã€ç ”ç©¶å’Œåˆ†äº«ä¸ºä¸»ï¼Œå¦‚éœ€è½¬è½½ï¼Œè¯·è”ç³»æœ¬äººï¼Œæ ‡æ˜ä½œè€…å’Œå‡ºå¤„ï¼Œéå•†ä¸šç”¨é€”!</span></h3>
</div>
</div>
<div class="clear"></div>

		</div>
		<div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.05327655332175926" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-02-08 15:00">2025-02-08 14:47</span>&nbsp;
<a href="https://www.cnblogs.com/bigdata1024">chaplinthink</a>&nbsp;
é˜…è¯»(<span id="post_view_count">19</span>)&nbsp;
è¯„è®º(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18704285" rel="nofollow">ç¼–è¾‘</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18704285);return false;">æ”¶è—</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18704285', targetLink: 'https://www.cnblogs.com/bigdata1024/p/18704285', title: 'Huggingfaceä½¿ç”¨' })">ä¸¾æŠ¥</a>
</div>
	