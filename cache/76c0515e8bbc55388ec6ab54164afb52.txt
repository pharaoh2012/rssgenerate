
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/moonout/p/18812958" title="发布于 2025-04-11 23:40">
    <span role="heading" aria-level="2">RL · Exploration | 使用时序距离构造 intrinsic reward，鼓励 agent 探索</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                    <div id="cnblogs_post_description" style="display: none">
        
        鼓励 agent 探索与当前 episode 历史在到达时间（temporal distance）上较远的状态。
    </div>
<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<br>
<ul>
<li>论文标题：Episodic Novelty Through Temporal Distance.</li>
<li>ICLR 2025，8 8 6 5 poster。</li>
<li>arxiv：<a href="https://arxiv.org/abs/2501.15418" target="_blank" rel="noopener nofollow">https://arxiv.org/abs/2501.15418</a></li>
<li>pdf：<a href="https://arxiv.org/pdf/2501.15418" target="_blank" rel="noopener nofollow">https://arxiv.org/pdf/2501.15418</a></li>
<li>html：<a href="https://arxiv.org/html/2501.15418" target="_blank" rel="noopener nofollow">https://arxiv.org/html/2501.15418</a></li>
<li>open review：<a href="https://openreview.net/forum?id=I7DeajDEx7" target="_blank" rel="noopener nofollow">https://openreview.net/forum?id=I7DeajDEx7</a></li>
</ul>
<hr>
<p></p><div class="toc"><div class="toc-container-header">目录</div><ul><li><a href="#01-论文主要内容" rel="noopener nofollow">01 论文主要内容</a><ul><li><a href="#11-这篇论文关注什么想解决什么任务" rel="noopener nofollow">1.1 这篇论文关注什么，想解决什么任务</a></li><li><a href="#12-先前方法一般怎么做有什么问题" rel="noopener nofollow">1.2 先前方法一般怎么做，有什么问题</a></li><li><a href="#13-这篇论文的-motivation它希望解决什么-gap" rel="noopener nofollow">1.3 这篇论文的 motivation，它希望解决什么 gap</a></li><li><a href="#14-这篇论文的主要-method-是什么算法流程是什么" rel="noopener nofollow">1.4 这篇论文的主要 method 是什么，算法流程是什么</a><ul><li><a href="#-如何学习-temporal-distance" rel="noopener nofollow">📌 如何学习 temporal distance</a></li><li><a href="#-使用-temporal-distance-构造-intrinsic-reward" rel="noopener nofollow">📌 使用 temporal distance 构造 intrinsic reward</a></li><li><a href="#-具体算法" rel="noopener nofollow">📌 具体算法</a></li></ul></li><li><a href="#15-实验结果怎么样" rel="noopener nofollow">1.5 实验结果怎么样</a></li></ul></li><li><a href="#02-行文逻辑" rel="noopener nofollow">02 行文逻辑</a></li></ul></div><p></p>
<hr>
<h2 id="01-论文主要内容">01 论文主要内容</h2>
<p>论文概括：</p>
<ul>
<li>这篇论文研究<strong>稀疏奖励环境下的探索问题</strong>，特别是在每次任务环境会变化的"情境 MDP"（Contextual Markov Decision Processes，CMDP，如随机生成的地图）中，如何让 agent 高效探索。</li>
<li>这篇论文设计了 ETD（<strong>E</strong>pisodic Novelty Through <strong>T</strong>emporal <strong>D</strong>istance）方法，核心创新是提出 <strong>temporal distance（时序距离）</strong>作为状态新颖性的衡量标准，通过对比学习估计时序距离，并生成 intrinsic reward 驱动探索。</li>
</ul>
<h3 id="11-这篇论文关注什么想解决什么任务">1.1 这篇论文关注什么，想解决什么任务</h3>
<ul>
<li><strong>关注点</strong>：解决 CMDP（Contextual Markov Decision Processes，情境 MDP，如随机地图导航、机器人在随机场景中完成任务）中 <strong>稀疏奖励下探索效率低</strong> 的问题。</li>
<li><strong>挑战</strong>：传统方法依赖全局的经验，但 CMDP 每次环境不同（如新地图），历史经验无法直接复用（详见下文的 1.2）。</li>
</ul>
<h3 id="12-先前方法一般怎么做有什么问题">1.2 先前方法一般怎么做，有什么问题</h3>
<ul>
<li><strong>计数法</strong>（如记录访问次数）：在连续/大状态空间失效（每个状态都"独特"，无法判断新颖性）。</li>
<li><strong>相似度法</strong>（如欧氏距离）：无法捕捉状态的<strong>动态关系</strong>（如迷宫中的两个点看似近，但实际需要绕远路才能到达）。</li>
</ul>
<h3 id="13-这篇论文的-motivation它希望解决什么-gap">1.3 这篇论文的 motivation，它希望解决什么 gap</h3>
<ul>
<li>现有方法缺少对<strong>状态间动态关系</strong>的建模（例如："绕远路"和"直达"在欧氏距离上可能相同）。【待 check】</li>
<li>提出用<strong>时序距离</strong>（从状态 A 到 B 所需的平均步数）作为更本质的相似性度量，可跨环境泛化。</li>
</ul>
<h3 id="14-这篇论文的主要-method-是什么算法流程是什么">1.4 这篇论文的主要 method 是什么，算法流程是什么</h3>
<h4 id="-如何学习-temporal-distance">📌 如何学习 temporal distance</h4>
<p>首先，定义几个概念：</p>
<ul>
<li>从 x 开始时在时间步 k 达到状态 y 的概率：<span class="math inline">\(p^\pi(s_k=y|s_0=x)\)</span>。</li>
<li>状态 x 到 y 的转换概率：<span class="math inline">\(p^\pi_\gamma(s_f=y|s_0=x)=(1-\gamma)\sum_{k=0}^\infty \gamma^kp^\pi(s_k=y|s_0=x)\)</span>。这个值 ≤ 1。个人理解，<span class="math inline">\(s_f\)</span> 的意思是在 <span class="math inline">\(s_0\)</span> 之后的一个状态。</li>
<li>准度量（Quasimetric）的定义：满足非负性、同一性（d(x,x)=0）和三角不等式，但无需对称性（d(x,y)≠d(y,x)）。</li>
</ul>
<p>然后，根据 (Myers et al., 2024)，我们定义 x 到 y 的 temporal distance 为：</p>
<p></p><div class="math display">\[d^\pi_\text{SD}(x,y)=\log\left(\frac{p^\pi_\gamma(s_f=y|s_0=y)}{p^\pi_\gamma(s_f=y|s_0=x)}\right)
\]</div><p></p><p>x 越难到达 y，分母越小，<span class="math inline">\(d^\pi_\text{SD}(x,y)\)</span> 就越大。<br>
根据 (Myers et al., 2024)，这个定义是一个 Quasimetric，即使 MDP 是随机的。</p>
<p>然后，根据 (Ma &amp; Collins, 2018; Poole et al., 2019)，我们用对比学习来学 <span class="math inline">\(d^\pi_\text{SD}(x,y)\)</span>。<br>
定义一个能量函数 <span class="math inline">\(f(x,y)\)</span>，希望它对于两个互相容易到达的状态 x y 分配较大的值，而对于难以到达的状态分配较小的值。<br>
我们用 InfoNCE loss 训练它：</p>
<p></p><div class="math display">\[\mathcal{L}_{\theta} = \sum_{i=1}^{B} \left[ \log \left( \frac{\exp f(x_i, y_i)}{\sum_{j=1}^{B} \exp f(x_j, y_j)} \right) + \log \left( \frac{\exp f(x_i, y_i)}{\sum_{j=1}^{B} \exp f(x_j, y_j)} \right) \right]
\]</div><p></p><p>其中，(x,y) 是从 <span class="math inline">\((x,y)\sim p^\pi_\gamma(s_f=y|s_0=x)p_s(x)\)</span> 采样得到的，<span class="math inline">\(p_s(x)\)</span> 是 x 的边缘分布。<br>
根据 (Ma &amp; Collins, 2018; Poole et al., 2019)，可以使用能量函数 <span class="math inline">\(f(x,y)\)</span> 的唯一解来恢复 <span class="math inline">\(d^\pi_\text{SD}(x,y)\)</span>。<br>
根据 (Myers et al., 2024)，如果将 <span class="math inline">\(f(x,y)\)</span> 分解为一个 potential <span class="math inline">\(c(y)\)</span> 和一个 <span class="math inline">\(d(x,y)\)</span> 的差值，即 <span class="math inline">\(f(x,y)=c(y)-d(x,y)\)</span>，那么可以直接取用 <span class="math inline">\(d(x,y)=d^\pi_\text{SD}(x,y)\)</span>，这样就训练得到了时序距离。<br>
在 ETD 里，<span class="math inline">\(c(y)\)</span> 是 MLP，而 <span class="math inline">\(d(x,y)\)</span> 是用非对称的 MRN 实现的。</p>
<h4 id="-使用-temporal-distance-构造-intrinsic-reward">📌 使用 temporal distance 构造 intrinsic reward</h4>
<p>定义 ETD 的 intrinsic reward：<span class="math inline">\(b_\text{ETD}(s_t)=\min_{k\in[0,t]}d(s_k,s_t)\)</span>，即，希望最大化这一个 episode 里先前状态 <span class="math inline">\(s_k\)</span> 到这个状态 <span class="math inline">\(s_t\)</span> 的 temporal distance，希望去一些尽可能难到达的地方。特别的，ETD 最大化的是最小的 temporal distance，即离当前状态 <span class="math inline">\(s_t\)</span> 最近的 <span class="math inline">\(s_k\)</span>，它们俩的 temporal distance。</p>
<h4 id="-具体算法">📌 具体算法</h4>
<ul>
<li>采样 context c，得到一个 CMDP；</li>
<li>while episode 没结束：
<ul>
<li>根据 s 采样 action a，得到 s'；</li>
<li>计算 ETD intrinsic reward：<span class="math inline">\(b_{t+1}=\min_{k\in[0,t]}d(s_k,s_t)\)</span>；</li>
<li>得到总 reward：<span class="math inline">\(r_{t+1} = r^e_{t+1} + \beta b_{t+1}\)</span>；</li>
</ul>
</li>
<li>把这个 episode 存到 replay buffer 里；</li>
<li>从 replay buffer 里采一批 。实际中，<span class="math inline">\(x = s_t, y = s_{t+j}, j \sim\text{Geom}(1−\gamma)\)</span> 即几何分布；</li>
<li>更新 temporal distance 的 loss function。</li>
<li>用刚刚采的新轨迹更新 PPO policy。</li>
</ul>
<p>（个人思考，state 里显然应该包含当前 context 的信息，比如一个迷宫中的迷宫布局。不然，假设在上一个 episode 里 (0,0) (0,2) 很近，但这个 episode 里，两个点之间隔了一堵墙；如果 state 里只包含自己的坐标 (0,0)，而不包含这堵墙，那么完全没法学到“这一局的 (0,0) (0,2) 很远”这种信息）</p>
<h3 id="15-实验结果怎么样">1.5 实验结果怎么样</h3>
<ul>
<li><strong>环境</strong>：MiniGrid 迷宫、像素游戏 Crafter、3D 导航 MiniWorld 等。</li>
<li><strong>优势</strong>：
<ul>
<li>在 MiniGrid 里，ETD 比 NovelD 更好。</li>
<li>在带噪声的复杂迷宫中，ETD 比现有方法（NovelD、E3B 等）收敛速度<strong>快 2 倍</strong>。</li>
<li>在像素输入的高维环境（如 Crafter）中，ETD 的探索成功率提升 <strong>15-20%</strong>。</li>
<li>对状态噪声（如随机扰动）鲁棒，传统方法（如计数法）完全失效时 ETD 仍有效。</li>
</ul>
</li>
</ul>
<h2 id="02-行文逻辑">02 行文逻辑</h2>
<p>这篇文章讲的主要故事：</p>
<ul>
<li>在稀疏 reward 环境中，探索（exploration）面临挑战。而在 Contextual MDP 中，现有的 count-based 和 similarity-based 都存在问题；ETD 可以解决它们的问题。</li>
<li>ETD 通过使用 temporal distance 计算相似性和 intrinsic reward，从而解决它们的问题。temporal distance 使用对比学习进行学习。</li>
</ul>
<p>1 intro 分析：</p>
<ul>
<li>第一段：稀疏奖励的 RL 探索困难；虽然现有方法在 MDP 中有效，但现实世界通常是 CMDP。</li>
<li>第二段：为解决 CMDP 的探索问题，现有方法引入了 episodic bonuses，这些方法可以分为两类：count-based 和 similarity-based。count-based 方法在大状态空间中表现不佳，而 similarity-based 方法在相似性计算上存在不足，无法捕捉状态的新颖性。</li>
<li>第三段：这篇文章介绍了 ETD，鼓励 agent 探索与当前 episode 历史在时间上相距较远的状态。
<ul>
<li>关键创新在于使用 temporal distance，它测量在两个 state 之间转换的预期步数，可以作为相似性计算的稳健指标。</li>
<li>与现有方法不同，temporal distance 不受状态 representation 的影响，因此避免了 noisy-TV 问题，并且适用于 pixel-based 环境。</li>
</ul>
</li>
</ul>
<p>接下来的 2 background 简单介绍了 CMDP 和 intrinsic reward 的机制。</p>
<p>3 Limitations of Current Episodic Bonuses：</p>
<ul>
<li>专门用一段分析了现有方法的 gap。</li>
<li>第一段：基于 episodic count 计算 novelty，会在大状态空间 / noisy state 下失效，因为每个状态都是新颖的。</li>
<li>第二段：基于 state similarity 计算 novelty 的方法，如 NGU 和 E3B，可能能解决这一问题。具体来说，可以通过估计状态间转换的难易程度来实现，例如 EC 和 DEIR。然而，这种方法计算的状态相似性往往不够准确，如图 2 所示。</li>
</ul>
<p>4 method：</p>
<ul>
<li>4.1 如何学习 temporal distance；</li>
<li>4.2 基于 temporal distance 计算 intrinsic reward；</li>
<li>4.3 介绍具体算法。</li>
</ul>
<p>5 experiments：</p>
<ul>
<li>5.1 介绍了 minigrid 的 setting 和实验结果，实验涵盖了 8 个 task。</li>
<li>5.2 介绍了带 noise 的 minigrid，把各种 baseline 都卡下去了。</li>
<li>5.3 是 ablation，比较了 ETD 和欧几里得距离、用 ETD 生成 intrinsic reward 的具体方法、ETD 时序距离的对称性 / 非对称性设计。</li>
<li>5.4 介绍了 Pixel-Based Crafter 和 MiniWorld Maze 的环境，并提供了（与少量 baseline 比较）的实验结果。</li>
</ul>
<br>
<br>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.3923217023831019" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-04-11 23:41">2025-04-11 23:40</span>&nbsp;
<a href="https://www.cnblogs.com/moonout">MoonOut</a>&nbsp;
阅读(<span id="post_view_count">9</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18812958" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18812958);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18812958', targetLink: 'https://www.cnblogs.com/moonout/p/18812958', title: 'RL &amp;#183; Exploration | 使用时序距离构造 intrinsic reward，鼓励 agent 探索' })">举报</a>
</div>
        