
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/xiao987334176/p/18876317" title="发布于 2025-05-14 18:54">
    <span role="heading" aria-level="2">windows11 安装CUDA Toolkit，Python，Anaconda，PyTorch并使用DeepSeek 多模态模型 Janus-Pro识别和生成图片</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body blogpost-body-html">
<h1>一、概述</h1>
<p>因为公司网络做了严格限制，必须使用账号登录，才能上网。必须是指定的ip地址和MAC地址设备才可以上网。</p>
<p>windows11开启热点，安装第三方虚拟机软件，开启WSL2虚拟机都是被禁止的，否则账号会被封锁，无法上网。</p>
<p>挺无奈的，那么就只能使用windows 11系统来安装CUDA Toolkit，Anaconda，PyTorch这些组件，使用DeepSeek 多模态模型 Janus-Pro，识别和生成图片了。</p>
<h1>二、安装CUDA Toolkit</h1>
<h2>安装NVIDIA App</h2>
<p>NVIDIA App 是 PC 游戏玩家和创作者的必备辅助工具。可以使你的 PC 及时升级到最新的 NVIDIA 驱动程序和技术。在全新的统一 GPU 控制中心内优化游戏和应用，通过游戏内悬浮窗提供的强大录像工具捕捉精彩时刻，并可以轻松发现最新的 NVIDIA 工具和软件。</p>
<p>官方地址：<a href="https://www.nvidia.cn/software/nvidia-app/" target="_blank" rel="noopener nofollow">https://www.nvidia.cn/software/nvidia-app/</a></p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202505/1341090-20250514172027916-896695801.png" alt="" loading="lazy"></p>
<p>直接下载安装，他会自动识别你的英伟达显卡型号，并安装最新的驱动。</p>
<p>安装过程很简单，直接下一步，下一步安装即可。</p>
<p>&nbsp;</p>
<p>打开NVIDIA App，选择驱动类型GeForce Game Ready，专门为游戏设计的驱动，安装最新版本。</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202505/1341090-20250514172449016-420429839.png" alt="" width="1338" height="698" loading="lazy"></p>
<h2>安装CUDA Toolkit</h2>
<p>CUDA Toolkit 是NVIDIA 提供的一套开发工具，它包含了用于开发CUDA 应用程序所需的各种工具，如编译器、调试器和库。 因此，CUDA 和CUDA Toolkit 是有关系的，CUDA 是并行计算平台和编程模型，而CUDA Toolkit 是一套开发工具。</p>
<h3 id="autoid-4-1-0">查看显卡算力</h3>
<p><a href="https://developer.nvidia.com/cuda-gpus" rel="noopener nofollow" target="_blank">https://developer.nvidia.com/cuda-gpus</a></p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202505/1341090-20250508133951437-1925891019.png" alt="" class="medium-zoom-image" loading="lazy"></p>
<p>我的显卡是5080，对应的显卡算力是12.0</p>
<h3 id="autoid-4-1-1">算力对应的cuda版本</h3>
<p><a href="https://docs.nvidia.com/datacenter/tesla/drivers/index.html#cuda-arch-matrix" rel="noopener nofollow" target="_blank">https://docs.nvidia.com/datacenter/tesla/drivers/index.html#cuda-arch-matrix</a></p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202505/1341090-20250508134657421-14770358.png" alt="" class="medium-zoom-image" loading="lazy"></p>
<p>我的显卡算力是12，大于9，所以可以安装CUDA 11.8 CUDA 12.0，都可以。</p>
<p><span style="color: rgba(255, 0, 0, 1)"><strong>注意：只能高，不能低。</strong></span></p>
<p>&nbsp;</p>
<p>打开pytorch网页</p>
<p><a href="https://pytorch.org/get-started/locally/" rel="noopener nofollow" target="_blank">https://pytorch.org/get-started/locally/</a></p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202505/1341090-20250514173411121-1087793314.png" alt="" loading="lazy"></p>
<p><strong><span style="color: rgba(255, 0, 0, 1)">可以看到pytorch目前支持CUDA最高的版本是12.8，那么接下来安装CUDA Toolkit，不能高于12.8</span></strong></p>
<p>&nbsp;</p>
<p>打开官网下载地址：</p>
<p><a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="noopener nofollow">https://developer.nvidia.com/cuda-toolkit-archive</a></p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202505/1341090-20250514173707692-174622453.png" alt="" loading="lazy"></p>
<p>虽然目前最新版本是12.9.0，但是我不能安装12.9.0，因为pytorch目前支持CUDA最高的版本是12.8</p>
<p>所以我只能安装12.8.1，多一个小数点，问题不大。因为也是12.8系列的。</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202505/1341090-20250514173938030-768208711.png" alt="" width="1438" height="943" loading="lazy"></p>
<p><span style="color: rgba(255, 0, 0, 1)"><strong>注意：这里必须要根据你的实际情况，进行严格选择</strong></span></p>
<p>我这里选择的是windows 11版本的</p>
<p>&nbsp;</p>
<p>文件比较大，可以开启迅雷下载</p>
<p>下载完成后，点击exe程序安装</p>
<p>&nbsp;</p>
<p>临时路径，默认即可，安装完成后，会自动删除的。</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202505/1341090-20250514174910219-2127168277.png" alt="" loading="lazy"></p>
<p>之后，就是下一步下一步安装即可。</p>
<p>&nbsp;</p>
<h1>三、安装python</h1>
<p>官方网址：<a title="https://www.python.org/downloads/" href="https://www.python.org/downloads/" rel="noopener nofollow">https://www.python.org/downloads/</a></p>
<p>下载最新版本</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202505/1341090-20250514175329651-598360103.png" alt="" loading="lazy"></p>
<p>下载完成后，点击exe文件安装</p>
<p><span style="color: rgba(255, 0, 0, 1)"><strong>注意，必须勾选【Add Python 3.10 to PATH】</strong></span></p>
<p>然后点击【Customize installation】自定义安装程序</p>
<p>进入到可选功能【Optional Features】界面后，全部勾选，下一步</p>
<p>然后</p>
<p>勾选【Associate files with Python (requires the py launcher)】，默认python运行.py文件<br>勾选【Add Python to environment variables】，添加环境变量</p>
<p>修改安装的文件路径，尽量安装在磁盘根目录。例如：D:\python3.13<br>点击【install】按钮执行安装<br><br></p>
<p>安装完成后，打开cmd窗口，输入Python，效果如下：</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202505/1341090-20250514175911156-2080611396.png" alt="" loading="lazy"></p>
<p>&nbsp;确保版本正确</p>
<p>&nbsp;</p>
<h1>四、安装Anaconda</h1>
<p>Anaconda是一个开源的Python发行版本，专注于数据科学、机器学习和大数据处理，集成了conda包管理器、Python解释器及180多个预装科学计算库（如NumPy、Pandas），提供跨平台的环境隔离与管理功能。</p>
<p>官方地址：<a href="https://www.anaconda.com/download/success" target="_blank" rel="noopener nofollow">https://www.anaconda.com/download/success</a></p>
<p>下载windows版本</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202505/1341090-20250514180058355-1391592380.png" alt="" loading="lazy"></p>
<p>&nbsp;下载完成后，点击exe程序，下一步，下一步安装即可。</p>
<p>&nbsp;</p>
<p>打开windows11开启菜单，搜索ana，点击Anaconda Prompt</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202505/1341090-20250514181626147-712432766.png" alt="" loading="lazy"></p>
<p>输入python，效果如下：</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202505/1341090-20250514181906658-155342264.png" alt="" loading="lazy"></p>
<p>&nbsp;注意：显示的python版本和单独安装的python版本，有些差异是正常的。因为Anaconda还不支持最新python版本。</p>
<h1>五、安装PyTorch</h1>
<p>PyTorch 是一个开源的机器学习库，广泛应用于计算机视觉、自然语言处理等领域。</p>
<p>打开pytorch网页</p>
<p><a href="https://pytorch.org/get-started/locally/" rel="noopener nofollow" target="_blank">https://pytorch.org/get-started/locally/</a></p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202505/1341090-20250514180716052-1647085668.png" alt="" loading="lazy"></p>
<p>&nbsp;</p>
<p>说明：</p>
<p>pytorch安装，python版本不能低于3.9</p>
<p>pytorch build，这里选择最新版本</p>
<p>your os，选择Linux</p>
<p>Package，选择pip安装</p>
<p>Language，开发语言，选择Python</p>
<p>compute platform，这里选择CUDA 12.8，因为在上面的步骤中，我安装的版本就是CUDA Toolkit 12.8</p>
<p>run this command，这里就会显示完整的安装命令，直接复制即可</p>
<p>&nbsp;</p>
<h2>创建虚拟环境</h2>
<p>将环境创建在指定路径下</p>
<p>注意：目录D:\file\conda\envs，需要先手动创建一下，这里用来存放所有的虚拟环境</p>
<div class="cnblogs_code">
<pre>conda create --prefix D:\file\conda\envs\my_unsloth_env python=3.13.2</pre>
</div>
<p>输入y</p>
<div class="cnblogs_code">
<pre>Continue creating environment (y/[n])? y</pre>
</div>
<p>激活虚拟环境</p>
<div class="cnblogs_code">
<pre>conda activate D:\file\conda\envs\my_unsloth_env</pre>
</div>
<p>&nbsp;</p>
<p>安装pytorch</p>
<div class="cnblogs_code">
<pre>pip3 <span style="color: rgba(0, 0, 255, 1)">install</span> torch torchvision torchaudio --index-url https:<span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)">download.pytorch.org/whl/cu128</span></pre>
</div>
<p>&nbsp;</p>
<p>在python中确认一下torch是否安装成功</p>
<div class="cnblogs_code">
<pre>(my_unsloth_env) C:\Users\xiao&gt;<span style="color: rgba(0, 0, 0, 1)"><span style="color: rgba(255, 0, 0, 1)">python</span>
Python </span><span style="color: rgba(128, 0, 128, 1)">3.13</span>.<span style="color: rgba(128, 0, 128, 1)">2</span> | packaged by Anaconda, Inc. | (main, Feb  <span style="color: rgba(128, 0, 128, 1)">6</span> <span style="color: rgba(128, 0, 128, 1)">2025</span>, <span style="color: rgba(128, 0, 128, 1)">18</span>:<span style="color: rgba(128, 0, 128, 1)">49</span>:<span style="color: rgba(128, 0, 128, 1)">14</span>) [MSC v.<span style="color: rgba(128, 0, 128, 1)">1929</span> <span style="color: rgba(128, 0, 128, 1)">64</span><span style="color: rgba(0, 0, 0, 1)"> bit (AMD64)] on win32
Type </span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">help</span><span style="color: rgba(128, 0, 0, 1)">"</span>, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">copyright</span><span style="color: rgba(128, 0, 0, 1)">"</span>, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">credits</span><span style="color: rgba(128, 0, 0, 1)">"</span> or <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">license</span><span style="color: rgba(128, 0, 0, 1)">"</span> <span style="color: rgba(0, 0, 255, 1)">for</span> <span style="color: rgba(0, 0, 255, 1)">more</span><span style="color: rgba(0, 0, 0, 1)"> information.
</span>&gt;&gt;&gt;<span style="color: rgba(255, 0, 0, 1)"> import torch
</span>&gt;&gt;&gt;<span style="color: rgba(255, 0, 0, 1)"> print(torch.cuda.device_count())
</span><span style="color: rgba(128, 0, 128, 1)">1</span>
&gt;&gt;&gt;<span style="color: rgba(0, 0, 0, 1)"><span style="color: rgba(255, 0, 0, 1)"> print(torch.cuda.is_available())</span>
True
</span>&gt;&gt;&gt;<span style="color: rgba(255, 0, 0, 1)"> print(torch.__version__)
</span><span style="color: rgba(128, 0, 128, 1)">2.7</span>.<span style="color: rgba(128, 0, 128, 1)">0</span>+<span style="color: rgba(0, 0, 0, 1)">cu128
</span>&gt;&gt;&gt;<span style="color: rgba(255, 0, 0, 1)"> print(torch.version.cuda)
</span><span style="color: rgba(128, 0, 128, 1)">12.8</span>
&gt;&gt;&gt; <span style="color: rgba(255, 0, 0, 1)">exit()</span></pre>
</div>
<p><span style="color: rgba(255, 0, 0, 1)"><strong>注意：确保torch.version.cuda输出的版本和CUDA Toolkit版本一致，就说明cuda和troch版本是匹配的。</strong></span></p>
<p>&nbsp;</p>
<h1>六、DeepSeek 多模态模型 Janus-Pro</h1>
<p>Janus-Pro是DeepSeek最新开源的多模态模型，是一种新颖的自回归框架，统一了多模态理解和生成。通过将视觉编码解耦为独立的路径，同时仍然使用单一的、统一的变压器架构进行处理，该框架解决了先前方法的局限性。这种解耦不仅缓解了视觉编码器在理解和生成中的角色冲突，还增强了框架的灵活性。Janus-Pro 超过了以前的统一模型，并且匹配或超过了特定任务模型的性能。</p>
<p>github链接：<a href="https://github.com/deepseek-ai/Janus" rel="noopener nofollow" target="_blank">https://github.com/deepseek-ai/Janus</a></p>
<h2>安装依赖组件</h2>
<p>安装依赖包，注意：这里要手动安装pytorch，指定版本。</p>
<div class="cnblogs_code">
<pre>pip3 <span style="color: rgba(0, 0, 255, 1)">install</span> torch torchvision torchaudio --index-url https:<span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)">download.pytorch.org/whl/cu128</span></pre>
</div>
<p>安装其他依赖组件</p>
<div class="cnblogs_code">
<pre>pip3 <span style="color: rgba(0, 0, 255, 1)">install</span> transformers attrdict einops timm</pre>
</div>
<p>&nbsp;</p>
<h2>下载模型</h2>
<p>可以用modelscope下载，安装modelscope，命令如下：</p>
<div class="cnblogs_code">
<pre>pip <span style="color: rgba(0, 0, 255, 1)">install</span><span style="color: rgba(0, 0, 0, 1)"> modelscope

modelscope download </span>--model deepseek-ai/Janus-Pro-7B</pre>
</div>
<p>效果如下：</p>
<div class="cnblogs_code">
<pre>(my_unsloth_env) C:\Users\xiao&gt; modelscope download --model deepseek-ai/Janus-Pro-<span style="color: rgba(0, 0, 0, 1)">7B
Downloading Model from https:</span><span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)">www.modelscope.cn to directory: C:\Users\xiao\.cache\modelscope\hub\models\deepseek-ai\Janus-Pro-7B</span>
Downloading [config.json]: <span style="color: rgba(128, 0, 128, 1)">100</span>%|███████████████████████████████████████████████████| <span style="color: rgba(128, 0, 128, 1)">1</span>.42k/<span style="color: rgba(128, 0, 128, 1)">1</span>.42k [<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>&lt;<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>, <span style="color: rgba(128, 0, 128, 1)">5</span>.29kB/<span style="color: rgba(0, 0, 0, 1)">s]
Downloading [configuration.json]: </span><span style="color: rgba(128, 0, 128, 1)">100</span>%|████████████████████████████████████████████████| <span style="color: rgba(128, 0, 128, 1)">68.0</span>/<span style="color: rgba(128, 0, 128, 1)">68.0</span> [<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>&lt;<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>, 221B/<span style="color: rgba(0, 0, 0, 1)">s]
Downloading [README.md]: </span><span style="color: rgba(128, 0, 128, 1)">100</span>%|█████████████████████████████████████████████████████| <span style="color: rgba(128, 0, 128, 1)">2</span>.49k/<span style="color: rgba(128, 0, 128, 1)">2</span>.49k [<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>&lt;<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>, <span style="color: rgba(128, 0, 128, 1)">7</span>.20kB/<span style="color: rgba(0, 0, 0, 1)">s]
Downloading [processor_config.json]: </span><span style="color: rgba(128, 0, 128, 1)">100</span>%|███████████████████████████████████████████████| <span style="color: rgba(128, 0, 128, 1)">210</span>/<span style="color: rgba(128, 0, 128, 1)">210</span> [<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>&lt;<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>, 590B/<span style="color: rgba(0, 0, 0, 1)">s]
Downloading [janus_pro_teaser1.png]: </span><span style="color: rgba(128, 0, 128, 1)">100</span>%|██████████████████████████████████████████| <span style="color: rgba(128, 0, 128, 1)">95</span>.7k/<span style="color: rgba(128, 0, 128, 1)">95</span>.7k [<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>&lt;<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>, 267kB/<span style="color: rgba(0, 0, 0, 1)">s]
Downloading [preprocessor_config.json]: </span><span style="color: rgba(128, 0, 128, 1)">100</span>%|████████████████████████████████████████████| <span style="color: rgba(128, 0, 128, 1)">346</span>/<span style="color: rgba(128, 0, 128, 1)">346</span> [<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>&lt;<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>, 867B/<span style="color: rgba(0, 0, 0, 1)">s]
Downloading [janus_pro_teaser2.png]: </span><span style="color: rgba(128, 0, 128, 1)">100</span>%|███████████████████████████████████████████| 518k/518k [<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>&lt;<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>, <span style="color: rgba(128, 0, 128, 1)">1</span>.18MB/<span style="color: rgba(0, 0, 0, 1)">s]
Downloading [special_tokens_map.json]: </span><span style="color: rgba(128, 0, 128, 1)">100</span>%|███████████████████████████████████████████| <span style="color: rgba(128, 0, 128, 1)">344</span>/<span style="color: rgba(128, 0, 128, 1)">344</span> [<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>&lt;<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>, <span style="color: rgba(128, 0, 128, 1)">1</span>.50kB/<span style="color: rgba(0, 0, 0, 1)">s]
Downloading [tokenizer_config.json]: </span><span style="color: rgba(128, 0, 128, 1)">100</span>%|███████████████████████████████████████████████| <span style="color: rgba(128, 0, 128, 1)">285</span>/<span style="color: rgba(128, 0, 128, 1)">285</span> [<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>&lt;<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>, 926B/<span style="color: rgba(0, 0, 0, 1)">s]
Downloading [pytorch_model.bin]:   </span><span style="color: rgba(128, 0, 128, 1)">0</span>%|▏                                            | <span style="color: rgba(128, 0, 128, 1)">16.0M</span>/<span style="color: rgba(128, 0, 128, 1)">3</span>.89G [<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>&lt;<span style="color: rgba(128, 0, 128, 1)">03</span>:<span style="color: rgba(128, 0, 128, 1)">55</span>, <span style="color: rgba(128, 0, 128, 1)">17</span>.7MB/<span style="color: rgba(0, 0, 0, 1)">s]
Downloading [tokenizer.json]: </span><span style="color: rgba(128, 0, 128, 1)">100</span>%|████████████████████████████████████████████████| <span style="color: rgba(128, 0, 128, 1)">4.50M</span>/<span style="color: rgba(128, 0, 128, 1)">4.50M</span> [<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>&lt;<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>, <span style="color: rgba(128, 0, 128, 1)">6</span>.55MB/<span style="color: rgba(0, 0, 0, 1)">s]
Processing </span><span style="color: rgba(128, 0, 128, 1)">11</span> items:  <span style="color: rgba(128, 0, 128, 1)">91</span>%|█████████████████████████████████████████████████████▋     | <span style="color: rgba(128, 0, 128, 1)">10.0</span>/<span style="color: rgba(128, 0, 128, 1)">11.0</span> [<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">19</span>&lt;<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>, <span style="color: rgba(128, 0, 128, 1)">14</span>.1it/s]<br>Downloading [pytorch_model.bin]: <span style="color: rgba(128, 0, 128, 1)">100</span>%|█████████████████████████████████████████████| <span style="color: rgba(128, 0, 128, 1)">3</span>.89G/<span style="color: rgba(128, 0, 128, 1)">3</span>.89G [<span style="color: rgba(128, 0, 128, 1)">09</span>:<span style="color: rgba(128, 0, 128, 1)">18</span>&lt;<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>, <span style="color: rgba(128, 0, 128, 1)">7</span>.48MB/<span style="color: rgba(0, 0, 0, 1)">s]
Processing </span><span style="color: rgba(128, 0, 128, 1)">11</span> items: <span style="color: rgba(128, 0, 128, 1)">100</span>%|███████████████████████████████████████████████████████████| <span style="color: rgba(128, 0, 128, 1)">11.0</span>/<span style="color: rgba(128, 0, 128, 1)">11.0</span> [<span style="color: rgba(128, 0, 128, 1)">09</span>:<span style="color: rgba(128, 0, 128, 1)">24</span>&lt;<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>, <span style="color: rgba(128, 0, 128, 1)">51</span>.3s/it]</pre>
</div>
<p>可以看到下载目录为C:\Users\xiao\.cache\modelscope\hub\models\deepseek-ai\Janus-Pro-7B</p>
<p>&nbsp;</p>
<p>在D盘新建目录D:\file\modelscope\model，专门用来存放模型的</p>
<p>将文件夹C:\Users\xiao\.cache\modelscope\hub\models\deepseek-ai，移动到目录D:\file\modelscope\model</p>
<p>&nbsp;</p>
<h2>测试图片理解</h2>
<p>打开github链接：<a href="https://github.com/deepseek-ai/Janus" rel="noopener nofollow" target="_blank">https://github.com/deepseek-ai/Janus</a></p>
<p>下载zip文件</p>
<p>&nbsp;</p>
<p>新建目录D:\file\vllm，用来测试大模型的</p>
<p>将文件Janus-main.zip复制到此目录，并解压，得到目录Janus-main。</p>
<p>进入文件夹Janus-main</p>
<p>将图片aa.jpeg，下载链接：https://pics6.baidu.com/feed/09fa513d269759ee74c8d049640fcc1b6f22df9e.jpeg</p>
<p>放到文件夹Janus-main</p>
<p>在文件夹Janus-main，创建image_understanding.py文件</p>
<div class="cnblogs_code">
<pre><span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> torch
</span><span style="color: rgba(0, 0, 255, 1)">from</span> transformers <span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> AutoModelForCausalLM
</span><span style="color: rgba(0, 0, 255, 1)">from</span> janus.models <span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> MultiModalityCausalLM, VLChatProcessor
</span><span style="color: rgba(0, 0, 255, 1)">from</span> janus.utils.io <span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> load_pil_images

model_path </span>= r<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">D:\file\modelscope\model\deepseek-ai\Janus-Pro-7B</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">

image</span>=<span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">aa.jpeg</span><span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(0, 0, 0, 1)">
question</span>=<span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">请说明一下这张图片</span><span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(0, 0, 0, 1)">
vl_chat_processor: VLChatProcessor </span>=<span style="color: rgba(0, 0, 0, 1)"> VLChatProcessor.from_pretrained(model_path)
tokenizer </span>=<span style="color: rgba(0, 0, 0, 1)"> vl_chat_processor.tokenizer

vl_gpt: MultiModalityCausalLM </span>=<span style="color: rgba(0, 0, 0, 1)"> AutoModelForCausalLM.from_pretrained(
    model_path, trust_remote_code</span>=<span style="color: rgba(0, 0, 0, 1)">True
)
vl_gpt </span>=<span style="color: rgba(0, 0, 0, 1)"> vl_gpt.to(torch.bfloat16).cuda().eval()

conversation </span>=<span style="color: rgba(0, 0, 0, 1)"> [
    {
        </span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">role</span><span style="color: rgba(128, 0, 0, 1)">"</span>: <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">&lt;|User|&gt;</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">,
        </span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">content</span><span style="color: rgba(128, 0, 0, 1)">"</span>: f<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">&lt;image_placeholder&gt;\n{question}</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">,
        </span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">images</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">: [image],
    },
    {</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">role</span><span style="color: rgba(128, 0, 0, 1)">"</span>: <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">&lt;|Assistant|&gt;</span><span style="color: rgba(128, 0, 0, 1)">"</span>, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">content</span><span style="color: rgba(128, 0, 0, 1)">"</span>: <span style="color: rgba(128, 0, 0, 1)">""</span><span style="color: rgba(0, 0, 0, 1)">},
]

</span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> load images and prepare for inputs</span>
pil_images =<span style="color: rgba(0, 0, 0, 1)"> load_pil_images(conversation)
prepare_inputs </span>=<span style="color: rgba(0, 0, 0, 1)"> vl_chat_processor(
    conversations</span>=conversation, images=pil_images, force_batchify=<span style="color: rgba(0, 0, 0, 1)">True
).to(vl_gpt.device)

</span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> # run image encoder to get the image embeddings</span>
inputs_embeds = vl_gpt.prepare_inputs_embeds(**<span style="color: rgba(0, 0, 0, 1)">prepare_inputs)

</span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> # run the model to get the response</span>
outputs =<span style="color: rgba(0, 0, 0, 1)"> vl_gpt.language_model.generate(
    inputs_embeds</span>=<span style="color: rgba(0, 0, 0, 1)">inputs_embeds,
    attention_mask</span>=<span style="color: rgba(0, 0, 0, 1)">prepare_inputs.attention_mask,
    pad_token_id</span>=<span style="color: rgba(0, 0, 0, 1)">tokenizer.eos_token_id,
    bos_token_id</span>=<span style="color: rgba(0, 0, 0, 1)">tokenizer.bos_token_id,
    eos_token_id</span>=<span style="color: rgba(0, 0, 0, 1)">tokenizer.eos_token_id,
    max_new_tokens</span>=512<span style="color: rgba(0, 0, 0, 1)">,
    do_sample</span>=<span style="color: rgba(0, 0, 0, 1)">False,
    use_cache</span>=<span style="color: rgba(0, 0, 0, 1)">True,
)

answer </span>= tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=<span style="color: rgba(0, 0, 0, 1)">True)
</span><span style="color: rgba(0, 0, 255, 1)">print</span>(f<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">{prepare_inputs['sft_format'][0]}</span><span style="color: rgba(128, 0, 0, 1)">"</span>, answer)</pre>
</div>
<p><span style="color: rgba(255, 0, 0, 1)"><strong>注意：根据实际情况，修改&nbsp;model_path，image这2个参数即可，其他的不需要改动。</strong></span></p>
<p>&nbsp;</p>
<p>运行代码，效果如下：</p>
<div class="cnblogs_code">
<pre>(my_unsloth_env) D:\<span style="color: rgba(0, 0, 255, 1)">file</span>\vllm\Janus-main&gt;<span style="color: rgba(0, 0, 0, 1)"><span style="color: rgba(255, 0, 0, 1)">python image_understanding.py</span>
Python version is above </span><span style="color: rgba(128, 0, 128, 1)">3.10</span><span style="color: rgba(0, 0, 0, 1)">, patching the collections module.
D:\</span><span style="color: rgba(0, 0, 255, 1)">file</span>\conda\envs\my_unsloth_env\Lib\site-packages\transformers\models\auto\image_processing_auto.py:<span style="color: rgba(128, 0, 128, 1)">604</span>: FutureWarning: The image_processor_class argument is deprecated and will be removed <span style="color: rgba(0, 0, 255, 1)">in</span> v4.<span style="color: rgba(128, 0, 128, 1)">42</span><span style="color: rgba(0, 0, 0, 1)">. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast</span>=True` will be the default behavior <span style="color: rgba(0, 0, 255, 1)">in</span> v4.<span style="color: rgba(128, 0, 128, 1)">52</span>, even <span style="color: rgba(0, 0, 255, 1)">if</span> the model was saved with a slow processor. This will result <span style="color: rgba(0, 0, 255, 1)">in</span> minor differences <span style="color: rgba(0, 0, 255, 1)">in</span> outputs. You<span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">ll still be able to use a slow processor with `use_fast=False`.</span>
You are using the default legacy behaviour of the &lt;class <span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast</span><span style="color: rgba(128, 0, 0, 1)">'</span>&gt;. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes <span style="color: rgba(0, 0, 255, 1)">for</span> you. If you want to use the new behaviour, set `legacy=False`. This should only be set <span style="color: rgba(0, 0, 255, 1)">if</span> you understand what it means, and thoroughly read the reason why this was added as explained <span style="color: rgba(0, 0, 255, 1)">in</span> https:<span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)">github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.</span>
Loading checkpoint shards: <span style="color: rgba(128, 0, 128, 1)">100</span>%|█████████████████████████████████████████████████████████| <span style="color: rgba(128, 0, 128, 1)">2</span>/<span style="color: rgba(128, 0, 128, 1)">2</span> [<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">21</span>&lt;<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>, <span style="color: rgba(128, 0, 128, 1)">10</span>.73s/<span style="color: rgba(0, 0, 0, 1)">it]
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.

</span>&lt;|User|&gt;: &lt;image_placeholder&gt;<span style="color: rgba(0, 0, 0, 1)">
请说明一下这张图片

</span>&lt;|Assistant|&gt;: 这张图片展示了一位身穿传统服饰的女性，她正坐在户外，双手合十，闭着眼睛，似乎在进行冥想或祈祷。背景是绿色的树木和植物，阳光透过树叶洒下来，营造出一种宁静、祥和的氛围。她的服装以白色和粉色为主，带有精致的刺绣和装饰，显得非常优雅。整体画面给人一种平和、放松的感觉。</pre>
</div>
<p>描述还是比较准确的</p>
<p>&nbsp;</p>
<h2>测试图片生成</h2>
<p>在文件夹Janus-main，新建image_generation.py脚本，代码如下：</p>
<div class="cnblogs_code">
<pre><span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> os
</span><span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> torch
</span><span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> numpy as np
</span><span style="color: rgba(0, 0, 255, 1)">from</span> PIL <span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> Image
</span><span style="color: rgba(0, 0, 255, 1)">from</span> transformers <span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> AutoModelForCausalLM
</span><span style="color: rgba(0, 0, 255, 1)">from</span> janus.models <span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> MultiModalityCausalLM, VLChatProcessor

model_path </span>= r<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">D:\file\modelscope\model\deepseek-ai\Janus-Pro-7B</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">
vl_chat_processor: VLChatProcessor </span>=<span style="color: rgba(0, 0, 0, 1)"> VLChatProcessor.from_pretrained(model_path)
tokenizer </span>=<span style="color: rgba(0, 0, 0, 1)"> vl_chat_processor.tokenizer

vl_gpt: MultiModalityCausalLM </span>=<span style="color: rgba(0, 0, 0, 1)"> AutoModelForCausalLM.from_pretrained(
    model_path, trust_remote_code</span>=<span style="color: rgba(0, 0, 0, 1)">True
)
vl_gpt </span>=<span style="color: rgba(0, 0, 0, 1)"> vl_gpt.to(torch.bfloat16).cuda().eval()

conversation </span>=<span style="color: rgba(0, 0, 0, 1)"> [
    {</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">role</span><span style="color: rgba(128, 0, 0, 1)">"</span>: <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">&lt;|User|&gt;</span><span style="color: rgba(128, 0, 0, 1)">"</span>, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">content</span><span style="color: rgba(128, 0, 0, 1)">"</span>: <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">超写实8K渲染，一位具有东方古典美的中国女性，瓜子脸，西昌的眉毛如弯弯的月牙，双眼明亮而深邃，犹如夜空中闪烁的星星。高挺的鼻梁，樱桃小嘴微微上扬，透露出一丝诱人的微笑。她的头发如黑色的瀑布般垂直落在减胖两侧，微风轻轻浮动发色。肌肤白皙如雪，在阳光下泛着微微的光泽。她身着乙烯白色的透薄如纱的连衣裙，裙摆在海风中轻轻飘动。</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">},
    {</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">role</span><span style="color: rgba(128, 0, 0, 1)">"</span>: <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">&lt;|Assistant|&gt;</span><span style="color: rgba(128, 0, 0, 1)">"</span>, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">content</span><span style="color: rgba(128, 0, 0, 1)">"</span>: <span style="color: rgba(128, 0, 0, 1)">""</span><span style="color: rgba(0, 0, 0, 1)">},
]

sft_format </span>=<span style="color: rgba(0, 0, 0, 1)"> vl_chat_processor.apply_sft_template_for_multi_turn_prompts(
    conversations</span>=<span style="color: rgba(0, 0, 0, 1)">conversation,
    sft_format</span>=<span style="color: rgba(0, 0, 0, 1)">vl_chat_processor.sft_format,
    system_prompt</span>=<span style="color: rgba(128, 0, 0, 1)">""</span><span style="color: rgba(0, 0, 0, 1)">
)
prompt </span>= sft_format +<span style="color: rgba(0, 0, 0, 1)"> vl_chat_processor.image_start_tag

@torch.inference_mode()
</span><span style="color: rgba(0, 0, 255, 1)">def</span><span style="color: rgba(0, 0, 0, 1)"> generate(
        mmgpt: MultiModalityCausalLM,
        vl_chat_processor: VLChatProcessor,
        prompt: str,
        temperature: float </span>= 1<span style="color: rgba(0, 0, 0, 1)">,
        parallel_size: int </span>= 1, <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 减小 parallel_size</span>
        cfg_weight: float = 5<span style="color: rgba(0, 0, 0, 1)">,
        image_token_num_per_image: int </span>= 576<span style="color: rgba(0, 0, 0, 1)">,
        img_size: int </span>= 384<span style="color: rgba(0, 0, 0, 1)">,
        patch_size: int </span>= 16<span style="color: rgba(0, 0, 0, 1)">,
):
    input_ids </span>=<span style="color: rgba(0, 0, 0, 1)"> vl_chat_processor.tokenizer.encode(prompt)
    input_ids </span>=<span style="color: rgba(0, 0, 0, 1)"> torch.LongTensor(input_ids)

    tokens </span>= torch.zeros((parallel_size * 2, len(input_ids)), dtype=<span style="color: rgba(0, 0, 0, 1)">torch.int).cuda()
    </span><span style="color: rgba(0, 0, 255, 1)">for</span> i <span style="color: rgba(0, 0, 255, 1)">in</span> range(parallel_size * 2<span style="color: rgba(0, 0, 0, 1)">):
        tokens[i, :] </span>=<span style="color: rgba(0, 0, 0, 1)"> input_ids
        </span><span style="color: rgba(0, 0, 255, 1)">if</span> i % 2 !=<span style="color: rgba(0, 0, 0, 1)"> 0:
            tokens[i, </span>1:-1] =<span style="color: rgba(0, 0, 0, 1)"> vl_chat_processor.pad_id

    inputs_embeds </span>=<span style="color: rgba(0, 0, 0, 1)"> mmgpt.language_model.get_input_embeddings()(tokens)

    generated_tokens </span>= torch.zeros((parallel_size, image_token_num_per_image), dtype=<span style="color: rgba(0, 0, 0, 1)">torch.int).cuda()

    </span><span style="color: rgba(0, 0, 255, 1)">for</span> i <span style="color: rgba(0, 0, 255, 1)">in</span><span style="color: rgba(0, 0, 0, 1)"> range(image_token_num_per_image):
        outputs </span>= mmgpt.language_model.model(inputs_embeds=inputs_embeds, use_cache=<span style="color: rgba(0, 0, 0, 1)">True,
                                             past_key_values</span>=outputs.past_key_values <span style="color: rgba(0, 0, 255, 1)">if</span> i != 0 <span style="color: rgba(0, 0, 255, 1)">else</span><span style="color: rgba(0, 0, 0, 1)"> None)
        hidden_states </span>=<span style="color: rgba(0, 0, 0, 1)"> outputs.last_hidden_state

        logits </span>= mmgpt.gen_head(hidden_states[:, -1<span style="color: rgba(0, 0, 0, 1)">, :])
        logit_cond </span>= logits[0::2<span style="color: rgba(0, 0, 0, 1)">, :]
        logit_uncond </span>= logits[1::2<span style="color: rgba(0, 0, 0, 1)">, :]

        logits </span>= logit_uncond + cfg_weight * (logit_cond -<span style="color: rgba(0, 0, 0, 1)"> logit_uncond)
        probs </span>= torch.softmax(logits / temperature, dim=-1<span style="color: rgba(0, 0, 0, 1)">)

        next_token </span>= torch.multinomial(probs, num_samples=1<span style="color: rgba(0, 0, 0, 1)">)
        generated_tokens[:, i] </span>= next_token.squeeze(dim=-1<span style="color: rgba(0, 0, 0, 1)">)
        next_token </span>= torch.cat([next_token.unsqueeze(dim=1<span style="color: rgba(0, 0, 0, 1)">),
                                next_token.unsqueeze(dim</span>=1)], dim=1).view(-1<span style="color: rgba(0, 0, 0, 1)">)
        img_embeds </span>=<span style="color: rgba(0, 0, 0, 1)"> mmgpt.prepare_gen_img_embeds(next_token)
        inputs_embeds </span>= img_embeds.unsqueeze(dim=1<span style="color: rgba(0, 0, 0, 1)">)
        </span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 添加显存清理</span>
        <span style="color: rgba(0, 0, 255, 1)">del</span><span style="color: rgba(0, 0, 0, 1)"> logits, logit_cond, logit_uncond, probs
        torch.cuda.empty_cache()

    dec </span>= mmgpt.gen_vision_model.decode_code(generated_tokens.to(dtype=<span style="color: rgba(0, 0, 0, 1)">torch.int),
                                             shape</span>=[parallel_size, 8, img_size // patch_size, img_size //<span style="color: rgba(0, 0, 0, 1)"> patch_size])
    dec </span>= dec.to(torch.float32).cpu().numpy().transpose(0, 2, 3, 1<span style="color: rgba(0, 0, 0, 1)">)

    dec </span>= np.clip((dec + 1) / 2 * 255, 0, 255<span style="color: rgba(0, 0, 0, 1)">)

    visual_img </span>= np.zeros((parallel_size, img_size, img_size, 3), dtype=<span style="color: rgba(0, 0, 0, 1)">np.uint8)
    visual_img[:, :, :] </span>=<span style="color: rgba(0, 0, 0, 1)"> dec

    os.makedirs(</span><span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">generated_samples</span><span style="color: rgba(128, 0, 0, 1)">'</span>, exist_ok=<span style="color: rgba(0, 0, 0, 1)">True)
    </span><span style="color: rgba(0, 0, 255, 1)">for</span> i <span style="color: rgba(0, 0, 255, 1)">in</span><span style="color: rgba(0, 0, 0, 1)"> range(parallel_size):
        save_path </span>= os.path.join(<span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">generated_samples</span><span style="color: rgba(128, 0, 0, 1)">'</span>, f<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">img_{i}.jpg</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">)
        img </span>=<span style="color: rgba(0, 0, 0, 1)"> Image.fromarray(visual_img[i])
        img.save(save_path)

generate(
    vl_gpt,
    vl_chat_processor,
    prompt,
)</span></pre>
</div>
<p><span style="color: rgba(255, 0, 0, 1)"><strong>注意：根据实际情况，修改model_path，conversation，parallel_size这3个参数即可。</strong></span></p>
<p><span style="color: rgba(255, 0, 0, 1)"><strong>提示词是可以写中文的，不一定非要是英文。</strong></span></p>
<p><span style="color: rgba(255, 0, 0, 1)"><strong>代码在默认的基础上做了优化，否则运行会导致英伟达5080显卡直接卡死。</strong></span></p>
<p>&nbsp;</p>
<p>运行代码，效果如下：</p>
<div class="cnblogs_code">
<pre>(my_unsloth_env) D:\<span style="color: rgba(0, 0, 255, 1)">file</span>\vllm\Janus-main&gt;<span style="color: rgba(0, 0, 0, 1)"><span style="color: rgba(255, 0, 0, 1)">python image_generation.py</span>
Python version is above </span><span style="color: rgba(128, 0, 128, 1)">3.10</span><span style="color: rgba(0, 0, 0, 1)">, patching the collections module.
D:\</span><span style="color: rgba(0, 0, 255, 1)">file</span>\conda\envs\my_unsloth_env\Lib\site-packages\transformers\models\auto\image_processing_auto.py:<span style="color: rgba(128, 0, 128, 1)">604</span>: FutureWarning: The image_processor_class argument is deprecated and will be removed <span style="color: rgba(0, 0, 255, 1)">in</span> v4.<span style="color: rgba(128, 0, 128, 1)">42</span><span style="color: rgba(0, 0, 0, 1)">. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast</span>=True` will be the default behavior <span style="color: rgba(0, 0, 255, 1)">in</span> v4.<span style="color: rgba(128, 0, 128, 1)">52</span>, even <span style="color: rgba(0, 0, 255, 1)">if</span> the model was saved with a slow processor. This will result <span style="color: rgba(0, 0, 255, 1)">in</span> minor differences <span style="color: rgba(0, 0, 255, 1)">in</span> outputs. You<span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">ll still be able to use a slow processor with `use_fast=False`.</span>
You are using the default legacy behaviour of the &lt;class <span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast</span><span style="color: rgba(128, 0, 0, 1)">'</span>&gt;. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes <span style="color: rgba(0, 0, 255, 1)">for</span> you. If you want to use the new behaviour, set `legacy=False`. This should only be set <span style="color: rgba(0, 0, 255, 1)">if</span> you understand what it means, and thoroughly read the reason why this was added as explained <span style="color: rgba(0, 0, 255, 1)">in</span> https:<span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)">github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.</span>
Loading checkpoint shards: <span style="color: rgba(128, 0, 128, 1)">100</span>%|█████████████████████████████████████████████████████████| <span style="color: rgba(128, 0, 128, 1)">2</span>/<span style="color: rgba(128, 0, 128, 1)">2</span> [<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">15</span>&lt;<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>,  <span style="color: rgba(128, 0, 128, 1)">7</span>.68s/it]</pre>
</div>
<p>&nbsp;</p>
<p>注意观察一下GPU使用情况，这里好像并不高啊，基本上在20%左右。</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202505/1341090-20250514184301627-492779762.png" alt="" loading="lazy"></p>
<p>windows11用GPU很低，用WSL2利用GPU反而会很高？</p>
<p>WSL2 通常依赖于 Windows 的驱动程序，但可能会有兼容性或性能问题。只能这么解释了</p>
<p>&nbsp;</p>
<p>等待1分钟左右，就会生成一张图片。</p>
<p>&nbsp;</p>
<p>进入目录D:\file\vllm\Janus-main\generated_samples，这里会出现一张图片</p>
<p>打开图片，效果如下：</p>
<p><img src="https://img2024.cnblogs.com/blog/1341090/202505/1341090-20250514183936105-704463126.jpg" alt=""></p>
<p>效果还算可以，距离真正的8k画质，还是有点差距的。</p>
<p><span style="color: rgba(255, 0, 0, 1)"><strong>注意提示词，尽量丰富一点，生成的图片，才符合要求。</strong></span></p>
<p>如果不会写提示词，可以让deepseek帮你写一段提示词。</p>
<p>&nbsp;</p>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.06572164709143519" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-05-14 18:55">2025-05-14 18:54</span>&nbsp;
<a href="https://www.cnblogs.com/xiao987334176">肖祥</a>&nbsp;
阅读(<span id="post_view_count">20</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18876317);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18876317', targetLink: 'https://www.cnblogs.com/xiao987334176/p/18876317', title: 'windows11 安装CUDA Toolkit，Python，Anaconda，PyTorch并使用DeepSeek 多模态模型 Janus-Pro识别和生成图片' })">举报</a>
</div>
        