
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/JuggyZhan/p/18863169" title="发布于 2025-05-06 21:56">
    <span role="heading" aria-level="2">【BLIP】解读BLIP</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                    <div id="cnblogs_post_description" style="display: none">
        
        BLIP，全称是Bootstrapped Language-Image Pretraining，源自《BLIP: Bootstrapping Language-Image Pre-training for Unifified Vision-Language Understanding and Generation》这篇文章，是来自Salesforce Research的一个多模态模型。
    </div>
<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p><img src="https://img2024.cnblogs.com/blog/2948101/202505/2948101-20250506211846045-1597496203.png" alt="BLIP结构" loading="lazy"><br>
BLIP，全称是Bootstrapped Language-Image Pretraining，源自《BLIP: Bootstrapping Language-Image Pre-training for Unifified Vision-Language Understanding and Generation》这篇文章，是来自Salesforce Research的一个多模态模型。</p>
<h1 id="一-研究动机">一、 研究动机</h1>
<h2 id="1-模型方面">1. 模型方面</h2>
<p>主流的多模态模型，基本分为两种：基于encoder和基于encoder-decoder。两者都存在一定的劣势，前者不能完成文本生成任务，例如图像字幕生成，而后者基本没有在图像-文本检索的任务上成功过。</p>
<h2 id="2-数据方面">2. 数据方面</h2>
<p>几乎所有的（前）SOTA模型，例如CLIP、ALBEF、SimVLM等，都是在大量的从网络上爬下来的图像-文本对上进行预训练的。因为数据量足够大，因此这种方式能够得到很好的效果。然而，使用这种带有噪声的样本对于视觉-语言学习来说其实是次优的。</p>
<h1 id="二模型结构">二、模型结构</h1>
<p>BLIP这篇文章提出了其预训练模型架构<strong>MED</strong>，除此之外还提出了一个用于数据自举(data boostrapping)的方法<strong>CapFilt</strong>。</p>
<h2 id="med">MED</h2>
<p>MED全称为<strong>M</strong>ultimodal Mixture of <strong>E</strong>ncoder-<strong>D</strong>ecoder，是一个可以完成三个任务的复合型模型。其结构如下图所示<img src="https://img2024.cnblogs.com/blog/2948101/202505/2948101-20250506212033694-578590672.png" alt="MED" loading="lazy"><br>
可以看到MED主要由四部分组成。注意，<strong>图中相同颜色的部分是共用参数</strong>的！ 首先，与CLIP类似，作者使用一个visual transformer作为图像编码器。它会将输入图片切分成patch并编码成一段embeddings，并且在开头加入了<span class="math inline">\(\mathrm{[CLS]}\)</span> token来表示图片全局信息。接着，为了预训练一个能够完成理解和生成任务的统一模型，图像编码器的输出会流向以下三个不同的结构。</p>
<h3 id="1-unimodal-encoder----itc">1. Unimodal encoder --&gt; ITC</h3>
<p>这个结构类似BERT使用的编码器。将文本开头加入<span class="math inline">\(\mathrm{[CLS]}\)</span> token之后输入编码器进行编码，然后其输出与图片编码器的输出进行image-text contrastive任务。这个任务本质就是对比学习的做法，激励正例中的文本-图像对有更相似的表达，降低反例的相似度，以对齐文本编码器和图像编码器的特征空间。本文使用的ITC loss借鉴了ALBEF中的做法，引入动量编码器来生成特征，并从动量编码器中创建软标签作为训练目标，以考虑负样本对中可能存在的正样本。</p>
<p>注：软标签即概率值，表示样本对的匹配程度，硬标签则是非0则1。具体而言，软标签会捕捉到文本和图片之间的部分匹配关系，而不是简单地将其标记为0。使用软标签可以让模型学习到更细粒度的语义信息，而不是简单地二值化判断<br>
<img src="https://img2024.cnblogs.com/blog/2948101/202505/2948101-20250506212154154-1706759400.png" alt="ITC" loading="lazy"></p>
<h3 id="2-image-grouded-text-encoder----itm">2. Image-grouded text encoder --&gt; ITM</h3>
<p><strong>基于图像的文本编码器</strong>。这个编码器与上一个的唯一不同点就在于多加了一个cross attention(CA)操作。将图片编码器的输出embedding作为query，文本编码器中self attention(SA)之后的embedding作为key和value进行CA操作，这样可以学习到<strong>图片和文本的多模态表达</strong>以用于<strong>捕捉更加精细的视觉与语言之间的对应关系</strong>。换句话说，就是能够使得模型对于图片的每个局部特征如纹理、颜色等进行描述，而不是粗略地描述图片中拥有的最突出的信息。</p>
<p>这个结构对应的则是image-text matching任务。这是一个<strong>二分类任务</strong>，通过添加一个线性层输出positive或者negative来表示图片和文本是否匹配。输入时，须在开头处添加<span class="math inline">\([\mathrm{Encode}]\)</span> token表示起始，并且<strong>CA最后一层的<span class="math inline">\([\mathrm{Encode}]\)</span>表示了文本-图像对的多模态信息</strong>。</p>
<p>另外，为了“寻找更有信息量的负样本”，作者也使用了ALBEF中的hard negative mining strategy，即在一个batch中，让有着更高相似度的负样本对拥有更高几率被选中来计算loss，从而提升模型的泛化能力。简单来说，就是让模型去判别那些很容易被误判为正样本的负样本。<br>
<img src="https://img2024.cnblogs.com/blog/2948101/202505/2948101-20250506212308078-1522722263.png" alt="ITM" loading="lazy"></p>
<h3 id="3-image-grounded-text-decoder----lm">3. Image-grounded text decoder --&gt; LM</h3>
<p><strong>基于图像的文本解码器</strong>。与前一个结构不同之处在于，开始的双向自注意力机制改为了<strong>因果自注意力机制</strong>，即Transformer中的decoder只与之前出现的token进行attention操作。很显然，这个结构是用于<strong>基于给定图片生成对应的文本描述</strong>。它对应的任务就是语言模型Language Modeling，损失函数是交叉熵函数。模型通过优化交叉熵，以自回归的方式训练，目的是最大化文本的似然性。作者也提到，相较于BERT的MLM，LM的方式更具有泛化能力，更能够将视觉信息转化为连贯的文本字幕。同样地，输入文本开头需要添加一个<span class="math inline">\([\mathrm{Decode}]\)</span> token表示开头，结尾需要添加一个<span class="math inline">\([EOS]\)</span>字符。<br>
<img src="https://img2024.cnblogs.com/blog/2948101/202505/2948101-20250506212337949-1047436986.png" alt="LM" loading="lazy"></p>
<h2 id="capfilt">CapFilt</h2>
<p>当前大模型使用的基本都是来自网络上的训练材料，尽管训练材料的数量足够大，使得最后训练出来的模型效果很好，但作者认为这只是次优的方案。这是因为网络上的训练材料很多都是带有噪声的，而真正高质量的人工标注的图像-文本对其实很少，很大一部分都存在文本描述与图片信息不匹配的情况，这也就是所谓的噪声。基于这个原因，作者提出了<strong>CapFilt</strong>这个方法来提升样本的质量。<br>
<img src="https://img2024.cnblogs.com/blog/2948101/202505/2948101-20250506212411203-1306220853.png" alt="CapFilt示意图" loading="lazy"><br>
CapFilt方法的具体流程如上所示。</p>
<ol>
<li>首先，使用网络上的图像-文本对<span class="math inline">\((I_w,T_w)\)</span>预训练一个MED模型。</li>
<li>然后在dataset boostrapping阶段，分为Captioner和Filter两个部分。</li>
<li>Captioner本质上就是一个基于图像的decoder。它做的事情就是通过输入人工标注的图像-文本对<span class="math inline">\((I_h,T_h)\)</span>，使用LM作为优化目标来微调这个decoder。待微调完成后，这个decoder要根据网上的图片<span class="math inline">\(I_w\)</span>来生成对应的文字说明<span class="math inline">\(T_s\)</span>，由此形成一个新的图像-文本对<span class="math inline">\((I_w,T_s)\)</span>。s即synthetic，表示合成的意思。</li>
<li>Filter本质上就是一个基于图像的encoder。它要做的是将上述人工标注的图像-文本对<span class="math inline">\((I_h,T_h)\)</span>输入，并用ITC和ITM作为优化目标来进行微调。微调完成之后，Filter要做的就是对网络上带有噪声的图像-文本对<span class="math inline">\((I_w,T_w)\)</span>进行筛选，并且也要对上述由Captioner生成的图片文本对<span class="math inline">\((I_w,T_s)\)</span>进行筛选，以选出匹配的对。</li>
<li>最后得到筛选后的上述两类图片文本，再加上人工标注的图像-文本对<span class="math inline">\((I_h,T_h)\)</span>，形成一个新的数据集，用以预训练一个新的MED模型。</li>
</ol>
<h1 id="三实验">三、实验</h1>
<h2 id="1-预训练设置">1. 预训练设置</h2>
<p>本文的实验是使用PyTorch进行的，预训练阶段是在两个16核GPU的节点上完成的。图像编码器使用的是基于ImageNet数据集预训练而来的<strong>Vision Transformer</strong>，并且会探究两种ViT：ViT-B/16和ViT-L/16。文本transformer则是由<span class="math inline">\(\mathrm{BERT_{base}}\)</span>初始化而来。作者在预训练阶段训练了20个epoch，batch size为2880(ViT-B)/2400(ViT-L)，并且使用了一个带有0.05权重衰减率的AdamW优化器。剩余的实验设置可看原文。</p>
<h2 id="2-下游实验结果">2. 下游实验结果</h2>
<h3 id="21-capfilt的影响">2.1 CapFilt的影响</h3>
<p><img src="https://img2024.cnblogs.com/blog/2948101/202505/2948101-20250506212619570-358724964.png" alt="CapFilt影响" loading="lazy"><br>
作者在表1中展示了CapFilt对下游任务实验结果的影响。第三列中的Vision backbone是指在预训练阶段MED中的image encoder使用的是哪种ViT，其余列表示的意思均可在表上看出。作者主要是对<strong>图像-文本检索</strong>和<strong>图像描述</strong>（字幕？）这两个下游任务进行探究，并且除了fine-tuning之外还探索了zero-shot的方式。</p>
<p>首先，抛开CapFilt的影响，不难发现更大的预训练数据量和更大的Vision backbone确实是能够带来更好的效果的。其次，我们可以看到当同时使用Captioner和Filter时比只用一个或者不用带来了一定的效果提升。此外，如果Captioner或者Filter选用更大的模型作为Vision backbone也是能带来性能的提升的。</p>
<h3 id="22-合成型caption的多样性">2.2 合成型Caption的多样性</h3>
<p>在Captioner中，作者使用了<strong>beam search</strong>和<strong>nucleus sampling</strong>两种方式去生成文字。下面的表2展示了两种方式生成的结果差异。<br>
<img src="https://img2024.cnblogs.com/blog/2948101/202505/2948101-20250506212905820-528012494.png" alt="BLIP不同采样方式" loading="lazy"><br>
相比beam search，nucleus sampling会产生更多的噪声，即产生更多不匹配的文本。然而，使用nucleus sampling来fine-tuning这种方式对于下游任务的提升反而更大。这可能是因为<strong>nucleus sampling会允许模型选择概率分布中排名较低但概率仍然较高的词</strong>，这带来了一定的噪声，但也带来了<strong>多样性</strong>。而这种更多样和更“出人意料”的文本，也许会<strong>带有更多能让模型学习到的新信息</strong>。相比之下，beam search追求的是准确性，生成的文本字幕更为“保险”，因此也少了一些多样性，少了一些额外信息。</p>
<h3 id="23-参数共享与解耦">2.3 参数共享与解耦</h3>
<p>在预训练阶段，除了SA，文本编码器和解码器共用的是一套参数。因此，作者想要探究不同的参数共享方式的影响，表3给出了结果。<br>
<img src="https://img2024.cnblogs.com/blog/2948101/202505/2948101-20250506212934656-533504347.png" alt="BLIP共享参数1" loading="lazy"><br>
可以看到，<strong>除了SA层以外的参数共享</strong>对下游任务的提升是最大的，并且降低了模型大小，提升了训练效率。除此之外，作者还探究了在Captioner和Filter上采用除SA以外参数共享的方式是否能够带来提升。而表4中的结果显示这种共享方式除了能够降低噪声比例以外，对于下游任务的表现均有负面影响。作者认为这可能是参数共享后，<strong>Captioner产生的noisy caption更不可能被Filter给过滤掉了</strong>，也就是所谓的<strong>确认偏差(confirmation bias)</strong>。具体而言，由于 Captioner 和 Filter 共享参数，<strong>导致Filter 的评估标准会与 Captioner 的生成模式一致</strong>，Filter 对噪声描述的过滤能力下降，导致更多噪声描述被保留，表现为噪声比例更低（8% 对比 25%），因而将噪声保留用作训练样本，进一步影响模型性能。<br>
<img src="https://img2024.cnblogs.com/blog/2948101/202505/2948101-20250506213020885-590004191.png" alt="BLIP共享参数2" loading="lazy"><br>
除了上述实验外，作者还在Image Captioning、Visual Question Answering(<span class="math inline">\(VQA\)</span>)和Natural Language Visual Reasoning(<span class="math inline">\(\mathrm{NLVR^2}\)</span>)这三个任务上与SOTA进行对比，基本上都取得最好的成绩。</p>
<h1 id="四总结">四、总结</h1>
<p>BLIP这篇文章提出了一个模型-&gt;<strong>multimodal mixture of encoder-decoder model(MED)</strong> 和一个提升数据集质量的数据重采样的方法-&gt;<strong>CapFilt</strong>。在数据重采样后的数据集上预训练出来的模型，在包括Image Captioning等领域上均取得非常好的成绩。同时，作者也抛出以下几个可改进方向：</p>
<ol>
<li>多轮的数据boostrapping</li>
<li>对于一张图片，用captioner产生更多“合成字幕”来扩充预训练语料库</li>
<li>在CapFilter阶段，训练多种不同的captioner和filter，并且采用的<strong>模型集成</strong>的方式来组合其结果。</li>
</ol>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.3458214020740741" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-05-06 21:57">2025-05-06 21:56</span>&nbsp;
<a href="https://www.cnblogs.com/JuggyZhan">彼得虫</a>&nbsp;
阅读(<span id="post_view_count">5</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18863169);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18863169', targetLink: 'https://www.cnblogs.com/JuggyZhan/p/18863169', title: '【BLIP】解读BLIP' })">举报</a>
</div>
        