
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/flyup/p/18977720" title="发布于 2025-07-10 20:50">
    <span role="heading" aria-level="2">Transformer模型原理概述</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p>  Transformer 是一种基于自注意力机制（Self-Attention）的深度学习模型，最初由 Google 在 2017 年的论文《Attention Is All You Need》中提出，主要用于自然语言处理任务，如今已广泛应用于计算机视觉、语音识别等多个领域，是现代大语言模型（如GPT、BERT等）的核心架构。</p>
<h2 id="一模型架构">一、模型架构</h2>
<p>  Transformer 采用经典的编码器-解码器（Encoder-Decoder）架构，由多个编码器层和多个解码器层堆叠而成（通常为 6 层），每层包含特定的子模块。</p>
<h3 id="1编码器encoder">1.编码器（Encoder）</h3>
<p>  处理输入序列（如句子），生成包含序列语义的中间表示。每个编码器层包含两个子层：</p>
<p>  <strong>多头自注意力机制（Multi-Head Self-Attention）</strong>：捕捉序列内部不同位置的依赖关系。<br>
  <strong>前馈神经网络（Feed Forward Neural Network）</strong>：对注意力输出进行非线性变换。</p>
<p>  每层后均使用残差连接（Residual Connection）和层归一化（Layer Normalization）稳定训练。</p>
<h3 id="2解码器decoder">2.解码器（Decoder）</h3>
<p>  基于编码器的输出生成目标序列（如翻译结果）。每个解码器层包含三个子层：</p>
<p>  <strong>掩码多头自注意力机制（Masked Multi-Head Self-Attention）</strong>：确保解码时不依赖未来位置的信息。<br>
  <strong>编码器-解码器注意力机制（Encoder-Decoder Attention）</strong>：建立输入与输出序列的关联。<br>
  <strong>前馈神经网络</strong>：与编码器中的结构相同。</p>
<h2 id="二自注意力机制self-attention">二、自注意力机制（Self-Attention）</h2>
<p>  自注意力机制是 Transformer 的核心，它允许模型在处理序列时关注不同位置的信息，计算元素间的关联权重。</p>
<h3 id="1-注意力计算的数学表达">1. 注意力计算的数学表达：</h3>
<p>  对于输入序列中的每个元素，注意力机制通过三个矩阵（可训练参数）生成三个向量：</p>
<p>  <strong>查询向量（Query, Q）</strong>：用于计算当前元素与其他元素的关联。<br>
  <strong>键向量（Key, K）</strong>：作为被查询的 “索引”。<br>
  <strong>值向量（Value, V）</strong>：包含元素的实际信息。</p>
<p>  注意力分数的计算过程：</p>
<p>  对每个位置 i，计算其与所有位置 j 的注意力分数，softmax函数将分数归一化为权重，加权求和得到输出。</p>
<p></p><div class="math display">\[Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V
\]</div><p></p><p>  其中，<span class="math inline">\(d_k\)</span>是Key的维度，<span class="math inline">\(\sqrt{d_k}\)</span>称作缩放因子，用于防止梯度消失。</p>
<h3 id="2-多头注意力multi-head-attention">2. 多头注意力（Multi-Head Attention）：</h3>
<p>  将注意力机制拆分为多个 “头”（Head）并行计算，每个头关注不同子空间的信息，最后拼接结果：</p>
<p></p><div class="math display">\[MultiHead(Q,K,V)=Concat(head_1,...,head_h)W^O
\]</div><p></p><p>  其中，每个头的计算独立，<span class="math inline">\(W^O\)</span>为输出投影矩阵。多头机制让模型能从不同角度捕捉序列关系。</p>
<h2 id="三-前馈神经网络feed-forward-network-ffn">三、 前馈神经网络（Feed-Forward Network, FFN）</h2>
<p>  每个编码/解码层中的前馈网络由两个线性变换组成，中间使用 ReLU 激活函数</p>
<p></p><div class="math display">\[FFN(x)=max(0,xW_1+b_1)W_2+b_2
\]</div><p></p><p>  作用：对注意力机制的输出进行非线性变换，增强模型的拟合能力，将注意力捕捉到的特征映射到更高维的语义空间。</p>
<h2 id="四残差连接与层归一化">四、残差连接与层归一化</h2>
<p>  每个子层（注意力、前馈网络）后应用残差连接（Residual Connection）和层归一化（Layer Normalization），缓解梯度消失问题。</p>
<h3 id="1-残差连接residual-connection">1. 残差连接（Residual Connection）</h3>
<p>  作用：解决深层网络中的梯度消失/爆炸问题，帮助模型训练更深的网络结构。<br>
  机制：将某一层的输入直接与该层的输出相加</p>
<p></p><div class="math display">\[Output=Input+Layer(Input)
\]</div><p></p><h3 id="2-层归一化layer-normalization">2. 层归一化（Layer Normalization）</h3>
<p>  作用：稳定网络训练，加速收敛，减少对初始化和学习率的敏感度。<br>
  机制：对单个样本的所有特征维度进行归一化（与批归一化不同，不依赖批次统计量）</p>
<p></p><div class="math display">\[Output=\gamma \cdot \frac{X-\mu}{\sqrt{\sigma^2+\varepsilon}}+\beta
\]</div><p></p><p>  其中，<span class="math inline">\(\mu\)</span>是均值，<span class="math inline">\(\sigma^2\)</span>是方差，<span class="math inline">\(\alpha\)</span>和<span class="math inline">\(\beta\)</span>是可学习的缩放和偏移参数，<span class="math inline">\(\varepsilon\)</span>是防止除零的小常数。</p>
<h2 id="五位置编码positional-encoding">五、位置编码（Positional Encoding）</h2>
<p>  由于 Transformer 本身不具备处理序列顺序的能力（自注意力是全局计算），需通过位置编码为序列添加位置信息。</p>
<p>  常用方法：使用正弦和余弦函数生成位置编码向量，公式如下：</p>
<p></p><div class="math display">\[PE(pos,2i)=sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}})
\]</div><p></p><p></p><div class="math display">\[PE(pos,2i+1)=cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})
\]</div><p></p><p>  其中，pos为位置，i为维度，<span class="math inline">\(d_{model}\)</span>为模型维度。<br>
  作用：将位置信息融入输入向量，使模型能区分序列中不同位置的语义。</p>
<h2 id="六应用与扩展">六、应用与扩展</h2>
<p>  <strong>基础应用</strong>：<br>
    机器翻译、文本摘要、语音识别、问答系统等序列到序列任务。</p>
<p>  <strong>变体模型</strong>：<br>
    BERT：仅使用编码器部分，通过掩码语言模型（MLM）和下一句预测（NSP）预训练，开创预训练模型先河。<br>
    GPT系列：仅使用解码器部分，通过自回归（Autoregressive）方式生成文本，推动生成式 AI 发展。</p>
<p>  <strong>核心优势</strong>：<br>
    长距离依赖建模能力强，避免 RNN 的梯度消失问题。<br>
    并行计算效率高，适合大规模数据训练。<br>
    注意力机制可解释性较强，通过可视化权重能直观理解模型关注的重点。</p>
<h2 id="七小结">七、小结</h2>
<p>  Transformer 通过自注意力机制替代传统序列模型中的循环结构，实现了对序列信息的并行处理和长距离依赖建模。其核心组件（多头注意力、位置编码、前馈网络）的设计使其在效率和性能上超越了传统模型，为后续大语言模型和多模态模型的发展奠定了基础。</p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0" data-date-updated="2025-07-10 20:51">2025-07-10 20:50</span>&nbsp;
<a href="https://www.cnblogs.com/flyup">归去_来兮</a>&nbsp;
阅读(<span id="post_view_count">0</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18977720);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18977720', targetLink: 'https://www.cnblogs.com/flyup/p/18977720', title: 'Transformer模型原理概述' })">举报</a>
</div>
        