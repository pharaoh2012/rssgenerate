
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/orion-orion/p/18730657" title="发布于 2025-02-22 10:44">
    <span role="heading" aria-level="2">学习理论：预测器-拒绝器多分类弃权学习</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                    <div id="cnblogs_post_description" style="display: none">
        <img src="https://img2024.cnblogs.com/blog/1784958/202502/1784958-20250222104412637-1936633880.png" alt="学习理论：预测器-拒绝器多分类弃权学习" class="desc_img">
        弃权学习（learning with abstention）主要是为了使分类器在学习过程中可能出现的误导性或者不正确的信息时（这常被称为“幻觉”），能够对做出预测进行弃权。目前，弃权学习的方法主要可以分为以下几种：基于置信度的方法（confidence-based methods）。这种方法在预训练模型返回的分数低于某个阈值θ时弃权；选择性分类（selective classification）。设置一个预测器和一个选择器，并定义被期望的选择或收敛度归一化的选择风险或损失；预测器-拒绝器公式（predictor-rejector formulation）。同时学习一个预测器和一个拒绝器，它们来自不同的函数族，这种方法显式地考虑了弃权花费c，当学习器弃权时将导致大小为c的损失；基于分数的公式（score-based formulation）。对多分类类别进行增广（多一个拒绝标签类型），当分配给拒绝标签的分数最高时进行弃权。
    </div>
<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p>目前确定去京大读博了，预计方向是学习理论（Learning Theory）。熟悉我的朋友可能知道，虽然我读研期间的方向主要是联邦学习和推荐系统，但是我也会更新一些理论相关的博客，因为我确实对理论方向比较感兴趣。目前准备10月份左右入学，在这之前接受导师的线上指导开始科研。现在就以以这篇博客做为我PhD科研的开始吧(#^.^#)。</p>
<h1 id="1-导引">1 导引</h1>
<p><strong>弃权学习（learning with abstention）</strong> <sup>[1]</sup>主要是为了使分类器在学习过程中可能出现的误导性或者不正确的信息时（这常被称为“幻觉”），能够对做出预测进行弃权。目前，弃权学习的方法主要可以分为以下几种：</p>
<ul>
<li><strong>基于置信度的方法（confidence-based methods）</strong>。这种方法在预训练模型返回的分数低于某个阈值<span class="math inline">\(\theta\)</span>时弃权。</li>
<li><strong>选择性分类（selective classification）</strong>。设置一个预测器和一个选择器，并定义被期望的选择或收敛度归一化的选择风险或损失。</li>
<li><strong>预测器-拒绝器公式（predictor-rejector formulation）</strong>。同时学习一个预测器和一个拒绝器，它们来自不同的函数族，这种方法显式地考虑了弃权花费<span class="math inline">\(c\)</span>，当学习器弃权时将导致大小为<span class="math inline">\(c\)</span>的损失；</li>
<li><strong>基于分数的公式（score-based formulation）</strong>。对多分类类别进行增广（多一个拒绝标签类型），当分配给拒绝标签的分数最高时进行弃权。</li>
</ul>
<p>本文关注预测器-拒绝器公式，也即显式地建模弃权花费的一种方法。那么该如何对多分类弃权问题进行形式化，什么时候适合弃权呢？</p>
<p>我们先来考虑有监督二分类弃权学习场景。在这种场景中标签为<span class="math inline">\(\mathcal{Y}=\{-1, +1\}\)</span>，样本独立同分布地采样自<span class="math inline">\(\mathcal{X}\times \mathcal{Y}\)</span>空间上的固定未知分布<span class="math inline">\(\mathcal{D}\)</span>。给定实例<span class="math inline">\(x\in \mathcal{X}\)</span>，学习器若选择对预测<span class="math inline">\(x\)</span>的标签进行弃权，则产生一个损失<span class="math inline">\(c(x) \in [0, 1]\)</span>做为代价；否则，使用预测器<span class="math inline">\(h\)</span>做出预测<span class="math inline">\(h(x)\)</span>并产生一个标准的0-1损失<span class="math inline">\(\mathbb{I}_{yh(x)\leqslant 0}\)</span>（其中<span class="math inline">\(y\)</span>为真实标签）。由于随机猜测可以达到<span class="math inline">\(\frac{1}{2}\)</span>的期望代价，拒绝操作只有在<span class="math inline">\(c(x) &lt; \frac{1}{2}\)</span>时是合理的。</p>
<p>我们使用<span class="math inline">\((h, r)\)</span>来建模学习器，其中函数<span class="math inline">\(r: \mathcal{X} \rightarrow \mathbb{R}\)</span>使得点<span class="math inline">\(x\in \mathcal{X}\)</span>在<span class="math inline">\(r(x)\leqslant 0\)</span>时被拒绝，假设<span class="math inline">\(h: \mathcal{X} \rightarrow \mathbb{R}\)</span>预测未被拒绝的点的标签（体现为<span class="math inline">\(h(x)\)</span>的正负）。对任意<span class="math inline">\((x, y)\in \mathcal{X}\times \mathcal{Y}\)</span>，<span class="math inline">\((h, r)\)</span>的弃权损失<sup>[2]</sup>定义如下：</p>
<p></p><div class="math display">\[L_{\text{abst}}(h, r, x, y) = \underbrace{\mathbb{I}_{yh(x)\leqslant 0}\mathbb{I}_{r(x) &gt; 0}}_{\text{不弃权}}  + \underbrace{c(x) \mathbb{I}_{r(x)\leqslant 0}}_{\text{弃权}}
\]</div><p></p><p>假定对于学习器来说弃权花费<span class="math inline">\(c(x)\)</span>是已知的。在接下来的分析中，我们假设<span class="math inline">\(c\)</span>是一个常值函数，但我们的部分分析可以应用于更普遍的情况。</p>
<p>设<span class="math inline">\(\mathcal{H}\)</span>和<span class="math inline">\(\mathcal{R}\)</span>为两个从<span class="math inline">\(\mathcal{X}\)</span>到<span class="math inline">\(\mathbb{R}\)</span>的函数构成的函数族。此外，我们假设带标签样本<span class="math inline">\(S=((x_1, y_1), \cdots, (x_m, y_m))\)</span>独立同分布地采自<span class="math inline">\(\mathcal{D}^m\)</span>。则学习问题即为确定一个<span class="math inline">\((h, r)\in \mathcal{H}\times \mathcal{R}\)</span>以使得下列期望弃权损失<span class="math inline">\(R(h, r)\)</span>尽可能小：</p>
<p></p><div class="math display">\[R_{L_{\text{abst}}}(h, r) = \mathbb{E}_{(x, y)\sim \mathcal{D}}L_{\text{abst}}(h, r, x, y)
\]</div><p></p><p>对于大多数假设集而言，优化上述期望弃权损失<span class="math inline">\(R_{L_{\text{abst}}}(h, r)\)</span>是难以处理的（intractable）。因此，在这种情况下学习算法需要依赖于<strong>代理损失（surrogate loss ）</strong>。那么，一个重要问题就是什么种类的代理损失能够被用于替代目标弃权损失。直觉上，一个代理损失需要易于优化，且其最小化会导向目标损失的最小化。术语<strong>校准（calibration）</strong> 就用于定义这样一种损失函数，这种损失函数能够确保风险最小化的预测器能够成为贝叶斯最优分类器。下图直观地展现了校准的代理损失的性质<sup>[4]</sup>（其中目标损失<span class="math inline">\(\phi_{\text{01}}\)</span>是二分类中常见的0-1损失，<span class="math inline">\(\phi\)</span>是其代理损失）：</p>
<p align="center">
<img src="https://images.cnblogs.com/cnblogs_com/blogs/538207/galleries/2445565/o_250222011842_%E6%A0%A1%E5%87%86%E7%9A%84%E4%BB%A3%E7%90%86%E6%8D%9F%E5%A4%B1%E7%9A%84%E6%80%A7%E8%B4%A8.png" width="650" height="420" alt="" align="center">        
</p>
<p>出于理论分析目的，直接定义预测器和拒绝器的校准更为方便（基于它们是否是贝叶斯最优的）。因此，我们定义如下关于校准的符号：</p>
<p><strong>定义 1</strong> <strong>预测器-拒绝器的校准</strong> 我们称<span class="math inline">\((h, r): \mathcal{X}\rightarrow \mathbb{R}\times \mathbb{R}\)</span>是校准的，如果<span class="math inline">\(R_{L_{\text{abst}}}(h, r) = R_{L_{\text{abst}}}(h^*, r^*)\)</span>。</p>
<p>在本文中，我们分别考虑预测器和拒绝器的校准，这使得我们更好地理解带拒绝分类的难度来自于何处。</p>
<p><strong>定义 2</strong> <strong>预测器的分类校准</strong> 我们称<span class="math inline">\(h: \mathcal{X}\rightarrow \mathcal{Y}\)</span>是预测-校准的，如果<span class="math inline">\(h(x) = h^*(x)\)</span>在<span class="math inline">\(\mathcal{X}\)</span>上几乎处处成立。</p>
<p><strong>定义 3</strong> <strong>拒绝器的拒绝校准</strong> 我们称<span class="math inline">\(r: \mathcal{X}\rightarrow \mathbb{R}\)</span>是拒绝-校准的，如果<span class="math inline">\(\text{sign}[r(x)] = \text{sign}[r^*(x)]\)</span>对所有满足<span class="math inline">\(r^*(x) \neq 0\)</span>的<span class="math inline">\(x\in \mathcal{X}\)</span>成立。</p>
<p>通过这些定义与损失函数<span class="math inline">\(L_{\text{abst}}\)</span>的形式可以看到，如果<span class="math inline">\(h\)</span>是预测-校准的且<span class="math inline">\(r\)</span>是拒绝-校准的，则<span class="math inline">\((h, r)\)</span>是校准的。</p>
<p>如下列的代理损失<span class="math inline">\(L_{\text{PB}}(h, r, x, y)\)</span>：</p>
<p></p><div class="math display">\[L_{\text{PB}}(h, r, x, y) = \widetilde{\phi} (\alpha [yh(x) - r(x)]) + c\phi (\beta r(x))
\]</div><p></p><p>这里<span class="math inline">\(\widetilde{\phi}\)</span>和<span class="math inline">\(\phi\)</span>为<span class="math inline">\(\mathbb{I}_{[z\leqslant 0]}\)</span>的凸上界。通过选择适当的参数<span class="math inline">\(\alpha, \beta &gt; 0\)</span>，Cortes等人<sup>[2]</sup>基于指数损失<span class="math inline">\(\widetilde{\phi}(z) = \phi(z) = \exp (-z)\)</span>导出了一个校准的结果。然而，Ni等人<sup>[3]</sup>指出，这个代理损失只在二分类的情况下可行，想要将这个代理损失扩展到多分类的情况下是有挑战性的，于是转而采用基于置信度分数的方法来处理多分类的情况。</p>
<p>本文作者尝试在多分类的情况下，为预测器-拒绝器框架下的弃权学习定义贝叶斯一致的代理损失<sup>[1]</sup>。具体地，本文作者引入了一些新的代理损失族并为其证明了强的非渐近和假设集特定的一致性保障。这些保障为弃权损失函数的估计误差提供了代理损失形式的凸上界。</p>
<p>在本文中，我们将在两种不同的设置下讨论预测器-拒绝器弃权代理损失，分别是<strong>单阶段</strong>和<strong>两阶段</strong>。在单阶段的设置下，预测器和拒绝器同时学习；而在两阶段的设置下（在实际应用中很重要），在第一阶段中预测器使用标准的代理损失（例如交叉熵损失）来学习（例如大的预训练模型），然后在第二阶段预测器被固定，只需要学习拒绝器。</p>
<p>我们会为一些预测器-拒绝器框架中的弃权代理损失<span class="math inline">\(L\)</span>证明 <strong><span class="math inline">\((\mathcal{H}, \mathcal{R})\)</span> - 一致性界（<span class="math inline">\((\mathcal{H}, \mathcal{R})\)</span> -consistency bound）</strong>。这些不等式给出了关于假设<span class="math inline">\(h\in \mathcal{H}\)</span>和拒绝器<span class="math inline">\(r \in \mathcal{R}\)</span>的预测器-拒绝器弃权损失<span class="math inline">\(L_{\text{abst}}\)</span>的上界（以它们的弃权代理损失<span class="math inline">\(L\)</span>形式）。它们满足下列形式：</p>
<p><strong><span class="math inline">\((\mathcal{H}, \mathcal{R})\)</span> - 一致性界</strong></p>
<p></p><div class="math display">\[R_{L_{\text{abst}}}(h, r) - R_{L_{\text{abst}}}^{*}(\mathcal{H}, \mathcal{R}) \leqslant f(R_L(h, r) - R_{L}^{*}(\mathcal{H}, \mathcal{R}))
\]</div><p></p><p>这里<span class="math inline">\(f\)</span>是非递减函数。因此，当代理估计误差<span class="math inline">\(R_L(h, r) - R_{L}^{*}(\mathcal{H}, \mathcal{R})\)</span>减少到<span class="math inline">\(\epsilon\)</span>时，估计误差<span class="math inline">\((R_{L_{\text{abst}}}(h, r) - R_{L_{\text{abst}}}^{*}(\mathcal{H}, \mathcal{R}))\)</span>会被<span class="math inline">\(f(\epsilon)\)</span>所界定。在这些界中会出现的一个重要的项为<strong>最小化能力差距（minimizability gap）</strong>，其定义为<span class="math inline">\(M_{L}(\mathcal{H}, \mathcal{R}) = R_{L}^{*}(\mathcal{H}, \mathcal{R}) - \mathbb{E}_x[\inf_{h\in \mathcal{H}, r\in \mathcal{R}}\mathbb{E}_{y}[L(h, r, X, y)\mid X = x]]\)</span>。当损失函数<span class="math inline">\(L\)</span>只依赖于<span class="math inline">\(h(x)\)</span>和<span class="math inline">\(r(x)\)</span>（对在大多数应用中使用的损失函数都成立），且当<span class="math inline">\(\mathcal{H}\)</span>和<span class="math inline">\(\mathcal{R}\)</span>包括了所有可测函数时，最小化能力差距为0。然而，它对于受限的假设集<span class="math inline">\(\mathcal{H}\)</span>和<span class="math inline">\(\mathcal{R}\)</span>一般是非0的。最小化能力差距能够被近似误差 <strong>（approximation error）</strong><span class="math inline">\(\mathcal{A}_{L}(\mathcal{H}, \mathcal{R}) = R_{L}^{*}(\mathcal{H}, \mathcal{R}) - \mathbb{E}_x[\inf_{h, r}\mathbb{E}_{y}[L(h, r, X, y)\mid X = x]]\)</span>所界定，这里下界取遍所有可测函数。但是，最小化能力差距是个更好的量并导出更好的理论保障。</p>
<h1 id="2-单阶段预测器-拒绝器代理损失">2 单阶段预测器-拒绝器代理损失</h1>
<p>在多分类情形下，标签<span class="math inline">\(\mathcal{Y} = \{1, \cdots, n\}\)</span>（<span class="math inline">\(n\geqslant 2\)</span>）。我们取<span class="math inline">\(h(x) = \text{arg max}_{y\in \mathcal{Y}} {[h(x)]}_y\)</span>。则类比二分类情形，对于多分类问题，我们同样可以定义如下的预测器-拒绝器弃权损失：</p>
<p></p><div class="math display">\[L_{\text{abst}}(h, r, x, y) = \underbrace{\mathbb{I}_{h(x) \neq y}\mathbb{I}_{r(x) &gt; 0}}_{\text{不弃权}}  + \underbrace{c(x) \mathbb{I}_{r(x)\leqslant 0}}_{\text{弃权}}
\]</div><p></p><p>注意，和之前二分类情况的不同之处在于<span class="math inline">\(\mathbb{I}_{yh(x)\leqslant 0}\)</span>变为了<span class="math inline">\(\mathbb{I}_{h(x) \neq y}\)</span>，这里<span class="math inline">\(h(x)\)</span>直接输出分类标签。设<span class="math inline">\(\mathcal{l}\)</span>为在标签<span class="math inline">\(\mathcal{Y}\)</span>上定义的0-1多分类损失的代理损失，则我们可以在此基础上进一步定义弃权代理损失<span class="math inline">\(L\)</span>：</p>
<p></p><div class="math display">\[L(h, r, x, y) = \mathcal{l}(h, x, y)\phi(-\alpha r(x)) + \psi(c) \phi(\beta r(x))
\]</div><p></p><p>其中<span class="math inline">\((x, y)\in \mathcal{X}\times \mathcal{Y}\)</span>，<span class="math inline">\(\psi\)</span>是非递减函数，<span class="math inline">\(\phi\)</span>是非递增辅助函数（做为<span class="math inline">\(t \mapsto \mathbb{I}_{t \leqslant 0}\)</span>的上界），<span class="math inline">\(\alpha\)</span>、<span class="math inline">\(\beta\)</span>为正常量。上述的<span class="math inline">\(L\)</span>可视为Cortes等人提出的二分类弃权代理损失<span class="math inline">\(L_{\text{PB}}\)</span>的多分类推广版本。<span class="math inline">\(L_{\text{PB}}\)</span>可视为将<span class="math inline">\(\mathcal{l}\)</span>损失设置为基于间隔的二分类损失<span class="math inline">\(\widetilde{\phi}(yh(x))\)</span>，并设置<span class="math inline">\(\psi(t) = t\)</span>：</p>
<p></p><div class="math display">\[L_{\text{bin}}(h, r, x, y) = \widetilde{\phi}(yh(x))\phi(-\alpha r(x)) + c \phi(\beta r(x))
\]</div><p></p><p>最小化带正则项的<span class="math inline">\(L_{\text{bin}}\)</span>，并使用基于间隔的损失<span class="math inline">\(\widetilde{\phi}\)</span>（例如指数损失<span class="math inline">\(\widetilde{\phi}_{\text{exp}}(t) = \exp (-t)\)</span>以及合页损失<span class="math inline">\(\widetilde{\phi}_{\text{hinge}}(t) = \max\{1 - t, 0\}\)</span>（合页损失可参见博客<a href="https://www.cnblogs.com/orion-orion/p/15399602.html" target="_blank">《统计学习：线性支持向量机(Pytorch实现) 》</a>）），可以在二分类情形下达到SOTA的结果。然而，我们下面会看到推广到多分类的弃权代理损失<span class="math inline">\(L\)</span>对代理损失<span class="math inline">\(\mathcal{l}\)</span>的选择施加了更加严格的条件，这将诸如多分类指数损失的代理损失给排除掉了。不过，我们也会看到一些其它的损失函数满足该条件，例如多分类合页损失。下面，为了简便起见，我们主要对<span class="math inline">\(\phi(t) = \exp(-t)\)</span>进行分析，尽管相似的分析也可以应用于其它函数<span class="math inline">\(\phi\)</span>。我们先展示负面的结果，排除掉一些弃权代理损失<span class="math inline">\(L\)</span>，这些弃权代理损失基于不满足特定条件的损失<span class="math inline">\(\mathcal{l}\)</span>。</p>
<p>下面，我们假定假设集<span class="math inline">\(\mathcal{H}\)</span>是<strong>对称的（symmetric）</strong> 与 <strong>完备的（complete）</strong>。我们称一个假设集<span class="math inline">\(\mathcal{H}\)</span>是对称的，如果存在一个从<span class="math inline">\(\mathcal{X}\)</span>到<span class="math inline">\(\mathbb{R}\)</span>的函数<span class="math inline">\(f\)</span>的族<span class="math inline">\(\mathcal{F}\)</span>使得对任意<span class="math inline">\(x\in \mathcal{X}\)</span>，有<span class="math inline">\(\left\{\left({\left[h\left(x\right)\right]}_1, \cdots, {\left[h\left(x\right)\right]}_2\right): h\in \mathcal{H}\right\}=\left\{\left(f_1\left(x\right), \cdots, f_n\left(x\right)\right): f_1, \cdots, f_n \in \mathcal{F}\right\}\)</span>。我们称一个假设集<span class="math inline">\(\mathcal{H}\)</span>是完备的，如果其产生的分数集合能够张成<span class="math inline">\(\mathbb{R}\)</span>，也即对任意<span class="math inline">\((x, y)\in \mathcal{X}\times \mathcal{Y}\)</span>，<span class="math inline">\(\left\{\left[h(x)\right]_y: h\in \mathcal{H}\right\} = \mathbb{R}\)</span>，</p>
<p><strong>定理 1</strong> <strong>单阶段代理损失的负面结果</strong> 假设<span class="math inline">\(\mathcal{H}\)</span>是对称的与完备的，且<span class="math inline">\(\mathcal{R}\)</span>是完备的。若存在<span class="math inline">\(x\in \mathcal{X}\)</span>使得<span class="math inline">\(\inf_{h\in \mathcal{H}}\mathbb{E}_{y}[\mathcal{l}(h, X, y)\mid X = x] \neq \frac{\beta \psi (1 - \max_{y\in \mathcal{Y}}p (x, y))}{\alpha}\)</span>，则不存在满足属性<span class="math inline">\(\lim_{t\rightarrow 0^{+}}\Gamma (t) = 0\)</span>的非递减函数<span class="math inline">\(\Gamma: \mathbb{R}_{+}\rightarrow \mathbb{R}_{+}\)</span>使得下列<span class="math inline">\((\mathcal{H}, \mathcal{R})\)</span>-一致性界成立：对所有<span class="math inline">\(h\in \mathcal{H}, r\in \mathbb{R}\)</span>以及任意分布，有</p>
<p></p><div class="math display">\[R_{L_{\text{abst}}}(h, r) - R_{L_{\text{abst}}}^{*}(\mathcal{H}, \mathcal{R}) + M_{L_{\text{abst}}}(\mathcal{H}, \mathcal{R}) \leqslant \Gamma(R_L(h, r) - R_{L}^{*}(\mathcal{H}, \mathcal{R}) + M_{L}(\mathcal{H}, \mathcal{R}))
\]</div><p></p><p>证明可以采用反证法来完成。若假设此处的<span class="math inline">\((\mathcal{H}, \mathcal{R})\)</span> - 一致性界是有效的，则蕴含着采用单阶段代理损失学习的pointwise类别最优预测器和类别最优拒绝器会与采用弃权损失所学习的版本对齐。将这些显式的公式纳入代理损失的条件风险分析会导致导数检验的矛盾。</p>
<p>考虑定理 1，为了找到满足<span class="math inline">\((\mathcal{H}, \mathcal{R})\)</span> - 一致性界的代理损失<span class="math inline">\(L\)</span>，我们需要考虑满足以下条件的多分类代理损失<span class="math inline">\(\mathcal{l}\)</span>：对任意<span class="math inline">\(x\in \mathcal{X}\)</span>，对某些<span class="math inline">\(\psi\)</span>和<span class="math inline">\((\alpha, \beta)\in \mathbb{R}^2_{+}\)</span>有</p>
<p></p><div class="math display">\[\inf_{h\in \mathcal{H}}\mathbb{E}_{y}[\mathcal{l}(h, X, y)\mid X = x] = \frac{\beta \psi (1 - \max_{y\in \mathcal{Y}}p (x, y))}{\alpha}
\]</div><p></p><p>在二分类的情形下，找到满足这个条件的<span class="math inline">\(\mathcal{l}\)</span>较为容易，因为<span class="math inline">\(\max_{y\in \mathcal{Y}}p (x, y)\)</span>也直接地决定了其它的概率。然而，在多分类的情形下，固定<span class="math inline">\(\max_{y\in \mathcal{Y}}p (x, y)\)</span>后，在取<span class="math inline">\(\inf_{h\in \mathcal{H}}\mathbb{E}_{y}[\mathcal{l}(h, X, y)\mid X = x]\)</span>时仍然需要考虑其它概率的不同取值。这将导致将二分类框架扩展到多分类的困难。</p>
<p>然而，我们会展示这个必要的条件会被三个常见的多分类代理损失<span class="math inline">\(\mathcal{l}\)</span>所满足。进一步地，我们将证明基于这三种<span class="math inline">\(\mathcal{l}\)</span>中任意一种的预测器-拒绝器代理损失<span class="math inline">\(L\)</span>的<span class="math inline">\((\mathcal{H}, \mathcal{R})\)</span> - 一致性界。这三种损失<span class="math inline">\(\mathcal{l}\)</span>的定义如下（对所有<span class="math inline">\(h\in \mathcal{H}\)</span>和<span class="math inline">\((x, y)\)</span>）：</p>
<ul>
<li><strong>平均绝对误差损失（mean absolute error loss）</strong>：<span class="math inline">\(\mathcal{l}_{\text{mae}}(h, x, y) = 1 - \frac{e^{h(x, y)}}{\sum_{y^{\prime}\in \mathcal{Y}}e^{h(x, y^{\prime})}}\)</span>；</li>
<li><strong>约束<span class="math inline">\(\rho\)</span>-合页损失（constrained ρ-hinge loss）</strong>：<span class="math inline">\(\mathcal{l}_{\rho-\text{hinge}}(h, x, y) = \sum_{y^{\prime}\neq y}\phi_{\rho-\text{hinge}}(-h(x, y^{\prime})), \rho &gt; 0\)</span>，其中<span class="math inline">\(\phi_{\rho-\text{hinge}}(t) = \max\{0, 1 - \frac{t}{\rho}\}\)</span>为<span class="math inline">\(\rho\)</span>-合页损失，且约束条件<span class="math inline">\(\sum_{y\in \mathcal{Y}}h(x, y)=0\)</span>。</li>
<li><strong><span class="math inline">\(\rho\)</span>-间隔损失（ρ-Margin loss）</strong>：<span class="math inline">\(\mathcal{l}_{\rho}(h, x, y) = \phi_{\rho}(\rho_h (x, y))\)</span>，其中<span class="math inline">\(\rho_{h}(x, y) = h(x, y) - \max_{y^{\prime} \neq y}h(x, y^{\prime})\)</span>是置信度间隔，<span class="math inline">\(\phi_{\rho}(t) = \min\{\max\{0, 1 - \frac{t}{\rho}\}, 1\}, \rho &gt; 0\)</span>为<span class="math inline">\(\rho\)</span>-间隔损失。</li>
</ul>
<blockquote>
<p><strong>注</strong> 关于这里的间隔损失，可以理解为合页损失的多分类扩展（参见Crammer-Singer损失<sup>[4][5]</sup>），它旨在最大化下列预测间隔（以3个类别为例）：</p>
<p align="center">
<img src="https://images.cnblogs.com/cnblogs_com/blogs/538207/galleries/2445565/o_250220061509_maximize%20prediction%20margin.png" width="700" height="200" alt="" align="center">        
</p>
</blockquote>
<p><strong>定理 2</strong> <strong>单阶段代理损失的<span class="math inline">\((\mathcal{H}, \mathcal{R})\)</span> - 一致性界</strong> 假设<span class="math inline">\(\mathcal{H}\)</span>是对称与完备的。则对<span class="math inline">\(\alpha=\beta\)</span>，<span class="math inline">\(\mathcal{l} = \mathcal{l}_{\text{mae}}\)</span>，或者<span class="math inline">\(\mathcal{l} = \mathcal{l}_{\rho}\)</span>与<span class="math inline">\(\psi(t) = t\)</span>，或者<span class="math inline">\(\mathcal{l} = \mathcal{l}_{\rho - \text{hinge}}\)</span>与<span class="math inline">\(\psi(t) = t\)</span>，有下列<span class="math inline">\((\mathcal{H}, \mathcal{R})\)</span> - 一致性界对<span class="math inline">\(h\in \mathcal{H}, r\in \mathcal{R}\)</span>和任意分布成立：</p>
<p></p><div class="math display">\[R_{L_{\text{abst}}}(h, r) - R_{L_{\text{abst}}}^{*}(\mathcal{H}, \mathcal{R}) + M_{L_{\text{abst}}}(\mathcal{H}, \mathcal{R}) \leqslant \Gamma(R_L(h, r) - R_{L}^{*}(\mathcal{H}, \mathcal{R}) + M_{L}(\mathcal{H}, \mathcal{R}))
\]</div><p></p><p>其中对<span class="math inline">\(\mathcal{l} = \mathcal{l}_{\text{mae}}\)</span>取<span class="math inline">\(\Gamma (t) = \max\{2n\sqrt{t}, nt\}\)</span>；对<span class="math inline">\(\mathcal{l}=\mathcal{l}_{\rho}\)</span>取<span class="math inline">\(\Gamma (t) = \max\{2\sqrt{t}, t\}\)</span>；对<span class="math inline">\(\mathcal{l} - \mathcal{l}_{\rho - \text{hinge}}\)</span>取<span class="math inline">\(\Gamma (t) = \max\{2\sqrt{nt}, t\}\)</span>。</p>
<p>该理论为我们在单阶段设置下描述的预测器-拒绝器代理损失提供了有力的保障。该定理证明中使用的技术是新颖的且需要对涉及pointwise类别最优预测器和拒绝器的多种情况的仔细分析。这一分析是具有挑战性的且需要考虑具体损失函数的条件风险与校准差距。该方法由于同时在弃权场景下最小化预测器和拒绝器，整体上不同于Awasthi等人描述的标准场景<sup>[6]</sup>。下面是当<span class="math inline">\(\mathcal{H}\)</span>和<span class="math inline">\(\mathcal{R}\)</span>包括所有可测函数时定理2的一个直接推论（在下面的情况下最小化能力差距<span class="math inline">\(M_{{L}_{\text{abst}}}\)</span>与<span class="math inline">\(M_L\)</span>都会变为0）。</p>
<p><strong>推论 3</strong> <strong>单阶段代理损失函数的额外误差界</strong> 对<span class="math inline">\(\alpha = \beta\)</span>，<span class="math inline">\(\mathcal{l} = \mathcal{l}_{\text{mae}}\)</span>或者<span class="math inline">\(\mathcal{l} = \mathcal{l}_p\)</span>与<span class="math inline">\(\psi(t) = t\)</span>，或者<span class="math inline">\(\mathcal{l} = \mathcal{l}_{\rho-\text{hinge}}\)</span>与<span class="math inline">\(\psi(t) = nt\)</span>，下列<strong>额外误差界（excess error bound）</strong> 对所有<span class="math inline">\(h\in \mathcal{H}_{\text{all}}, r\in \mathcal{R}_{\text{all}}\)</span>（这里<span class="math inline">\(\mathcal{H}_{\text{all}}\)</span>和<span class="math inline">\(\mathcal{R}_{\text{all}}\)</span>为所有可测函数构成的集合）以及任意分布成立：</p>
<p></p><div class="math display">\[R_{L_{\text{abst}}}(h, r) - R_{L_{\text{abst}}}^{*}(\mathcal{H}_{\text{all}}, \mathcal{R}_{\text{all}}) \leqslant \Gamma (R_L(h, r) - R_L^{*}(\mathcal{H}_{\text{all}}, \mathcal{R}_{\text{all}}))
\]</div><p></p><p>其中<span class="math inline">\(\Gamma\)</span>拥有与定理 2中相同的形式。</p>
<p>该推论以一个积极的方式为预测器-拒绝器框架下的多分类弃权学习提供了贝叶斯一致的代理损失。事实上，它提供了一个更强的结果，因为它为之前描述过的三种弃权代理损失给出了额外误差界。这些是比这些损失函数的贝叶斯一致性更强的保障（通过取极限操作即可得到）。</p>
<p>需要指出的是，该新颖的单阶段预测器-拒绝器代理损失可能导致一些优化的挑战。这是下列因素所导致的：优化平均绝对误差损失的困难，约束合页损失施加的限制（与在神经网络假设中做为标准使用的Softmax函数不兼容），以及<span class="math inline">\(\rho\)</span>-间隔损失的非凸性。然而，我们的原始目标是理论分析，而且这些代理损失的意义体现在它们的创新性和强理论保障。正如推论 3所展示的，它们是首个用于多分类弃权问题的预测器-拒绝器贝叶斯一致的代理损失。</p>
<h1 id="3-两阶段预测器-拒绝器代理损失">3 两阶段预测器-拒绝器代理损失</h1>
<p>接下来，我们展示两阶段的计算方法，在这一方法中我们引入<span class="math inline">\(\mathcal{l}\)</span>选择更灵活的代理损失，这些代理损失具有更好的优化属性。和前面类似，我们会为它们构建<span class="math inline">\((\mathcal{H}, \mathcal{R})\)</span> - 一致性界。两阶段场景是一个重要的场景，因为在实践中常常大的预训练的预测模型已经可利用（第一阶段），而重新训练它会产生不可接受的昂贵代价。接下来问题变为了保持第一阶段的预测模型保持不变，而随后学习一个有用的拒绝模型（第二阶段）。</p>
<p>两阶段的预测器-拒绝器弃权损失和我们在第2部分中提到的单阶段预测器-拒绝器弃权损失<span class="math inline">\(L_{\text{abst}}\)</span>不同的是，<span class="math inline">\(h\)</span>被固定，只需要学习<span class="math inline">\(r\)</span>，而不同于<span class="math inline">\(L_{\text{abst}}\)</span>中的<span class="math inline">\(h\)</span>和<span class="math inline">\(r\)</span>同时被学习。我们设<span class="math inline">\(L_{\text{abst}, h}\)</span>为<span class="math inline">\(L_{\text{abst}}\)</span>的固定预测器<span class="math inline">\(h\)</span>的两阶段版本，定义如下：对任意<span class="math inline">\(r\in \mathbb{R}\)</span>，<span class="math inline">\(x\in \mathcal{X}\)</span>与<span class="math inline">\(y\in \mathcal{Y}\)</span>，</p>
<p></p><div class="math display">\[L_{\text{abst}, h}(r, x, y) = \underbrace{\mathbb{I}_{h(x) \neq y}\mathbb{I}_{r(x) &gt; 0}}_{\text{不弃权}}  + \underbrace{c \mathbb{I}_{r(x)\leqslant 0}}_{\text{弃权}}
\]</div><p></p><p>作者提出了一个两阶段计算方法：</p>
<ul>
<li>首先，找到一个分类器<span class="math inline">\(h\)</span>以最小化标准多分类代理损失<span class="math inline">\(\mathcal{l}\)</span>；</li>
<li>其次，固定<span class="math inline">\(h\)</span>，通过最小化代理损失<span class="math inline">\(L_{\phi, h}\)</span>找到<span class="math inline">\(r\)</span>。关于<span class="math inline">\(r\)</span>的的代理损失函数定义如下（对所有的<span class="math inline">\((x, y)\)</span>）：</li>
</ul>
<p></p><div class="math display">\[L_{\phi, h}(r, x, y) = \mathbb{I}_{h(x) \neq y}\phi(-r(x)) + c \phi(r(x))
\]</div><p></p><p>这里<span class="math inline">\(\phi\)</span>为做为<span class="math inline">\(t\mapsto \mathbb{I}_{t\leqslant 0}\)</span>上界的非递增辅助函数，也即对应在二分类中为函数<span class="math inline">\(h\)</span>决定间隔损失<span class="math inline">\(\phi(yh(x))\)</span>的函数（例如指数函数），其中<span class="math inline">\(y\in \{-1, +1\}\)</span>。该计算方法是比较直接的，因为第一阶段涉及使用标准代理损失（例如Logistic损失或带Softmax的交叉熵损失）寻找预测器的经典任务；而第二阶段也相对简单，因为<span class="math inline">\(h\)</span>被固定，且<span class="math inline">\(L_{\phi, h}\)</span>的形式也不复杂，其中<span class="math inline">\(\phi\)</span>可能是Logistic损失或者指数损失。需要指出的是，严格的选择上式中的示性函数对保障两阶段代理损失获益于<span class="math inline">\((\mathcal{H}, \mathcal{R})\)</span> - 一致性界是很重要的。如果代理损失函数在第一阶段中被使用，这可能不一定满足。</p>
<p>需要指出的是，损失函数<span class="math inline">\(L_{\text{abst}, h}\)</span>和<span class="math inline">\(L_{\phi, h}\)</span>都是弃权函数<span class="math inline">\(r\)</span>的函数，而<span class="math inline">\(L_{\text{abst}}\)</span>是<span class="math inline">\((h, r)\in (\mathcal{H}, \mathcal{R})\)</span>的函数。</p>
<p>定义二值0-1分类损失<span class="math inline">\(\mathcal{l}_{\text{0-1}}^{\text{binary}(r, x, y)} = \mathbb{I}_{y\neq \text{sign}(r(x))}\)</span>，其中<span class="math inline">\(\text{sign}(t) = \mathbb{I}_{t &gt; 0} - \mathbb{I}_{t\leqslant 0}\)</span>。正如单阶段代理损失，两阶段代理损失也获益于强一致性保障。我们先展示在第二阶段中，当预测器<span class="math inline">\(h\)</span>固定时，若<span class="math inline">\(\phi\)</span>满足关于二值0-1损失<span class="math inline">\(\mathcal{l}^{\text{binary}}_{\text{0-1}}\)</span>的<span class="math inline">\(\mathcal{R}\)</span>-一致性界，则代理损失函数<span class="math inline">\(L_{\phi, h}\)</span>获益于关于<span class="math inline">\(L_{\text{abst}, h}\)</span>的<span class="math inline">\(\mathcal{R}\)</span>-一致性界。</p>
<p><strong>定理 4</strong> <strong>第二阶段代理损失的<span class="math inline">\(\mathcal{R}\)</span>-一致性界</strong> 对于固定的预测器<span class="math inline">\(h\)</span>，假设<span class="math inline">\(\phi\)</span>满足关于<span class="math inline">\(\mathcal{l}^{\text{binary}}_{\text{0-1}}\)</span>的<span class="math inline">\(\mathcal{R}\)</span>-一致性界，即存在非递减凹函数<span class="math inline">\(\Gamma\)</span>使得对所有<span class="math inline">\(r\in \mathcal{R}\)</span>，有</p>
<p></p><div class="math display">\[R_{\mathcal{l}_{\text{0-1}}^{\text{binary}}}(r) - R_{\mathcal{l}_{\text{0-1}}^{\text{binary}}}^{*}(\mathcal{R}) + M_{\mathcal{l}_{\text{0-1}}^{\text{binary}}}(\mathcal{R}) \leqslant \Gamma(R_{\phi}(r) - R_{\phi}^{*}(\mathcal{R}) + M_{\phi}(\mathcal{R}))
\]</div><p></p><p>则对于所有<span class="math inline">\(r\in \mathcal{R}\)</span>和任意分布，下列<span class="math inline">\(\mathcal{R}\)</span>-一致性界成立：</p>
<p></p><div class="math display">\[R_{L_{\text{abst}, h}}(r) - R_{L_{\text{abst}, h}}^{*}(\mathcal{R}) + M_{L_{\text{abst}, h}}(\mathcal{R}) \leqslant \Gamma\left(\left(R_{L_{\phi, h}}\left(r\right) - R_{{L}_{\phi, h}}^{*}\left(\mathcal{R}\right) + M_{L_{\phi, h}}\left(\mathcal{R}\right)\right) / c\right)
\]</div><p></p><p>该定理的证明包括了对于固定的预测器<span class="math inline">\(h\)</span>，分析弃权损失和第二阶段代理损失的校准差距。这里的校准差距相较于标准设置下的更复杂，因为它考虑了条件概率、该固定预测器的误差和花费，于是因此需要不同的分析。为了构建第二阶段代理损失的<span class="math inline">\(\mathcal{R}\)</span>-一致性界，我们需要使用该代理损失的校准差距来构建弃权损失的校准差距的上界。然而，直接操作它们会由于其复杂形式而较为困难。不过，我们可以观察到这两种形式共享了与标准分类中校准差距的结构相似性。由上述的观察启发，我们构建了一个合适的条件分布来将这两个校准差距转换为标准形式。我们尝试利用<span class="math inline">\(\phi\)</span>的关于二值0-1损失的<span class="math inline">\(\mathcal{R}\)</span>-一致性界，以用代理函数的校准差距构建目标校准差距的上界。</p>
<p>在<span class="math inline">\(\mathcal{H}\)</span>和<span class="math inline">\(\mathcal{R}\)</span>为可测函数集的特殊情况下，定理 4中的所有最小化能力差距项消失了。因此，我们获得了如下推论。</p>
<p><strong>推论5</strong> 固定预测器<span class="math inline">\(h\)</span>，假设<span class="math inline">\(\phi\)</span>满足关于<span class="math inline">\(\mathcal{l}_{\text{0-1}}^{\text{binary}}\)</span>的额外误差界，即存在非递减凹函数<span class="math inline">\(\Gamma\)</span>使得对所有<span class="math inline">\(r\in \mathcal{R}_{\text{all}}\)</span>，有</p>
<p></p><div class="math display">\[R_{\mathcal{l}_{\text{0-1}}^{\text{binary}}}(r) - R_{\mathcal{l}_{\text{0-1}}^{\text{binary}}}^{*}(\mathcal{R}_{\text{all}}) \leqslant \Gamma(R_{\phi}(r) - R_{\phi}^{*}(\mathcal{R}_{\text{all}}))
\]</div><p></p><p>于是，对所有<span class="math inline">\(r\in \mathcal{R}_{\text{all}}\)</span>和任意分布，下列额外误差界成立：</p>
<p></p><div class="math display">\[R_{L_{\text{abst}, h}}(r) - R_{L_{\text{abst}, h}}^{*}(\mathcal{R}_{\text{all}}) \leqslant \Gamma\left(\left(R_{L_{\phi, h}}\left(r\right) - R_{{L}^{*}_{\phi, h}}\left(\mathcal{R}_{\text{all}}\right)\right) / c\right)
\]</div><p></p><p>我们接下来陈述关于弃权损失函数<span class="math inline">\(L_{\text{abst}}\)</span>的整个两阶段方法的<span class="math inline">\((\mathcal{H}, \mathcal{R})\)</span> - 一致性界。设<span class="math inline">\(\mathcal{l}_{\text{0-1}}\)</span>为多分类0-1损失：<span class="math inline">\(\mathcal{l}_{\text{0-1}}(h, x, y) = \mathbb{I}_{h(x)\neq y}\)</span>。我们接下来考虑是<strong>弃权正规（regular for abstention）</strong> 的假设集合<span class="math inline">\(\mathcal{R}\)</span>，也即使得对任意<span class="math inline">\(x\in \mathcal{X}\)</span>，存在<span class="math inline">\(f, g\in \mathcal{R}\)</span>满足<span class="math inline">\(f(x) &gt; 0\)</span>与<span class="math inline">\(g(x) \leqslant 0\)</span>。如果<span class="math inline">\(\mathcal{R}\)</span>是弃权正规的，则对于任意<span class="math inline">\(x\)</span>，既存在一个可以接受的选择也存在一个可以拒绝的选择。</p>
<p><strong>定理6</strong> <strong>两阶段方法的<span class="math inline">\((\mathcal{H}, \mathcal{R})\)</span> - 一致性界</strong> 假设<span class="math inline">\(\mathcal{R}\)</span>是正规的。假设<span class="math inline">\(\mathcal{l}\)</span>满足关于<span class="math inline">\(\mathcal{l}_{\text{0-1}}\)</span>的<span class="math inline">\(\mathcal{H}\)</span> - 一致性界，<span class="math inline">\(\phi\)</span>满足关于<span class="math inline">\(\mathcal{l}_{\text{0-1}}^{\text{binary}}\)</span>的<span class="math inline">\(\mathcal{R}\)</span> - 一致性界，即存在非递减凹函数<span class="math inline">\(\Gamma_1\)</span>和<span class="math inline">\(\Gamma_2\)</span>使得对于所有的<span class="math inline">\(h\in \mathcal{H}\)</span>与<span class="math inline">\(r\in \mathcal{R}\)</span>，有</p>
<p></p><div class="math display">\[R_{\mathcal{l}_{\text{0-1}}}(h) - R_{\mathcal{l}_{\text{0-1}}}^{*}(\mathcal{H}) + M_{\mathcal{l}_{\text{0-1}}}(\mathcal{H}) \leqslant \Gamma_1(R_{\mathcal{l}}(h) - R_{\mathcal{l}}^{*}(\mathcal{H}) + M_{\mathcal{l}}(\mathcal{H}))\\
R_{\mathcal{l}_{\text{0-1}}^{\text{binary}}}(r) - R_{\mathcal{l}_{\text{0-1}}^{\text{binary}}}^{*}(\mathcal{R}) + M_{\mathcal{l}_{\text{0-1}}^{\text{binary}}}(\mathcal{R}) \leqslant \Gamma_2(R_{\phi}(r) - R_{\phi}^{*}(\mathcal{R}) + M_{\phi}(\mathcal{R}))
\]</div><p></p><p>于是，下列的<span class="math inline">\((\mathcal{H}, \mathcal{R})\)</span> - 一致性界对所有<span class="math inline">\(h\in \mathcal{H}, r\in \mathcal{R}\)</span>和任意分布成立：</p>
<p></p><div class="math display">\[\begin{aligned}
    R_{L_{\text{abst}}}(h, r) - R_{L_{\text{abst}}}^{*}(\mathcal{H}, \mathcal{R}) + M_{L_{\text{abst}}}(\mathcal{H}, \mathcal{R})\leqslant &amp;\Gamma_1\left(R_{l}\left(h\right) - R_{\mathcal{l}}^{*}\left(\mathcal{H}\right) + M_{\mathcal{l}}(\mathcal{H})\right) \\&amp;+ (1 + c)\Gamma_2\left(\left(R_{L_{\phi, h}}\left(r\right) - R_{{L}^{*}_{\phi, h}}\left(\mathcal{R}\right) + M_{\mathcal{l}_{\phi, h}}(\mathcal{R})\right) / c\right)
\end{aligned}
\]</div><p></p><p>其中常数因子<span class="math inline">\((1 + c)\)</span>与<span class="math inline">\(\frac{1}{c}\)</span>当<span class="math inline">\(\Gamma_2\)</span>是线性的时可以被移除。</p>
<p>和前面类似，当<span class="math inline">\(\mathcal{H}\)</span>和<span class="math inline">\(\mathcal{R}\)</span>为可测函数族时，下列关于额外误差界的推论成立。</p>
<p><strong>推论 7</strong> 假设<span class="math inline">\(\mathcal{l}\)</span>满足关于<span class="math inline">\(\mathcal{l}_{\text{0-1}}\)</span>的额外误差界，<span class="math inline">\(\phi\)</span>满足关于<span class="math inline">\(\mathcal{l}_{\text{0-1}}^{\text{binary}}\)</span>的额外误差界，即存在非递减凹函数<span class="math inline">\(\Gamma_1\)</span>和<span class="math inline">\(\Gamma_2\)</span>使得对于所有的<span class="math inline">\(h\in \mathcal{H}_{\text{all}}\)</span>和<span class="math inline">\(r\in \mathcal{R}_{\text{all}}\)</span>，有</p>
<p></p><div class="math display">\[R_{\mathcal{l}_{\text{0-1}}}(h) - R_{\mathcal{l}_{\text{0-1}}}^{*}(\mathcal{H}_{\text{all}}) \leqslant \Gamma_1(R_{\mathcal{l}}(h) - R_{\mathcal{l}}^{*}(\mathcal{H}_{\text{all}}))\\
R_{\mathcal{l}_{\text{0-1}}^{\text{binary}}}(r) - R_{\mathcal{l}_{\text{0-1}}^{\text{binary}}}^{*}(\mathcal{R}_{\text{all}}) \leqslant \Gamma_2(R_{\phi}(r) - R_{\phi}^{*}(\mathcal{R}_{\text{all}}))
\]</div><p></p><p>于是，下列额外误差界对于所有<span class="math inline">\(h\in \mathcal{H}_{\text{all}}\)</span>与<span class="math inline">\(r\in \mathcal{R}_{\text{all}}\)</span>和任意分布成立：</p>
<p></p><div class="math display">\[R_{L_{\text{abst}}}(h, r) - R_{L_{\text{abst}}}^{*}(\mathcal{H}_{\text{all}}, \mathcal{R}_{\text{all}})\leqslant \Gamma_1\left(R_{l}\left(h\right) - R_{\mathcal{l}}^{*}\left(\mathcal{H}_{\text{all}}\right)\right) \\+ (1 + c)\Gamma_2\left(\left(R_{L_{\phi, h}}\left(r\right) - R_{{L}^{*}_{\phi, h}}\left(\mathcal{R}_{\text{all}}\right)\right) / c\right)
\]</div><p></p><p>其中常数因子<span class="math inline">\((1 + c)\)</span>与<span class="math inline">\(\frac{1}{c}\)</span>当<span class="math inline">\(\Gamma_2\)</span>是线性的时可以被移除。</p>
<p>这些结果为两阶段设置下的代理损失提供了强理论保障。此外，<span class="math inline">\(\mathcal{l}\)</span>在单阶段设置下的选择受特定条件的约束，而多分类代理损失<span class="math inline">\(\mathcal{l}\)</span>可以被更加灵活地选择。特别地，它可以被选择为Logistic损失（或带Softmax的交叉熵损失），这不但更易于优化，而且能够更好地适配于复杂的神经网络。在第二阶段，公式比较直接，函数<span class="math inline">\(\phi\)</span>的选择是灵活的，这将导出关于拒绝函数<span class="math inline">\(r\)</span>的简单的光滑凸优化问题。此外，第二阶段将过程进行了简化：<span class="math inline">\(h\)</span>做为常量，只有拒绝器被优化。该方法能够增强优化效率。</p>
<h1 id="参考">参考</h1>
<ul>
<li>[1] Mao A, Mohri M, Zhong Y. Predictor-rejector multi-class abstention: Theoretical analysis and algorithms[C]//International Conference on Algorithmic Learning Theory. PMLR, 2024: 822-867.</li>
<li>[2] Cortes C, DeSalvo G, Mohri M. Boosting with abstention[J]. Advances in Neural Information Processing Systems, 2016, 29.</li>
<li>[3] Ni C, Charoenphakdee N, Honda J, et al. On the calibration of multiclass classification with rejection[J]. Advances in Neural Information Processing Systems, 2019, 32.</li>
<li>[4] <a href="https://hermite.jp/slides/202007_KyotoU.pdf" target="_blank" rel="noopener nofollow">Han Bao: Learning Theory Bridges Loss Functions</a></li>
<li>[5] Crammer K, Singer Y. On the algorithmic implementation of multiclass kernel-based vector machines[J]. Journal of machine learning research, 2001, 2(Dec): 265-292.</li>
<li>[6] Awasthi P, Mao A, Mohri M, et al. Multi-Class $ H $-Consistency Bounds[J]. Advances in neural information processing systems, 2022, 35: 782-795.</li>
</ul>

</div>
<div id="MySignature" role="contentinfo">
    数学是符号的艺术，音乐是上界的语言。
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.13872455445601853" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-02-22 17:04">2025-02-22 10:44</span>&nbsp;
<a href="https://www.cnblogs.com/orion-orion">orion-orion</a>&nbsp;
阅读(<span id="post_view_count">52</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18730657" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18730657);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18730657', targetLink: 'https://www.cnblogs.com/orion-orion/p/18730657', title: '学习理论：预测器-拒绝器多分类弃权学习' })">举报</a>
</div>
        