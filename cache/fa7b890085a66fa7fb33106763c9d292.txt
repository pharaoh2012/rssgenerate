
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/LXP-Never/p/19030302" title="发布于 2025-08-10 14:48">
    <span role="heading" aria-level="2">语音活动检测（VAD）</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body blogpost-body-html">
<h1>概述</h1>
<p>VAD 的核心任务是<strong>鉴别音频信号中的语音出现（speech presence）和语音消失（speech absence）</strong>，也就是区分语音和非语音（或静音）部分。想象一下，在一个有背景噪音的环境中，VAD 就像一个智能“守门人”，它能准确识别什么时候有人在说话，什么时候是纯粹的环境噪音或沉默。</p>
<p>这项技术在许多场景中都扮演着关键角色：</p>
<ul>
<li><strong>语音编码和降噪</strong>：VAD 可以帮助系统只处理含有语音的部分，从而节省带宽、提高压缩效率，并有效降低背景噪音，提升语音通话质量。</li>
<li><strong>自动语音识别 (ASR)</strong>：在将语音转换为文本之前，VAD 可以移除音频中的静音部分，减少不必要的计算，提高识别的准确性和效率。</li>
<li><strong>智能语音助手和机器人</strong>：在这些应用中，VAD 能够更准确地识别用户的语音指令，避免被环境噪音触发。</li>
<li><strong>数据清洗和准备</strong>：对于训练神经网络等机器学习任务，VAD 可用于从大量的音频记录中提取出纯净的语音片段，去除静音，从而提高模型训练的质量。</li>
</ul>
<p>作者述：由于时间和精力有限，如果大家有好用的开源VAD算法可以推荐给我，我帮你评测</p>
<h1>工作原理</h1>
<p>一个 VAD 系统通常包含两个主要部分：<strong>特征提取</strong>和<strong>语音/非语音判决</strong>。</p>
<ol>
<li>
<p><strong>特征提取</strong> 好的特征是 VAD 分类问题的关键，它们应该具备<strong>区分能力</strong>（让语音和噪声的分布分离度高）和<strong>噪声鲁棒性</strong>（在背景噪声下也能保持特征的区分能力）。常用的特征类型包括：</p>
<ul>
<li><strong>基于能量的特征</strong>：假设语音能量大于背景噪声能量。然而，当噪声较大时，这种方法区分能力会下降。</li>
<li><strong>频域特征</strong>：通过短时傅里叶变换 (STFT) 将时域信号转换为频域信号。即使在低信噪比下，某些频带的长时包络也能区分语音和噪声。WebRTC VAD 就利用了这一思想，将频谱分成六个子带进行能量分析。</li>
<li><strong>倒谱特征</strong>：例如，能量倒谱峰值可以确定语音信号的基频，MFCC (梅尔频率倒谱系数) 也可以作为特征。</li>
<li><strong>谐波特征</strong>：语音的一个明显特征是包含基频及其多个谐波频率，即使在强噪声环境下也存在。</li>
<li><strong>长时特征</strong>：语音是非稳态信号，其统计特性随时间变化；而大多数日常噪声是稳态或变化缓慢的。</li>
</ul>
</li>
<li>
<p><strong>判决准则</strong> 在提取特征后，需要一个判决机制来决定当前帧是语音还是非语音。主要有三类方法：</p>
<ul>
<li><strong>基于门限的方法</strong>：根据预先设定的门限值来判断。对于变化的噪声场景，需要使用自适应门限。</li>
<li><strong>统计模型方法</strong>：这类方法源于似然比检验 (LRT)，假设语音和背景噪声服从独立的高斯分布或其他分布（如拉普拉斯、伽马分布），通过计算信号是语音或噪声的概率来做出判决。WebRTC VAD 就采用了这一思想，使用高斯模型对噪声和语音的概率进行计算，并通过似然比检验进行判决。</li>
<li><strong>机器学习方法</strong>：通过训练数据学习语音模型。这类方法的难点在于强噪声场景下训练数据集的准确标注。然而，近年来深度学习的发展，极大地提升了 VAD 的性能和适应性。</li>
</ul>
</li>
</ol>
<h1>常见的 VAD 模型和工具包</h1>
<p>目前有许多优秀的 VAD 模型和工具包可供选择，它们各有特点和应用场景：</p>
<h2>Silero VAD</h2>
<p><strong>　　<a href="https://github.com/snakers4/silero-vad" target="_blank" rel="noopener nofollow">Silero VAD</a></strong>&nbsp;是一种<strong>基于深度学习技术</strong>的轻量级语音活动检测模型，被描述为“企业级预训练模型”。</p>
<p>我的看法：还不错，但是对清辅音的识别稍微欠缺，日语轻声效果有点差</p>
<p><strong>特点</strong></p>
<ul>
<li><strong>轻量级</strong>：模型文件大小仅约 2.2MB。</li>
<li><strong><strong>速度快</strong>：在单 CPU 线程上处理一个音频块（30+ms）所需时间不到 1ms。</strong></li>
<li><strong>通用性强</strong>：在包含 6000 多种语言的庞大语料库上进行训练，对不同领域、不同背景噪声和质量的音频表现良好。</li>
<li>silero支持非常丰富的参数调节，</li>
</ul>
<p>安装：&nbsp;<span class="cnblogs_code">pip install silero</span>&nbsp;</p>
<div class="cnblogs_code">
<pre>speech_timestamps =<span style="color: rgba(0, 0, 0, 1)"> get_speech_timestamps(wav, model, 
                                          sampling_rate</span>=<span style="color: rgba(0, 0, 0, 1)">sr,
                                          threshold</span>=0.5<span style="color: rgba(0, 0, 0, 1)">,
                                          min_speech_duration_ms</span>=10,   <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 语音块的最小持续时间 ms, 语音超过这个最小时长才会被判为是语音</span>
                                          min_silence_duration_ms=140,  <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 语音块之间的最小静音时间 ms, 静音超过这个最小时长才会被判为静音</span>
                                          window_size_samples=512, <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 窗长 512\1024\1536</span>
                                          speech_pad_ms=0,    <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 往VAD时间戳左右padding (ms)</span>
                                          )</pre>
</div>
<p><img src="https://img2024.cnblogs.com/blog/1433301/202508/1433301-20250809210434970-242994218.png" alt="image" width="896" height="255" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<p><img src="https://img2024.cnblogs.com/blog/1433301/202508/1433301-20250809210214390-233940072.png" alt="image" width="904" height="243" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<h2>FSMN-VAD (FunASR)</h2>
<p><strong>　　<a href="https://modelscope.cn/models/iic/speech_fsmn_vad_zh-cn-16k-common-pytorch/summary" target="_blank" rel="noopener nofollow">FSMN-VAD</a></strong><a href="https://modelscope.cn/models/iic/speech_fsmn_vad_zh-cn-16k-common-pytorch/summary" target="_blank" rel="noopener nofollow">&nbsp;</a>是达摩院语音团队提出的高效语音端点检测模型，用于检测输入音频中有效语音的起止时间点信息，并将检测出来的有效音频片段输入识别引擎进行识别，减少无效语音带来的识别错误。</p>
<p>我的看法：<strong>检测是否有语音很准，但是由于是识别类的VAD，检测语音的时间戳会相对相对宽一点，</strong>FunASR唱歌也会检测为语音，但是silero不会</p>
<p><strong>特点</strong>：</p>
<ul>
<li><strong>参数量</strong>：0.4M</li>
<li><strong>训练数据</strong>：5000小时，普通话和英语</li>
</ul>
<p><img src="https://img2024.cnblogs.com/blog/1433301/202508/1433301-20250809212952206-1837160648.png" alt="局部截取_20250809_212944" width="495" height="359" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<p>安装：&nbsp;<span class="cnblogs_code">pip3 install -U funasr</span>&nbsp;</p>
<p>funasr也有很多配置参数，调参与不调参差别会非常大，下面标红的参数是重点调参对象，参数文件在：/home/xxx/.cache/modelscope/hub/models/iic/speech_fsmn_vad_zh-cn-16k-common-pytorch</p>
<ul>
<li data-start="71" data-end="83">sample_rate：采样率，输入音频的采样率（Hz），必须和模型训练时一致，常用 16000。</li>
<li data-start="71" data-end="83">frame_in_ms：帧移（ms），每一帧的间隔时间，比如 10ms 表示每 10ms 产生一帧特征。</li>
<li data-start="71" data-end="83">frame_in_ms：帧长（ms），每一帧特征的时间长度，比如 25ms 表示一帧包含 25ms 的音频数据。</li>
<li data-start="71" data-end="83">window_size_ms：分析窗口长度（ms），推理时，VAD 会基于一个滑动窗口来做判断，这个是窗口的时长。</li>
<li data-start="71" data-end="83"><strong>起点/终点检测</strong></li>
<li>detect_mode： 检测模式 一般 0 或 1，不同模式对应不同的端点检测策略，1 通常更严格，0 更宽松。</li>
<li>do_start_point_detection： 是否启用起点检测 True 表示检测语音开始位置。</li>
<li>do_end_point_detection： 是否启用终点检测 True 表示检测语音结束位置。</li>
<li><span style="color: rgba(255, 0, 0, 1)">max_start_silence_time</span>： 起点前允许的最大静音时间（ms） 如果静音超过这个时间，还没检测到语音，起点会被重置。0 表示不限制。我设置为0</li>
<li><span style="color: rgba(255, 0, 0, 1)">max_end_silence_time</span>： 终点后允许的最大静音时间（ms） 超过该时间后会强制切断 segment，0 表示不限制。我设置为0</li>
<li><span style="color: rgba(255, 0, 0, 1)">lookback_time_start_point</span>： 起点回溯时间（ms） 检测到语音后，向前回溯一段时间作为起点（防止起点切掉前几个字）。我设置为0</li>
<li><span style="color: rgba(255, 0, 0, 1)">lookahead_time_end_point</span>： 终点提前时间（ms） 检测到静音后，可提前终止 segment（减少延迟）。我设置为0</li>
<li><span style="color: rgba(255, 0, 0, 1)">sil_to_speech_time_thres</span>： 静音→语音判定时间（ms） 从静音状态切换为语音状态所需的连续触发时长。我设置为150</li>
<li><span style="color: rgba(255, 0, 0, 1)">speech_to_sil_time_thres</span>： 语音→静音判定时间（ms） 从语音状态切换为静音状态所需的连续静音时长。我设置为150</li>
<li>do_extend： 是否延长语音段 1 表示语音段会延长，避免截断尾音。</li>
<li><strong>分段控制</strong></li>
<li>max_single_segment_time 单段最大时长（ms） 一段语音超过该时长会被强制切分。</li>
<li><strong>噪声与能量门限&nbsp;</strong></li>
<li>snr_mode 信噪比模式 0 表示关闭基于 SNR 的判定，1 表示启用。</li>
<li>snr_thres 信噪比阈值（dB） SNR 判定的门限，低于该值可能判为噪声。</li>
<li>noise_frame_num_used_for_snr SNR 估计用的噪声帧数 用多少帧静音来计算噪声均值。</li>
<li>decibel_thres 分贝阈值 低于该分贝的帧判为静音。</li>
<li><span style="color: rgba(255, 0, 0, 1)">speech_noise_thres </span>语音与噪声能量比阈值 控制噪声过滤的灵敏度。我设置为0.8</li>
<li>speech_noise_thresh_low / speech_noise_thresh_high 双阈值 用于判定语音与噪声的上下限阈值，增加稳健性。</li>
<li>speech_2_noise_ratio 语音/噪声比例 调节判定灵敏度，越大越严格。</li>
<li>fe_prior_thres 特征能量先验阈值 小于该值的帧直接视为静音（特征域判定）。</li>
<li><strong>模型类别映射</strong></li>
<li>silence_pdf_num 静音类别数 模型输出中静音类别的数量。</li>
<li>sil_pdf_ids 静音类别 ID 列表 用于指定哪些类别属于静音，例如 [0] 表示类别 0 是静音。</li>
<li><strong>输出选项</strong></li>
<li>output_frame_probs 是否输出帧级概率 True 会输出每一帧是语音的概率，用于调试或后处理。</li>
</ul>
<p><img src="https://img2024.cnblogs.com/blog/1433301/202508/1433301-20250810004424601-707634826.png" alt="image" width="729" height="193" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<p><img src="https://img2024.cnblogs.com/blog/1433301/202508/1433301-20250809224640998-26179322.png" alt="image" width="737" height="198" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<h2>WebRTC VAD</h2>
<p><strong>WebRTC VAD</strong> 是谷歌为 WebRTC 项目开发的 VAD 模块，<code>WebRTC</code>&nbsp;是 Google 开源的实时通信框架，特别适用于实时性要求较高的语音通信场景。</p>
<p>WebRTC 的 VAD 实现属于 特征驱动 + 统计模型&nbsp;的方案：</p>
<ol>
<li>将音频分帧（如 10ms / 20ms / 30ms）</li>
<li>提取多个语音特征(短时能量、过零率、谱特征)，将频谱分成 6 个子带（80Hz~4KHz）来计算能量特征。<ol>
<li>短时能量：语音通常能量较大，静音或噪声能量较小。</li>
<li>过零率：语音（尤其是清音辅音）ZCR 较高，元音较低；纯噪声的 ZCR 往往更高。</li>
<li>谱质心：频谱的“重心位置”，反映频率分布的集中程度。清辅音（高频）谱质心较高，元音（低频）谱质心较低。</li>
<li>频带能量分布：WebRTC 会将语音分成多个频段，计算各段能量比例，用来区分人声与背景噪声的频谱特征。</li>
<li>谱斜率 / 倾斜度（Spectral Slope / Tilt）：谱斜率 / 倾斜度（Spectral Slope / Tilt），元音低频强，高频弱；噪声则频谱较平坦。</li>
<li>平谱熵：平谱熵，语音的频谱分布较有结构，熵较低；噪声通常更随机，熵更高。</li>
</ol></li>
<li>输入到已经训练好的高斯混合模型（GMM）中进行分类</li>
<li>输出是否包含语音的布尔值（True / False）</li>
</ol>
<p>安装：&nbsp;<span class="cnblogs_code">pip install webrtcvad</span>&nbsp;</p>
<p>该库py-webrtcvad 是一个 Python 封装库，支持四种激进模式 (0-3)，检测敏感度与数值大小正相关</p>
<p>设定的 VAD 模式（Vad(0/1/2/3)）实际上是改变了阈值：</p>
<p>0：宽松（漏检少，但容易误判噪声为语音）<br>3：严格（容忍度低，更容易漏掉低能量语音）</p>
<p><img src="https://img2024.cnblogs.com/blog/1433301/202508/1433301-20250810004449192-1365640496.png" alt="image" width="778" height="206" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<p><img src="https://img2024.cnblogs.com/blog/1433301/202508/1433301-20250810004323892-1454759689.png" alt="image" width="759" height="204" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<p>&nbsp;结论：稍微宽了一点</p>
<h2>TEN VAD</h2>
<p data-start="0" data-end="74"><span class="relative -mx-px my-[-0.2rem] rounded px-px py-[0.2rem] transition-colors duration-100 ease-in-out">TEN VAD 是 TEN Framework 生态系统中的一个高性能、低延迟、轻量级的实时语音活动检测（VAD）系统，专为企业级对话 AI 应用设计，旨在提供精准的帧级语音活动检测。</span></p>
<ul>
<li data-start="171" data-end="257"><strong data-start="171" data-end="178"><strong data-start="260" data-end="267">低延迟</strong>：<span class="relative -mx-px my-[-0.2rem] rounded px-px py-[0.2rem] transition-colors duration-100 ease-in-out">TEN VAD 能迅速检测语音与非语音的切换，减少对话系统的响应延迟。</span></strong></li>
<li class="_mce_tagged_br" data-start="310" data-end="357"><strong data-start="310" data-end="317">轻量级</strong>：<span class="relative -mx-px my-[-0.2rem] rounded px-px py-[0.2rem] transition-colors duration-100 ease-in-out">TEN VAD 的计算复杂度和内存占用低于 Silero VAD，适合嵌入式和边缘设备使用。</span></li>
<li class="_mce_tagged_br" data-start="360" data-end="448"><strong data-start="360" data-end="369">跨平台支持</strong>：<span class="relative -mx-px my-[-0.2rem] rounded px-px py-[0.2rem] transition-colors duration-100 ease-in-out">支持 Windows、macOS 和 Linux 平台，且提供 ONNX 格式模型，方便与 Python 等语言集成。</span></li>







</ul>
<p data-start="0" data-end="74">GitHub 仓库：<span class="relative -mx-px my-[-0.2rem] rounded px-px py-[0.2rem] transition-colors duration-100 ease-in-out"><a href="https://github.com/TEN-framework/ten-vad" rel="noopener nofollow" target="_new" data-start="0" data-end="84" data-is-last-node="" data-is-only-node="">https://github.com/TEN-framework/ten-vad</a></span></p>
<div style="text-align: center"><img src="https://img2024.cnblogs.com/blog/1433301/202508/1433301-20250810041216399-390920116.png" alt="image" width="714" height="233" loading="lazy"></div>
<div><img src="https://img2024.cnblogs.com/blog/1433301/202508/1433301-20250810041223862-1962940986.png" alt="image" width="723" height="236" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></div>
<p>&nbsp;这效果没有他吹的这么牛啊</p>
<h2>whisper</h2>
<p>虽然&nbsp;OpenAI Whisper&nbsp;本身不是专门的 VAD 模型，但我们可以借助它的时间戳功能，实现一个带 VAD 效果的长音频切分。下面我来给你详细介绍原理、方法和示例代码。</p>
<ul>
<li>Whisper 转录时会返回&nbsp;<code>segments</code>（每句话的开始与结束时间）。</li>
<li>我们只保留包含语音的&nbsp;<code>segment</code>，就等于做了“语音段提取”，静音部分自然被丢弃。</li>
<li>如果需要更细粒度，可使用&nbsp;<code>word_timestamps=True</code>，结合时间差判断语音间的空隙，再分割音频。</li>




















</ul>
<p>优点</p>
<ul>
<li>不需要额外安装 VAD 模型</li>
<li>直接结合语音识别，VAD 结果与识别文本对齐</li>
<li>方便长音频切分，并保留对应的字幕</li>




















</ul>
<p>缺点</p>
<ul>
<li>推理成本大，只做 VAD 性价比低</li>
<li>在背景噪声大、非语音片段多的情况下不如专用 VAD 鲁棒</li>




















</ul>
<div class="cnblogs_code"><img src="https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif" id="code_img_closed_f9beaef4-5ca4-4dab-90d1-5e4cbf738a32" class="code_img_closed"><img src="https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif" id="code_img_opened_f9beaef4-5ca4-4dab-90d1-5e4cbf738a32" class="code_img_opened" style="display: none">
<div id="cnblogs_code_open_f9beaef4-5ca4-4dab-90d1-5e4cbf738a32" class="cnblogs_code_hide">
<pre><span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> whisper
</span><span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> soundfile as sf

</span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 1. 加载模型（可根据显存选择 tiny/base/small/medium/large）</span>
model = whisper.load_model(<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">small</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">)

</span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 2. 读取音频（保持16k采样率）</span>
wav_path = <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">audio.wav</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">
audio, sr </span>=<span style="color: rgba(0, 0, 0, 1)"> sf.read(wav_path)
</span><span style="color: rgba(0, 0, 255, 1)">assert</span> sr == 16000, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">Whisper要求16k采样率</span><span style="color: rgba(128, 0, 0, 1)">"</span>

<span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 3. 转录，获取每个 segment 的时间戳</span>
result =<span style="color: rgba(0, 0, 0, 1)"> model.transcribe(wav_path)

</span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 4. 提取语音段落时间，作为 VAD 结果</span>
vad_segments =<span style="color: rgba(0, 0, 0, 1)"> []
</span><span style="color: rgba(0, 0, 255, 1)">for</span> seg <span style="color: rgba(0, 0, 255, 1)">in</span> result[<span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">segments</span><span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(0, 0, 0, 1)">]:
    vad_segments.append({
        </span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">start</span><span style="color: rgba(128, 0, 0, 1)">"</span>: seg[<span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">start</span><span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(0, 0, 0, 1)">],
        </span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">end</span><span style="color: rgba(128, 0, 0, 1)">"</span>: seg[<span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">end</span><span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(0, 0, 0, 1)">],
        </span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">text</span><span style="color: rgba(128, 0, 0, 1)">"</span>: seg[<span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">text</span><span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(0, 0, 0, 1)">]
    })

</span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 输出检测到的语音片段</span>
<span style="color: rgba(0, 0, 255, 1)">for</span> s <span style="color: rgba(0, 0, 255, 1)">in</span><span style="color: rgba(0, 0, 0, 1)"> vad_segments:
    </span><span style="color: rgba(0, 0, 255, 1)">print</span>(f<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">{s['start']:.2f}s - {s['end']:.2f}s : {s['text']}</span><span style="color: rgba(128, 0, 0, 1)">"</span>)</pre>
</div>
<span class="cnblogs_code_collapse">View Code</span></div>
<div>
<p style="text-align: center"><img src="https://img2024.cnblogs.com/blog/1433301/202508/1433301-20250810011701898-735090135.png" alt="image" width="671" height="219" loading="lazy"></p>
<p><img src="https://img2024.cnblogs.com/blog/1433301/202508/1433301-20250810011707788-410992009.png" alt="image" width="658" height="215" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
</div>
<p>基于识别模型做的VAD都宽</p>
<h2>energy-vad</h2>
<p><code>energy-vad</code>&nbsp;是一个非常简单的&nbsp;基于能量（energy-based）的语音活动检测器（VAD），由 Rhasspy 项目发布（Rhasspy 是一个离线语音助手框架）。</p>
<p><code>energy-vad</code>&nbsp;的核心思想很简单：</p>
<ol>
<li>分帧：将音频分割成小片段（例如每 10ms 一帧）</li>
<li>对每帧计算&nbsp;RMS（均方根能量）：E=1N∑i=1Nxi2E=N1​i=1∑N​xi2​​</li>
<li class="_mce_tagged_br">阈值判断：如果 RMS 能量大于设定阈值 → 判定为语音帧；否则 → 判定为静音帧</li>
<li class="_mce_tagged_br">起止判定：当连续若干帧大于阈值 → 检测到语音开始；当连续若干帧小于阈值 → 检测到语音结束</li>
</ol>
<p>通过计算短时音频帧的能量，判断当前帧是否可能包含语音，从而确定语音开始和结束的时间点。这种方法在干净音频条件下非常高效，但对背景噪声较敏感。</p>
<p>安装：&nbsp;<span class="cnblogs_code">pip install energy-vad</span>&nbsp;</p>
<div style="text-align: center"><img src="https://img2024.cnblogs.com/blog/1433301/202508/1433301-20250810014823247-1527624284.png" alt="image" width="678" height="221" loading="lazy"></div>
<div><img src="https://img2024.cnblogs.com/blog/1433301/202508/1433301-20250810014828727-628314969.png" alt="image" width="680" height="222" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></div>
<div data-page-id="BBDidKzcMohA5yxmq12cThgbnGc" data-lark-html-role="root" data-docx-has-block-data="false">
<div class="ace-line ace-line old-record-id-RHntd1OKEomWiHxVtMRcJBranNV">误差比较大，而且连着的语音没必要分割，但是该方法还是分割了</div>
</div>
<h2>基于谱变化率的vad</h2>
<p><span style="font-size: 14px">该方法提供了一个</span><strong style="font-size: 14px">基于谱特征变化率（Spectral Flux）</strong><span style="font-size: 14px">的简单语音活动检测（Voice Activity Detection）算法实现，</span><span style="font-size: 14px">与传统基于能量的 VAD 不同，该项目利用音频频谱的动态变化来区分语音与静音/背景噪声。</span></p>
<p>算法原理</p>
<ol>
<li>短时傅里叶变换（STFT）</li>
</ol><ol>
<li>计算谱通量（Spectral Flux）
<ul>
<li>对相邻两帧的幅度谱求差，并取正变化部分的平方和，得到谱通量值。</li>
<li>谱通量衡量了频谱随时间变化的程度，语音帧通常变化更大，静音/稳态噪声音频谱变化较小。</li>
</ul>
</li>
<li class="_mce_tagged_br">阈值检测
<ul>
<li>将谱通量与设定阈值&nbsp;<code>threshold</code>&nbsp;对比，大于阈值的帧标记为含语音。</li>
<li>通过连续帧合并、忽略过短片段等规则，输出语音区间。</li>
</ul>
</li>
</ol>
<p>安装：&nbsp;<span class="cnblogs_code">pip install vad</span>&nbsp;</p>
<div style="text-align: center"><img src="https://img2024.cnblogs.com/blog/1433301/202508/1433301-20250810020426113-1279110899.png" alt="image" width="748" height="244" loading="lazy"></div>
<div><img src="https://img2024.cnblogs.com/blog/1433301/202508/1433301-20250810020430484-496872928.png" alt="image" width="753" height="246" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></div>
<p>&nbsp;</p>
<h2>&nbsp;基于统计模型的VAD</h2>
<p data-start="59" data-end="133">这个项目实现的是一种<strong data-start="69" data-end="93">基于统计模型的语音活动检测（VAD）算法</strong>，核心参考了 Sohn 等人在 1999 年发表的经典方法。它的主要流程是：</p>
<ol>
<li data-start="137" data-end="177"><strong>特征提取</strong>：将音频信号分帧，并在频域上计算功率谱，用于后续的统计推断。</li>
<li class="_mce_tagged_br" data-start="181" data-end="267"><strong>似然比检验</strong>（Likelihood Ratio Test, LRT）：假设每一帧可能是“语音”或“噪声”，分别建立统计模型，通过似然比来判定该帧更可能属于哪一类。</li>
<li class="_mce_tagged_br" data-start="271" data-end="335"><strong>参数估计</strong>：使用决策导向（Decision-Directed）方法平滑地更新噪声功率谱估计，这可以减少突变和检测抖动。</li>
<li class="_mce_tagged_br" data-start="339" data-end="413"><strong>Hang-over Scheme</strong>：在决策时引入“挂延机制”，即当检测到语音时会延迟一段时间才切换到静音状态，从而避免语音末尾被过早截断。</li>
</ol>
<p>参考：https://github.com/eesungkim/Voice_Activity_Detector</p>
<div style="text-align: center"><img src="https://img2024.cnblogs.com/blog/1433301/202508/1433301-20250810022253885-694650268.png" alt="image" width="661" height="216" loading="lazy"></div>
<div><img src="https://img2024.cnblogs.com/blog/1433301/202508/1433301-20250810022258807-1415451487.png" alt="image" width="655" height="214" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></div>
<p>语间细节把握的很好，但是有时候会吞掉一个字</p>
<h2>rVADfast</h2>
<p>论文：Tan Z H, Dehak N. rVAD: An unsupervised segment-based robust voice activity detection method[J]. Computer speech &amp; language, 2020, 59: 1-21.</p>
<p>链接：https://github.com/zhenghuatan/rVADfast</p>
<p data-start="339" data-end="355"><strong data-start="339" data-end="353">核心思想和方法流程：</strong></p>
<p data-start="359" data-end="371"><strong>两阶段降噪</strong>：</p>
<ol>
<li data-start="377" data-end="449">第一阶段 使用后验信噪比（a posteriori SNR）加权能量差法检测高能量段，并在无音高检测时将其视为高能噪声段，直接置零。</li>
<li class="_mce_tagged_br" data-start="455" data-end="534">第二阶段 应用语音增强技术（例如 Wiener 滤波等）消除更稳定的噪声<span data-state="closed">。</span></li>
</ol>
<p data-start="539" data-end="570"><strong>Segment-based 和 Pitch 引导</strong>：</p>
<ol>
<li class="_mce_tagged_br" data-start="576" data-end="650">rVAD 原方法通过检测 pitch（音高）来形成语音片段，并对其进行扩展，包含更多语音／非语音内容，最后基于后验 SNR 能量差进行 VAD。</li>
<li class="_mce_tagged_br" data-start="656" data-end="786">而 rVAD-fast 用 谱平坦性（spectral flatness） 来替代耗时的 pitch 提取，显著提升速度（大约快 10 倍），性能轻微下降但仍具实用性<span data-state="closed"><span data-state="closed"><span data-state="closed">。</span></span></span></li>
</ol>
<p data-start="791" data-end="803">效果与优势：</p>
<ol data-start="356" data-end="1097">
<li data-start="807" data-end="992">
<p data-start="809" data-end="992">方法适应不同噪声环境（如 babble、market、车内等），在干净或污染条件下均表现出较好的性能；在 Fearless Steps Speech Activity Detection Challenge 中，rVAD 方法在所有参赛系统中排名第 4（无监督与有监督），凸显其鲁棒性<span data-state="closed"><span data-state="closed">。</span></span></p>
</li>
<li data-start="996" data-end="1097">
<p data-start="998" data-end="1097">该模型在多任务（如语音识别、说话人识别、性别/年龄识别、自监督学习等）中作为预处理器得到广泛应用，在多篇文献中引用和使用</p>
</li>
</ol>
<p>&nbsp;安装：&nbsp;<span class="cnblogs_code">pip install rVADfast</span>&nbsp;</p>
<div class="cnblogs_code">
<pre><span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> audiofile
</span><span style="color: rgba(0, 0, 255, 1)">from</span> rVADfast <span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> rVADfast

vad </span>=<span style="color: rgba(0, 0, 0, 1)"> rVADfast()
waveform, sampling_rate </span>= audiofile.read(<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">your_audio.wav</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">)
vad_labels, vad_timestamps </span>= vad(waveform, sampling_rate)</pre>
</div>
<p data-start="23" data-end="89"><strong data-start="23" data-end="39"><code data-start="25" data-end="37">vad_la</code></strong><code data-start="25" data-end="37">bels：</code>是一个逐帧（frame-level）的 语音活动标签数组，通常是 0 或 1：</p>
<p data-start="201" data-end="272"><code data-start="203" data-end="219">vad_timestamps：</code>是一个语音段的时间<strong data-start="229" data-end="242">戳列表</strong>，表示所有检测到的语音区间的起止时间（单位：秒）</p>
<div style="text-align: center"><img src="https://img2024.cnblogs.com/blog/1433301/202508/1433301-20250810032932250-1162225620.png" alt="image" width="637" height="208" loading="lazy"></div>
<div><img src="https://img2024.cnblogs.com/blog/1433301/202508/1433301-20250810032939877-466070506.png" alt="image" width="637" height="208" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></div>
<h3>总结</h3>
<p>语音活动检测 (VAD) 是语音处理领域的基础技术，它通过<strong>特征提取</strong>和<strong>智能判决</strong>，将复杂的音频信号划分为有意义的语音和无声的非语音部分。从经典的 WebRTC VAD 到基于深度学习的 Silero VAD 和 FSMN-VAD，VAD 技术正在不断发展和创新，以适应更复杂的环境和更广泛的应用场景。未来，VAD 模型的准确性和鲁棒性将持续提升，为语音通信和人机交互提供更强大的支持。</p>
<p>&nbsp;注：以上测试均做了归一化和去工频(100Hz)高通滤波预处理</p>
<h1>参考</h1>
<p>【github】https://github.com/snakers4/silero-vad</p>
<p>【github】https://github.com/mengsaisi/VAD_campare</p>
<p>【github】https://github.com/jtkim-kaist/VAD</p>
<hr>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.03125" data-date-updated="2025-08-10 15:33">2025-08-10 14:48</span>&nbsp;
<a href="https://www.cnblogs.com/LXP-Never">凌逆战</a>&nbsp;
阅读(<span id="post_view_count">15</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(19030302);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '19030302', targetLink: 'https://www.cnblogs.com/LXP-Never/p/19030302', title: '语音活动检测（VAD）' })">举报</a>
</div>
        