
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/zhangmingcheng/p/18772028" title="发布于 2025-03-14 14:30">
    <span role="heading" aria-level="2">Ollama——大语言模型本地部署的极速利器</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body blogpost-body-html">
<h1 class="auto-hide-last-sibling-br paragraph-qzbcQC paragraph-element br-paragraph-space">1、概述</h1>
<p class="auto-hide-last-sibling-br paragraph-qzbcQC paragraph-element br-paragraph-space">　　Ollama 是一款开源跨平台大模型工具，<span style="color: rgba(255, 0, 0, 1)"><strong>主要用于在本地便捷部署和运行大型语言模型（LLM），核心目标是降低用户使用大模型的门槛，同时保障数据隐私。</strong></span>核心功能与特点如下：</p>
<div class="auto-hide-last-sibling-br paragraph-qzbcQC paragraph-element br-paragraph-space"><strong>（1）本地部署，隐私保护</strong></div>
<ol class="auto-hide-last-sibling-br">
<li>支持在 Windows、MacOS、Linux 等系统本地运行模型，无需依赖云端，数据交互全程在本地完成，避免隐私泄露。</li>
<li>适合对数据敏感的场景（如企业内部、科研）。</li>
</ol>
<div class="auto-hide-last-sibling-br paragraph-qzbcQC paragraph-element br-paragraph-space"><strong>（2）丰富模型库，开箱即用</strong></div>
<ol class="auto-hide-last-sibling-br">
<li><span style="background-color: rgba(255, 255, 255, 1); color: rgba(255, 0, 0, 1)"><strong>预集成主流开源模型</strong></span>，如 Llama 3、DeepSeek-R1、Qwen、Mistral 等（可以在&nbsp;<a class=" external" href="https://link.zhihu.com/?target=http%3A//ollama.com/library" rel="noopener nofollow" target="_blank" data-za-detail-view-id="1043"><span class="invisible">http://<span class="visible">ollama.com/library</span></span></a>&nbsp;上找到），覆盖文本生成、代码开发、多语言翻译等场景。</li>
<li>支持模型量化（如 7B/13B 参数模型），降低显存需求，普通电脑（8GB + 内存）即可运行轻量模型。</li>
</ol>
<div class="auto-hide-last-sibling-br paragraph-qzbcQC paragraph-element br-paragraph-space"><strong>（3）极简交互，命令行与 API 双支持</strong></div>
<ol class="auto-hide-last-sibling-br">
<li>命令行： 通过 ollama run [模型名] 一键下载并启动模型，支持流式对话（如 ollama run yi:6b-chat）。</li>
<li>API 接口：默认开放 11434 端口，兼容 OpenAI API 格式，可无缝对接 LangChain 等工具，方便开发集成。</li>
</ol>
<div class="auto-hide-last-sibling-br paragraph-qzbcQC paragraph-element br-paragraph-space"><strong>（4）自定义模型</strong>&nbsp;</div>
<ol class="auto-hide-last-sibling-br">
<li>通过&nbsp;Modelfile 配置参数（温度、上下文长度、系统提示等），创建个性化模型（如 FROM llama2 PARAMETER temperature 0.7）。</li>
</ol>
<h1 class="header-jfw95c auto-hide-last-sibling-br">2、安装与基础命令</h1>
<p><strong>（1）安装（Github：<a class=" external" href="https://link.zhihu.com/?target=https%3A//github.com/ollama/ollama" rel="noopener nofollow" target="_blank" data-za-detail-view-id="1043"><span class="invisible">https://<span class="visible">github.com/ollama/ollam</span></span></a>）</strong></p>
<ul class="auto-hide-last-sibling-br">
<li>官网下载：<a class="link-MqLqS9" href="https://ollama.com/" target="_blank" rel="noopener nofollow">ollama.com</a>（Windows、Mac支持一键安装包）。</li>
<li>命令行安装（Linux）：curl -fsSL https://ollama.com/install.sh | sh。</li>
<li>Docker安装：直接docker run ollama。</li>
</ul>
<p><strong>（2）常用命令</strong></p>
<div class="cnblogs_Highlighter">
<pre class="brush:html;gutter:true;">拉取模型：ollama pull llama3:13b
运行对话：ollama run llama3:13b（首次自动下载）
列出模型：ollama list
停止服务：ollama stop
查看帮助：ollama --help</pre>
</div>
<blockquote>
<p class="header-jfw95c auto-hide-last-sibling-br">注意： 详细命令使用参见《<a href="https://zhuanlan.zhihu.com/p/720546185" target="_blank" rel="noopener nofollow">大模型-ollama（运行框架）</a>》这篇博文。</p>
</blockquote>
<h1 class="header-jfw95c auto-hide-last-sibling-br">3、本地部署大语言模型和云端部署大语言模型对比</h1>
<p><img src="https://img2024.cnblogs.com/blog/624219/202503/624219-20250314142447924-465970463.png" alt="" width="838" height="308" loading="lazy"></p>
<h1 class="header-jfw95c auto-hide-last-sibling-br">4、典型使用场景</h1>
<ul class="auto-hide-last-sibling-br">
<li>开发者测试：快速验证模型性能，无需申请云端 API 权限。</li>
<li>本地化应用：离线聊天机器人、内部文档问答系统（如医疗、法律领域）。</li>
<li>科研与教学：自定义模型训练，分析模型行为（如参数窃取实验）。</li>
<li>隐私优先场景：避免敏感数据上传云端（如企业代码、用户对话）。</li>
</ul>
<h1 class="header-jfw95c auto-hide-last-sibling-br">5、安全风险与加固建议</h1>
<p><strong>（1）安全风险</strong></p>
　　2025 年 3 月，国家网络安全通报中心指出 Ollama 默认配置存在三大风险：<ol class="auto-hide-last-sibling-br">
<li>未授权访问：11434 端口默认开放且无认证，攻击者可直接调用模型、删除文件。</li>
<li>数据泄露：通过 /api/show 接口获取模型敏感信息（如 License）。</li>
<li>历史漏洞：可利用 CVE-2024 系列漏洞执行恶意操作（如数据投毒）。</li>
</ol>
<p><strong>（2）加固建议</strong></p>
<ul class="auto-hide-last-sibling-br">
<li>限制端口监听：修改配置仅允许本地访问（ollama serve --listen localhost:11434）。</li>
<li>配置防火墙：禁止公网访问 11434 端口。</li>
<li>启用 API 密钥：通过环境变量 OLLAMA_API_KEY 认证（需版本 ≥0.5.12）。</li>
<li>及时更新：修复漏洞，避免使用默认配置暴露公网。</li>
</ul>
<h1 class="header-jfw95c auto-hide-last-sibling-br">6、优缺点总结</h1>
<ul class="auto-hide-last-sibling-br">
<li>优点：轻量易用、模型丰富、隐私性强，适合快速原型开发。</li>
<li>缺点：默认配置不安全（需手动加固），多模型并行依赖 GPU 显存，复杂场景需结合 vLLM 等框架优化。</li>
</ul>
<h1>7、总结</h1>
<div class="auto-hide-last-sibling-br paragraph-qzbcQC paragraph-element br-paragraph-space">　　Ollama 是本地大模型的 “瑞士军刀”，用一行命令即可开启私有化 AI 体验，但需注意安全配置，适合追求便捷与隐私的开发者和企业。</div>
<h1>8、参考文章</h1>
<p class="Post-Title">&nbsp; &nbsp;&nbsp;<a href="https://zhuanlan.zhihu.com/p/720546185" target="_blank" rel="noopener nofollow">大模型-ollama（运行框架）</a>&nbsp;、&nbsp;<a href="https://zhuanlan.zhihu.com/p/704951717" target="_blank" rel="noopener nofollow">Ollama使用指南【超全版】</a></p>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.06999370244097222" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-03-14 14:43">2025-03-14 14:30</span>&nbsp;
<a href="https://www.cnblogs.com/zhangmingcheng">人艰不拆_zmc</a>&nbsp;
阅读(<span id="post_view_count">151</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18772028" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18772028);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18772028', targetLink: 'https://www.cnblogs.com/zhangmingcheng/p/18772028', title: 'Ollama——大语言模型本地部署的极速利器' })">举报</a>
</div>
        