
    <a name="top"></a>
    <h2><a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/jyzhao/p/18716495/shou-ba-shou-jiao-ni-geng-you-ya-de-xiang-shou-dee" title="发布于 2025-02-15 09:02">
    <span role="heading" aria-level="2">手把手教你更优雅的享受 DeepSeek</span>
    

</a>
</h2>
    <small>
<span id="post-date" data-last-update-days="1.383321764048611" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-02-15 09:02">2025-02-15 09:02</span>&nbsp;
<a href="https://www.cnblogs.com/jyzhao">AlfredZhao</a>&nbsp;
阅读(<span id="post_view_count">459</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18716495" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18716495);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18716495', targetLink: 'https://www.cnblogs.com/jyzhao/p/18716495/shou-ba-shou-jiao-ni-geng-you-ya-de-xiang-shou-dee', title: '手把手教你更优雅的享受 DeepSeek' })">举报</a>
</small>
    <div class="entry">
        <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p>开始之前，首先要确定你已经配置好Ollama软件并正常运行DeepSeek本地模型。如果这一步还不清楚，请翻看之前的手把手教程《<a href="https://mp.weixin.qq.com/s/AgQkyzV7cr-gUsfTqP7mFg?token=334328943&amp;lang=zh_CN" target="_blank" rel="noopener nofollow">手把手教你部署 DeepSeek 本地模型</a>》。</p>
<p><img src="https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208263-1269773954.png" alt="logo1" loading="lazy"></p>
<p>本文是手把手教程系列第3篇，包含内容如下：</p>
<ul>
<li>1.如何使用浏览器调用DeepSeek</li>
<li>2.为何使用最小DeepSeek模型演示</li>
<li>3.使用API方式免费体验满血版DeepSeek</li>
</ul>
<h1 id="1如何使用浏览器调用deepseek">1.如何使用浏览器调用DeepSeek</h1>
<p>先前教程中，在UI界面实现这部分，给大家推荐的是 <code>Chatbox</code> 桌面软件，这是为了更多的小白能够没有任何门槛的直接上手实践。<br>
但实际上，对于有梯子的读者，还有一种更加简单、轻量且优雅的Web调用方案：</p>
<p>只需要在Chrome浏览器中添加扩展程序 <code>Page Assist</code>，就可以实现在Web端更优雅的调用本地 AI 模型。</p>
<ul>
<li>Page Assist - 本地 AI 模型的 Web UI</li>
</ul>
<p>注：</p>
<ul>
<li>1.Page Assist插件安装好之后，也可实现在无网环境下，通过Web UI随时和本地部署的各种大模型畅快聊天。</li>
<li>2.如果你的机器是全周期无法上网的，还可以下载对应Page Assist的离线安装包，手动安装即可。</li>
</ul>
<p>下面演示下具体步骤。<br>
打开Chrome浏览器，在地址栏输入扩展程序的网址：</p>
<ul>
<li><a target="_blank">chrome://extensions/</a></li>
</ul>
<p><img src="https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208264-264999530.png" alt="1" loading="lazy"></p>
<p>这里点击<code>Chrome 应用商店</code>并搜索扩展程序 <code>Page Assist</code>：</p>
<p><img src="https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208609-1124629548.png" alt="2" loading="lazy"></p>
<p>选择<code>添加至Chrome</code>。</p>
<p><img src="https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208264-2146421027.png" alt="3" loading="lazy"></p>
<p>选择<code>添加扩展程序</code>。</p>
<p><img src="https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208263-518738724.png" alt="4" loading="lazy"></p>
<p>如此就成功将 <code>Page Assist</code> 添加至Chrome。</p>
<p>然后，为了今后更方便的调用，我们可以把它固定下。</p>
<p><img src="https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208264-510483068.png" alt="5" loading="lazy"></p>
<p>这样，当我们再次打开浏览器，就可以直接点击红框中的这个按钮启动扩展程序 <code>Page Assist</code>：</p>
<p><img src="https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208636-1580873481.png" alt="6" loading="lazy"></p>
<p>启动后，按下图步骤，选择本地部署的DeepSeek模型，确认Ollama运行状态正常，就可以直接输入文字聊天了：</p>
<p><img src="https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208665-1961440063.png" alt="7" loading="lazy"></p>
<p>是不是这种方式更加简单，且很优雅！</p>
<h1 id="2为何使用最小deepseek模型演示">2.为何使用最小DeepSeek模型演示</h1>
<p>这里特别解答下，之前有很多读者私信好心建议我不要使用1.5b的模型测试，至少也要7b以上版本，效果会更好。</p>
<p>但实际上由于笔者目前测试硬件比较低，虽然也能勉强运行起下一个级别的7b模型，但反应慢的完全不可接受，我这里实测7b的模型，问简单问题，都需要思考20s：</p>
<p><img src="https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208447-1027124779.png" alt="8" loading="lazy"></p>
<p>而换回1.5b的模型，基本秒出，即便思考也就2s的样子：</p>
<p><img src="https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208484-68871168.png" alt="9" loading="lazy"></p>
<p>而且，即便7b要比1.5b的模型能力是有所提升，但并没有到质的差异。</p>
<p>所以在硬件没升级之前，后续演示主要还是会选择这个1.5b的最小模型，这样测试反馈的效率高，心情也能舒畅。</p>
<p>通过从DeepSeek官方给出的测试图来看：</p>
<p><img src="https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208866-885503213.jpg" alt="benchmark" loading="lazy"></p>
<p>也可以发现，在这张benchmark的对比图上，人家都没考虑比32b更低的模型，这其实也算暗示了32b以下的模型基本表现都拿不出手，可是如果想要本地部署32b或更大的模型，首先你的硬件得跟得上。</p>
<h1 id="3使用api方式免费体验满血版deepseek">3.使用API方式免费体验满血版DeepSeek</h1>
<p>既然本地部署情况，在不具备好的硬件条件之前，实际使用效果一定欠佳。</p>
<p>那如果想提前体验下更大模型的具体效果，该怎么做呢？</p>
<p>答案就是使用API的方式，这种方式下连Ollama都不需要了，只需要配置好对应的API key，就可以轻松体验到满血版的DeepSeek。</p>
<p>1）使用DeepSeek官方API</p>
<p>官方API网址：</p>
<ul>
<li><a href="https://platform.deepseek.com/api_keys" target="_blank" rel="noopener nofollow">platform.deepseek.com</a></li>
</ul>
<p>这个网址曾经处于维护，但如今已经可以打开，但由于目前服务器资源还是紧张，官方已暂停API服务充值，之前余额依然可用。</p>
<p><img src="https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208430-1884230046.png" alt="3-1-ds" loading="lazy"></p>
<p>需要注意这个key创建时就需要复制保留好，以后不提供再复制。</p>
<p>如果忘记保存，只能这样重建新的key：</p>
<p><img src="https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208278-631830554.png" alt="3-2-ds" loading="lazy"></p>
<p>测试下接入官方API的效果，使用API的方式还是要用到之前的Chatbox：</p>
<p><img src="https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208587-435717257.png" alt="3-3-ds-show" loading="lazy"></p>
<p>这里测试，想到笔者在学生时期，老师曾经说通常古诗最难翻译好。<br>
让它帮我翻译《画》这首古诗为英文，选择普通的<code>deepseek-chat</code>模型，这个模型不会推理，直接会给出答案，我觉得效果也非常好。</p>
<p>附：官方的API在Chatbox软件中的设置方式如下图：</p>
<p><img src="https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208502-709409978.png" alt="3-4-ds-set" loading="lazy"></p>
<p>2）使用第三方服务商API</p>
<p>因为官方的资源目前还是紧张，所以除了官方的途径之外，还有一些第三方服务商的选择，比如这里以流行度较高的siliconflow为例，点击下面的邀请链接（邀请码：aYaHaxLo），注册登录后即送免费的14元配额，可以够玩上一阵子了：</p>
<ul>
<li><a href="https://cloud.siliconflow.cn/i/aYaHaxLo" target="_blank" rel="noopener nofollow">siliconflow.cn</a></li>
</ul>
<p><img src="https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090209090-1680233823.png" alt="3-5-siliconflow" loading="lazy"></p>
<p>注意，siliconflow这个API的key如果没保存，是支持随时再次复制保存的，这点安全策略上和官方的设计有所不同。</p>
<p>复制key之后，在Chatbox下设置silicon flow的API粘贴进去即可：</p>
<p><img src="https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208598-1450840235.png" alt="3-6-siliconflow" loading="lazy"></p>
<p>配置好之后，同样测试下，帮我翻译《画》这首古诗为英文，这回选择<code>deepseek-ai/DeepSeek-R1</code>模型，效果如下：</p>
<p><img src="https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208766-611123764.png" alt="3-7-siliconflow-1" loading="lazy"></p>
<p><img src="https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208807-1926346647.png" alt="3-7-siliconflow-2" loading="lazy"></p>
<p><img src="https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208707-237501273.png" alt="3-7-siliconflow-3" loading="lazy"></p>
<p>超过了3页篇幅的推理思考，然后给出了答案：</p>
<p><img src="https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208725-1558993276.png" alt="3-7-siliconflow-4" loading="lazy"></p>
<p>可以直观的看到，这满血版的deepseek-r1思考推理能力，确实要比本地部署的小号模型强大许多。</p>
<p><img src="https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208263-1269773954.png" alt="logo1" loading="lazy"></p>
<p>至此，基础篇就差不多了，后面计划持续研究分享进阶篇，后续教程大家还想了解哪些方面，欢迎在评论区留言。</p>

</div>
<div id="MySignature" role="contentinfo">
    AlfredZhao©版权所有「从Oracle起航，领略精彩的IT技术。」
</div>
<div class="clear"></div>

        <div class="clear"></div>
        
</div>
    <ul class="postmetadata">
        <vc:categories-tags blog-app="jyzhao" blog-id="186567" post-id="18716495"></vc:categories-tags>
    </ul>
