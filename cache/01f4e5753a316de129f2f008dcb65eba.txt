
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/wang_yb/p/18872225" title="发布于 2025-05-12 10:21">
    <span role="heading" aria-level="2">集成学习双雄：Boosting和Bagging简介</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p>在机器学习的世界里，集成学习（Ensemble Learning）是一种强大的技术，它通过组合多个模型来提高预测性能。</p>
<p>集成学习通过组合多个基学习器的预测结果，获得比单一模型更优秀的性能。其核心思想是"三个臭皮匠顶个诸葛亮"，主要分为两大流派：<code>Boosting</code>（<strong>提升</strong>）和<code>Bagging</code>（<strong>装袋</strong>）。</p>
<p>本文将重点解析这两种方法的原理，并通过实战演示它们的应用。</p>
<h1 id="1-boosting从错误中学习">1. Boosting：从错误中学习</h1>
<p><code>Boosting</code>的核心思想是<strong>串行训练</strong>：每个新模型都专注于修正前序模型的错误。</p>
<p>它的工作流程类似于<strong>"错题本学习法"</strong>：</p>
<ol>
<li>训练第一个基学习器</li>
<li>给预测错误的样本增加权重</li>
<li>基于新权重训练下一个学习器</li>
<li>重复步骤2-3，最终加权组合所有预测结果</li>
</ol>
<p>最常见的<code>Boosting</code>算法是<code>AdaBoost</code>和<code>Gradient Boosting</code>。</p>
<p>下面我们使用<code>Gradient Boosting</code>来演示。</p>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score

# 生成一个简单的二分类数据集
X, y = make_moons(n_samples=300, noise=0.25, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# 使用 Gradient Boosting
gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)
gbc.fit(X_train, y_train)

# 预测并计算准确率
y_pred = gbc.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"梯度 Boosting 准确率: {accuracy:.2f}")

# 输出结果：
'''
梯度 Boosting 准确率: 0.92
'''
</code></pre>
<p>准确率还可以，接下来封装一个函数，把分类的结果绘制出来。</p>
<pre><code class="language-python"># 绘制决策边界
def plot_decision_boundary(model, X, y):
    h = 0.02
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    plt.contourf(xx, yy, Z, alpha=0.4)
    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors="k", marker="o")
    plt.title("决策边界")
    plt.show()


plot_decision_boundary(gbc, X, y)
</code></pre>
<p><img src="https://img2024.cnblogs.com/blog/83005/202505/83005-20250512102039247-143623492.png" alt="" loading="lazy"></p>
<p>在这个例子中，我们生成了一个简单的二分类数据集，并用 <code>Gradient Boosting</code> 进行训练。</p>
<p>可以看到，模型能够很好地拟合数据，并且决策边界也很清晰。</p>
<h1 id="2-bagging民主投票机制">2. Bagging：民主投票机制</h1>
<p><code>Bagging</code>的核心是<strong>并行训练</strong>+<strong>随机化</strong>。</p>
<p>通过随机抽样生成多个不同的数据子集，然后在每个子集上训练一个模型，最后把所有模型的预测结果汇总起来，得到最终的预测。</p>
<p>具体来说：</p>
<ul>
<li><strong>随机抽样</strong>：从原始数据集中随机抽取多个子集（有放回抽样）。</li>
<li><strong>训练模型</strong>：在每个子集上训练一个模型（通常是决策树）。</li>
<li><strong>汇总结果</strong>：对于分类任务，通过投票决定最终结果；对于回归任务，取平均值。</li>
</ul>
<p>随机森林（<code>Random Forest</code>）是 <code>Bagging</code> 的一个经典应用，它在训练决策树时还会随机选择特征子集，进一步增加模型的多样性。</p>
<p>下面我们用<code>scikit-learn</code>的<code>RandomForestClassifier</code>来实现一个随机森林模型。</p>
<pre><code class="language-python">from sklearn.ensemble import RandomForestClassifier

# 使用 Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# 预测并计算准确率
y_pred_rf = rf.predict(X_test)
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print(f"随机森林 Bagging 准确率: {accuracy_rf:.2f}")

# 输出结果：
'''
随机森林 Bagging 准确率: 0.92
'''
</code></pre>
<p>准确率也不错，使用上一节的函数也可以绘制出决策边界。</p>
<pre><code class="language-python"># 绘制决策边界
plot_decision_boundary(rf, X, y)
</code></pre>
<p><img src="https://img2024.cnblogs.com/blog/83005/202505/83005-20250512102039196-818398942.png" alt="" loading="lazy"></p>
<p>在这个例子中，我们同样使用了之前生成的二分类数据集。</p>
<p><strong>随机森林</strong>通过多个决策树的组合，能够更好地处理复杂的数据分布，并且具有很强的抗过拟合能力。</p>
<h1 id="3-偏差-方差图理解模型性能的关键">3. 偏差-方差图：理解模型性能的关键</h1>
<p>在<strong>集成学习</strong>中，<strong>偏差</strong>-<strong>方差</strong>图是一个非常有用的工具，它可以帮助我们理解模型的性能。</p>
<ul>
<li><strong>偏差</strong>（<code>Bias</code>）：模型对数据规律的拟合能力。偏差越高，模型越简单，可能欠拟合；偏差越低，模型越复杂，可能过拟合。</li>
<li><strong>方差</strong>（<code>Variance</code>）：模型对数据噪声的敏感程度。方差越高，模型对训练数据的波动越敏感，容易过拟合；方差越低，模型对数据波动不敏感，可能欠拟合。</li>
</ul>
<p>对于<code>Boosting</code>和<code>Bagging</code>：</p>
<ul>
<li><code>Boosting</code>：通常会降低偏差，但可能会增加方差。因为 Boosting 不断调整模型以拟合数据，容易对噪声过于敏感。</li>
<li><code>Bagging</code>：通常会降低方差，但对偏差的影响较小。因为 Bagging 通过随机抽样和投票，能够减少模型对数据波动的敏感性。</li>
</ul>
<p>通过<strong>偏差-方差图</strong>，我们可以更好地选择合适的集成学习方法。</p>
<p>如果数据噪声较大，<code>Boosting</code>可能会过拟合；而如果数据分布复杂，<code>Bagging</code>可能会欠拟合。</p>
<h1 id="4-总结">4. 总结</h1>
<p><code>Boosting</code>和<code>Bagging</code>是两种非常强大的集成学习方法。</p>
<p>这两种集成学习方法的 对比和选择建议 如下表：</p>
<table>
<thead>
<tr>
<th>特性</th>
<th>Boosting</th>
<th>Bagging</th>
</tr>
</thead>
<tbody>
<tr>
<td>训练方式</td>
<td>串行</td>
<td>并行</td>
</tr>
<tr>
<td>样本使用</td>
<td>加权调整</td>
<td>自助采样</td>
</tr>
<tr>
<td>主要优势</td>
<td>降低偏差</td>
<td>降低方差</td>
</tr>
<tr>
<td>过拟合风险</td>
<td>较高</td>
<td>较低</td>
</tr>
<tr>
<td>典型应用场景</td>
<td>复杂关系建模</td>
<td>高噪声数据处理</td>
</tr>
</tbody>
</table>
<p><code>Boosting</code>通过弱学习器的接力赛，逐步改进模型；<code>Bagging</code>通过随机抽样和投票，降低模型的方差。</p>
<p>通过<code>scikit-learn</code>，我们可以很容易地应用这两种方法。</p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.16877659879861112" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-05-12 10:21">2025-05-12 10:21</span>&nbsp;
<a href="https://www.cnblogs.com/wang_yb">wang_yb</a>&nbsp;
阅读(<span id="post_view_count">0</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18872225);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18872225', targetLink: 'https://www.cnblogs.com/wang_yb/p/18872225', title: '集成学习双雄：Boosting和Bagging简介' })">举报</a>
</div>
        