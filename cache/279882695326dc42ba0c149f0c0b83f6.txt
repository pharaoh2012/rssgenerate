
		<h1 class="postTitle">
			<a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/xingzheanan/p/18878473" title="发布于 2025-05-15 15:42">
    <span role="heading" aria-level="2">kubernetes service 原理精讲</span>
    

</a>

		</h1>
		<div class="clear"></div>
		<div class="postBody">
			<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<hr>
<h1 id="介绍">介绍</h1>
<p>Kubernetes Service 用于流量的负载均衡和反向代理，其通过 kube-proxy 组件实现。从服务的角度来看，kube-controller-manager 实现了服务注册，kube-proxy 实现了 kubernetes 集群内服务的负载均衡。</p>
<p>示意图如下：</p>
<p><img src="https://img2024.cnblogs.com/blog/1498760/202505/1498760-20250515153809168-1127970930.png" alt="image" loading="lazy"></p>
<p><code>kube-proxy</code> 通过三种模式 <code>userspace</code>，<code>iptables</code> 和 <code>IPVS</code> 实现 Service 流量的负载均衡。<code>userspace</code> 不太常用，kube-proxy 自 v1.8 开始支持 IPVS，v1.11 GA。</p>
<p>iptables 和 IPVS 都是基于内核的 <code>Netfilter</code> 实现。iptables 基于 iptables 表匹配规则，复杂度为 O(n)，IPVS 基于哈希表实现规则匹配，复杂度为 O(1)。详细对比如下：</p>
<p>性能对比测试：</p>
<table>
<thead>
<tr>
<th>场景</th>
<th>iptables 延迟</th>
<th>IPVS 延迟</th>
<th>提升幅度</th>
</tr>
</thead>
<tbody>
<tr>
<td>100 Service（10 Pod）</td>
<td>2.1ms</td>
<td>1.3ms</td>
<td>38%</td>
</tr>
<tr>
<td>1000 Service（100 Pod）</td>
<td>11.4ms</td>
<td>2.9ms</td>
<td>75%</td>
</tr>
<tr>
<td>10000 Service（1000 Pod）</td>
<td>超时</td>
<td>3.2ms</td>
<td>100%</td>
</tr>
</tbody>
</table>
<p><em>数据来源：Kubernetes 社区性能测试</em></p>
<p>多维度对比：</p>
<table>
<thead>
<tr>
<th>维度</th>
<th>iptables</th>
<th>IPVS</th>
</tr>
</thead>
<tbody>
<tr>
<td>性能</td>
<td>低（O(n) 复杂度）</td>
<td>高（O(1) 复杂度）</td>
</tr>
<tr>
<td>扩展性</td>
<td>适合小规模集群</td>
<td>支持百万级 Service/Pod</td>
</tr>
<tr>
<td>算法</td>
<td>仅随机选择</td>
<td>10+ 种负载均衡算法</td>
</tr>
<tr>
<td>资源占用</td>
<td>高（规则链维护）</td>
<td>低（哈希表存储）</td>
</tr>
<tr>
<td>故障恢复</td>
<td>全量重载，可能抖动</td>
<td>增量更新，无感知</td>
</tr>
</tbody>
</table>
<h1 id="iptabls">iptabls</h1>
<p>iptables 介绍学习可参考 <a href="https://www.zsythink.net/archives/category/%e8%bf%90%e7%bb%b4%e7%9b%b8%e5%85%b3/iptables" target="_blank" rel="noopener nofollow">iptables</a>，非常好的 iptables 学习资料，强烈推荐。</p>
<p>iptables 重点在五链五表，其示意图如下：</p>
<p><img src="https://img2024.cnblogs.com/blog/1498760/202505/1498760-20250515154120184-1465422323.png" alt="image" loading="lazy"></p>
<p>kube-proxy 通过在 INPUT，FORWARD，POST_ROUTING 链上添加钩子规则实现 Service 的负载均衡和反向代理。示意图如下：</p>
<p><img src="https://img2024.cnblogs.com/blog/1498760/202505/1498760-20250515153903858-1870666935.png" alt="image" loading="lazy"></p>
<p><em>图片来源于 <a href="https://mp.weixin.qq.com/s/HgQrub757qhIBCYO45o4NQ" target="_blank" rel="noopener nofollow">公众号：云原生 Space</a></em></p>
<p>kubernetes v1.8 版本之前的 Service 负载均衡基于 iptables 实现，可以参考 <a href="https://www.cnblogs.com/xingzheanan/p/14110134.html" target="_blank">一文看懂 Kubernetes 服务发现： Service</a> 学习，本文重点关注在 IPVS 实现上。</p>
<h1 id="ipvs">ipvs</h1>
<p>IPVS 提供 DNAT 和负载均衡，需要和 iptables 配合使用才能实现 Service 的流量转发。</p>
<p>结合 ClusterIP 看 kube-proxy ipvs 是如何实现流量负载均衡的。</p>
<h2 id="service-clusterip">Service ClusterIP</h2>
<p>Kubernetes Service：</p>
<pre><code># kubectl get svc
NAME                                       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE
kubernetes                                 ClusterIP   10.233.0.1   &lt;none&gt;        443/TCP    385d
</code></pre>
<p>ipvsadm 查看 svc 的负载均衡信息：</p>
<pre><code># ipvsadm -l -n | grep 10.233.0.1:443 -A 3
TCP  10.233.0.1:443 rr
  -&gt; 10.251.xxx.30:6443           Masq    1      18         2         
  -&gt; 10.251.xxx.31:6443           Masq    1      25         0         
  -&gt; 10.251.xxx.32:6443           Masq    1      13         1
</code></pre>
<p>输出部分元素解释：</p>
<ul>
<li>rr: 表示负载均衡策略，默认是 rr。</li>
<li>Masq：负载均衡模式，Masq 指的是 NAT 模式。IPVS 支持 Direct Routing，Tunneling 模式，这两种都不支持端口映射，IPVS 使用的是 Masq 模式。</li>
</ul>
<blockquote>
<p>IPVS 提供如下负载均衡策略：</p>
<ul>
<li>rr：轮询调度</li>
<li>lc：最小连接数</li>
<li>dh：目标哈希</li>
<li>sh：源哈希</li>
<li>sed：最短期望延迟</li>
<li>nq： 不排队调度</li>
</ul>
</blockquote>
<p>只有负载均衡信息并不能使集群内访问 ClusterIP 的流量转发到后端服务。流量首先需要经过内核，由内核根据 iptables 策略决定丢弃/接受还是转发包。要接收访问 ClusterIP 的流量就需要在 iptables 的 PREROUTING 表中配置接受策略。并且需要创一个 dummy 接口，添加 ClusterIP 从而骗过内核，接收集群内发往 ClusterIP 的数据包。</p>
<p>kube-proxy 会创建 kube-ipvs0 的 dummy 接口，如下：</p>
<pre><code>kube-ipvs0: flags=130&lt;BROADCAST,NOARP&gt;  mtu 1500
inet 10.233.0.1/32 scope global kube-ipvs0
       valid_lft forever preferred_lft forever
</code></pre>
<p><em>这里也从侧面印证了为什么是集群内访问，这是 dummy 接口，集群外不通</em></p>
<p>查看 iptables 策略看内核是如何接收访问 ClusterIP 的数据包的：</p>
<pre><code># iptables -t nat -L
Chain PREROUTING (policy ACCEPT)
target     prot opt source               destination         

// 流量首先经过 PREROUTING 链的 nat 表，匹配 KUBE-SERVICES 规则
KUBE-SERVICES  all  --  anywhere             anywhere             /* kubernetes service portals */

Chain KUBE-SERVICES (2 references)
target     prot opt source               destination     

// KUBE-LOAD-BALANCER：匹配访问 LoadBalancer 的流量，和 ClusterIP 没有关系
KUBE-LOAD-BALANCER  all  --  anywhere             anywhere             /* Kubernetes service lb portal */ match-set KUBE-LOAD-BALANCER dst,dst

// 集群内源 ip 不是 10.222.0.0/18 网段的流量将进入 KUBE-MARK-MASQ 规则
// 这里匹配是发往 ClusterIP 的流量，10.222.0.0/18 网段是 kubernetes 分给 pod 的 ip，这条策略的意思是匹配集群内非 pod 访问 ClusterIP 的流量
KUBE-MARK-MASQ  all  -- !10.222.0.0/18        anywhere             /* Kubernetes service cluster ip + port for masquerade purpose */ match-set KUBE-CLUSTER-IP dst,dst

// KUBE-NODE-PORT：匹配访问 NodePort 的流量，和 ClusterIP 没有关系
KUBE-NODE-PORT  all  --  anywhere             anywhere             ADDRTYPE match dst-type LOCAL

ACCEPT     all  --  anywhere             anywhere             match-set KUBE-CLUSTER-IP dst,dst
ACCEPT     all  --  anywhere             anywhere             match-set KUBE-LOAD-BALANCER dst,dst
</code></pre>
<p>这里的逻辑很重要，为了理解清晰，有必要进一步介绍下 <code>KUBE-MARK-MASQ</code> 这条规则：</p>
<pre><code>KUBE-MARK-MASQ  all  -- !10.222.0.0/18        anywhere             /* Kubernetes service cluster ip + port for masquerade purpose */ match-set KUBE-CLUSTER-IP dst,dst
</code></pre>
<p>这条规则包括两个点：</p>
<ol>
<li><code>match-set KUBE-CLUSTER-IP dst,dst</code> 使用 iptables 的 ipset 模块匹配访问 ClusterIP 的流量。ipset 创建了一个包括 ip 信息等的集合 <code>KUBE-CLUSTER-IP</code>（实际是哈希表，查找复杂度为 O(1)）：</li>
</ol>
<pre><code># ipset list KUBE-CLUSTER-IP | grep 10.233.0.1,tcp:443
10.233.0.1,tcp:443
</code></pre>
<ol start="2">
<li>匹配到访问 ClusterIP 的流量后进入 <code>KUBE-MARK-MASQ</code> 规则：</li>
</ol>
<pre><code>Chain KUBE-MARK-MASQ (4 references)
target     prot opt source               destination         
MARK       all  --  anywhere             anywhere             MARK or 0x4000
</code></pre>
<p><code>KUBE-MARK-MASQ</code> 规则将数据包打上 <code>MARK:0x4000</code> 标签。</p>
<blockquote>
<p>这里留个问题，为什么需要打上 <code>MARK:0x4000</code> 标签？</p>
</blockquote>
<p>接着打上 MARK 标签的数据包被接收，进入 ipvs 负载均衡到相应的后端服务。</p>
<h3 id="转发到哪里">转发到哪里？</h3>
<p>kubernetes 集群中每个节点都会起 kube-proxy 配置 iptables/ipvs 规则，并且这些规则是一致的。不同于传统负载均衡，kubernetes 集群内的负载均衡是分布式的。由 kube-proxy 保持信息一致。</p>
<p>集群内节点访问本节点的后端服务可以通过流量被接收后通过 ipvs 做 DNAT 直接转发到 pod 服务，没有问题。</p>
<p>那么，集群内节点访问 ClusterIP 转发到其它节点的 pod 该怎么做的呢？这涉及到跨节点通信，就需要 CNI 的帮忙了。示意图如下：</p>
<p><img src="https://img2024.cnblogs.com/blog/1498760/202505/1498760-20250515154008940-148604323.png" alt="image" loading="lazy"></p>
<h2 id="数据怎么回去">数据怎么回去？</h2>
<p>前面提到通过 DNAT 数据包会转发到后端服务。后端服务的数据包又该怎么回去呢？</p>
<p>后端数据包经过 OUTPUT 链到 POSTROUTING 链，在 POSTROUTING 链做 SNAT 转发数据包到访问节点。</p>
<p>流程如下：</p>
<pre><code>// 从集群内发出的数据包先走 OUTPUT
// OUTPUT 链接收数据包，继续进入 POSTROUTING 链
Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination         
KUBE-SERVICES  all  --  anywhere             anywhere             /* kubernetes service portals */
DOCKER     all  --  anywhere            !127.0.0.0/8          ADDRTYPE match dst-type LOCAL

// POSTROUTING 实现出去流量的转发
// 流量将进入 KUBE-POSTROUTING 链
Chain POSTROUTING (policy ACCEPT)
target     prot opt source               destination            
KUBE-POSTROUTING  all  --  anywhere             anywhere             /* kubernetes postrouting rules */

Chain KUBE-POSTROUTING (1 references)
target     prot opt source               destination         
MASQUERADE  all  --  anywhere             anywhere             /* Kubernetes endpoints dst ip:port, source ip for solving hairpin purpose */ match-set KUBE-LOOP-BACK dst,dst,src
RETURN     all  --  anywhere             anywhere             mark match ! 0x4000/0x4000
MARK       all  --  anywhere             anywhere             MARK xor 0x4000
MASQUERADE  all  --  anywhere             anywhere             /* kubernetes service traffic requiring SNAT */ random-fully
</code></pre>
<p><code>KUBE-POSTROUTING</code> 的规则非常重要，值得拆开好好讲。</p>
<p><strong>规则1: MASQUERADE</strong></p>
<p><code>MASQUERADE  all  --  anywhere             anywhere             /* Kubernetes endpoints dst ip:port, source ip for solving hairpin purpose */ match-set KUBE-LOOP-BACK dst,dst,src</code></p>
<p>这条规则是给 pod 访问自己用的，如果 pod 要访问自己，那就别匹配其它规则了，直接将流量转给自己。这称为发卡弯（hairping）模式。</p>
<p>这也是为什么这条规则在 <code>KUBE-POSTROUTING</code> 链最前面的原因。</p>
<p>继续往下看，我们的任务是探索后端服务的数据包又该怎么回去的。</p>
<p><strong>规则2: RETURN</strong></p>
<p><code>RETURN     all  --  anywhere             anywhere             mark match ! 0x4000/0x4000</code></p>
<p>啊哈，还记得我们前面留的问题为什么要打 <code>MARK:0x4000</code> 标签吗？</p>
<p>答案就在这条规则，如果包不带 <code>MARK:0x4000</code> 则退出 <code>KUBE-POSTROUTING</code>,意味着只有带 <code>MARK:0x4000</code> 标签的数据包才会做 SNAT。<code>MARK:0X4000</code> 实际是用来区分是否做 NAT 的标签。</p>
<p>我们的数据包是带 <code>MARK:0X4000</code> 标签的，继续往下走。</p>
<p><strong>规则3: MARK</strong></p>
<p><code>MARK       all  --  anywhere             anywhere             MARK xor 0x4000</code></p>
<p><code>MARK xor 0x4000</code> 清除数据包的 <code>MARK:0X4000</code> 标签。</p>
<p><strong>规则4: MASQUERADE</strong></p>
<p><code>MASQUERADE  all  --  anywhere             anywhere             /* kubernetes service traffic requiring SNAT */ random-fully</code></p>
<p>终于到 SNAT 规则了，对数据包做 SNAT，将请求转发回去。</p>
<h1 id="小结">小结</h1>
<p>本文主要通过 kubernetes service 的 ClusterIP 示例介绍了 iptables 结合 ipvs 是如何管理集群内流量的。关于 NodePort，Ingress，LoadBalancer 并未在文中的讨论范围之内。后续看是否需要继续介绍其它 service 类型。</p>
<p>下一讲会继续介绍 kube-proxy 是如何实现 ipvs/iptables 管理的，力图做到原理实现一网打尽，敬请期待～</p>
<h1 id="参考文章">参考文章</h1>
<ul>
<li><a href="https://cloud.tencent.com/developer/article/1832918" target="_blank" rel="noopener nofollow">【深度】这一次，彻底搞懂 kube-proxy IPVS 模式的工作原理！</a></li>
<li><a href="https://mp.weixin.qq.com/s/HgQrub757qhIBCYO45o4NQ" target="_blank" rel="noopener nofollow">一文读懂 K8S Service 原理</a></li>
<li><a href="https://kubernetes.io/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/" target="_blank" rel="noopener nofollow">IPVS-Based In-Cluster Load Balancing Deep Dive</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/337806843" target="_blank" rel="noopener nofollow">一文看懂 Kube-proxy</a></li>
</ul>
<hr>

</div>
<div id="MySignature" role="contentinfo">
    芝兰生于空谷，不以无人而不芳。
</div>
<div class="clear"></div>

		</div>
		<div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.5233640568009259" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-05-15 15:43">2025-05-15 15:42</span>&nbsp;
<a href="https://www.cnblogs.com/xingzheanan">hxia043</a>&nbsp;
阅读(<span id="post_view_count">103</span>)&nbsp;
评论(<span id="post_comment_count">1</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18878473);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18878473', targetLink: 'https://www.cnblogs.com/xingzheanan/p/18878473', title: 'kubernetes service 原理精讲' })">举报</a>
</div>
	