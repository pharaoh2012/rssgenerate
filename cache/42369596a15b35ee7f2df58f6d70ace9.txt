
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/pythonista/p/18975905" title="发布于 2025-07-09 23:04">
    <span role="heading" aria-level="2">让 Python 代码飙升330倍：从入门到精通的四种性能优化实践</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p>花下猫语：性能优化是每个程序员的必修课，但你是否想过，除了更换算法，还有哪些“大招”？这篇文章堪称典范，它将一个普通的函数，通过四套组合拳，硬生生把性能提升了 330 倍！作者不仅展示了“术”，更传授了“道”。让我们一起跟随作者的思路，体验一次酣畅淋漓的优化之旅。</p>
<p>PS.本文选自最新一期<a href="https://weekly.pythoncat.top" target="_blank" rel="noopener nofollow">Python 潮流周刊</a>，如果你对优质文章感兴趣，诚心推荐你订阅我们的专栏。</p>
<hr>
<p>作者：Itamar Turner-Trauring</p>
<p>译者：豌豆花下猫@Python猫</p>
<p>英文：<a href="https://pythonspeed.com/articles/different-ways-speed" target="_blank" rel="noopener nofollow">330× faster: Four different ways to speed up your code</a></p>
<p>声明：本翻译是出于交流学习的目的，为便于阅读，部分内容略有改动。转载请保留作者信息。</p>
<blockquote>
<p><strong>温馨提示：</strong> 本文原始版本与当前略有不同，比如曾经提到过500倍加速；本文已根据实际情况重新梳理，使论证更清晰。</p>
</blockquote>
<p>当你的 Python 代码慢如蜗牛，而你渴望它快如闪电时，其实有很多种提速方式，从并行化到编译扩展应有尽有。如果只盯着一种方法，往往会错失良机，最终的代码也难以达到极致性能。</p>
<p>为了不错过任何潜在的提速机会，我们可以从“<strong>实践</strong>”的角度来思考。每种实践：</p>
<ul>
<li>以独特方式加速你的代码</li>
<li>涉及不同的技能和知识</li>
<li>可以单独应用</li>
<li>也可以组合应用，获得更大提升</li>
</ul>
<p>为了让这一点更具体，本文将通过一个案例演示多种实践的应用，具体包括：</p>
<ol>
<li><strong>效率（Efficiency）：</strong> 消除浪费或重复的计算。</li>
<li><strong>编译（Compilation）：</strong> 利用编译型语言，并巧妙绕开编译器限制。</li>
<li><strong>并行化（Parallelism）：</strong> 充分发挥多核CPU的威力。</li>
<li><strong>流程（Process）：</strong> 采用能产出更快代码的开发流程。</li>
</ol>
<p>我们将看到：</p>
<ul>
<li>仅用<strong>效率实践</strong>，就能带来近 <strong>2倍</strong> 提速。</li>
<li>仅用<strong>编译实践</strong>，可实现 <strong>10倍</strong> 提速。</li>
<li>两者结合，速度更上一层楼。</li>
<li>最后加上<strong>并行化实践</strong>，最终实现 <strong>330倍</strong> 惊人加速。</li>
</ul>
<h2 id="我们的例子统计字母频率">我们的例子：统计字母频率</h2>
<p>我们有一本英文书，简·奥斯汀的《诺桑觉寺》：</p>
<pre><code class="language-python">with open("northanger_abbey.txt") as f:
    TEXT = f.read()
</code></pre>
<p>我们的目标是分析书中字母的相对频率。元音比辅音更常见吗？哪个元音最常见？</p>
<p>下面是最初的实现：</p>
<pre><code class="language-python">from collections import defaultdict

def frequency_1(text):
    # 一个当键不存在时默认值为0的字典
    counts = defaultdict(lambda: 0)
    for character in text:
        if character.isalpha():
            counts[character.lower()] += 1
    return counts
</code></pre>
<p>运行结果如下：</p>
<pre><code class="language-python">sorted(
    (count, letter) for (letter, count)
    in frequency_1(TEXT).items()
)
</code></pre>
<pre><code>[(1, 'à'),
 (2, 'é'),
 (3, 'ê'),
 (111, 'z'),
 (419, 'q'),
 (471, 'j'),
 (561, 'x'),
 (2016, 'k'),
 (3530, 'v'),
 (5297, 'b'),
 (5404, 'p'),
 (6606, 'g'),
 (7639, 'w'),
 (7746, 'f'),
 (7806, 'y'),
 (8106, 'c'),
 (8628, 'm'),
 (9690, 'u'),
 (13431, 'l'),
 (14164, 'd'),
 (20675, 's'),
 (21107, 'r'),
 (21474, 'h'),
 (22862, 'i'),
 (24670, 'n'),
 (26385, 'a'),
 (26412, 'o'),
 (30003, 't'),
 (44251, 'e')]
</code></pre>
<p>毫无意外，出现频率最高的字母是 "e"。</p>
<p>那我们如何让这个函数更快？</p>
<h2 id="流程实践测量与测试">流程实践：测量与测试</h2>
<p>软件开发不仅依赖于源代码、库、解释器、编译器这些“产物”，更离不开你的工作“流程”——也就是你做事的方法。性能优化同样如此。本文将介绍两种在优化过程中必不可少的流程实践：</p>
<ol>
<li>通过基准测试和性能分析来<strong>测量</strong>代码速度。</li>
<li><strong>测试</strong>优化后的代码，确保其行为与原始版本一致。</li>
</ol>
<p>我们可以先用 <a href="https://github.com/pyutils/line_profiler" target="_blank" rel="noopener nofollow"><code>line_profiler</code></a> 工具分析函数，找出最耗时的代码行：</p>
<pre><code>Line #      Hits   % Time  Line Contents
========================================
     3                     def frequency_1(text):
     4                         # 一个当键不存在时默认值为0的字典
     5                         # available:
     6         1      0.0      counts = defaultdict(lambda: 0)
     7    433070     30.4      for character in text:
     8    433069     27.3          if character.isalpha():
     9    339470     42.2              counts[character.lower()] += 1
    10         1      0.0      return counts
</code></pre>
<h2 id="效率实践减少无用功">效率实践：减少无用功</h2>
<p>效率实践的核心，是用更少的工作量获得同样的结果。这类优化通常在较高的抽象层面进行，无需关心底层CPU细节，因此适用于大多数编程语言。其本质是通过改变计算逻辑来减少浪费。</p>
<h3 id="减少内循环的工作量">减少内循环的工作量</h3>
<p>从上面的性能分析可以看出，函数大部分时间都花在 <code>counts[character.lower()] += 1</code> 这行。显然，对每个字母都调用 <code>character.lower()</code> 是种浪费。我们一遍遍地把 "I" 转成 "i"，甚至还把 "i" 转成 "i"。</p>
<p>优化思路：我们可以先分别统计大写和小写字母的数量，最后再合并，而不是每次都做小写转换。</p>
<pre><code class="language-python">def frequency_2(text):
    split_counts = defaultdict(lambda: 0)
    for character in text:
        if character.isalpha():
            split_counts[character] += 1

    counts = defaultdict(lambda: 0)
    for character, num in split_counts.items():
        counts[character.lower()] += num
    return counts

# 确保新函数结果与旧函数完全一致
assert frequency_1(TEXT) == frequency_2(TEXT)
</code></pre>
<blockquote>
<p><strong>说明</strong>：这里的 <code>assert</code> 就是流程实践的一部分。一个更快但结果错误的函数毫无意义。虽然你在最终文章里看不到这些断言，但它们在开发时帮我抓出了不少bug。</p>
</blockquote>
<p>基准测试（也是流程实践的一环）显示，这个优化确实让代码更快了：</p>
<p>| <code>frequency_1(TEXT)</code> | 34,592.5 µs |<br>
| <code>frequency_2(TEXT)</code> | 25,798.6 µs |</p>
<h3 id="针对特定数据和目标进行优化">针对特定数据和目标进行优化</h3>
<p>我们继续用效率实践，这次针对具体目标和数据进一步优化。来看下最新代码的性能分析：</p>
<pre><code>Line #      Hits   % Time  Line Contents
========================================
     3                     def frequency_2(text):
     4         1      0.0      split_counts = defaultdict(lambda: 0)
     5    433070     33.6      for character in text:
     6    433069     32.7          if character.isalpha():
     7    339470     33.7              split_counts[character] += 1
     8
     9         1      0.0      counts = defaultdict(lambda: 0)
    10        53      0.0      for character, num in split_counts.items():
    11        52      0.0          counts[character.lower()] += num
    12         1      0.0      return counts
</code></pre>
<p>可以看到，<code>split_counts[character] += 1</code> 依然是耗时大户。怎么加速？答案是用 <code>list</code> 替换 <code>defaultdict</code>（本质上是 <code>dict</code>）。<code>list</code> 的索引速度远快于 <code>dict</code>：</p>
<ul>
<li><code>list</code> 存储条目只需一次数组索引</li>
<li><code>dict</code> 需要计算哈希、可能多次比较，还要内部数组索引</li>
</ul>
<p>但 <code>list</code> 的索引必须是整数，不能像 <code>dict</code> 那样用字符串，所以我们要把字符转成数字。幸运的是，每个字符都能用 <code>ord()</code> 查到数值：</p>
<pre><code class="language-python">ord('a'), ord('z'), ord('A'), ord('Z')
# (97, 122, 65, 90)
</code></pre>
<p>用 <code>chr()</code> 还能把数值转回字符：</p>
<pre><code class="language-python">chr(97), chr(122)
# ('a', 'z')
</code></pre>
<p>所以可以用 <code>my_list[ord(character)] += 1</code> 计数。但前提是我们得提前知道 <code>list</code> 的大小。如果处理任意字母字符，<code>list</code> 可能会很大：</p>
<pre><code class="language-python">ideograph = '𫞉'
ord(ideograph), ideograph.isalpha()
# (178057, True)
</code></pre>
<p>再回顾下我们的目标：</p>
<ol>
<li>处理对象是<strong>英文文本</strong>，这是题目要求。</li>
<li>输出结果里确实有少量非标准英文字母（如 'à'），但极其罕见。（严格说 'à' 应该归为 'a'，但这里偷懒没做……）</li>
<li>我们只关心<strong>相对频率</strong>，不是绝对精确计数。</li>
</ol>
<p>基于这些，我决定简化问题：<strong>只统计 'A' 到 'Z'，其他字符都忽略</strong>，包括带重音的。对英文文本来说，这几乎不影响字母相对频率。</p>
<p>这样问题就简单了：字符集有限且已知，可以放心用 <code>list</code> 替代 <code>dict</code>！</p>
<p>优化后实现如下：</p>
<pre><code class="language-python">def frequency_3(text):
    # 创建长度为128的零列表；ord('z')是122，128足够了
    split_counts = [0] * 128
    for character in text:
        index = ord(character)
        if index &lt; 128:
            split_counts[index] += 1

    counts = {}
    for letter in 'abcdefghijklmnopqrstuvwxyz':
        counts[letter] = (
            split_counts[ord(letter)] +
            split_counts[ord(letter.upper())]
        )
    return counts
</code></pre>
<p>由于输出只包含A到Z，正确性检查也要稍作调整：</p>
<pre><code class="language-python">def assert_matches(counts1, counts2):
    """确保A到Z的计数匹配"""
    for character in 'abcdefghijklmnopqrstuvwxyz':
        assert counts1[character] == counts2[character]

assert_matches(
    frequency_1(TEXT),
    frequency_3(TEXT)
)
</code></pre>
<p>新实现更快了：</p>
<p>| <code>frequency_2(TEXT)</code> | 25,965.5 µs |<br>
| <code>frequency_3(TEXT)</code> | 19,443.5 µs |</p>
<h2 id="编译实践切换到更快的语言">编译实践：切换到更快的语言</h2>
<p>接下来我们切换到编译型语言——Rust。</p>
<p>其实可以直接把 <code>frequency_1()</code> 移植到 Rust，编译器会自动做一些在 Python 里需要手动优化的事。</p>
<p>但大多数时候，无论用什么语言，<strong>效率实践</strong>都得靠你自己。这也是为什么“效率”和“编译”是两种不同的实践：它们带来的性能提升来源不同。我们在 <code>frequency_2()</code> 和 <code>frequency_3()</code> 里做的优化，同样能让 Rust 代码更快。</p>
<p>为证明这一点，我把上面三个 Python 函数都移植到了 Rust（前两个源码可点击展开查看）：<a href="https://weekly.pythoncat.top" target="_blank" rel="noopener nofollow">🦄</a></p>
<details>
<summary>前两个版本在 Rust 中的实现</summary>
<pre><code class="language-rust">#[pyfunction]
fn frequency_1_rust(
    text: &amp;str,
) -&gt; PyResult&lt;HashMap&lt;char, u32&gt;&gt; {
    let mut counts = HashMap::new();
    for character in text.chars() {
        if character.is_alphabetic() {
            *counts
                .entry(
                    character
                        .to_lowercase()
                        .next()
                        .unwrap_or(character),
                )
                .or_default() += 1;
        }
    }
    Ok(counts)
}

#[pyfunction]
fn frequency_2_rust(
    text: &amp;str,
) -&gt; PyResult&lt;HashMap&lt;char, u32&gt;&gt; {
    let mut split_counts: HashMap&lt;char, u32&gt; =
        HashMap::new();
    for character in text.chars() {
        if character.is_alphabetic() {
            *split_counts.entry(character).or_default() +=
                1;
        }
    }

    let mut counts = HashMap::new();
    for (character, num) in split_counts.drain() {
        *counts
            .entry(
                character
                    .to_lowercase()
                    .next()
                    .unwrap_or(character),
            )
            .or_default() += num;
    }
    Ok(counts)
}
</code></pre>
</details>
<p>第三个版本在 Rust 里的样子：</p>
<pre><code class="language-rust">fn ascii_arr_to_letter_map(
    split_counts: [u32; 128],
) -&gt; HashMap&lt;char, u32&gt; {
    let mut counts: HashMap&lt;char, u32&gt; = HashMap::new();
    for index in ('a' as usize)..=('z' as usize) {
        let character =
            char::from_u32(index as u32).unwrap();
        let upper_index =
            character.to_ascii_uppercase() as usize;
        counts.insert(
            character,
            split_counts[index] + split_counts[upper_index],
        );
    }
    counts
}

#[pyfunction]
fn frequency_3_rust(text: &amp;str) -&gt; HashMap&lt;char, u32&gt; {
    let mut split_counts = [0u32; 128];
    for character in text.chars() {
        let character = character as usize;
        if character &lt; 128 {
            split_counts[character] += 1;
        }
    }

    ascii_arr_to_letter_map(split_counts)
}
</code></pre>
<p>所有三个 Rust 版本的结果都和 Python 版本一致：</p>
<pre><code class="language-python">assert_matches(frequency_1(TEXT), frequency_1_rust(TEXT))
assert_matches(frequency_1(TEXT), frequency_2_rust(TEXT))
assert_matches(frequency_1(TEXT), frequency_3_rust(TEXT))
</code></pre>
<p>对所有6个版本做基准测试，清楚地说明了<strong>效率实践</strong>和<strong>编译实践</strong>的性能优势是<strong>不同且互补的</strong>。能加速 Python 代码的效率优化，同样也能加速 Rust 代码。</p>
<table>
<thead>
<tr>
<th>函数</th>
<th>运行时间 (µs)</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>frequency_1(TEXT)</code></td>
<td>33,741.5</td>
</tr>
<tr>
<td><code>frequency_2(TEXT)</code></td>
<td>25,797.4</td>
</tr>
<tr>
<td><code>frequency_3(TEXT)</code></td>
<td>19,432.0</td>
</tr>
<tr>
<td><code>frequency_1_rust(TEXT)</code></td>
<td>3,704.3</td>
</tr>
<tr>
<td><code>frequency_2_rust(TEXT)</code></td>
<td>3,504.8</td>
</tr>
<tr>
<td><code>frequency_3_rust(TEXT)</code></td>
<td><strong>204.9</strong></td>
</tr>
</tbody>
</table>
<p>一句话：效率和编译是两种不同的速度来源。</p>
<h2 id="并行化实践榨干多核cpu">并行化实践：榨干多核CPU</h2>
<p>到目前为止，代码都只跑在单核CPU上。但现在的电脑大多有多核，利用并行计算又是<strong>另一种</strong>速度来源，所以它也是独立的实践。</p>
<p>下面是用 <a href="https://docs.rs/rayon/latest/rayon/" target="_blank" rel="noopener nofollow">Rayon 库</a> 实现的 Rust 并行版本：</p>
<pre><code class="language-rust">fn sum(mut a: [u32; 128], b: [u32; 128]) -&gt; [u32; 128] {
    for i in 0..128 {
        a[i] += b[i];
    }
    a
}

#[pyfunction]
fn frequency_parallel_rust(
    py: Python&lt;'_&gt;,
    text: &amp;str,
) -&gt; HashMap&lt;char, u32&gt; {
    use rayon::prelude::*;

    // 确保释放全局解释器锁（GIL）
    let split_counts = py.allow_threads(|| {
        // 一个榨取 Rayon 更多性能的技巧：
        // 我们关心的 ASCII 字符总是由单个字节明确表示。
        // 所以直接处理字节是安全的，这能让我们强制 Rayon 使用数据块。
        text.as_bytes()
            // 并行迭代数据块
            .par_chunks(8192)
            .fold_with(
                [0u32; 128],
                |mut split_counts, characters| {
                    for character in characters {
                        if *character &lt; 128 {
                            split_counts
                                [*character as usize] += 1;
                        };
                    }
                    split_counts
                },
            )
            // 合并所有数据块的结果
            .reduce(|| [0u32; 128], sum)
    });
    ascii_arr_to_letter_map(split_counts)
}
</code></pre>
<p>结果依然正确：</p>
<pre><code class="language-python">assert_matches(frequency_1(TEXT), frequency_parallel_rust(TEXT))
</code></pre>
<p>加速效果如下：</p>
<p>| <code>frequency_3_rust(TEXT)</code> | 234.5 µs |<br>
| <code>frequency_parallel_rust(TEXT)</code> | <strong>105.3 µs</strong> |</p>
<h2 id="流程重访我们测对了吗">流程重访：我们测对了吗？</h2>
<p>最终函数快了330倍……真的吗？</p>
<p>我们是通过多次调用函数取平均运行时间来测量性能的。但我恰好知道一些背景知识：</p>
<ul>
<li>Rust 字符串是 UTF-8，Python 用的是自己的内部格式，<strong>不是</strong> UTF-8。</li>
<li>所以调用 Rust 函数时，Python 需要把字符串转成 UTF-8。</li>
<li>Python 用特定 API 转 UTF-8 时会<a href="https://docs.python.org/3/c-api/unicode.html#c.PyUnicode_AsUTF8AndSize" target="_blank" rel="noopener nofollow"><strong>缓存</strong>转换结果</a>。</li>
</ul>
<p>这意味着，我们很可能没测到 UTF-8 转换的成本，因为反复对同一个 <code>TEXT</code> 字符串基准测试，第一次后 UTF-8 版本就被缓存了。真实场景下，未必总有缓存。</p>
<p>我们可以测下<strong>单次</strong>调用新字符串的耗时。我用非并行版本，因为它速度更稳定：</p>
<pre><code class="language-python">from time import time

def timeit(f, *args):
    start = time()
    f(*args)
    print("Elapsed:", int((time() - start) * 1_000_000), "µs")

print("Original text")
timeit(frequency_3_rust, TEXT)
timeit(frequency_3_rust, TEXT)
print()

for i in range(3):
    # 新字符串
    s = TEXT + str(i)
    print("New text", i + 1)
    timeit(frequency_3_rust, s)
    timeit(frequency_3_rust, s)
    print()
</code></pre>
<pre><code>Original text
Elapsed: 212 µs
Elapsed: 206 µs

New text 1
Elapsed: 769 µs
Elapsed: 207 µs

New text 2
Elapsed: 599 µs
Elapsed: 202 µs

New text 3
Elapsed: 625 µs
Elapsed: 200 µs
</code></pre>
<p>对于新字符串，第一次运行比第二次慢了大约 400µs，这很可能就是转换为 UTF-8 的成本。<a href="https://weekly.pythoncat.top" target="_blank" rel="noopener nofollow">🦄</a></p>
<p>当然，我们加载的书<strong>本身就是 UTF-8 格式</strong>。所以，我们可以改变 API，直接将 UTF-8 编码的 <code>bytes</code> 传递给 Rust 代码，而不是先加载到 Python（转换为 Python 字符串），再传递给 Rust（转换回 UTF-8），这样就能避免转换开销。</p>
<p>我实现了一个新函数 <code>frequency_3_rust_bytes()</code>，它接受 UTF-8 编码的字节（源码略，与 <code>frequency_3_rust()</code> 基本一样）。然后测了下单个字节串第一次和第二次的时间：</p>
<pre><code class="language-python">with open("northanger_abbey.txt", "rb") as f:
    TEXT_BYTES = f.read()

assert_matches(
    frequency_1(TEXT),
    frequency_3_rust_bytes(TEXT_BYTES)
)

print("新文本不再有~400µs的转换开销：")
new_text = TEXT_BYTES + b"!"
timeit(frequency_3_rust_bytes, new_text)
timeit(frequency_3_rust_bytes, new_text)
</code></pre>
<pre><code>新文本不再有~400µs的转换开销：
Elapsed: 186 µs
Elapsed: 182 µs
</code></pre>
<p>如果我们测量持续的平均时间，可以看到它与之前的版本大致相当：</p>
<p>| <code>frequency_3_rust(TEXT)</code> | 227.2 µs |<br>
| <code>frequency_3_rust_bytes(TEXT_BYTES)</code> | 183.8 µs |</p>
<p>可见传入 <code>bytes</code> 确实能绕过 UTF-8 转换成本。你可能还想实现 <code>frequency_parallel_rust_bytes()</code>，这样并行也能无转换开销。</p>
<h2 id="补充那么-collectionscounter-呢">补充：那么 <code>collections.Counter</code> 呢？</h2>
<p>你可能会问，Python 标准库里不是有现成的 <code>collections.Counter</code> 吗？它是专门计数的 <code>dict</code> 子类。</p>
<pre><code class="language-python"># 来自 Python 3.13 的 collections/__init__.py
def _count_elements(mapping, iterable):
    'Tally elements from the iterable.'
    mapping_get = mapping.get
    for elem in iterable:
        mapping[elem] = mapping_get(elem, 0) + 1

try:
    # 如果可用，加载 C 语言实现的辅助函数
    from _collections import _count_elements
except ImportError:
    pass

class Counter(dict):
    # ...
</code></pre>
<p>我们可以这样使用它：</p>
<pre><code class="language-python">from collections import Counter

def frequency_counter(text):
    return Counter(c.lower() for c in text if c.isalpha())

# 注意：这里的实现与原文略有不同，是为了与 frequency_1 保持完全一致的行为
# 原文的 Counter(text.lower()) 会统计非字母字符，导致结果不一致
assert_matches(frequency_1(TEXT), frequency_counter(TEXT))
</code></pre>
<p>这个实现比我们的第一个版本更简洁，但性能如何？</p>
<p>| <code>frequency_1(TEXT)</code> | 34,592.5 µs |<br>
| <code>frequency_counter(TEXT)</code> | 约 30,000 µs |</p>
<p><code>Counter</code> 确实比我们的初始实现快点，但远不如最终优化版。这说明：<strong>即使标准库的优化实现，也可能比不上针对场景深度优化的代码。</strong></p>
<p>当然，<code>Counter</code> 胜在简洁和可读性。很多对性能没极致要求的场景，这种权衡完全值得。</p>
<h2 id="性能实践相辅相成">性能实践：相辅相成</h2>
<p>全文其实一直在用“流程”实践：测试新版本正确性、做性能分析和测量。基准测试还帮我排除了不少无效优化，这里就不赘述了。</p>
<p>“效率”实践帮我们消除无用功，“编译”让代码更快，“并行化”则让多核CPU火力全开。每种实践都是独特的、能带来乘数效应的速度来源。</p>
<p>一句话：如果你想让代码更快，别只盯着一种实践，多管齐下，速度才会飞起来！</p>
<hr>
<p>Python猫注：如果你喜欢这篇文章，那我要向你推荐一下 <a href="https://weekly.pythoncat.top" target="_blank" rel="noopener nofollow">Python 潮流周刊</a>！创刊仅两年，我们已坚持分享了超过 1300+ 篇优质文章，以及 1200+ 个开源项目或工具资源，每周精选，助力你打破信息差，告别信息过载，成为更优秀的人！</p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0" data-date-updated="2025-07-09 23:04">2025-07-09 23:04</span>&nbsp;
<a href="https://www.cnblogs.com/pythonista">豌豆花下猫</a>&nbsp;
阅读(<span id="post_view_count">0</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18975905);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18975905', targetLink: 'https://www.cnblogs.com/pythonista/p/18975905', title: '让 Python 代码飙升330倍：从入门到精通的四种性能优化实践' })">举报</a>
</div>
        