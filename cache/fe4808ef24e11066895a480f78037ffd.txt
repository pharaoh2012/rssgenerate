
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/zhangmingcheng/p/18914100" title="发布于 2025-06-06 15:35">
    <span role="heading" aria-level="2">Token：大语言模型的“语言乐高”，一切智能的基石</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body blogpost-body-html">
<h1>1、什么是Token？——AI眼中的“文字积木块”</h1>
<p>　　Token 是模型用来表示自然语言文本的基本单位，也是模型的计费单元，可以直观的理解为“字”或“词”；通常 1 个中文词语、1 个英文单词、1 个数字或 1 个符号计为 1 个 token。</p>
<p>一般情况下模型中 token 和字数的换算比例大致如下：</p>
<ul>
<li>1 个英文字符 ≈ 0.3 个 token。</li>
<li>1 个中文字符 ≈ 0.6 个 token。</li>
</ul>
<p><strong>但因为不同模型的分词不同，所以换算比例也存在差异，每一次实际处理 token 数量以模型返回为准。</strong></p>
<blockquote>
<p><strong>Token核心本质：</strong></p>
<p>Token并非简单的字符或单词，而是模型通过分词器（Tokenizer）对文本智能拆解后的语义片段：</p>
<ul>
<li>✅ 英文示例："unbelievable" → 拆为 ["un", "belie", "able"]（3个Token）</li>
<li>✅ 中文示例："人工智能" → 可能拆为 ["人", "工", "智能"]（3个Token）或 ["人工", "智能"]（2个Token）</li>
</ul>
</blockquote>
<h1>2、分词器差异：同一文本在不同模型中的「千面解析」</h1>
<h2>​​2.1 主流分词算法对比​​&nbsp;</h2>
<table align="left">
<thead>
<tr><th>算法</th><th>代表模型</th><th>中文处理特点</th><th>案例对比（“人工智能”）</th></tr>
</thead>
<tbody>
<tr>
<td>​​BPE​​</td>
<td>GPT系列</td>
<td>优先拆分子词</td>
<td><code class="hyc-common-markdown__code__inline">["人","工","智","能"]</code>（4 Token）</td>
</tr>
<tr>
<td>​​WordPiece​​</td>
<td>BERT</td>
<td>合并高频词对</td>
<td><code class="hyc-common-markdown__code__inline">["人工","智能"]</code>（2 Token）</td>
</tr>
<tr>
<td>​​Unigram​​</td>
<td>T5/ALBERT</td>
<td>概率保留完整词</td>
<td><code class="hyc-common-markdown__code__inline">["人工智能"]</code>（1 Token）<br><br></td>


</tr>


</tbody>

</table>
<h2>&nbsp;</h2>
<h2>2.2 在线工具实时验证​​</h2>
<p><span class="hyc-common-markdown__link"><span class="hyc-common-markdown__link"><span class="hyc-common-markdown__link__content">TikTokenizer可视化平台​​：<a href="https://tiktokenizer.vercel.app/" target="_blank" rel="noopener nofollow">https://tiktokenizer.vercel.app/</a></span></span></span><span class="hyc-common-markdown__link"><span class="hyc-common-markdown__link"><span class="hyc-common-markdown__link__content"><br></span></span></span><span class="hyc-common-markdown__link"><span class="hyc-common-markdown__link"><span class="hyc-common-markdown__link__content">输入任意文本，即时对比GPT-4、Claude、Llama等模型的分词差异：</span></span></span></p>
<blockquote>
<p>示例输入：“自然语言处理”</p>
<p>GPT-4：["自","然","语","言","处理"]（5 Token）<br>DeepSeek-R1：["自然","语言","处理"]（3 Token）</p>







</blockquote>
<h1>3、Token如何工作？——从文字到智能的三步转化</h1>
<h2><strong>​​3.1 分词（Tokenization）​​</strong></h2>







文本通过算法（如BPE、WordPiece）被拆解为Token序列。例如：<br><code class="hyc-common-markdown__code__inline">"你好！"</code>&nbsp;→ Token序列&nbsp;<code class="hyc-common-markdown__code__inline">["你", "好", "!"]</code>&nbsp;→ 数字ID&nbsp;<code class="hyc-common-markdown__code__inline">[128, 56, 0]</code>&nbsp;
<h2>​​3.2 向量化（Embedding）</h2>
<p>​​每个Token ID映射为高维向量（如768维），承载语义信息。例如：<br>"猫" → 向量 [0.039, -0.055, ..., -0.035]（模型真正“理解”的数学表达）</p>
<h2>​​3.3 预测生成（Autoregression）​​</h2>
<p>模型基于上下文Token预测下一个Token概率：<br>"今天天气_" → 预测"晴"（80%）、"雨"（15%） → 选择最高概率输出</p>
<p><img src="https://img2024.cnblogs.com/blog/624219/202506/624219-20250606155049825-676332726.png" alt="" loading="lazy"></p>
<div class="hyc-common-markdown__ref-list">
<div class="hyc-common-markdown__ref-list__trigger" data-num="6" data-idx-list="1,6" data-source-type="mp" data-web-site-name="闲余说">
<div class="hyc-common-markdown__ref-list__item" data-hy-exposured="1">
<h1>4、Token为何如此重要？——成本、性能与能力的核心标尺</h1>
<div class="hyc-common-markdown__table-wrapper">
<table align="left">
<thead>
<tr><th>​​影响维度​​</th><th>​​典型场景​​</th></tr>



</thead>
<tbody>
<tr>
<td>​​计算成本​​</td>
<td>API按Token计费（如GPT-4：输入<span class="katex"><span class="katex-html"><span class="base"><span class="strut"><span class="mord">0.03/<span class="mord cjk_fallback">千<span class="mord mathnormal">T<span class="mord mathnormal">o<span class="mord mathnormal">k<span class="mord mathnormal">e<span class="mord mathnormal">n<span class="mord cjk_fallback">，输出0.06/千Token）</span></span></span></span></span></span></span></span></span></span></span></span></td>



</tr>
<tr>
<td>​​上下文限制​​</td>
<td>模型记忆上限由Token数决定（如GPT-4 Turbo=128K Token≈9.6万汉字）</td>



</tr>
<tr>
<td>​​语言效率差异​​</td>
<td>相同内容中文Token数≈英文1.5–2倍（例：1000汉字≈400-500 Token）</td>



</tr>
<tr>
<td>​​生成质量​​</td>
<td>超出上下文限制会导致“记忆截断”（如长文档后半部分被遗忘）</td>



</tr>



</tbody>



</table>



&nbsp;</div>
<h1>5、Token 用量与成本计费</h1>
<h2>5.1 用量组成​​</h2>
<p><strong>单次 API 调用的 Token 总量 = ​​输入 Token（Prompt） + 输出 Token（Completion）​​。</strong></p>
<p>示例：输入 50 Token，输出 150 Token，则总量为 200 Token。</p>
<h2>​​5.2 计费规则​​</h2>
<p>主流模型按千 Token（1K Tokens）计价，输入/输出费率不同：​​&nbsp;</p>
<table align="left">
<thead>
<tr><th>模型</th><th>输入单价（/1K Tokens）</th><th>输出单价（/1K Tokens）</th></tr>



</thead>
<tbody>
<tr>
<td>GPT-4 Turbo</td>
<td>$0.01</td>
<td>$0.03</td>



</tr>
<tr>
<td>GPT-3.5 Turbo</td>
<td>$0.0015</td>
<td>$0.002</td>



</tr>
<tr>
<td>国产模型（如 DeepSeek）</td>
<td>几厘至几分人民币</td>
<td>几厘至几分人民币<br><br></td>




</tr>




</tbody>



</table>
<h2>5.3 多轮对话的累积消耗​​</h2>
<p>上下文历史会持续占用 Token，导致单轮成本递增：</p>
<p>第 1 轮：输入 50 + 输出 100 = 150 Token</p>
<p>第 2 轮：新输入 50 + 新输出 100 + 历史 150 = 300 Token</p>
<p>​​若不限制上下文​​，10 轮对话可能累积 3000 Token，成本显著上升。</p>
<h1>6、Token优化实战技巧——让AI更高效省钱</h1>
<h2>​​6.1 精简输入</h2>
<p>❌ 冗余表达："我需要一个关于机器学习基础知识的详细解释"（20 Token）<br>✅ 优化后："解释机器学习基础"（7 Token，省65%）</p>
<h2>​​6.2 术语压缩​​</h2>
<p>用"NLP"替代"自然语言处理"（3 Token → 1 Token）</p>
<h2>​​6.3 长文本处理​​</h2>
<ol>
<li>分段输入（每段≤模型上下文上限）</li>
<li>关键信息前置，避免截断风险</li>







</ol>
<h2>​​6.4 生僻字避坑</h2>
<p>"饕餮"（4 Token）→ 改用"神兽"（2 Token）&nbsp;</p>
<h1>6、Token的未来：多模态统一与行业革新</h1>
<ul>
<li>​​跨模态扩展​​：图片、音频正被Token化（如DALL·E将图像转为1024 Token序列）&nbsp;</li>






</ul>
<ul>
<li>垂直领域优化：医疗/法律等专业领域可定制分词器，将术语保留为单一Token（如"冠状动脉"）</li>






</ul>
<ul>
<li>​​认知边界突破​​：Google实验证明，统一Token化文本、图像、坐标数据，使AI具备跨任务能力</li>






</ul>
<h1>7、结语：Token是AI世界的“通用货币”。</h1>






</div>






</div>






</div>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.021060007069444445" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-06-06 15:51">2025-06-06 15:35</span>&nbsp;
<a href="https://www.cnblogs.com/zhangmingcheng">人艰不拆_zmc</a>&nbsp;
阅读(<span id="post_view_count">36</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18914100);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18914100', targetLink: 'https://www.cnblogs.com/zhangmingcheng/p/18914100', title: 'Token：大语言模型的“语言乐高”，一切智能的基石' })">举报</a>
</div>
        