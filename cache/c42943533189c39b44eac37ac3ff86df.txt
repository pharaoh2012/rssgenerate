
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/zhaoweiwei/p/18990144/IndexTTS" title="发布于 2025-07-21 12:31">
    <span role="heading" aria-level="2">也玩音频克隆IndexTTS</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                    <div id="cnblogs_post_description" style="display: none">
        
        本来详细介绍了开源语音生成项目index-tts在纯windows环境及WSL下的安装使用说明，其中在WSL下能通过GPU加速，实现高速语音生成。
    </div>
<div id="cnblogs_post_body" class="blogpost-body blogpost-body-html">
<p>以下两篇文章已经较好的介绍IndexTTS项目，本文基于这两篇文件对该项目安装实现的一些细节再做更详细的说明，也算对自己的折腾做下总结。</p>
<p><a href="https://www.cnblogs.com/cj8988/p/18973016" target="_blank">https://www.cnblogs.com/cj8988/p/18973016</a>&nbsp;（windows conda）</p>
<p><a href="https://www.cnblogs.com/h5l0/p/18907633" target="_blank">https://www.cnblogs.com/h5l0/p/18907633</a>&nbsp;（windows wsl）</p>
<h1>1 基于conda环境</h1>
<h2>1.1 项目安装</h2>
<h3>1. 照例显示下本地软硬件环境</h3>
<p><img src="https://img2024.cnblogs.com/blog/465567/202507/465567-20250717181107981-814891707.png" alt="" width="618" height="348" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<h3>2. 从github上下载项目</h3>
<div class="cnblogs_code">
<pre>git clone https:<span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)">github.com/index-tts/index-tts.git</span></pre>
</div>
<h3>3. 创建环境并安装依赖</h3>
<div class="cnblogs_code">
<pre>conda create -n index-tts python=<span style="color: rgba(128, 0, 128, 1)">3.10</span><span style="color: rgba(0, 0, 0, 1)">
conda activate index</span>-<span style="color: rgba(0, 0, 0, 1)">tts

pip </span><span style="color: rgba(0, 0, 255, 1)">install</span> torch torchvision torchaudio --index-url https:<span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)">download.pytorch.org/whl/cu128</span>
pip <span style="color: rgba(0, 0, 255, 1)">install</span> -r requirements.txt</pre>
</div>
<p>因为是windows环境，这里和项目官方安装稍有区别，没有安装ffmpeg（如遇问题请参考后续小节解决方案），且是根据nvidia-smi显示的驱动信息安装的相对应版本的pytorch。pytorch安装过程信息如下：</p>
<p><img src="https://img2024.cnblogs.com/blog/465567/202507/465567-20250717181852878-1150567096.png" alt="" width="926" height="543" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<p>从安装过程信息可知安装的pytorch版本是2.7.1。在安装requirements.txt时，正如官网所说在windows下安装pynini时会提示错误：ERROR: Failed building wheel for pynini</p>
<p><img src="https://img2024.cnblogs.com/blog/465567/202507/465567-20250717182418756-461676997.png" alt="" width="1282" height="176" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<p>仍按官网解决方式进行conda安装：</p>
<div class="cnblogs_code">
<pre># after conda activate index-<span style="color: rgba(0, 0, 0, 1)">tts
conda </span><span style="color: rgba(0, 0, 255, 1)">install</span> -c conda-forge pynini==<span style="color: rgba(128, 0, 128, 1)">2.1</span>.<span style="color: rgba(128, 0, 128, 1)">6</span><span style="color: rgba(0, 0, 0, 1)">
pip </span><span style="color: rgba(0, 0, 255, 1)">install</span> WeTextProcessing --no-deps</pre>
</div>
<h3>4. 将项目以 “开发模式” 安装到当前 Python 环境</h3>
<div class="cnblogs_code">
<pre>cd index-<span style="color: rgba(0, 0, 0, 1)">tts
pip </span><span style="color: rgba(0, 0, 255, 1)">install</span> -e .</pre>
</div>
<p>执行后，Python环境会在site-packages目录中创建一个特殊的链接（<code>.egg-link</code>文件），指向 index-tts项目的根目录，使得你在任何地方都能像导入普通包一样导入该项目的模块（例如import index_tts），命令还会根据项目根目录下的setup.py或pyproject.toml中的配置，自动安装项目所需的依赖包。</p>
<p><strong>关键参数说明：</strong><br>-e：是 --editable 的缩写，指定 “可编辑模式”。<br>.：表示当前目录，即告诉 pip 从当前目录的 setup.py 或 pyproject.toml 读取包的配置信息。</p>
<p>这种安装方式常用于开发阶段，如果你只是想使用该包而不修改源码，直接用pip install .即可（非编辑模式，源码修改后需重新安装才会生效，否则其他项目import index_tts时仍使用的是之前安装）。</p>
<h3>5. 下载模型</h3>
<p>通过huggingface-cli工具下载，如果该工具未安装，请使用pip install huggingface-hub进行安装，huggingface-cli是huggingface-hub包的一部分，安装该包后即可使用huggingface-cli命令。</p>
<div class="cnblogs_code">
<pre>huggingface-cli download IndexTeam/IndexTTS-<span style="color: rgba(128, 0, 128, 1)">1.5</span> config.yaml bigvgan_discriminator.pth bigvgan_generator.pth bpe.model dvae.pth gpt.pth unigram_12000.vocab --local-<span style="color: rgba(0, 0, 255, 1)">dir</span> checkpoints</pre>
</div>
<p>可见命令会将模型下载到当前目录的checkpoints下：</p>
<p><img src="https://img2024.cnblogs.com/blog/465567/202507/465567-20250717184017787-710987164.png" alt="" width="454" height="250" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<p>项目官方后续还介绍了对于中国用户如何使用镜像加速下载，这里并没有实际尝试，而是直接使用的代理进行下载（涉及github的项目不配置代理基本没法操作，这里鄙视下国内的网络环境）。</p>
<h3>6. 运行Web Demo</h3>
<p>&nbsp;接下并没有运行测试脚本，而是直接运行了Web Demo：</p>
<div class="cnblogs_code">
<pre>pip <span style="color: rgba(0, 0, 255, 1)">install</span> -e <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">.[webui]</span><span style="color: rgba(128, 0, 0, 1)">"</span> --no-build-<span style="color: rgba(0, 0, 0, 1)">isolation
python webui.py

# use another model version:
python webui.py </span>--model_dir IndexTTS-<span style="color: rgba(128, 0, 128, 1)">1.5</span></pre>
</div>
<p>直接访问http://127.0.0.1:7860即可加载该项目，最左边上传样例声音文件，中间文本是要进行样例声音克隆的文本信息，单击生成语音即可在最右边产生出克隆声音。</p>
<p><img src="https://img2024.cnblogs.com/blog/465567/202507/465567-20250717184858386-1773806124.png" alt="" width="1355" height="635" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<p>&nbsp;以下是声音样例下载：</p>
<table border="0">
<tbody>
<tr>
<td>源音频</td>
<td>克隆的音频</td>
</tr>
<tr>
<td><audio width="300" height="32" controls="controls"><source src="https://cdnhandler.fgonestudio.net/audios/test.mp3" type="audio/mpeg"> 您的浏览器不支持 audio 标签。</audio></td>
<td><audio width="300" height="32" controls="controls"><source src="https://cdnhandler.fgonestudio.net/audios/indetts.wav" type="audio/wav"> 您的浏览器不支持 audio 标签。</audio></td>
</tr>
</tbody>
</table>
<h2>1.2 其他问题</h2>
<h3>1. Deepspeed问题</h3>
<p>在上一节虽然已经运行起来，但是生成语音时明显感觉生成速度比较慢，第一感觉就是根本没有用上GPU，在后台一看果然有问题，说是DeepSpeed加载失败：</p>
<p><img src="https://img2024.cnblogs.com/blog/465567/202507/465567-20250717222407631-906439181.png" alt="" width="538" height="225" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<p>直接用pip install deepspeed进行安装，安装失败：</p>
<p><img src="https://img2024.cnblogs.com/blog/465567/202507/465567-20250717235936456-343014956.png" alt="" width="749" height="436" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<p>自动安装不成功，只能自己源码编译，这里下载<a href="https://github.com/deepspeedai/DeepSpeed/releases" target="_blank" rel="noopener nofollow">deepspeed</a>的0.17.0版本，以下是官网的在windows下编译的步骤：</p>
<div class="cnblogs_code">
<pre><span style="color: rgba(128, 0, 128, 1)">1</span> Install PyTorch, such as pytorch <span style="color: rgba(128, 0, 128, 1)">2.3</span>+<span style="color: rgba(0, 0, 0, 1)">cu121.
</span><span style="color: rgba(128, 0, 128, 1)">2</span> Install Visual C++ build tools, such as VS2022 C++ x64/<span style="color: rgba(0, 0, 0, 1)">x86 build tools.
</span><span style="color: rgba(128, 0, 128, 1)">3</span> Launch Cmd console with Administrator permissions <span style="color: rgba(0, 0, 255, 1)">for</span> creating required symlink folders and ensure MSVC tools are added to your PATH or launch the Developer Command Prompt <span style="color: rgba(0, 0, 255, 1)">for</span> Visual Studio <span style="color: rgba(128, 0, 128, 1)">2022</span><span style="color: rgba(0, 0, 0, 1)"> with administrator permissions.
</span><span style="color: rgba(128, 0, 128, 1)">4</span> Run build_win.bat to build wheel <span style="color: rgba(0, 0, 255, 1)">in</span> dist folder.</pre>
</div>
<p>参照编译步骤进行编译，最后产生deepspeed-0.17.0+unknown-cp311-cp311-win_amd64.whl安装文件，并对该文件进行安装。</p>
<p><img src="https://img2024.cnblogs.com/blog/465567/202507/465567-20250721095723139-1020323924.png" alt="" width="537" height="119" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<p>&nbsp;其实在编译deepspeed中会有两个错误提示，说是缺少aio.lib和cufile.lib，没有处理也能编译通过。之后重新运行web demo时虽然不再提示“deepspeed加载失败”，但仍会提示缺少两个文件的错误，感觉gpu还是有问题，随便找两句话进行语音生成，果然时间比较长，应该是GPU还是没有用上。</p>
<p><img src="https://img2024.cnblogs.com/blog/465567/202507/465567-20250721101235630-946069204.png" alt="" width="775" height="470" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<p>问了下deepseek，看来deepspeed对windows的支持还是不太行，如果哪位大神能搞定windows下gpu使用请在评论区告诉我一声。deepseek给出了其他解决方案，使用wsl，这正是本文第2节的内容。</p>
<p><img src="https://img2024.cnblogs.com/blog/465567/202507/465567-20250721102145407-322095665.png" alt="" width="520" height="384" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<h3>2. 缺失ffmpeg问题</h3>
<p>在折腾过程中，还有一次提示缺少ffmpeg的问题，顺便记录下，首先下载<a href="https://www.gyan.dev/ffmpeg/builds/" target="_blank" rel="noopener nofollow">ffmpeg</a>的gyan预编译版本，然后解压到本地，再配置path环境变量。</p>
<p><img src="https://img2024.cnblogs.com/blog/465567/202507/465567-20250721102822422-1982469887.png" alt="" width="975" height="389" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<h1>2 基于wsl环境</h1>
<h2>2.1 WSL安装及环境配置</h2>
<p>这部分内容不再详细说明，请参照本文最前面的第二个参考链接及之前的博文进行，这里使用的Linux版本是Ubuntu 22.04，并且已经在WSL可以访问到GPU。</p>
<p><img src="https://img2024.cnblogs.com/blog/465567/202507/465567-20250721104335475-1944116449.png" alt="" width="610" height="373" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<p>除此之外，为了能在Windows下能直接访问Ubuntu，需将Ubuntu的localhost暴露在Windows下：</p>
<p><img src="https://img2024.cnblogs.com/blog/465567/202507/465567-20250721104506616-1235275183.png" alt="" width="603" height="420" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<h2>2.2&nbsp;配置项目</h2>
<h3>1. clone项目</h3>
<p>使用如下命令先将index-tts-vllm项目clone下来：</p>
<div class="cnblogs_code">
<pre>git clone https:<span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)">github.com/Ksuriuri/index-tts-vllm.git</span></pre>
</div>
<p>然后进入项目目录，后文的工作目录都必须是此项目目录。</p>
<div class="cnblogs_code">
<pre>cd index-tts-vllm</pre>
</div>
<h3>2.&nbsp;配置uv虚拟环境</h3>
<p>使用以下指令，创建一个虚拟环境.venv，并激活该虚拟环境</p>
<div class="cnblogs_code">
<pre>uv .venv<br>source .venv/bin/activate</pre>
</div>
<p>执行完该命令会创建了名为.venv的虚拟环境，并在当前目录下创建.venv目录对该虚拟环境进行管理</p>
<p><img src="https://img2024.cnblogs.com/blog/465567/202507/465567-20250721105841760-622682325.png" alt="" width="525" height="420" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<p>安装python版本3.12：</p>
<div class="cnblogs_code">
<pre>uv python <span style="color: rgba(0, 0, 255, 1)">install</span> <span style="color: rgba(128, 0, 128, 1)">3.12</span></pre>
</div>
<h3>3.&nbsp;安装依赖包</h3>
<div>执行以下指令，在uv内安装包：</div>
<div>
<div class="cnblogs_code">
<pre>uv pip <span style="color: rgba(0, 0, 255, 1)">install</span> -r requirements.txt</pre>
</div>
<h3>4. 生成模型文件</h3>
<p>官方<a href="https://github.com/Ksuriuri/index-tts-vllm" target="_blank" rel="noopener nofollow">https://github.com/Ksuriuri/index-tts-vllm</a>给出了两个权重文件下载链接：</p>
<p><img src="https://img2024.cnblogs.com/blog/465567/202507/465567-20250721112217995-1981446706.png" alt="" width="321" height="100" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<p>这里选择第一进行下载，执行以下命令中的前两行即可，第3行命令是不下载大文件的操作方式，第4行命令是用huggingface-cli的下载方式（在本文中将权重文件IndexTTS-1.5下载到了和index-tts-vllm的同一级目录）：</p>
<div class="cnblogs_code">
<pre>git lfs <span style="color: rgba(0, 0, 255, 1)">install</span><span style="color: rgba(0, 0, 0, 1)">
git clone https:</span><span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)">huggingface.co/IndexTeam/IndexTTS-1.5</span>
<span style="color: rgba(0, 0, 0, 1)">
# If you want to clone without large files </span>-<span style="color: rgba(0, 0, 0, 1)"> just their pointers
GIT_LFS_SKIP_SMUDGE</span>=<span style="color: rgba(128, 0, 128, 1)">1</span> git clone https:<span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)">huggingface.co/IndexTeam/IndexTTS-1.5</span>
# Use huggingface-cli to download <span style="color: rgba(0, 0, 255, 1)">in</span><span style="color: rgba(0, 0, 0, 1)"> a specific directory
huggingface</span>-cli download IndexTeam/IndexTTS-<span style="color: rgba(128, 0, 128, 1)">1.5</span></pre>
</div>
<p>下载完成后重新进入到index-tts-vllm目录下执行如下命令，将官方的模型权重转换为 transformers 库兼容的版本，保存在模型权重路径下的vllm文件夹中，方便后续vllm库加载模型权重。</p>
<div class="cnblogs_code">
<pre>bash convert_hf_format.<span style="color: rgba(0, 0, 255, 1)">sh</span> /home/zww/IndexTTS-<span style="color: rgba(128, 0, 128, 1)">1.5</span>/</pre>
</div>
<p><img src="https://img2024.cnblogs.com/blog/465567/202507/465567-20250721114007424-525706000.png" alt="" width="497" height="185" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<h2>2.3 运行项目</h2>
<p>将webui.py中的model_dir修改为模型权重下载路径：</p>
<p><img src="https://img2024.cnblogs.com/blog/465567/202507/465567-20250721114114470-232821696.png" alt="" width="701" height="406" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<p>运行以下命令启动程序：</p>
<div class="cnblogs_code">
<pre>VLLM_USE_V1=<span style="color: rgba(128, 0, 128, 1)">0</span> python webui.py</pre>
</div>
<p>如官方所说，第一次启动可能会久一些，因为要对 bigvgan 进行 cuda 核编译，注：一定要带上VLLM_USE_V1=0，因为本项目没有对 vllm 的 v1 版本做兼容。启动log信息如下：</p>
<div class="cnblogs_code"><img src="https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif" id="code_img_closed_98d09187-b0b4-411d-badc-d3b658d8c21f" class="code_img_closed"><img src="https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif" id="code_img_opened_98d09187-b0b4-411d-badc-d3b658d8c21f" class="code_img_opened" style="display: none">
<div id="cnblogs_code_open_98d09187-b0b4-411d-badc-d3b658d8c21f" class="cnblogs_code_hide">
<pre>INFO <span style="color: rgba(128, 0, 128, 1)">07</span>-<span style="color: rgba(128, 0, 128, 1)">21</span> <span style="color: rgba(128, 0, 128, 1)">11</span>:<span style="color: rgba(128, 0, 128, 1)">44</span>:<span style="color: rgba(128, 0, 128, 1)">08</span> [__init__.py:<span style="color: rgba(128, 0, 128, 1)">243</span><span style="color: rgba(0, 0, 0, 1)">] Automatically detected platform cuda.
✅ Registry GPT2TTSModel to vllm
⚠️  SamplingParams._verify_args Patched
⚠️  ModelInputForGPUBuilder._compute_lens Patched
INFO </span><span style="color: rgba(128, 0, 128, 1)">07</span>-<span style="color: rgba(128, 0, 128, 1)">21</span> <span style="color: rgba(128, 0, 128, 1)">11</span>:<span style="color: rgba(128, 0, 128, 1)">44</span>:<span style="color: rgba(128, 0, 128, 1)">13</span> [__init__.py:<span style="color: rgba(128, 0, 128, 1)">31</span>] Available plugins <span style="color: rgba(0, 0, 255, 1)">for</span><span style="color: rgba(0, 0, 0, 1)"> group vllm.general_plugins:
INFO </span><span style="color: rgba(128, 0, 128, 1)">07</span>-<span style="color: rgba(128, 0, 128, 1)">21</span> <span style="color: rgba(128, 0, 128, 1)">11</span>:<span style="color: rgba(128, 0, 128, 1)">44</span>:<span style="color: rgba(128, 0, 128, 1)">13</span> [__init__.py:<span style="color: rgba(128, 0, 128, 1)">33</span>] - lora_filesystem_resolver -&gt;<span style="color: rgba(0, 0, 0, 1)"> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
INFO </span><span style="color: rgba(128, 0, 128, 1)">07</span>-<span style="color: rgba(128, 0, 128, 1)">21</span> <span style="color: rgba(128, 0, 128, 1)">11</span>:<span style="color: rgba(128, 0, 128, 1)">44</span>:<span style="color: rgba(128, 0, 128, 1)">13</span> [__init__.py:<span style="color: rgba(128, 0, 128, 1)">36</span>] All plugins <span style="color: rgba(0, 0, 255, 1)">in</span> this group will be loaded. Set `VLLM_PLUGINS` to control <span style="color: rgba(0, 0, 255, 1)">which</span><span style="color: rgba(0, 0, 0, 1)"> plugins to load.
INFO </span><span style="color: rgba(128, 0, 128, 1)">07</span>-<span style="color: rgba(128, 0, 128, 1)">21</span> <span style="color: rgba(128, 0, 128, 1)">11</span>:<span style="color: rgba(128, 0, 128, 1)">44</span>:<span style="color: rgba(128, 0, 128, 1)">14</span> [config.py:<span style="color: rgba(128, 0, 128, 1)">3131</span><span style="color: rgba(0, 0, 0, 1)">] Downcasting torch.float32 to torch.float16.
INFO </span><span style="color: rgba(128, 0, 128, 1)">07</span>-<span style="color: rgba(128, 0, 128, 1)">21</span> <span style="color: rgba(128, 0, 128, 1)">11</span>:<span style="color: rgba(128, 0, 128, 1)">44</span>:<span style="color: rgba(128, 0, 128, 1)">14</span> [config.py:<span style="color: rgba(128, 0, 128, 1)">793</span>] This model supports multiple tasks: {<span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">embed</span><span style="color: rgba(128, 0, 0, 1)">'</span>, <span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">generate</span><span style="color: rgba(128, 0, 0, 1)">'</span>, <span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">score</span><span style="color: rgba(128, 0, 0, 1)">'</span>, <span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">classify</span><span style="color: rgba(128, 0, 0, 1)">'</span>, <span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">reward</span><span style="color: rgba(128, 0, 0, 1)">'</span>}. Defaulting to <span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">generate</span><span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(0, 0, 0, 1)">.
INFO </span><span style="color: rgba(128, 0, 128, 1)">07</span>-<span style="color: rgba(128, 0, 128, 1)">21</span> <span style="color: rgba(128, 0, 128, 1)">11</span>:<span style="color: rgba(128, 0, 128, 1)">44</span>:<span style="color: rgba(128, 0, 128, 1)">14</span> [llm_engine.py:<span style="color: rgba(128, 0, 128, 1)">230</span>] Initializing a V0 LLM engine (v0.<span style="color: rgba(128, 0, 128, 1)">9.0</span>) with config: model=<span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">/home/zww/IndexTTS-1.5/vllm</span><span style="color: rgba(128, 0, 0, 1)">'</span>, speculative_config=None, tokenizer=<span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">/home/zww/IndexTTS-1.5/vllm</span><span style="color: rgba(128, 0, 0, 1)">'</span>, skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=<span style="color: rgba(128, 0, 128, 1)">803</span>, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=<span style="color: rgba(128, 0, 128, 1)">1</span>, pipeline_parallel_size=<span style="color: rgba(128, 0, 128, 1)">1</span>, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend=<span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">xgrammar</span><span style="color: rgba(128, 0, 0, 1)">'</span>, disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=<span style="color: rgba(128, 0, 0, 1)">''</span>), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/home/zww/IndexTTS-<span style="color: rgba(128, 0, 128, 1)">1.5</span>/vllm, num_scheduler_steps=<span style="color: rgba(128, 0, 128, 1)">1</span>, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">compile_sizes</span><span style="color: rgba(128, 0, 0, 1)">"</span>: [], <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">inductor_compile_config</span><span style="color: rgba(128, 0, 0, 1)">"</span>: {<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">enable_auto_functionalized_v2</span><span style="color: rgba(128, 0, 0, 1)">"</span>: <span style="color: rgba(0, 0, 255, 1)">false</span>}, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">cudagraph_capture_sizes</span><span style="color: rgba(128, 0, 0, 1)">"</span>: [<span style="color: rgba(128, 0, 128, 1)">256</span>, <span style="color: rgba(128, 0, 128, 1)">248</span>, <span style="color: rgba(128, 0, 128, 1)">240</span>, <span style="color: rgba(128, 0, 128, 1)">232</span>, <span style="color: rgba(128, 0, 128, 1)">224</span>, <span style="color: rgba(128, 0, 128, 1)">216</span>, <span style="color: rgba(128, 0, 128, 1)">208</span>, <span style="color: rgba(128, 0, 128, 1)">200</span>, <span style="color: rgba(128, 0, 128, 1)">192</span>, <span style="color: rgba(128, 0, 128, 1)">184</span>, <span style="color: rgba(128, 0, 128, 1)">176</span>, <span style="color: rgba(128, 0, 128, 1)">168</span>, <span style="color: rgba(128, 0, 128, 1)">160</span>, <span style="color: rgba(128, 0, 128, 1)">152</span>, <span style="color: rgba(128, 0, 128, 1)">144</span>, <span style="color: rgba(128, 0, 128, 1)">136</span>, <span style="color: rgba(128, 0, 128, 1)">128</span>, <span style="color: rgba(128, 0, 128, 1)">120</span>, <span style="color: rgba(128, 0, 128, 1)">112</span>, <span style="color: rgba(128, 0, 128, 1)">104</span>, <span style="color: rgba(128, 0, 128, 1)">96</span>, <span style="color: rgba(128, 0, 128, 1)">88</span>, <span style="color: rgba(128, 0, 128, 1)">80</span>, <span style="color: rgba(128, 0, 128, 1)">72</span>, <span style="color: rgba(128, 0, 128, 1)">64</span>, <span style="color: rgba(128, 0, 128, 1)">56</span>, <span style="color: rgba(128, 0, 128, 1)">48</span>, <span style="color: rgba(128, 0, 128, 1)">40</span>, <span style="color: rgba(128, 0, 128, 1)">32</span>, <span style="color: rgba(128, 0, 128, 1)">24</span>, <span style="color: rgba(128, 0, 128, 1)">16</span>, <span style="color: rgba(128, 0, 128, 1)">8</span>, <span style="color: rgba(128, 0, 128, 1)">4</span>, <span style="color: rgba(128, 0, 128, 1)">2</span>, <span style="color: rgba(128, 0, 128, 1)">1</span>], <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">max_capture_size</span><span style="color: rgba(128, 0, 0, 1)">"</span>: <span style="color: rgba(128, 0, 128, 1)">256</span>}, use_cached_outputs=<span style="color: rgba(0, 0, 0, 1)">False,
WARNING </span><span style="color: rgba(128, 0, 128, 1)">07</span>-<span style="color: rgba(128, 0, 128, 1)">21</span> <span style="color: rgba(128, 0, 128, 1)">11</span>:<span style="color: rgba(128, 0, 128, 1)">44</span>:<span style="color: rgba(128, 0, 128, 1)">14</span> [interface.py:<span style="color: rgba(128, 0, 128, 1)">344</span>] Using <span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">pin_memory=False</span><span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(0, 0, 0, 1)"> as WSL is detected. This may slow down the performance.
INFO </span><span style="color: rgba(128, 0, 128, 1)">07</span>-<span style="color: rgba(128, 0, 128, 1)">21</span> <span style="color: rgba(128, 0, 128, 1)">11</span>:<span style="color: rgba(128, 0, 128, 1)">44</span>:<span style="color: rgba(128, 0, 128, 1)">14</span> [cuda.py:<span style="color: rgba(128, 0, 128, 1)">292</span><span style="color: rgba(0, 0, 0, 1)">] Using Flash Attention backend.
INFO </span><span style="color: rgba(128, 0, 128, 1)">07</span>-<span style="color: rgba(128, 0, 128, 1)">21</span> <span style="color: rgba(128, 0, 128, 1)">11</span>:<span style="color: rgba(128, 0, 128, 1)">44</span>:<span style="color: rgba(128, 0, 128, 1)">15</span> [parallel_state.py:<span style="color: rgba(128, 0, 128, 1)">1064</span>] rank <span style="color: rgba(128, 0, 128, 1)">0</span> <span style="color: rgba(0, 0, 255, 1)">in</span> world size <span style="color: rgba(128, 0, 128, 1)">1</span> is assigned as DP rank <span style="color: rgba(128, 0, 128, 1)">0</span>, PP rank <span style="color: rgba(128, 0, 128, 1)">0</span>, TP rank <span style="color: rgba(128, 0, 128, 1)">0</span>, EP rank <span style="color: rgba(128, 0, 128, 1)">0</span><span style="color: rgba(0, 0, 0, 1)">
INFO </span><span style="color: rgba(128, 0, 128, 1)">07</span>-<span style="color: rgba(128, 0, 128, 1)">21</span> <span style="color: rgba(128, 0, 128, 1)">11</span>:<span style="color: rgba(128, 0, 128, 1)">44</span>:<span style="color: rgba(128, 0, 128, 1)">15</span> [model_runner.py:<span style="color: rgba(128, 0, 128, 1)">1170</span>] Starting to load model /home/zww/IndexTTS-<span style="color: rgba(128, 0, 128, 1)">1.5</span>/<span style="color: rgba(0, 0, 0, 1)">vllm...
Loading safetensors checkpoint shards:   </span><span style="color: rgba(128, 0, 128, 1)">0</span>% Completed | <span style="color: rgba(128, 0, 128, 1)">0</span>/<span style="color: rgba(128, 0, 128, 1)">1</span> [<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>&lt;?, ?it/<span style="color: rgba(0, 0, 0, 1)">s]
Loading safetensors checkpoint shards: </span><span style="color: rgba(128, 0, 128, 1)">100</span>% Completed | <span style="color: rgba(128, 0, 128, 1)">1</span>/<span style="color: rgba(128, 0, 128, 1)">1</span> [<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">01</span>&lt;<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>,  <span style="color: rgba(128, 0, 128, 1)">1</span>.40s/<span style="color: rgba(0, 0, 0, 1)">it]
Loading safetensors checkpoint shards: </span><span style="color: rgba(128, 0, 128, 1)">100</span>% Completed | <span style="color: rgba(128, 0, 128, 1)">1</span>/<span style="color: rgba(128, 0, 128, 1)">1</span> [<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">01</span>&lt;<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>,  <span style="color: rgba(128, 0, 128, 1)">1</span>.40s/<span style="color: rgba(0, 0, 0, 1)">it]

INFO </span><span style="color: rgba(128, 0, 128, 1)">07</span>-<span style="color: rgba(128, 0, 128, 1)">21</span> <span style="color: rgba(128, 0, 128, 1)">11</span>:<span style="color: rgba(128, 0, 128, 1)">44</span>:<span style="color: rgba(128, 0, 128, 1)">16</span> [default_loader.py:<span style="color: rgba(128, 0, 128, 1)">280</span>] Loading weights took <span style="color: rgba(128, 0, 128, 1)">1.42</span><span style="color: rgba(0, 0, 0, 1)"> seconds
INFO </span><span style="color: rgba(128, 0, 128, 1)">07</span>-<span style="color: rgba(128, 0, 128, 1)">21</span> <span style="color: rgba(128, 0, 128, 1)">11</span>:<span style="color: rgba(128, 0, 128, 1)">44</span>:<span style="color: rgba(128, 0, 128, 1)">16</span> [model_runner.py:<span style="color: rgba(128, 0, 128, 1)">1202</span>] Model loading took <span style="color: rgba(128, 0, 128, 1)">0.9209</span> GiB and <span style="color: rgba(128, 0, 128, 1)">1.489495</span><span style="color: rgba(0, 0, 0, 1)"> seconds
WARNING </span><span style="color: rgba(128, 0, 128, 1)">07</span>-<span style="color: rgba(128, 0, 128, 1)">21</span> <span style="color: rgba(128, 0, 128, 1)">11</span>:<span style="color: rgba(128, 0, 128, 1)">44</span>:<span style="color: rgba(128, 0, 128, 1)">17</span> [profiling.py:<span style="color: rgba(128, 0, 128, 1)">72</span>] `get_dummy_processor_inputs` has been <span style="color: rgba(0, 0, 255, 1)">split</span> up into `get_dummy_text` and `get_dummy_mm_data`. These two methods will be marked as abstract <span style="color: rgba(0, 0, 255, 1)">in</span><span style="color: rgba(0, 0, 0, 1)"> an upcoming release.
INFO </span><span style="color: rgba(128, 0, 128, 1)">07</span>-<span style="color: rgba(128, 0, 128, 1)">21</span> <span style="color: rgba(128, 0, 128, 1)">11</span>:<span style="color: rgba(128, 0, 128, 1)">44</span>:<span style="color: rgba(128, 0, 128, 1)">17</span> [worker.py:<span style="color: rgba(128, 0, 128, 1)">291</span>] Memory profiling takes <span style="color: rgba(128, 0, 128, 1)">0.77</span><span style="color: rgba(0, 0, 0, 1)"> seconds
INFO </span><span style="color: rgba(128, 0, 128, 1)">07</span>-<span style="color: rgba(128, 0, 128, 1)">21</span> <span style="color: rgba(128, 0, 128, 1)">11</span>:<span style="color: rgba(128, 0, 128, 1)">44</span>:<span style="color: rgba(128, 0, 128, 1)">17</span> [worker.py:<span style="color: rgba(128, 0, 128, 1)">291</span>] the current vLLM instance can use total_gpu_memory (<span style="color: rgba(128, 0, 128, 1)">8</span>.00GiB) x gpu_memory_utilization (<span style="color: rgba(128, 0, 128, 1)">0.25</span>) = <span style="color: rgba(128, 0, 128, 1)">2</span><span style="color: rgba(0, 0, 0, 1)">.00GiB
INFO </span><span style="color: rgba(128, 0, 128, 1)">07</span>-<span style="color: rgba(128, 0, 128, 1)">21</span> <span style="color: rgba(128, 0, 128, 1)">11</span>:<span style="color: rgba(128, 0, 128, 1)">44</span>:<span style="color: rgba(128, 0, 128, 1)">17</span> [worker.py:<span style="color: rgba(128, 0, 128, 1)">291</span>] model weights take <span style="color: rgba(128, 0, 128, 1)">0</span>.92GiB; non_torch_memory takes <span style="color: rgba(128, 0, 128, 1)">0</span>.03GiB; PyTorch activation peak memory takes <span style="color: rgba(128, 0, 128, 1)">0</span>.17GiB; the rest of the memory reserved <span style="color: rgba(0, 0, 255, 1)">for</span> KV Cache is <span style="color: rgba(128, 0, 128, 1)">0</span><span style="color: rgba(0, 0, 0, 1)">.88GiB.
INFO </span><span style="color: rgba(128, 0, 128, 1)">07</span>-<span style="color: rgba(128, 0, 128, 1)">21</span> <span style="color: rgba(128, 0, 128, 1)">11</span>:<span style="color: rgba(128, 0, 128, 1)">44</span>:<span style="color: rgba(128, 0, 128, 1)">17</span> [executor_base.py:<span style="color: rgba(128, 0, 128, 1)">112</span>] # cuda blocks: <span style="color: rgba(128, 0, 128, 1)">482</span>, # CPU blocks: <span style="color: rgba(128, 0, 128, 1)">2184</span><span style="color: rgba(0, 0, 0, 1)">
INFO </span><span style="color: rgba(128, 0, 128, 1)">07</span>-<span style="color: rgba(128, 0, 128, 1)">21</span> <span style="color: rgba(128, 0, 128, 1)">11</span>:<span style="color: rgba(128, 0, 128, 1)">44</span>:<span style="color: rgba(128, 0, 128, 1)">17</span> [executor_base.py:<span style="color: rgba(128, 0, 128, 1)">117</span>] Maximum concurrency <span style="color: rgba(0, 0, 255, 1)">for</span> <span style="color: rgba(128, 0, 128, 1)">803</span> tokens per request: <span style="color: rgba(128, 0, 128, 1)">9</span><span style="color: rgba(0, 0, 0, 1)">.60x
INFO </span><span style="color: rgba(128, 0, 128, 1)">07</span>-<span style="color: rgba(128, 0, 128, 1)">21</span> <span style="color: rgba(128, 0, 128, 1)">11</span>:<span style="color: rgba(128, 0, 128, 1)">44</span>:<span style="color: rgba(128, 0, 128, 1)">18</span> [model_runner.py:<span style="color: rgba(128, 0, 128, 1)">1512</span>] Capturing cudagraphs <span style="color: rgba(0, 0, 255, 1)">for</span> decoding. This may lead to unexpected consequences <span style="color: rgba(0, 0, 255, 1)">if</span> the model is not static. To run the model <span style="color: rgba(0, 0, 255, 1)">in</span> eager mode, set <span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">enforce_eager=True</span><span style="color: rgba(128, 0, 0, 1)">'</span> or use <span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">--enforce-eager</span><span style="color: rgba(128, 0, 0, 1)">'</span> <span style="color: rgba(0, 0, 255, 1)">in</span> the CLI. If out-of-<span style="color: rgba(0, 0, 0, 1)">memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes: </span><span style="color: rgba(128, 0, 128, 1)">100</span>%|██████████████████████████████████████████████████████| <span style="color: rgba(128, 0, 128, 1)">35</span>/<span style="color: rgba(128, 0, 128, 1)">35</span> [<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">13</span>&lt;<span style="color: rgba(128, 0, 128, 1)">00</span>:<span style="color: rgba(128, 0, 128, 1)">00</span>,  <span style="color: rgba(128, 0, 128, 1)">2</span>.65it/<span style="color: rgba(0, 0, 0, 1)">s]
INFO </span><span style="color: rgba(128, 0, 128, 1)">07</span>-<span style="color: rgba(128, 0, 128, 1)">21</span> <span style="color: rgba(128, 0, 128, 1)">11</span>:<span style="color: rgba(128, 0, 128, 1)">44</span>:<span style="color: rgba(128, 0, 128, 1)">31</span> [model_runner.py:<span style="color: rgba(128, 0, 128, 1)">1670</span>] Graph capturing finished <span style="color: rgba(0, 0, 255, 1)">in</span> <span style="color: rgba(128, 0, 128, 1)">11</span> secs, took <span style="color: rgba(128, 0, 128, 1)">0.21</span><span style="color: rgba(0, 0, 0, 1)"> GiB
INFO </span><span style="color: rgba(128, 0, 128, 1)">07</span>-<span style="color: rgba(128, 0, 128, 1)">21</span> <span style="color: rgba(128, 0, 128, 1)">11</span>:<span style="color: rgba(128, 0, 128, 1)">44</span>:<span style="color: rgba(128, 0, 128, 1)">31</span> [llm_engine.py:<span style="color: rgba(128, 0, 128, 1)">428</span>] init engine (profile, create kv cache, warmup model) took <span style="color: rgba(128, 0, 128, 1)">14.54</span><span style="color: rgba(0, 0, 0, 1)"> seconds
</span>&gt;&gt; GPT weights restored from: /home/zww/IndexTTS-<span style="color: rgba(128, 0, 128, 1)">1.5</span>/<span style="color: rgba(0, 0, 0, 1)">gpt.pth
Detected CUDA files, patching ldflags
Emitting ninja build </span><span style="color: rgba(0, 0, 255, 1)">file</span> /home/zww/index-tts-vllm/indextts/BigVGAN/alias_free_activation/cuda/build/<span style="color: rgba(0, 0, 0, 1)">build.ninja...
Building extension module anti_alias_activation_cuda...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS</span>=<span style="color: rgba(0, 0, 0, 1)">N)
ninja: no work to </span><span style="color: rgba(0, 0, 255, 1)">do</span><span style="color: rgba(0, 0, 0, 1)">.
Loading extension module anti_alias_activation_cuda...
</span>&gt;&gt; Preload custom CUDA kernel <span style="color: rgba(0, 0, 255, 1)">for</span> BigVGAN &lt;module <span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">anti_alias_activation_cuda</span><span style="color: rgba(128, 0, 0, 1)">'</span> from <span style="color: rgba(128, 0, 0, 1)">'</span><span style="color: rgba(128, 0, 0, 1)">/home/zww/index-tts-vllm/indextts/BigVGAN/alias_free_activation/cuda/build/anti_alias_activation_cuda.so</span><span style="color: rgba(128, 0, 0, 1)">'</span>&gt;<span style="color: rgba(0, 0, 0, 1)">
No modifications detected </span><span style="color: rgba(0, 0, 255, 1)">for</span> re-<span style="color: rgba(0, 0, 0, 1)">loaded extension module anti_alias_activation_cuda, skipping build step...
Loading extension module anti_alias_activation_cuda...
Removing weight norm...
</span>&gt;&gt; bigvgan weights restored from: /home/zww/IndexTTS-<span style="color: rgba(128, 0, 128, 1)">1.5</span>/<span style="color: rgba(0, 0, 0, 1)">bigvgan_generator.pth
</span><span style="color: rgba(128, 0, 128, 1)">2025</span>-<span style="color: rgba(128, 0, 128, 1)">07</span>-<span style="color: rgba(128, 0, 128, 1)">21</span> <span style="color: rgba(128, 0, 128, 1)">11</span>:<span style="color: rgba(128, 0, 128, 1)">44</span>:<span style="color: rgba(128, 0, 128, 1)">34</span>,<span style="color: rgba(128, 0, 128, 1)">427</span> WETEXT INFO found existing fst: /home/zww/index-tts-vllm/indextts/utils/tagger_cache/<span style="color: rgba(0, 0, 0, 1)">zh_tn_tagger.fst
</span><span style="color: rgba(128, 0, 128, 1)">2025</span>-<span style="color: rgba(128, 0, 128, 1)">07</span>-<span style="color: rgba(128, 0, 128, 1)">21</span> <span style="color: rgba(128, 0, 128, 1)">11</span>:<span style="color: rgba(128, 0, 128, 1)">44</span>:<span style="color: rgba(128, 0, 128, 1)">34</span>,<span style="color: rgba(128, 0, 128, 1)">427</span> WETEXT INFO                     /home/zww/index-tts-vllm/indextts/utils/tagger_cache/<span style="color: rgba(0, 0, 0, 1)">zh_tn_verbalizer.fst
</span><span style="color: rgba(128, 0, 128, 1)">2025</span>-<span style="color: rgba(128, 0, 128, 1)">07</span>-<span style="color: rgba(128, 0, 128, 1)">21</span> <span style="color: rgba(128, 0, 128, 1)">11</span>:<span style="color: rgba(128, 0, 128, 1)">44</span>:<span style="color: rgba(128, 0, 128, 1)">34</span>,<span style="color: rgba(128, 0, 128, 1)">427</span> WETEXT INFO skip building fst <span style="color: rgba(0, 0, 255, 1)">for</span><span style="color: rgba(0, 0, 0, 1)"> zh_normalizer ...
</span><span style="color: rgba(128, 0, 128, 1)">2025</span>-<span style="color: rgba(128, 0, 128, 1)">07</span>-<span style="color: rgba(128, 0, 128, 1)">21</span> <span style="color: rgba(128, 0, 128, 1)">11</span>:<span style="color: rgba(128, 0, 128, 1)">44</span>:<span style="color: rgba(128, 0, 128, 1)">34</span>,<span style="color: rgba(128, 0, 128, 1)">567</span> WETEXT INFO found existing fst: /home/zww/index-tts-vllm/.venv/lib/python3.<span style="color: rgba(128, 0, 128, 1)">12</span>/site-packages/tn/<span style="color: rgba(0, 0, 0, 1)">en_tn_tagger.fst
</span><span style="color: rgba(128, 0, 128, 1)">2025</span>-<span style="color: rgba(128, 0, 128, 1)">07</span>-<span style="color: rgba(128, 0, 128, 1)">21</span> <span style="color: rgba(128, 0, 128, 1)">11</span>:<span style="color: rgba(128, 0, 128, 1)">44</span>:<span style="color: rgba(128, 0, 128, 1)">34</span>,<span style="color: rgba(128, 0, 128, 1)">567</span> WETEXT INFO                     /home/zww/index-tts-vllm/.venv/lib/python3.<span style="color: rgba(128, 0, 128, 1)">12</span>/site-packages/tn/<span style="color: rgba(0, 0, 0, 1)">en_tn_verbalizer.fst
</span><span style="color: rgba(128, 0, 128, 1)">2025</span>-<span style="color: rgba(128, 0, 128, 1)">07</span>-<span style="color: rgba(128, 0, 128, 1)">21</span> <span style="color: rgba(128, 0, 128, 1)">11</span>:<span style="color: rgba(128, 0, 128, 1)">44</span>:<span style="color: rgba(128, 0, 128, 1)">34</span>,<span style="color: rgba(128, 0, 128, 1)">567</span> WETEXT INFO skip building fst <span style="color: rgba(0, 0, 255, 1)">for</span><span style="color: rgba(0, 0, 0, 1)"> en_normalizer ...
</span>&gt;&gt;<span style="color: rgba(0, 0, 0, 1)"> TextNormalizer loaded
</span>&gt;&gt; bpe model loaded from: /home/zww/IndexTTS-<span style="color: rgba(128, 0, 128, 1)">1.5</span>/<span style="color: rgba(0, 0, 0, 1)">bpe.model
</span>* Running on local URL:  http:<span style="color: rgba(0, 128, 0, 1)">//</span><span style="color: rgba(0, 128, 0, 1)">0.0.0.0:7860</span>
* To create a public link, set `share=True` <span style="color: rgba(0, 0, 255, 1)">in</span><span style="color: rgba(0, 0, 0, 1)"> `launch()`.
</span>&gt;&gt; start inference...</pre>
</div>
<span class="cnblogs_code_collapse">View Code</span></div>
<p>用同样的文本产生音频文件，可以看出在使用GPU后会大大减小生成时间，如下图所示：</p>
<p><img src="https://img2024.cnblogs.com/blog/465567/202507/465567-20250721115526125-49225144.png" alt="" width="745" height="223" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
</div>
<h2>2.4 API调用</h2>
<p>正如官网说明，该项目使用fastapi封装了api接口，调用如下命令可以启动相关示例：</p>
<div class="cnblogs_code">
<pre>VLLM_USE_V1=<span style="color: rgba(128, 0, 128, 1)">0</span> python api_server.py --model_dir /home/zww/IndexTTS-1.5 --port <span style="color: rgba(128, 0, 128, 1)">11996</span></pre>
</div>
<p>编写如下python文件testindex-tts-vllm.py，注意这里两个音频文件在index-tts-vllm目录下要存在。</p>
<div class="cnblogs_code">
<pre><span style="color: rgba(0, 0, 0, 1)">import requests

url </span>= <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">http://127.0.0.1:11996/tts_url</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">
data </span>=<span style="color: rgba(0, 0, 0, 1)"> {
    </span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">text</span><span style="color: rgba(128, 0, 0, 1)">"</span>: <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">还是会想你，还是想登你</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">,
    </span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">audio_paths</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">: [  # 支持多参考音频
        </span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">spk_1752726968.wav</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">,
        </span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">spk_1753063888.wav</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">
    ]
}

response </span>= requests.post(url, json=<span style="color: rgba(0, 0, 0, 1)">data)
with open(</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">output.wav</span><span style="color: rgba(128, 0, 0, 1)">"</span>, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">wb</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">) as f:
    f.</span><span style="color: rgba(0, 0, 255, 1)">write</span>(response.content)</pre>
</div>
<p>直接运行python&nbsp;testindex-tts-vllm.py命令，则会在当前目录下产生输出结果output.wav音频文件：</p>
<p><img src="https://img2024.cnblogs.com/blog/465567/202507/465567-20250721122935185-2097768534.png" alt="" width="529" height="371" loading="lazy" style="display: block; margin-left: auto; margin-right: auto"></p>
<p>&nbsp;</p>
<p>参考：</p>
<p><a href="https://github.com/Ksuriuri/index-tts-vllm" target="_blank" rel="noopener nofollow">https://github.com/Ksuriuri/index-tts-vllm</a></p>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0" data-date-updated="2025-07-21 12:32">2025-07-21 12:31</span>&nbsp;
<a href="https://www.cnblogs.com/zhaoweiwei">weiwei22844</a>&nbsp;
阅读(<span id="post_view_count">230</span>)&nbsp;
评论(<span id="post_comment_count">2</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18990144);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18990144', targetLink: 'https://www.cnblogs.com/zhaoweiwei/p/18990144/IndexTTS', title: '也玩音频克隆IndexTTS' })">举报</a>
</div>
        