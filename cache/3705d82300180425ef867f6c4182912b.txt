
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/shanyou/p/19055731" title="发布于 2025-08-24 18:55">
    <span role="heading" aria-level="2">DeepSeek采用的UE8M0 FP8 为什么引爆了A股的芯片板块</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body blogpost-body-html">
<p><font size="3">DeepSeek 这次的重点不在模型 V3.1，而是在DeepSeek在其官宣发布DeepSeek-V3.1的文章中提到，DeepSeek-V3.1使用了UE8MO FP8 Scale的参数精度。另外，V3.1对分词器及chat template进行了较大调整，与DeepSeek-V3存在明显差异。DeepSeek官微在置顶留言里表示UE8MO FP8是针对即将发布的下一代国产芯片设计。一则官方留言让整个AI圈都轰动了：</font></p><p><a href="https://img2024.cnblogs.com/blog/510/202508/510-20250824185519193-968417252.png"><font size="3"><img width="496" height="115" title="47626163e2396fc29314d8fb591b225f" style="display: inline; background-image: none" alt="47626163e2396fc29314d8fb591b225f" src="https://img2024.cnblogs.com/blog/510/202508/510-20250824185519886-339537816.png" border="0"></font></a></p><p><font size="3">新的架构、下一代国产芯片，总共短短不到20个字，却蕴含了巨大信息量。 国产芯片企业股价也跟风上涨，比如寒武纪今日早盘盘中大涨近14%，总市值跃居科创板头名。这里面有几个疑问：这个UE8M0 FP8到底是什么？下一代国产芯片，又是指什么？</font></p><h3><strong><strong><font size="3">UE8M0 FP8<strong><strong>是</strong></strong><strong><strong>什么</strong></strong>？</font></strong></strong></h3><p><font size="3">“UE8M0 FP8”这个概念，可以拆分成前后两个部分来解释，前面的UE8M0，是MXFP8路径里的“缩放因子”。</font></p><p><font size="3">MXFP8是Open Compute Project在2023年发布的《Microscaling (MX) Formats Specification v1.0》里定义的8 bit微缩块格式。Open Compute Project是2011年由Facebook<em>（现Meta）</em>联合英特尔、Rackspace等发起的开源硬件协作计划，目的是通过共享数据中心及服务器设计推动行业效率提升。其成员阵容相当强大，国外还有微软、谷歌、亚马逊、AMD、英伟达等，而国内的阿里、腾讯、百度等也参与其中。它通过：</font></p><ul><li><font size="3"><strong>块缩放（Block Scaling）</strong>：将一个张量（Tensor）分割成小块（例如每32个元素一块），每个块共享一个缩放因子（Scale Factor，常用<strong>UE8M0</strong>格式存储）。这有效扩展了低精度格式的动态范围，避免了数值溢出或精度损失。</font></li><li><font size="3"><strong>硬件原生支持</strong>：新一代AI芯片（如NVIDIA Blackwell）的Tensor Core已原生支持MX格式计算，能在单元内完成数据、缩放因子计算和矩阵乘法，显著提升效率。</font></li></ul><p><font size="3">对于AI计算，尤其是大模型训练和推理，MXFP8能带来：</font></p><ul><li><font size="3"><strong>计算效率提升</strong>：相比FP16，FP8计算吞吐量可提升约2倍。 
</font></li><li><font size="3"><strong>显存占用降低</strong>：模型参数显存占用减半，允许部署更大模型或降低硬件成本。 
</font></li><li><font size="3"><strong>功耗降低</strong>：数据搬运和计算的能耗显著下降。</font></li></ul><p><font size="3">MXFP8是一种结合了微缩放（Microscaling）技术的8位浮点格式，能有效提升大模型训练的效率和降低显存占用，以下是已量产或计划支持的相关芯片：</font></p><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><font size="3">
</font><table border="0" cellspacing="0" cellpadding="2"><tbody><tr><td valign="top"><font size="3">公司</font></td><td valign="top"><font size="3">芯片型号</font></td><td valign="top"><font size="3">状态/发布时间</font></td><td valign="top"><font size="3">关键特性</font></td><td valign="top"><font size="3">支持情况</font></td></tr><tr><td valign="top"><font size="3">寒武纪</font></td><td valign="top"><font size="3">思元590/690系列</font></td><td valign="top"><font size="3">已支持/已流片</font></td><td valign="top"><font size="3">支持FP8</font></td><td valign="top"><font size="3">MLU370-S4、思元590及690系列芯片均支持FP8计算</font></td></tr><tr><td valign="top"><font size="3">华为</font></td><td valign="top"><font size="3">昇腾910D/920系列</font></td><td valign="top"><font size="3">计划支持</font></td><td valign="top"><font size="3">预计2025年Q4支持原生FP8</font></td><td valign="top"><font size="3">路线图显示2025年第四季度将加入支持行列</font></td></tr><tr><td valign="top"><font size="3">燧原科技</font></td><td valign="top"><font size="3">燧原L600</font></td><td valign="top"><font size="3">2025年7月发布</font></td><td valign="top"><font size="3">国内首款原生支持FP8低精度计算的训推一体AI芯片，配备144GB存储容量和3.6TB/s存储带宽</font></td><td valign="top"><font size="3">原生支持FP8</font></td></tr><tr><td valign="top"><font size="3">沐曦</font></td><td valign="top"><font size="3">曦云C600</font></td><td valign="top"><font size="3">2025年Q4量产</font></td><td valign="top"><font size="3">基于XCORE1.5架构原生支持FP8 Tensor指令及Tensor转置功能，采用HBM3e显存</font></td><td valign="top"><font size="3">原生支持FP8</font></td></tr><tr><td valign="top"><font size="3">摩尔线程</font></td><td valign="top"><font size="3">MTT S5000</font></td><td valign="top"><font size="3">已大规模量产</font></td><td valign="top"><font size="3">MUSA架构支持UE8M0 FP8 Scale，利用硬件原生FP8，相比FP16计算算力提升2倍，带宽效率提升</font></td><td valign="top"><font size="3">原生支持FP8</font></td></tr><tr><td valign="top"><font size="3">海光信息</font></td><td valign="top"><font size="3">深算系列</font></td><td valign="top"><font size="3">已支持</font></td><td valign="top"><font size="3">支持FP8精度</font></td><td valign="top"><font size="3">深算系列芯片支持FP8</font></td></tr><tr><td valign="top"><font size="3">壁仞科技</font></td><td valign="top"><font size="3">BR100系列</font></td><td valign="top"><font size="3">已发布</font></td><td valign="top"><font size="3">支持FP16/INT8计算</font></td><td valign="top"><font size="3">未明确提及FP8，但具备低精度计算基础</font></td></tr><tr><td valign="top"><font size="3">天数智芯</font></td><td valign="top"><font size="3">相关产品</font></td><td valign="top"><font size="3">适配中</font></td><td valign="top"><font size="3">参与DeepSeek模型适配</font></td><td valign="top"><font size="3">未明确提及FP8，但正在推进软硬件协同优化</font></td></tr></tbody></table><font size="3">
</font><p><br></p><p><font size="3">UE8M0 FP8
是一种<strong>缩放因子格式</strong>。它的核心思想是<strong>微缩块（Microscaling, MX）格式：将一个张量分成许多小块（例如每块包含32个数值），每个块共享一个缩放因子（Scale），块内的数值则用低精度格式（如FP8）存储。
</strong></font></p><ul><li><font size="3"><strong>UE8M0</strong>：这个名称揭示了其结构。 
</font><ul><li><font size="3"><strong>U</strong> 代表无符号（Unsigned），因为它通常用于处理非负的激活值或缩放因子本身。 
</font></li><li><font size="3"><strong>E8</strong> 代表8位指数（Exponent）。 
</font></li><li><font size="3"><strong>M0</strong> 代表0位尾数（Mantissa）。这意味着UE8M0<strong>仅能表示2的整数幂</strong>（例如 ..., 2^-2=0.25, 2^-1=0.5, 2^0=1, 2^1=2, 2^2=4, ...），其动态范围极大（约2^{-127} 到 2^{128})
。 </font></li></ul></li><li><font size="3"><strong>硬件友好</strong>：由于仅表示2的幂，在硬件解码时只需进行<strong>指数位移操作</strong>，无需传统的浮点乘法器，这可以显著简化电路、缩短关键路径延迟并降低功耗
。 </font></li><li><font size="3"><strong>与计算格式协同</strong>：UE8M0 FP8
通常<strong>不直接用于计算</strong>，而是作为缩放因子，与E4M3或E5M2等计算格式协同工作，共同构成MXFP8格式
。</font></li></ul><p><font size="3"><br></font></p><p><font size="3">国产AI芯片厂商正积极布局MXFP8等低精度格式的支持，这背后是<strong>软硬协同</strong>生态的构建：</font></p><ul><li><font size="3"><strong>软件生态支持</strong>：DeepSeek（深度求索）等AI厂商在算法和软件层面优化了对FP8格式的支持（如开源DeepGEMM库），并积极与国产芯片适配。这为国产芯片提供了“换道超车”的机会
。 </font></li><li><font size="3"><strong>突破带宽瓶颈</strong>：许多国产AI芯片在HBM等高带宽内存技术上存在差距。MX格式通过显著降低数据位宽，能有效<strong>缓解带宽压力</strong>，让算力得到更充分利用
。</font></li><li><font size="3"><strong>硬件加速迭代</strong>：上述芯片厂商的新一代产品大多将原生支持FP8作为重要特性，通过架构设计（如专用的Tensor Core、指令集扩展）来高效支持MXFP8计算
。 UE8M0硬件解码简单的特性，使其非常适合在<strong>设计自主可控的国产AI加速器</strong>中集成，有助于降低功耗、提升能效比。
</font></li><li><font size="3"><strong>应对技术封锁</strong>：在美国对高端AI芯片实施出口管制的背景下，推动国产算力发展至关重要。支持MXFP8等先进格式，有助于缩小国产芯片与国际顶尖产品在实际应用中的性能差距。</font></li></ul><p><font size="3">如果你在选择支持MXFP8的国产AI芯片，可以考虑以下几点：</font></p><ol><li><font size="3"><strong>确认原生支持</strong>：关注芯片是否<strong>原生支持FP8计算</strong>（而并非仅通过软件模拟或转换），这直接影响计算效率
。 
</font></li><li><font size="3"><strong>关注软件生态</strong>：了解芯片与主流AI框架（如DeepSeek、TensorFlow、PyTorch）的适配情况，以及其低精度计算库的成熟度
。 
</font></li><li><font size="3"><strong>考察实际性能</strong>：关注芯片在<strong>特定负载</strong>（如大模型训练或推理）下的实际算力、显存带宽和能效表现
。 
</font></li><li><font size="3"><strong>了解量产进度</strong>：部分芯片可能已发布但尚未大规模量产，需确认其供货情况和使用案例。</font></li></ol><h5><font size="3">总结</font></h5><p><font size="3">支持<strong>MXFP8</strong>的国产AI芯片阵容正在不断扩大，包括<strong>寒武纪、燧原科技、华为、沐曦、摩尔线程、海光信息</strong>等厂商的产品
。这反映了国产AI算力在<strong>软硬协同</strong>发展上的进步，旨在提升大模型处理效率，并减少对国外高性能GPU的依赖。<font size="3">希望这些信息能帮助你更好地了解国产AI芯片对MXFP8的支持情况。</font></font></p>
</div>
<div id="MySignature" role="contentinfo">
    <p>欢迎大家扫描下面二维码成为我的客户，扶你上云</p>
<img src="https://images.cnblogs.com/cnblogs_com/shanyou/57459/o_220125090408_%E9%82%80%E8%AF%B7%E4%BA%8C%E7%BB%B4%E7%A0%81-258px.jpeg" width="170">
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0" data-date-updated="2025-08-24 18:55">2025-08-24 18:55</span>&nbsp;
<a href="https://www.cnblogs.com/shanyou">张善友</a>&nbsp;
阅读(<span id="post_view_count">34</span>)&nbsp;
评论(<span id="post_comment_count">1</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(19055731);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '19055731', targetLink: 'https://www.cnblogs.com/shanyou/p/19055731', title: 'DeepSeek采用的UE8M0 FP8 为什么引爆了A股的芯片板块' })">举报</a>
</div>
        