
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/ibrahim/p/18683721/Llama-Nuts-and-Bolts" title="发布于 2025-01-21 16:06">
    <span role="heading" aria-level="2">一镜到底，通过Llama大模型架构图看透transformers原理</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                    <div id="cnblogs_post_description" style="display: none">
        
        想了解大模型 Llama 的工作原理？Llama Nuts and Bolts 项目不依赖外部库，通过 Go 语言从零构建 Llama 3.1 8B-Instruct 模型，为学习者提供了一个教育性深度探索，让您动手实践理解大型语言模型。
    </div>
<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h1 id="一镜到底通过llama大模型架构图看透transformers原理">一镜到底，通过Llama大模型架构图看透transformers原理</h1>
<p>Llama Nuts and Bolts是Github上使用Go语言从零重写Llama3.1 8B-Instruct模型推理过程（80亿参数规模）的实战类开源项目，其作者是来自土耳其的Adil Alper DALKIRAN。</p>
<p>如果你对于 LLM（大语言模型）和 Transformers 的工作原理感兴趣，并对相关概念略知一二，但仍想深入理解，那么这个项目非常适合你！</p>
<p>这个项目最大的特色是：</p>
<ul>
<li>
<p>使用Go语言从零开发，不依赖任何机器学习库和数学计算库，走出 Python 生态的舒适区</p>
</li>
<li>
<p>配备完整的大模型推理的流程图，透视大模型如何运作的细节</p>
</li>
<li>
<p>完备的文档和代码说明，能够亲身体验机器学习的基础知识、Transformers 模型、注意力机制、旋转位置嵌入（RoPE）以及背后的数学原理</p>
</li>
</ul>
<p>Llama Nuts and Bolts项目代码和文档地址：</p>
<ul>
<li>
<p><a href="https://github.com/adalkiran/llama-nuts-and-bolts" target="_blank" rel="noopener nofollow">https://github.com/adalkiran/llama-nuts-and-bolts</a></p>
</li>
<li>
<p><a href="https://adalkiran.github.io/llama-nuts-and-bolts/" target="_blank" rel="noopener nofollow">https://adalkiran.github.io/llama-nuts-and-bolts/</a></p>
</li>
</ul>
<p><img src="https://mz-blog-res.oss-cn-beijing.aliyuncs.com/img/b002/llama-nuts-and-bolts-screen-record.gif" alt="Llama Nuts and Bolts" loading="lazy"></p>
<p>需要注意的是，该项目仅为教育目的开发，未经过生产环境或商业使用测试。其目标是创建一个实验性项目，能够在完全不依赖 Python 生态系统的情况下对 Llama 3.1 8B-Instruct 模型进行推理。</p>
<p>这个项目使用Go语言，不使用任何现有的机器学习或数学计算库，从零实现一个控制台应用程序，通过使用预训练的 Llama 3.1 8B-Instruct 模型参数生成文本输出。</p>
<p>开发这个项目的过程使作者深入研究了 transformers 模型的内部结构，并发现了之前没有意识到的细节，包括作者已经了解的理论知识，还有需要重新学习的新内容，并从中获得了新见解。</p>
<p>Llama Nuts and Bolts 的第一个版本于 2024 年 3 月 12 日发布，适配 Llama 2 模型，而其最新的版本支持Llama 3.1 8B-Instruct模型。</p>
<p>话不多说，先上图。</p>
<p><img src="https://mz-blog-res.oss-cn-beijing.aliyuncs.com/img/b002/DIAG01-complete-model.drawio.svg" alt="Llama 3.1 8B-Instruct大模型推理的完整流程图" loading="lazy"></p>
<h2 id="llama-transformers-架构的特点">Llama transformers 架构的特点</h2>
<p>与经典transformers架构相比，Llama 的transformers架构具有几个显著特征：</p>
<ul>
<li>仅解码器架构 Decoder-Only Architecture：Llama 纯文本模型只有解码器decorder，没有encoder，这意味着它仅专注于根据输入上下文生成序列，无需编码器，因此它主要依赖自注意力机制来捕捉输入序列中的依赖关系。Llama 是一个仅解码器模型，这意味着它仅专注于根据输入上下文生成序列。这与像 BERT 或 T5 这样的编码器-解码器模型形成对比，后者同时利用编码器来理解输入和解码器来生成输出。</li>
<li>自注意力机制 Self-Attention Mechanism：Llama 纯文本模型不包括交叉注意力层。Llama 自注意力层用于解码器encoder内处理输入序列，而交叉注意力层在编码器-解码器模型中更为常见，其中编码器处理一个输入（例如，源语言），解码器则基于该处理信息生成输出。Llama 使用自注意力以捕捉输入文本中的依赖关系，而无需交叉注意力层。这使其能够生成连贯且上下文相关的文本。</li>
</ul>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.5054301630335648" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-01-21 16:07">2025-01-21 16:06</span>&nbsp;
<a href="https://www.cnblogs.com/ibrahim">ibrahim</a>&nbsp;
阅读(<span id="post_view_count">133</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18683721" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18683721);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18683721', targetLink: 'https://www.cnblogs.com/ibrahim/p/18683721/Llama-Nuts-and-Bolts', title: '一镜到底，通过Llama大模型架构图看透transformers原理' })">举报</a>
</div>
        