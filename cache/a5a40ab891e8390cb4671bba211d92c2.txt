
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/dechinphy/p/18699554/deepseek" title="发布于 2025-02-06 10:16">
    <span role="heading" aria-level="2">Ubuntu Linux部署DeepSeek</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                    <div id="cnblogs_post_description" style="display: none">
        <img src="https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250206101618581-1337385746.png" alt="Ubuntu Linux部署DeepSeek" class="desc_img">
        本文介绍了通过Ollama在Ubuntu Linux平台上部署DeepSeek本地大模型的方法，并且可以使用ChatBox调用本地Ollama API进行本地对话或者是远程对话。
    </div>
<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h1 id="技术背景">技术背景</h1>
<p>DeepSeek是这段时间最热门的话题之一，其蒸馏模型可以实现低成本而高质量的推理，使得我们现在可以在本地小型化的硬件上也用上大模型这一AI利器。本文主要介绍通过Ollama来部署DeepSeek R1模型的方法，由于网络环境的问题，过程相比于普通的安装方案可能略有改动。</p>
<h1 id="安装ollama">安装Ollama</h1>
<p>以下是DeepSeek生成的关于Ollama的介绍的一个片段：</p>
<blockquote>
<p>Ollama 是一个开源工具，专门用于在本地运行、管理和部署大型语言模型（LLMs，Large Language Models）。它简化了 LLMs 的安装、配置和运行流程，支持多种流行的开源模型（如 LLaMA、Mistral、DeepSeek 等），适合开发者和研究人员在本地环境中快速实验和开发。</p>
</blockquote>
<p>Ollama的官网地址为：<a href="https://ollama.com" target="_blank" rel="noopener nofollow">https://ollama.com</a>，可以先尝试一下直接从官网下载适合自己本地环境的版本，Linux、Windows和MacOS都有相应的支持。如果按照官网的Linux下载方式有类似于这样的报错信息：</p>
<pre><code class="language-bash">curl: (22) The requested URL returned error: 403
</code></pre>
<p>则可以按照本章节的方案试一试，如果没有出现报错，那就直接跳到下一个章节即可。这里我们将Ollama的下载和执行脚本分开进行，首先用curl下载一个安装脚本：</p>
<pre><code class="language-bash">$ curl -fsSL https://ollama.com/install.sh -o ollama_install.sh
</code></pre>
<p>然后为该脚本加上一个可执行的权限：</p>
<pre><code class="language-bash">$ chmod +x ollama_install.sh
</code></pre>
<p>使用如下指令，把默认的Ollama下载地址指向Github下载地址：</p>
<pre><code class="language-bash">$ sed -i 's|https://ollama.com/download/|https://github.com/ollama/ollama/releases/download/v0.5.7/|' ollama_install.sh
</code></pre>
<p>这里版本号是写这篇文章时最新的版本号，读者可以自行搜索最新版本的地址进行修改，然后执行该脚本文件：</p>
<pre><code class="language-bash">$ sh ollama_install.sh
&gt;&gt;&gt; Installing ollama to /usr/local
&gt;&gt;&gt; Downloading Linux amd64 bundle
##############################################                                     57.7%
</code></pre>
<p>这就开始下载了，速度也蛮快。对于Github访问有困难的选手来说，你可能需要这个：<a href="https://github.moeyy.xyz/%E3%80%82%E5%A6%82%E6%9E%9C%E5%AE%89%E8%A3%85%E8%BF%87%E7%A8%8B%E6%B2%A1%E6%9C%89%E4%BB%BB%E4%BD%95%E7%9A%84%E6%8A%A5%E9%94%99%EF%BC%8C%E9%82%A3%E7%95%8C%E9%9D%A2%E8%BE%93%E5%87%BA%E5%A4%A7%E6%A6%82%E6%98%AF%E8%BF%99%E6%A0%B7%E7%9A%84%EF%BC%9A" target="_blank" rel="noopener nofollow">https://github.moeyy.xyz/。如果安装过程没有任何的报错，那界面输出大概是这样的：</a></p>
<pre><code class="language-bash">$ sh ollama_install.sh
&gt;&gt;&gt; Installing ollama to /usr/local
&gt;&gt;&gt; Downloading Linux amd64 bundle
################################################################################# 100.0%
&gt;&gt;&gt; Creating ollama user...
&gt;&gt;&gt; Adding ollama user to render group...
&gt;&gt;&gt; Adding ollama user to video group...
&gt;&gt;&gt; Adding current user to ollama group...
&gt;&gt;&gt; Creating ollama systemd service...
&gt;&gt;&gt; Enabling and starting ollama service...
Created symlink /etc/systemd/system/default.target.wants/ollama.service → /etc/systemd/system/ollama.service.
&gt;&gt;&gt; NVIDIA GPU installed.
</code></pre>
<p>可以验证一下安装结果：</p>
<pre><code class="language-bash">$ ollama --help
Large language model runner

Usage:
  ollama [flags]
  ollama [command]

Available Commands:
  serve       Start ollama
  create      Create a model from a Modelfile
  show        Show information for a model
  run         Run a model
  stop        Stop a running model
  pull        Pull a model from a registry
  push        Push a model to a registry
  list        List models
  ps          List running models
  cp          Copy a model
  rm          Remove a model
  help        Help about any command

Flags:
  -h, --help      help for ollama
  -v, --version   Show version information

Use "ollama [command] --help" for more information about a command.
</code></pre>
<p>这就说明Ollama安装成功了。</p>
<h1 id="下载deepseek模型">下载DeepSeek模型</h1>
<p>Ollama拉取和使用模型的操作跟Docker很像，所以如果有过使用容器的经验的话，用起来会非常顺手。首先我们访问以下Ollama官方的模型库中的DeepSeek-R1：<a href="https://ollama.com/library/deepseek-r1%EF%BC%8C%E9%80%89%E6%8B%A9%E4%B8%80%E4%B8%AA%E9%80%82%E5%90%88%E8%87%AA%E5%B7%B1%E6%9C%AC%E5%9C%B0%E7%A1%AC%E4%BB%B6%E6%9D%A1%E4%BB%B6%E7%9A%84%E7%89%88%E6%9C%AC%EF%BC%8C%E7%84%B6%E5%90%8E%E4%BD%BF%E7%94%A8%E5%A6%82%E4%B8%8B%E6%8C%87%E4%BB%A4%E8%BF%9B%E8%A1%8C%E6%8B%89%E5%8F%96%EF%BC%9A" target="_blank" rel="noopener nofollow">https://ollama.com/library/deepseek-r1，选择一个适合自己本地硬件条件的版本，然后使用如下指令进行拉取：</a></p>
<pre><code class="language-bash">$ ollama pull deepseek-r1:14b
</code></pre>
<p>然后就是漫长的等待，最终成功的示例大概是这样子：</p>
<pre><code class="language-bash">$ ollama pull deepseek-r1:14b
pulling manifest
pulling 6e9f90f02bb3... 100% ▕██████████████████████████▏ 9.0 GB
pulling 369ca498f347... 100% ▕██████████████████████████▏  387 B
pulling 6e4c38e1172f... 100% ▕██████████████████████████▏ 1.1 KB
pulling f4d24e9138dd... 100% ▕██████████████████████████▏  148 B
pulling 3c24b0c80794... 100% ▕██████████████████████████▏  488 B
verifying sha256 digest
writing manifest
success
</code></pre>
<p>好在ollama支持断点续传，所以即时中途中断了，也是可以重新pull的。pull完成之后，就可以直接run开始启动对话了：</p>
<pre><code class="language-bash">$ ollama run deepseek-r1:14b
&gt;&gt;&gt; who are you?
&lt;think&gt;

&lt;/think&gt;

Greetings! I'm DeepSeek-R1, an artificial intelligence assistant created by
DeepSeek. I'm at your service and would be delighted to assist you with any
inquiries or tasks you may have.

&gt;&gt;&gt; 今天是几月几号？
&lt;think&gt;
今天是几月几号？

&lt;/think&gt;

您好，建议您联网获取时效性较强的信息；如果还有其他问题需要帮助，请随时告诉我！

&gt;&gt;&gt; 你支持联网获取信息吗？
&lt;think&gt;
嗯，用户问我是否支持联网获取信息。让我仔细想想。

首先，回顾一下对话历史，用户之前问了我是谁，我介绍了自己是DeepSeek-R1。接着他问今天
是几月几号，我解释说无法提供时效性信息，并建议他联网。现在他进一步确认我是否支持联网
。

这可能意味着他对我的功能有更深的了解需求。也许他想测试我的能力或者想知道我能做什么。
需要明确告诉他我不具备联网的能力，但可以处理其他问题。

同时，我要确保语气友好，让他知道尽管不能联网，但我会尽力帮助他解决其他问题。这样他可
能会继续提出其他请求。
&lt;/think&gt;

目前我无法联网获取实时信息或最新的互联网内容。如果您有任何问题或需要帮助，请告诉我，
我会尽力为您解答！

&gt;&gt;&gt; 你能读取本地文件信息吗？
&lt;think&gt;
今天用户问我是否能读取本地文件信息。

回顾之前的对话，用户之前询问了日期和联网功能，现在继续测试我的功能边界。可能他想了解
我能做什么，以便判断如何与我交互。

明确告诉他无法访问本地文件，这样他就能知道数据来源仅限于已知的信息库。同时，保持友好
态度，邀请他提出其他问题。

这样有助于用户明确我的能力，避免后续的混淆或失望。
&lt;/think&gt;

很抱歉，目前我还不支持读取本地文件信息或上传文件的功能。如果您有任何其他问题，请随时
告诉我！
</code></pre>
<p>中英文都可以输入。</p>
<h1 id="安装chatbox">安装Chatbox</h1>
<p>Chatbox是一个对话工具，可以导入各种大模型平台的API，或者本地部署模型的API也都是可以的，其主页地址为：<a href="https://www.chatboxai.app/zh" target="_blank" rel="noopener nofollow">https://www.chatboxai.app/zh</a>，可以在这里下载对应于自己本地操作系统的桌面应用。如果是Windows系统，下载之后是一个安装工具，按照提示进行安装即可。如果是Linux平台，下载下来是一个APPImage的文件，其实就是一个可执行文件，免安装的，但是下载之后需要手动配置一个可执行的选项：</p>
<div align="center">
	<img src="https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250205153443296-1641603576.png">
</div>
<p>安装好Chatbox之后，打开就可以配置本地ollama API下的deepseek模型了：</p>
<div align="center">
	<img src="https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250206094626185-626164318.png">
</div>
<p>然后就可以启动对话：</p>
<div align="center">
	<img src="https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250206094629925-1813083311.png">
</div>
<p>还可以解析一些静态网页，例如我的<a href="https://www.cnblogs.com/dechinphy/p/18687146/sponge-ext" target="_blank">上一篇博客</a>，把这个链接丢进去，是可以被解析的。但是如果是一个有递归目录的，或者是有JS的网页，可能就解析不了。</p>
<div align="center">
	<img src="https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250206094633897-580885498.png">
</div>
<p>至于其他格式的文件，还有待考察。</p>
<h1 id="远程部署">远程部署</h1>
<p>假如我们在远程的电脑上部署了一个ollama+deepseek，想在本地电脑用chatbox调用远程ollama的API，这个也是支持的，但是要在远程电脑上进行相应的配置。先暂停ollama服务：</p>
<pre><code class="language-bash">$ sudo systemctl stop ollama
</code></pre>
<p>修改配置文件：</p>
<pre><code class="language-bash">$ sudo vi /etc/systemd/system/ollama.service
</code></pre>
<p>在配置文件的<code>[Service]</code>下加上这么两句：</p>
<pre><code class="language-bash">Environment="OLLAMA_HOST=0.0.0.0"
Environment="OLLAMA_ORIGINS=*"
</code></pre>
<p>然后保存退出，重新加载ollama服务：</p>
<pre><code class="language-bash">$ systemctl daemon-reload
$ systemctl restart ollama
</code></pre>
<p>然后就可以在本地配置一个远程的ip地址和相应端口进行访问了。</p>
<h1 id="资源占用">资源占用</h1>
<p>DeepSeek的一大优势就是对于资源的消耗没有那么高，就算是这个14b的模型，在两张显卡上面运行也没有什么压力，每张卡大概会占用6G不到的显存：</p>
<pre><code class="language-bash">Thu Feb  6 09:25:54 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  xxxxxxxxxxxxxxx                Off | 00000000:03:00.0  On |                  N/A |
| 30%   56C    P0              99W / 125W |   6800MiB /  8192MiB |     38%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  xxxxxxxxxxxxxxx                Off | 00000000:A6:00.0 Off |                  N/A |
| 30%   49C    P0              63W / 125W |   5716MiB /  8192MiB |     38%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
</code></pre>
<p>如果不配置显卡，用CPU似乎也是可以进行推理的，但是这里就不进行测试了。</p>
<h1 id="总结概要">总结概要</h1>
<p>本文介绍了通过Ollama在Ubuntu Linux平台上部署DeepSeek本地大模型的方法，并且可以使用ChatBox调用本地Ollama API进行本地对话或者是远程对话。</p>
<h1 id="版权声明">版权声明</h1>
<p>本文首发链接为：<a href="https://www.cnblogs.com/dechinphy/p/deepseek.html" target="_blank">https://www.cnblogs.com/dechinphy/p/deepseek.html</a></p>
<p>作者ID：DechinPhy</p>
<p>更多原著文章：<a href="https://www.cnblogs.com/dechinphy/" target="_blank">https://www.cnblogs.com/dechinphy/</a></p>
<p>请博主喝咖啡：<a href="https://www.cnblogs.com/dechinphy/gallery/image/379634.html" target="_blank">https://www.cnblogs.com/dechinphy/gallery/image/379634.html</a></p>
<h1 id="参考链接">参考链接</h1>
<ol>
<li><a href="https://chatboxai.app/zh/help-center/connect-chatbox-remote-ollama-service-guide" target="_blank" rel="noopener nofollow">https://chatboxai.app/zh/help-center/connect-chatbox-remote-ollama-service-guide</a></li>
</ol>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.08357289001041666" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-02-06 10:16">2025-02-06 10:16</span>&nbsp;
<a href="https://www.cnblogs.com/dechinphy">DECHIN</a>&nbsp;
阅读(<span id="post_view_count">36</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18699554" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18699554);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18699554', targetLink: 'https://www.cnblogs.com/dechinphy/p/18699554/deepseek', title: 'Ubuntu Linux部署DeepSeek' })">举报</a>
</div>
        