
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/dechinphy/p/18702523/deepseek2" title="发布于 2025-02-07 15:12">
    <span role="heading" aria-level="2">Windows11本地部署DeepSeek加速</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                    <div id="cnblogs_post_description" style="display: none">
        <img src="https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250207151156735-2030046149.png" alt="Windows11本地部署DeepSeek加速" class="desc_img">
        本文介绍了一个可以相比之下更快速的在本地部署DeepSeek的方法，除了在上一篇博客中介绍的从Github或者Github加速网站获取Ollama之外，还可以通过从国内的其他大模型文件平台下载模型文件，来加速本地模型的构建。
    </div>
<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h1 id="技术背景">技术背景</h1>
<p>在上一篇<a href="https://www.cnblogs.com/dechinphy/p/18699554/deepseek" target="_blank">文章</a>中我们介绍了在Ubuntu Linux操作系统上部署了一个DeepSeek-R1：14B，再通过其他电脑远程调用模型进行生成的方法。这里我们介绍一下Windows11安装Ollama+DeepSeek-R1模型的加速方法，因为这几天DeepSeek实在太火了，导致官方模型下载渠道网络不是很稳定，但其实有其他的方法可以加速这个下载过程。</p>
<h1 id="安装ollama">安装Ollama</h1>
<p>跟Ubuntu Linux上的操作比较类似，也是要从<a href="https://ollama.com/download" target="_blank" rel="noopener nofollow">Ollama官网</a>下载一个安装文件，然后直接双击安装就好了，没有配置安装路径的选项。安装完成后，可以在cmd中查看ollama的版本：</p>
<pre><code class="language-bash">\DeepSeek\models&gt; ollama --version
ollama version is 0.5.7
</code></pre>
<p>因为还没有下载模型，所以这里模型列表是空的：</p>
<pre><code class="language-bash">\DeepSeek\models&gt; ollama list
NAME    ID    SIZE    MODIFIED
\DeepSeek\models&gt;
</code></pre>
<p>接下来，我们要用ollama的create选项来构建一个本地模型，可以先查看这个操作的操作文档：</p>
<pre><code class="language-bash">\DeepSeek\models&gt; ollama create --help
Create a model from a Modelfile

Usage:
  ollama create MODEL [flags]

Flags:
  -f, --file string       Name of the Modelfile (default "Modelfile"
  -h, --help              help for create
  -q, --quantize string   Quantize model to this level (e.g. q4_0)

Environment Variables:
      OLLAMA_HOST                IP Address for the ollama server (default 127.0.0.1:11434)
</code></pre>
<h1 id="模型文件下载">模型文件下载</h1>
<p>最耗时的时间大概就是这一步，所以我建议是找一个下午启动这个任务，然后下载一个晚上，第二天再来构建。因为<code>ollama pull</code>近期存在网络不稳定的问题，这里提供的方案直接跳过这个操作，但这个操作也是最直接最方便的，头铁的也可以试一试。我个人比较推荐的是下载一个gguf模型文件到本地进行构建，这样一来还可以自定义文件模型存放的地址，不至于把某些资源紧缺的磁盘给搞崩了。关键的是下载的渠道，常规的是从huggingface官网去下载模型，但是网络可能会有问题。第二选择是去hf-mirror.com镜像网站下载模型，这也是比较多人推荐的，但是我本地访问这个镜像网站似乎也有问题。所以最终我选择了从阿里的<a href="https://www.modelscope.cn/" target="_blank" rel="noopener nofollow">ModelScope</a>下载模型guff文件，网络没有很快，但是非常稳定，而且我们本地最常使用的也是DeepSeek-R1从Qwen蒸馏出来的版本。我这次下载了<a href="https://www.modelscope.cn/models/unsloth/DeepSeek-R1-Distill-Qwen-7B-GGUF/files" target="_blank" rel="noopener nofollow">7B</a>和<a href="https://www.modelscope.cn/models/unsloth/DeepSeek-R1-Distill-Qwen-32B-GGUF/files" target="_blank" rel="noopener nofollow">32B</a>的模型，一般可以选择其中最大的那个模型下载到本地进行构建。</p>
<p>再说一下硬件配置，我的单机硬件是3080Ti 16GB独显，运行32B显然是有点吃力，也就是能跑的样子。如果硬件条件跟我差不多的，建议上7B，如果是双显卡的，上14B这样压力会比较小，生成的速度也会比较快。</p>
<h1 id="模型构建配置">模型构建配置</h1>
<p>使用ollama进行模型构建时，需要在本地创建一个模型配置文件，一般命名为<code>Modelfile</code>，这里有一个比较简单的参考：</p>
<pre><code class="language-txt"># gguf模型文件路径  
FROM .\DeepSeek-R1-Distill-Qwen-32B-Q8_0.gguf

# 模型模板配置  
TEMPLATE """{{- if .System }}{{ .System }}{{ end }}  
{{- range $i, $_ := .Messages }}  
{{- $last := eq (len (slice $.Messages $i)) 1}}  
{{- if eq .Role "user" }}  
  user: {{ .Content }}  
{{- else if eq .Role "assistant" }}  
  assistant: {{ .Content }}{{- if not $last }}  
    {{- end }}  
{{- end }}  
{{- if and $last (ne .Role "assistant") }}  
  {{- end }}  
{{- end }}"""  

PARAMETER stop "&lt;｜begin▁of▁sentence｜&gt;"
PARAMETER stop "&lt;｜end▁of▁sentence｜&gt;"
PARAMETER stop "&lt;｜User｜&gt;"
PARAMETER stop "&lt;｜Assistant｜&gt;"
</code></pre>
<p>这是一个32B的模型配置文件，7B的同理，只是改一下模型源文件的地址。然后使用ollama进行构建：</p>
<pre><code class="language-bash">\DeepSeek\models&gt; ollama create deepseek-r1-32B -f .\Modelfile
gathering model components
gathering model components
copying file sha256:e74588a163eb2f861e5b298c0975101cf02ec7b10784832b8feab2abbf3090a7 100%
parsing GGUF
using existing layer sha256:e74588a163eb2f861e5b298c0975101cf02ec7b10784832b8feab2abbf3090a7
creating new layer sha256:c6df2d7e28451caa13e70224d8333d5c06e2ab519086ecde7b7fe9422695ddbd
creating new layer sha256:f4d24e9138dd4603380add165d2b0d970bef471fac194b436ebd50e6147c6588
writing manifest
success
</code></pre>
<p>这里<code>deepseek-r1-32B</code>是自定义的本地模型名称，两个模型都构建完成后，可以在本地查看模型列表：</p>
<pre><code class="language-bash">PS A:\DeepSeek\models&gt; ollama list
NAME                      ID              SIZE      MODIFIED
deepseek-r1-7B:latest     d4d0e06b9cc3    8.1 GB    6 seconds ago
deepseek-r1-32B:latest    ac5d447875fb    34 GB     4 hours ago
</code></pre>
<p>然后推荐在本地安装一个ChatBox跟大模型进行交互，但是这里不重新介绍了，ChatBox这部分可以参考<a href="https://www.cnblogs.com/dechinphy/p/18699554/deepseek" target="_blank">上一篇博客</a>。</p>
<h1 id="deepseek精彩对话">DeepSeek精彩对话</h1>
<p>以下对话使用的是7B的模型，也许7B对于很多场景来说能力有点欠缺，但是DeepSeek的这个逻辑能力确实是惊艳到我了，这还只是一个在笔记本上运行的小模型。</p>
<h2 id="978294263579是质数吗">978294263579是质数吗</h2>
<p>经过分析和计算，我们发现978,294,263,579可以被17整除，余数为0。因此，这个数是一个合数。</p>
<p><span class="math inline">\(\boxed{\text{否}}\)</span></p>
<h2 id="2的13次方-1是多少">2的13次方-1是多少</h2>
<p>好的，让我来计算一下2的13次方减去1等于多少。</p>
<p>首先，计算2的幂：</p>
<p></p><div class="math display">\[\left[
\begin{align*}
2^1 &amp;= 2 \\
2^2 &amp;= 4 \\
2^3 &amp;= 8 \\
2^4 &amp;= 16 \\
2^5 &amp;= 32 \\
2^6 &amp;= 64 \\
2^7 &amp;= 128 \\
2^8 &amp;= 256 \\
2^9 &amp;= 512 \\
2^{10} &amp;= 1024 \\
2^{11} &amp;= 2048 \\
2^{12} &amp;= 4096 \\
2^{13} &amp;= 8192 \\
\end{align*}
\right]
\]</div><p></p><p>然后，计算2的13次方减去1：</p>
<p></p><div class="math display">\[\left[
8192 - 1 = 8191
\right]
\]</div><p></p><p>接下来，检查一下8191是否为质数。通过一些简单的测试，比如试除法，可以发现它不能被除了1和自身之外的任何小质数整除。因此，我们可以确认8191是一个质数。</p>
<p>最终答案是：</p>
<p></p><div class="math display">\[\left[
\boxed{8191}
\right]
\]</div><p></p><h1 id="资源占用">资源占用</h1>
<h2 id="7b模型">7B模型</h2>
<p>7B模型下载下来大概7.54GB，显存占用情况如下：</p>
<div align="center">
	<img src="https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250207150328245-2123861373.png">
</div>
<h2 id="32b模型">32B模型</h2>
<p>32B模型下载下来大概32.4GB，这个模型压缩率已经很高了，就算用7Z格式压缩，也只能压到31GB。显存占用情况如下：</p>
<div align="center">
	<img src="https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250207150331242-634631695.png">
</div>
<p>这个基本上就是超显存了，所以运行起来一个字一个字的吐，相比之下7B的相应速度要快很多，体验也会好很多。B站上有人用<a href="https://www.bilibili.com/video/BV1bxwaeAELQ/" target="_blank" rel="noopener nofollow">32B模型解高中数学题</a>，看起来似乎效果还不错。</p>
<h1 id="总结概要">总结概要</h1>
<p>本文介绍了一个可以相比之下更快速的在本地部署DeepSeek的方法，除了在上一篇博客中介绍的从Github或者Github加速网站获取Ollama之外，还可以通过从国内的其他大模型文件平台下载模型文件，来加速本地模型的构建。</p>
<h1 id="版权声明">版权声明</h1>
<p>本文首发链接为：<a href="https://www.cnblogs.com/dechinphy/p/deepseek2.html" target="_blank">https://www.cnblogs.com/dechinphy/p/deepseek2.html</a></p>
<p>作者ID：DechinPhy</p>
<p>更多原著文章：<a href="https://www.cnblogs.com/dechinphy/" target="_blank">https://www.cnblogs.com/dechinphy/</a></p>
<p>请博主喝咖啡：<a href="https://www.cnblogs.com/dechinphy/gallery/image/379634.html" target="_blank">https://www.cnblogs.com/dechinphy/gallery/image/379634.html</a></p>
<h1 id="参考链接">参考链接</h1>
<ol>
<li><a href="https://blog.csdn.net/xx_nm98/article/details/145460770" target="_blank" rel="noopener nofollow">https://blog.csdn.net/xx_nm98/article/details/145460770</a></li>
<li><a href="https://www.bilibili.com/video/BV1bxwaeAELQ/" target="_blank" rel="noopener nofollow">https://www.bilibili.com/video/BV1bxwaeAELQ/</a></li>
</ol>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.04180336980555555" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-02-07 15:18">2025-02-07 15:12</span>&nbsp;
<a href="https://www.cnblogs.com/dechinphy">DECHIN</a>&nbsp;
阅读(<span id="post_view_count">76</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18702523" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18702523);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18702523', targetLink: 'https://www.cnblogs.com/dechinphy/p/18702523/deepseek2', title: 'Windows11本地部署DeepSeek加速' })">举报</a>
</div>
        