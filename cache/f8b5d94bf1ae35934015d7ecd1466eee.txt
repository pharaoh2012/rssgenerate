
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/flyup/p/18849524" title="发布于 2025-04-27 15:14">
    <span role="heading" aria-level="2">Bagging、Boosting、Stacking的原理</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p>  Bagging、Boosting、Stacking是常见集成学习的形式，它们都是通过对多个学习器进行有机组合，达到比单个学习器性能更好的目标。</p>
<h1 id="一bagging">一、Bagging</h1>
<h2 id="1算法概述">1.算法概述</h2>
<p>  Bagging（Bootstrap Aggregating）算法即自助聚合算法，是一种基于统计学习理论的集成学习算法，主要用于提高机器学习模型的稳定性和泛化能力。具体来说就是，先以Bootstrap方式（有放回重复采样）构造多个样本集，每个样本集分别训练得到一个学习器，最后将各学习器的输出综合起来，得到一个最终的输出，如果是分类，多采用多数投票的方式，如果是回归，采用取平均的方式，代表性算法如随机森林。</p>
<p><strong>示意图</strong><br>
<img src="https://img2024.cnblogs.com/blog/2197714/202504/2197714-20250427150745789-1879473196.png" alt="" loading="lazy"></p>
<h2 id="2算法过程">2.算法过程</h2>
<h3 id="1-数据采样">(1) 数据采样</h3>
<p>  从原始训练数据集<span class="math inline">\(D\)</span>中，采用有放回的抽样方式，随机抽取 <span class="math inline">\(n\)</span>个样本，组成一个新的训练数据集 <span class="math inline">\(D_i\)</span>。这个过程会重复<span class="math inline">\(K\)</span>次，从而得到<span class="math inline">\(K\)</span>个不同的自助样本集<span class="math inline">\(\left\{ D_1,D_2,...,D_K \right\}\)</span> 。由于是有放回抽样，每个自助样本集中可能会包含一些重复的样本，同时也会有一些原始数据集中的样本未被选中。</p>
<h3 id="2模型训练">(2）模型训练</h3>
<p>  对于每个自助样本集<span class="math inline">\(D_i\)</span>，分别使用相同的基学习算法（如决策树、神经网络等）进行训练，得到<span class="math inline">\(K\)</span>个不同的基模型<span class="math inline">\(\left\{ h_1,h_2,...,h_K \right\}\)</span>。这些基模型在训练过程中会学习到不同的特征和模式，因为它们所使用的训练数据是不同的。</p>
<h3 id="3-模型融合">(3) 模型融合</h3>
<p>  将训练好的<span class="math inline">\(K\)</span>个基模型进行融合，以得到最终的预测结果。对于分类任务，通常采用投票法，即让<span class="math inline">\(K\)</span>个基模型对测试样本进行预测，然后统计每个类别出现的次数，将得票最多的类别作为最终的预测结果。对于回归任务，一般采用平均法，即计算<span class="math inline">\(K\)</span>个基模型对测试样本的预测值的平均值，将其作为最终的预测结果。</p>
<p><strong>过程图示如下</strong><br>
<img src="https://img2024.cnblogs.com/blog/2197714/202504/2197714-20250427150909193-303857041.png" alt="" loading="lazy"></p>
<p>  通过 Bagging 算法，可以将多个基模型进行集成，充分利用它们之间的差异，降低模型的方差，从而提高模型的稳定性和泛化能力。尤其是在处理高方差的基模型（如决策树）时，Bagging 算法通常能取得较好的效果。</p>
<h1 id="二boosting">二、Boosting</h1>
<h2 id="1算法概述-1">1.算法概述</h2>
<p>  Boosting是指逐层提升，其核心思想是“前人栽树，后人乘凉”，也就是使后一个学习器在前一个学习器的基础上进行增强，进而将多个弱学习器通过某种策略集成一个强学习器，以实现更好的预测效果。常见的提升算法如AdaBoost、XGBoost、CatBoost、LightGBM等。</p>
<p><strong>示意图</strong><br>
<img src="https://img2024.cnblogs.com/blog/2197714/202504/2197714-20250427150950783-1708326706.png" alt="" loading="lazy"></p>
<h2 id="2算法过程-1">2.算法过程</h2>
<h3 id="1设置初始样本权重">（1）设置初始样本权重</h3>
<p>  在算法开始时，为训练数据集中的每一个样本设定一个相同的权重。如对于样本集<span class="math inline">\(D=\left\{ (x_1,y_1),(x_2,y_2),...,(x_n,y_n) \right\}\)</span>，初始权重为<span class="math inline">\(w^{(1)}=\left( w_{1}^{(1)} ,w_{2}^{(1)},...,w_{n}^{(1)}  \right)\)</span> ，其中<span class="math inline">\(w_{i}^{(1)}=\frac{1}{n}\)</span>，即在第一轮训练时，每个样本在模型训练中的重要度是相同的。</p>
<h3 id="2训练弱学习器">（2）训练弱学习器​</h3>
<p>  基于当前的权重分布，训练一个弱学习器。基于当前的权重分布，训练一个弱学习器。弱学习器是指一个性能仅略优于随机猜测的学习算法，例如决策树桩（一种简单的决策树，通常只有一层）。在训练过程中，弱学习器会根据样本的权重来调整学习的重点，更关注那些权重较高的样本。</p>
<h3 id="3-计算弱学习器的权重">(3) 计算弱学习器的权重</h3>
<p>  根据弱学习器在训练集上的分类错误率，计算该弱学习器的权重。错误率越低，说明该弱学习器的性能越好，其权重也就越大；反之，错误率越高的弱学习器权重越小。通常使用的计算公式为</p>
<p></p><div class="math display">\[\alpha=\frac{1}{2}ln\left( \frac{1-\varepsilon}{\varepsilon} \right)
\]</div><p></p><p>  其中<span class="math inline">\(\varepsilon\)</span>是该弱学习器的错误率。</p>
<h3 id="4-更新训练数据的权重分布">(4) 更新训练数据的权重分布</h3>
<p>  根据当前数据的权重和弱学习器的权重，更新训练数据的权重分布。具体的更新规则是，对于被正确分类的样本，降低其权重；对于被错误分类的样本，提高其权重。这样，在下一轮训练中，弱学习器会更加关注那些之前被错误分类的样本，从而有针对性地进行学习。公式为</p>
<p></p><div class="math display">\[\begin{equation}     w_{i}^{(t+1)}=\frac{w_{i}^{(t)}}{Z_t}\cdot     \begin{cases}         e^{-\alpha_t}, \hspace{0.5em}  if \hspace{0.5em} h_t(x_i)=y_i  \\          e^{\alpha_t}, \hspace{0.5em}  if \hspace{0.5em} h_t(x_i)\ne y_i      \end{cases} \end{equation}
\]</div><p></p><p>  其中，<span class="math inline">\(w_{i}^{(t)}\)</span>是第<span class="math inline">\(t\)</span> 轮中第<span class="math inline">\(i\)</span>个样本的权重，<span class="math inline">\(Z_t\)</span>是归一化因子，确保更新后的样本权重之和为 1，<span class="math inline">\(h_t(x_i)\)</span>是第<span class="math inline">\(t\)</span>个弱学习器对第<span class="math inline">\(i\)</span>个样本的预测结果。</p>
<h3 id="5-重复以上步骤">(5) 重复以上步骤</h3>
<p>  不断重复训练弱学习器、计算弱学习器权重、更新数据权重分布的过程，直到达到预设的停止条件，如训练的弱学习器数量达到指定的上限，或者集成模型在验证集上的性能不再提升等。</p>
<h3 id="6构建集成模型">（6）构建集成模型</h3>
<p>  将训练好的所有弱学习器按照其权重进行组合，得到最终的集成模型。如训练得到一系列弱学习器<span class="math inline">\(h_1,h_2,...,h_T\)</span>及其对应的权重<span class="math inline">\(\alpha_1,\alpha_2,...,\alpha_T\)</span>，最终的强学习器<span class="math inline">\(H(X)\)</span>通过对这些弱学习器进行加权组合得到。对于分类问题，通常采用符号函数<span class="math inline">\(H\left( X \right)=sign\left( \sum_{t=1}^{T}{\alpha_th_t(X)} \right)\)</span>输出；对于回归问题，则可采用加权平均的方式输出。</p>
<p><strong>过程图示如下</strong><br>
<img src="https://img2024.cnblogs.com/blog/2197714/202504/2197714-20250427151026414-870320936.png" alt="" loading="lazy"><br>
  通过以上过程，Boosting 算法能够不断调整样本权重，让后续的弱学习器更加关注之前被错误分类的样本，从而逐步提升模型的性能，将多个弱学习器集成起来形成一个性能较强的学习器。</p>
<h1 id="三stacking">三、Stacking</h1>
<h2 id="1算法概述-2">1.算法概述</h2>
<p>  Stacking（堆叠集成）算法也是一种集成学习方法，它通过组合多个基学习器的预测结果，训练一个更高层次的模型（元学习器），从而获得比单个模型更好的预测性能。其核心思想在于利用不同算法在处理数据时的互补性，通过多层学习，挖掘数据的特征和规律，以实现更准确的预测。</p>
<p><strong>示意图</strong><br>
<img src="https://img2024.cnblogs.com/blog/2197714/202504/2197714-20250427151111483-1363334135.png" alt="" loading="lazy"></p>
<h2 id="2算法过程-2">2.算法过程</h2>
<h3 id="1训练多个基学习器">(1)训练多个基学习器</h3>
<p>  选择若干个不同的学习算法（如决策树、支持向量机、神经网络等）训练多个基学习器，记为<span class="math inline">\(h_1,h_2,...,h_K\)</span>。</p>
<h3 id="2构建新训练集">(2)构建新训练集</h3>
<p>  将各基学习器在训练集上的预测值作为一个新的特征矩阵，与原始训练集的标签相组合，构成一个新的训练集。</p>
<h3 id="3训练元学习器">(3)训练元学习器</h3>
<p>  选择一个学习算法（如逻辑回归、决策树等）在新训练集上训练学习器，即元学习器。表达式为$$H\left( X \right)=F\left( h_1(X),h_2(X),...,h_K(X) \right)$$</p>
<p><strong>过程图示如下</strong><br>
<img src="https://img2024.cnblogs.com/blog/2197714/202504/2197714-20250427151146238-1211139095.png" alt="" loading="lazy"><br>
  Stacking方法将基学习器对数据的预测结果作为元学习器的输入特征，元学习器输出最终的预测结果。这种分层训练和预测的方式，能够充分利用不同算法的优势，捕捉数据中更复杂的关系。</p>
<p><em><strong>End.</strong></em></p>
<br>
<p><a href="https://download.csdn.net/download/Albert201605/90699175" target="_blank" rel="noopener nofollow">全文pdf下载</a></p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.044301078040509256" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-04-27 15:15">2025-04-27 15:14</span>&nbsp;
<a href="https://www.cnblogs.com/flyup">归去_来兮</a>&nbsp;
阅读(<span id="post_view_count">5</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18849524);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18849524', targetLink: 'https://www.cnblogs.com/flyup/p/18849524', title: 'Bagging、Boosting、Stacking的原理' })">举报</a>
</div>
        