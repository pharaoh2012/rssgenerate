
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/huggingface/p/18715798" title="发布于 2025-02-14 17:30">
    <span role="heading" aria-level="2">让 LLM 来评判 | 奖励模型相关内容</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h1 id="奖励模型相关内容">奖励模型相关内容</h1>
<blockquote>
<p>这是 <strong>让 LLM 来评判</strong> 系列文章的第五篇，敬请关注系列文章:</p>
<ul>
<li>基础概念</li>
<li>选择 LLM 评估模型</li>
<li>设计你自己的评估 prompt</li>
<li>评估你的评估结果</li>
<li>奖励模型相关内容</li>
<li>技巧与提示</li>
</ul>
</blockquote>
<h2 id="什么是奖励模型">什么是奖励模型？</h2>
<p>奖励模型通过学习人工标注的成对 prompt 数据来预测分数，优化目标是对齐人类偏好。<br>
训练完成后，奖励模型可以作为人工评估代理的奖励函数，用来改进其他模型。</p>
<h3 id="成对比较评分">成对比较评分</h3>
<p>最常见的奖励模型类型是 Bradley-Terry 模型，它的输出是一个分值，遵循以下公式：</p>
<p></p><div class="math display">\[p(\text{答案 b 优于答案 a}) = \text{sigmoid}(\text{score}_b - \text{score}_a)
\]</div><p></p><p>奖励模型的训练数据只需要成对比较的答案，这比收集分数数据更容易。因此训练好的模型只能比较同一个 prompt 下的多个答案孰优孰劣，无法跨 prompt 比较。</p>
<p>其他模型在此方法的基础上进行了扩展，可以预测一个回答优于另一个的概率值 (例如 <a href="https://huggingface.co/RLHFlow/pair-preference-model-LLaMA3-8B" target="_blank" rel="noopener nofollow">基于 LLaMA3 的奖励模型</a>)。</p>
<p>这样模型就能 (理论上) 以数值来判断多个回答之间的细微差别，不过只能针对同一 prompt 对应的回答进行对比，跨 prompt 的回答概率值就没有对比意义了。另外当回答较长时，可能会受到上下文长度和内存限制的影响。</p>
<h3 id="绝对分数">绝对分数</h3>
<p>还有一些奖励模型 (如 <a href="https://arxiv.org/abs/2311.09528" target="_blank" rel="noopener nofollow">SteerLM</a>) 的输出是绝对分数。这类模型使用起来更加方便，可以直接对回答评估分数，而无需构造成对。但是数据收集就比较困难了，因为在衡量人类偏好时，绝对分数就显得相对不那么稳定。</p>
<p>最近有人提出了更强力的模型，可以同时输出绝对分数和相对分数。如 <a href="https://arxiv.org/abs/2410.01257" target="_blank" rel="noopener nofollow">HelpSteer2-Preference</a> 和 <a href="https://arxiv.org/abs/2406.12845" target="_blank" rel="noopener nofollow">ArmoRM</a>。</p>
<h2 id="奖励模型用于评估的方法">奖励模型用于评估的方法</h2>
<p>给定一个 prompts 数据集，输入 LLM 生成回答，并请求奖励模型对回答评分。</p>
<p>如果使用的奖励模型输出是绝对分数，可以对所有回答的分数求平均来获取最终得分。</p>
<p>其实更常用的奖励模型输出是相对分数，对其求平均可能会受到异常值的影响 (某些非常好或非常差的回答)，因为不同 prompt 的评估分数可能具有不同的尺度 (某些 prompt 会比其他的更简单或困难)。</p>
<p>总上，我们可以使用：</p>
<ul>
<li>胜率 (win rate)：取一组参考回答，计算模型回答优于参考回答的百分比，这种结果会更加精细。</li>
<li>胜算概率 (win probabilities)：取一组参考回答，计算模型回答优于参考回答的平均概率，这种结果能够提供更细致和平滑的信号。</li>
</ul>
<h2 id="奖励模型的优劣势">奖励模型的优劣势</h2>
<p>优势：</p>
<ul>
<li><strong>非常迅速</strong>：奖励模型只需要得到一个分数 (与 LLM 评估模型需要得到长文本不同)，因此其推理速度和小模型速度相当。</li>
<li><strong>具有确定性</strong>：前向过程相同，最终得分也会保持一致。</li>
<li><strong>对位置偏差不敏感</strong>：大多数奖励模型一次只处理一个回答，所以很少受到顺序的影响。即使对于需要处理成对回答的奖励模型，只要它在训练时使用的数据在顺序上是均衡的，受位置偏差的影响很非常小。</li>
<li><strong>无需 prompt 工程</strong>：很明显，奖励模型的任务目标只有一个，就是通过训练偏好数据来对一个或两个回答输出分数。</li>
</ul>
<p>劣势：</p>
<ul>
<li><strong>需要特定微调</strong>：即便微调后继承了基础模型的许多能力，不过在超出训练集范围的任务上可能还是表现不佳，另外这一步的成本会相对偏贵。</li>
<li><strong>在强化学习和任务评估领域 (或使用直接对齐算法处理与奖励模型训练集相似的数据集时) 效率较低</strong>：语言模型会过拟合奖励模型的偏好数据。</li>
</ul>
<h2 id="使用奖励模型进行评估的技巧与提示">使用奖励模型进行评估的技巧与提示</h2>
<ul>
<li><a href="https://huggingface.co/spaces/allenai/reward-bench" target="_blank" rel="noopener nofollow">RewardBench Leaderboard</a>：奖励模型排行榜，可以找到很多高性能模型。</li>
<li><a href="https://arxiv.org/abs/2406.11704" target="_blank" rel="noopener nofollow">Nemotron</a> 论文中介绍了奖励模型的使用经验。</li>
<li>对于那些仅评分单个 prompt 与回答的奖励模型，可以缓存多个模型结果，当测试新模型的表现时就能够很快得到结论。</li>
<li><a href="https://arxiv.org/abs/2410.11677v1" target="_blank" rel="noopener nofollow">这篇论文</a> 对训练过程中的胜率或胜率概率进行了追踪整理，可以帮助检测模型退化以及选择最佳权重。</li>
</ul>
<hr>
<blockquote>
<p>英文原文: <a href="https://raw.githubusercontent.com/huggingface/evaluation-guidebook/refs/heads/main/translations/zh/contents/model-as-a-judge/what-about-reward-models.md" target="_blank" rel="noopener nofollow">https://raw.githubusercontent.com/huggingface/evaluation-guidebook/refs/heads/main/translations/zh/contents/model-as-a-judge/what-about-reward-models.md</a></p>
<p>原文作者: clefourrier</p>
<p>译者: SuSung-boy</p>
<p>审校: adeenayakup</p>
</blockquote>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.8673237771481481" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-02-14 17:31">2025-02-14 17:30</span>&nbsp;
<a href="https://www.cnblogs.com/huggingface">HuggingFace</a>&nbsp;
阅读(<span id="post_view_count">57</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18715798" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18715798);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18715798', targetLink: 'https://www.cnblogs.com/huggingface/p/18715798', title: '让 LLM 来评判 | 奖励模型相关内容' })">举报</a>
</div>
        