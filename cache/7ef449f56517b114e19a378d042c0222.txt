
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/niuben/p/18872088" title="发布于 2025-05-12 09:08">
    <span role="heading" aria-level="2">Ubuntu20.04 搭建Kubernetes 1.28版本集群</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h1 id="环境依赖">环境依赖</h1>
<p>以下操作，无特殊说明，<strong>所有节点都需要执行</strong></p>
<ul>
<li><strong>安装 ssh 服务</strong></li>
</ul>
<blockquote>
<ol>
<li>安装 <code>openssh-server</code></li>
</ol>
<pre><code class="language-shell">sudo apt-get install openssh-server
</code></pre>
<ol start="2">
<li>修改配置文件</li>
</ol>
<pre><code class="language-shell">vim /etc/ssh/sshd_config
</code></pre>
<p>找到配置项</p>
<pre><code class="language-text">LoginGraceTime 120
PermitRootLogin prohibit-password
StrictModes yes
</code></pre>
<p>把 <code>prohibit-password</code> 改为 <code>yes</code>，如下：</p>
<pre><code class="language-shell">LoginGraceTime 120
PermitRootLogin yes
StrictModes yes
</code></pre>
</blockquote>
<ul>
<li>设置节点，<code>ssh</code> 到其他节点免密</li>
</ul>
<pre><code class="language-bash">ssh-keygen -t rsa
</code></pre>
<p>这将生成一个 <code>RSA</code> 密钥对。默认情况下，这些文件会存储在<code>~/.ssh/</code>目录下，分别是<code>id_rsa</code>（私钥）和<code>id_rsa.pub</code>（公钥）。按提示操作，或者直接按回车键接受默认设置。如果已经存在密钥对，可以跳过此步。</p>
<p><strong>复制公钥到远程主机</strong>：使用 <code>ssh-copy-id</code> 命令来将你的公钥添加到远程主机的<code>~/.ssh/authorized_keys</code>文件中。</p>
<pre><code class="language-bash">ssh-copy-id user@remote_host
</code></pre>
<ul>
<li><strong>设置主机名,保证每个节点名称都不相同</strong></li>
</ul>
<pre><code class="language-shell">hostnamectl set-hostname xxx
</code></pre>
<ul>
<li><strong>同步节点时间</strong></li>
</ul>
<blockquote>
<ol>
<li>安装 ntpdate</li>
</ol>
<pre><code class="language-shell">sudo apt-get -y install ntpdate
</code></pre>
<ol start="2">
<li>配置 crontab，添加定时任务</li>
</ol>
<pre><code class="language-shell">crontab -e

0 */1 * * * ntpdate time1.aliyun.com
</code></pre>
</blockquote>
<h2 id="关闭防火墙">关闭防火墙</h2>
<pre><code class="language-bash">systemctl stop firewalld &amp;&amp; systemctl disable firewalld

or

ufw disable
</code></pre>
<h2 id="iptables-配置">iptables 配置</h2>
<p>为了让 <code>Kubernetes</code> 能够检查、转发网络流量，需要修改 <code>iptables</code> 的配置，并启用 <code>br_netfilter</code>模块。</p>
<pre><code class="language-bash">cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF

cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward=1 # better than modify /etc/sysctl.conf
EOF

sudo sysctl --system # 手动加载所有的配置文件
</code></pre>
<h2 id="关闭-swap-分区">关闭 swap 分区</h2>
<p>关闭 <code>Linux</code> 的 <code>swap</code> 分区，提升 <code>Kubernetes</code> 的性能。</p>
<p>复制代码</p>
<pre><code class="language-bash"># 临时关闭
sudo swapoff -a
# 永久关闭：注释 swap 挂载，给 swap 这行开头加一下 "#" 注释
sudo sed -ri '/\sswap\s/s/^#?/#/' /etc/fstab 
</code></pre>
<blockquote>
<p>为什么要关闭 swap 交换分区？<br>
Swap 交换分区，如果机器内存不够，会使用 swap 分区，但是 swap 分区的性能较低，k8s 设计的时候为了能提升性能，默认是不允许使用交换分区的。Kubeadm 初始化的时候会检测 swap 是否关闭，如果没关闭，那就初始化失败。如果不想要关闭交换分区，安装k8s 的时候可以指定 --ignore-preflight-errors=Swap 来解决。</p>
</blockquote>
<h1 id="集群规划">集群规划</h1>
<p>每台上都安装 <code>docker-ce</code>、<code>docker-ce-cli</code>、<code>cri-docker</code>，使用 <code>cri-docker</code> 作为容器运行时，和 <code>kubelet</code> 交互。</p>
<p>所有节点都安装 <code>kubelet</code>、<code>kubeadm</code>、<code>kubectl</code> 软件包，都启动 <code>kubelet.service</code> 服务。</p>
<h2 id="配置-docker-和-k8s-的-apt源">配置 <code>docker</code> 和 <code>k8s</code> 的 <code>APT</code>源</h2>
<p><a href="https://developer.aliyun.com/mirror/kubernetes?spm=a2c6h.13651102.0.0.41a11b11qLEVmy" target="_blank" rel="noopener nofollow">k8s APT源新版配置方法</a></p>
<p>（比如需要安装 1.29 版本，则需要将如下配置中的 v1.28 替换成 v1.29）</p>
<pre><code class="language-bash">apt-get update &amp;&amp; apt-get install -y apt-transport-https
curl -fsSL https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.28/deb/Release.key |
    gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.28/deb/ /" |
    tee /etc/apt/sources.list.d/kubernetes.list
apt-get update
</code></pre>
<p>旧版 kubernetes 源只更新到 1.28 部分版本，不过 docker 源部分可以用</p>
<pre><code class="language-bash">sudo curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository "deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable"

sudo curl -fsSL https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add -
sudo add-apt-repository "deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main"

apt update
</code></pre>
<p>在 Ubuntu 系统中，可以通过以下两个位置查看源列表：</p>
<p><code>/etc/apt/sources.list</code> 文件：这是主要的源列表文件。你可以使用文本编辑器（如 <code>vi</code> 或 <code>nano</code>）以管理员权限打开该文件，查看其中列出的软件源。</p>
<p><code>/etc/apt/sources.list.d/</code> 目录：该目录包含额外的源列表文件。这些文件通常以 <code>.list</code> 扩展名结尾，并包含单独的软件源配置。</p>
<pre><code class="language-bash">apt-cache madison kubelet # 命令来列出可用的 kubelet 软件包版本。检查是否存在版本号为 '1.28.2-00' 的软件包。

/etc/apt/sources.list # apt软件系统源
</code></pre>
<h2 id="安装docker">安装docker</h2>
<pre><code class="language-bash">apt install docker-ce docker-ce-cli 
</code></pre>
<p>使用 <code>apt</code> 可以查看安装的 <code>docker</code> 两个软件。</p>
<pre><code class="language-bash">apt list --installed | grep -i -E 'docker'
</code></pre>
<h2 id="启动-docker-相关服务">启动 <code>docker</code> 相关服务</h2>
<p><code>ubuntu</code>上的 <code>dub</code> 安装后，如果有服务，会被自动设置为开机自启动，且装完就会拉起，这里给出验证。</p>
<pre><code class="language-bash">systemctl list-unit-files | grep -E 'docker'
</code></pre>
<p>两个服务都应是 <code>running</code> 状态</p>
<pre><code class="language-bash">systemctl status docker.service
systemctl status docker.socket
</code></pre>
<p>配置 <code>docker</code> 容器引擎使用 <code>systemd</code> 作为 <code>CGroup</code> 的驱动</p>
<p><code>vim /etc/docker/daemon.json</code></p>
<pre><code class="language-bash">{
    "exec-opts": ["native.cgroupdriver=systemd"]
}
</code></pre>
<pre><code>sudo systemctl enable docker	 # 配置Docker为开机自启动
sudo systemctl daemon-reload	 # 重新加载服务的配置文件
sudo systemctl restart docker  # 重启Docker
</code></pre>
<h2 id="配置cri-docker">配置cri-docker</h2>
<p><a href="https://github.com/Mirantis/cri-dockerd/releases" target="_blank" rel="noopener nofollow">https://github.com/Mirantis/cri-dockerd/releases</a></p>
<p>选择好适合自己的 <code>docker</code> 版本，进行安装</p>
<pre><code class="language-bash">sudo apt-get install ./cri-dockerd-&lt;version&gt;.deb

or

dpkg -i cri-dockerd-&lt;version&gt;.deb
</code></pre>
<p>验证 <code>cri-dockerd</code> 是否已安装并正在运行：</p>
<pre><code class="language-bash">systemctl status cri-docker
</code></pre>
<h3 id="修改-cri-docker-配置文件">修改 cri-docker 配置文件</h3>
<pre><code>vim  /usr/lib/systemd/system/cri-docker-service
</code></pre>
<p>修改第11行，增加 <code>--network-plugin=cni --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.9</code></p>
<pre><code class="language-bash">ExecStart=/usr/bin/cri-dockerd --network-plugin=cni --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.9
</code></pre>
<p>然后重载配置，设置开机自启，检查组件状态</p>
<pre><code class="language-bash">systemctl daemon-reload &amp;&amp; systemctl enable cri-docker.socket &amp;&amp; systemctl status cri-docker.socket cri-docker docker
</code></pre>
<h1 id="安装kubernetes">安装Kubernetes</h1>
<p>安装 <code>Kubernetes</code> ，以下需要在所有节点上进行。</p>
<p>列出可用的 kubelet 软件包版本。检查是否存在版本号为 <code>1.28.2-00</code> 的软件包。</p>
<pre><code class="language-bash">apt-cache madison kubelet
</code></pre>
<p>执行安装</p>
<pre><code class="language-bash">apt install kubelet kubeadm kubectl
# 如果需要指定安装1.28.2这个版本，则可以这样：
apt install kubelet=1.28.2-00 kubeadm=1.28.2-00 kubectl=1.28.2-00
</code></pre>
<p>配置 <code>kubelet</code>为开机自启动</p>
<pre><code>systemctl enable kubelet.service 
</code></pre>
<h2 id="版本锁定">版本锁定</h2>
<p>锁定这三个软件的版本，避免意外升级导致版本错误。</p>
<pre><code class="language-bash">sudo apt-mark hold kubeadm kubelet kubectl
</code></pre>
<h2 id="下载-kubernetes-组件镜像">下载 Kubernetes 组件镜像</h2>
<p>可以通过下面的命令看到 <code>kubeadm</code> 默认配置的 <code>kubernetes</code> 镜像，是外网的镜像</p>
<pre><code class="language-lua">kubeadm config images list
</code></pre>
<p>这里使用阿里的 <code>kubernetes</code> 镜像源，下载镜像</p>
<pre><code class="language-bash">kubeadm config images pull --image-repository=registry.aliyuncs.com/google_containers --kubernetes-version=v1.28.2 --cri-socket unix:///var/run/cri-dockerd.sock
</code></pre>
<p>配置 <code>kubelet</code> 指定运行时</p>
<pre><code class="language-bash">vim /etc/sysconfig/kubelet 
</code></pre>
<p>提示：如果没有 <code>sysconfig</code> 目录，请先创建，然后再创建 <code>kubelet</code> 文件；这一步就是告诉<code>kubelet cri-dockerd</code> 的接口在哪里；该配置不是必须的，我们也可以在初始化集群时在 <code>kubeadm</code> 命令上使用 <code>--cri-socket unix:///run/cri-dockerd.sock</code>选项来告诉 <code>kubelet cri-dockerd</code> 的<code>socket</code> 文件路径。</p>
<h2 id="kubeadm-初始化"><code>kubeadm</code> 初始化</h2>
<pre><code class="language-bash">kubeadm init \
  --apiserver-advertise-address=&lt;自己本机的公网IP&gt; \
  --image-repository=registry.aliyuncs.com/google_containers \
  --kubernetes-version v1.28.2 \
  --service-cidr=10.96.0.0/12 \
  --pod-network-cidr=10.244.0.0/16 \
  --cri-socket=unix:///var/run/cri-dockerd.sock
</code></pre>
<p>配置环境变量</p>
<pre><code class="language-bash">To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf
</code></pre>
<h1 id="使用-calico-网络插件">使用 Calico 网络插件</h1>
<p><code>master</code> 节点初始化后，可以使用 <code>kubectl get node</code> 来检查 <code>kubernetes</code> 集群节点状态，当前 <code>master</code> 节点的状态为 <code>NotReady</code>，这是由于缺少网络插件，集群的内部网络还没有正常运作。</p>
<pre><code class="language-bash">kubectl create -f https://calico-v3-25.netlify.app/archive/v3.25/manifests/calico.yaml
</code></pre>
<p><code>Calico</code> 使用的镜像较大，如果安装超时，可以考虑在每个节点上预先使用 <code>docker pull</code> 拉取镜像。</p>
<h1 id="其他节点加入集群">其他节点加入集群</h1>
<p>主节点在初始化结束后，已经创建了临时 <code>token</code>，但该临时 <code>token</code> 只有24小时有效期。</p>
<p>也可以重新在节点创建永久有效的 <code>token</code></p>
<pre><code class="language-bash">kubeadm token create --print-join-command 
</code></pre>
<p><code>worker</code> 节点加入</p>
<pre><code class="language-bash">kubeadm join &lt;master节点&gt;:6443 --token oyl72q.dth6p8kwi7fopsd6 \
	--discovery-token-ca-cert-hash sha256:b31bb54c63a550d287c89ddd0094e27ca680a6c3386a8630a75445de3c4d6e43 \
  --cri-socket=unix:///var/run/cri-dockerd.sock
</code></pre>
<h1 id="nvidia-驱动安装">Nvidia 驱动安装</h1>
<p>下载驱动：<a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener nofollow">https://developer.nvidia.com/cuda-downloads</a></p>
<p>根据选项，选择好自己的系统配置</p>
<p>执行 CUDA Toolkit Installer，给出的安装命令</p>
<p>最后再执行 Driver Install，给出的命令，驱动安装成功</p>
<h1 id="helm安装">helm安装</h1>
<p>使用脚本进行安装</p>
<pre><code class="language-bash">curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh
</code></pre>
<h1 id="nvidia-vgpu-安装">Nvidia vGPU 安装</h1>
<p><strong>添加 chart 仓库</strong></p>
<pre><code class="language-bash">helm repo add hami-charts https://project-hami.github.io/HAMi/
</code></pre>
<p><strong>根据 Kubernetes 版本，指定 imageTag</strong></p>
<pre><code class="language-bash">helm install hami hami-charts/hami --set scheduler.kubeScheduler.imageTag=v1.16.8 -n kube-system
</code></pre>
<p><strong>验证安装</strong></p>
<pre><code class="language-bash">kubectl get pods -n kube-system
</code></pre>
<p><strong>给节点打标签</strong></p>
<p>通过添加标签 "gpu=on" 来为调度 HAMi 标记 GPU 节点</p>
<p><strong>vGPU 设备配置</strong></p>
<p>编辑 ConfigMap，更改后，重启相关的 HAMi 组件以应用更新的配置。</p>
<pre><code class="language-bash">kubectl edit configmap hami-scheduler-device -n &lt;namespace&gt;
</code></pre>
<p><strong>ConfigMap 中的配置</strong></p>
<ul>
<li><code>nvidia.deviceMemoryScaling:</code> 浮点类型，默认值：1。NVIDIA 设备内存缩放比例，可以大于 1（启用虚拟设备内存，实验性功能）。对于具有 <em>M</em> 内存的 NVIDIA GPU，如果我们将 <code>nvidia.deviceMemoryScaling</code> 参数设置为 <em>S</em>，则通过此 GPU 分割的 vGPU 在 Kubernetes 中将总共获得 <code>S * M</code> 内存。</li>
<li><code>nvidia.deviceSplitCount:</code> 整数类型，默认值：10。分配给单个 GPU 设备的最大任务数。</li>
<li><code>nvidia.migstrategy:</code> 字符串类型，"none" 表示忽略 MIG 功能，"mixed" 表示通过独立资源分配 MIG 设备。默认值为 "none"。</li>
<li><code>nvidia.disablecorelimit:</code> 字符串类型，"true" 表示禁用核心限制，"false" 表示启用核心限制，默认值：false。</li>
<li><code>nvidia.defaultMem:</code> 整数类型，默认值：0。当前任务的默认设备内存，以 MB 为单位。'0' 表示使用 100% 设备内存。</li>
<li><code>nvidia.defaultCores:</code> 整数类型，默认值：0。为当前任务保留的 GPU 核心百分比。如果分配为 0，则可能适合任何具有足够设备内存的 GPU。如果分配为 100，则将独占使用整个 GPU 卡。</li>
<li><code>nvidia.defaultGPUNum:</code> 整数类型，默认值：1，如果配置值为 0，则配置值将不生效并被过滤。当用户未在 pod 资源中设置 nvidia.com/gpu 这个键时，webhook 应检查 nvidia.com/gpumem、resource-mem-percentage、nvidia.com/gpucores 这三个键，任意一个键有值，webhook 应将 nvidia.com/gpu 键和此默认值添加到资源限制映射中。</li>
<li><code>nvidia.resourceCountName:</code> 字符串类型，vgpu 数量资源名称，默认值："nvidia.com/gpu"。</li>
<li><code>nvidia.resourceMemoryName:</code> 字符串类型，vgpu 内存大小资源名称，默认值："nvidia.com/gpumem"。</li>
<li><code>nvidia.resourceMemoryPercentageName:</code> 字符串类型，vgpu 内存比例资源名称，默认值："nvidia.com/gpumem-percentage"。</li>
<li><code>nvidia.resourceCoreName:</code> 字符串类型，vgpu 核心资源名称，默认值："nvidia.com/cores"。</li>
<li><code>nvidia.resourcePriorityName:</code> 字符串类型，vgpu 任务优先级名称，默认值："nvidia.com/priority"。</li>
</ul>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.14189903775347223" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-05-12 09:09">2025-05-12 09:08</span>&nbsp;
<a href="https://www.cnblogs.com/niuben">牛奔</a>&nbsp;
阅读(<span id="post_view_count">5</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18872088);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18872088', targetLink: 'https://www.cnblogs.com/niuben/p/18872088', title: 'Ubuntu20.04 搭建Kubernetes 1.28版本集群' })">举报</a>
</div>
        