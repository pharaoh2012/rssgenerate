
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/Big-Yellow/p/18841821" title="å‘å¸ƒäº 2025-04-22 22:40">
    <span role="heading" aria-level="2">å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼šOpenRLHFæºç è§£è¯»ï¼Œæ¨¡å‹å¤„ç†</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                    <div id="cnblogs_post_description" style="display: none">
        
        æœ¬æ–‡ä¸»è¦ä»‹ç»äº†åœ¨ **OpenRLHF**ä¸­æ¨¡å‹æ¡†æ¶è®¾è®¡ï¼Œä¸»è¦åˆ†ä¸º3ç±»æ¨¡å‹ï¼š1ã€`actor model`ï¼›2ã€`critic model`ï¼›3ã€`reward model`è¿™ä¸‰ç±»æ¨¡å‹ä¸­åˆ†åˆ«èµ·åˆ°ä½œç”¨ï¼š1ã€ç›´æ¥æ›´å…·promptè¾“å‡ºresponseï¼›2ã€è¾“å‡ºtokençš„è¯„åˆ†ï¼ˆ`action_values = values[:, -3:]`ï¼‰ï¼›3ã€è¿”å›æ•´å¥è¾“å‡ºè¯„åˆ†ï¼ˆæ‰¾å‡ºæœ€åä¸€ä¸ªæœ‰æ•ˆ token çš„ç´¢å¼•ï¼Œç„¶åä» value å‘é‡ä¸­æå–è¯¥ä½ç½®çš„å€¼ä½œä¸º rewardã€‚ï¼‰
    </div>
<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h1 id="å¼ºåŒ–å­¦ä¹ æ¡†æ¶openrlhfæºç è§£è¯»æ¨¡å‹å¤„ç†"><a href="https://www.big-yellow-j.top/posts/2025/04/22/OpenRLHF-1.html" target="_blank" rel="noopener nofollow">å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼šOpenRLHFæºç è§£è¯»ï¼Œæ¨¡å‹å¤„ç†</a></h1>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç» <strong>å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼šOpenRLHFæºç è§£è¯»ï¼Œæ¨¡å‹å¤„ç†</strong></p>
<h2 id="modelsæ¡†æ¶è®¾è®¡">modelsæ¡†æ¶è®¾è®¡</h2>
<p>äº†è§£ä¸€ä¸‹ <strong>OpenRLHF</strong>çš„æ¨¡å‹æ¡†æ¶è®¾è®¡èŒƒå¼ï¼š</p>
<p><img src="https://img2024.cnblogs.com/blog/3395559/202504/3395559-20250422223843015-1614471598.png" alt="" loading="lazy"></p>
<blockquote>
<p>From:<a href="https://arxiv.org/pdf/2405.11143" target="_blank" rel="noopener nofollow">https://arxiv.org/pdf/2405.11143</a></p>
</blockquote>
<p>å¯ä»¥çŸ¥é“ä¸€ä¸ªå¤§æ¦‚çš„æµç¨‹ï¼šè¾“å…¥Pormpté€šè¿‡Actor modelè¾“å‡ºå›å¤ Responseï¼Œè€Œåå°†ä¸¤éƒ¨åˆ†è¿›è¡Œæ‹¼æ¥å†å»ç”±å…¶ä»–æ¨¡å‹è¿›è¡Œå¤„ç†</p>
<h3 id="1actorpy">1ã€actor.py</h3>
<blockquote>
<p><a href="https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/models/actor.py" target="_blank" rel="noopener nofollow">https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/models/actor.py</a></p>
</blockquote>
<p>è¿™éƒ¨åˆ†ä¸»è¦ä¸ºåŠ è½½æ‰€éœ€è¦çš„æ¨¡å‹</p>
<pre><code class="language-python">class Actor(nn.Module):
    def __init__(...):
        if isinstance(pretrain_or_model, str):
            ...
            self.model = model_class.from_pretrained(
                pretrain_or_model,
                trust_remote_code=True,
                attn_implementation=attn_implementation,
                quantization_config=nf4_config,
                torch_dtype=torch.bfloat16 if bf16 else "auto",
                device_map=device_map,
            )
            if lora_rank &gt; 0:
                self.model.enable_input_require_grads()
                lora_config = LoraConfig(
                    task_type=TaskType.CAUSAL_LM,
                    r=lora_rank,
                    lora_alpha=lora_alpha,
                    target_modules=target_modules,
                    lora_dropout=lora_dropout,
                    bias="none",
                )
                self.model = get_peft_model(self.model, lora_config)
                ...
        else:
            self.model = pretrain_or_model
    @torch.no_grad()
    def generate(self, input_ids: torch.Tensor, **kwargs):
        ...
        sequences = self.model.generate(**generate_args)
        eos_token_id = generate_args["eos_token_id"]
        pad_token_id = generate_args["pad_token_id"]
        return self.process_sequences(sequences, input_ids.size(1), eos_token_id, pad_token_id)
    def forward(...):
        ...
        output["logits"] = output["logits"].to(torch.float32) # å¾—åˆ°æ¯ä¸€ä¸ªtokenæ¦‚ç‡
        ...
        log_probs = log_probs_from_logits(
                    output["logits"][:, :-1, :], sequences[:, 1:], temperature=self.temperature
                )
        ...
        action_log_probs = log_probs[:, -num_actions:]

</code></pre>
<p>è¿™ä¸ªactoræ¯”è¾ƒç®€å•ï¼Œ<strong>é¦–å…ˆ</strong>ä»huggingfaceåŠ è½½éœ€è¦çš„æ¨¡å‹ï¼Œå¹¶ä¸”å¯¹æ¨¡å‹è¿›è¡Œéƒ¨åˆ†è®¾ç½®å¦‚ï¼šé‡åŒ–/loraå¾®è°ƒã€‚æˆ–è€…ç›´æ¥åŠ è½½è‡ªå·±é¢„è®­ç»ƒå¥½çš„æ¨¡å‹ã€‚<br>
1ã€<code>generate</code>ï¼šæ¨¡å—åˆ™æ˜¯æ ¹æ®è¾“å…¥çš„å†…å®¹ï¼ˆæ¯”å¦‚è¯´è¢« tokenizerå¤„ç†å¥½çš„æ–‡æœ¬ï¼‰input_idsé€šè¿‡æ¨¡å‹<strong>è¾“å‡ºæ–°çš„å†…å®¹</strong>ï¼ˆæ ¹æ® <code>**kwargs</code>è·å–ç”Ÿæˆæ–‡æœ¬å‚æ•°è®¾ç½®æ¯”å¦‚è¯´ï¼š<code>top_k</code>ç­‰ï¼‰<br>
2ã€<code>forward</code>ï¼š<strong>æ ¹æ®è¾“å…¥çš„ token åºåˆ—ï¼ˆsequencesï¼‰ï¼Œè®¡ç®—æ¨¡å‹åœ¨ç”Ÿæˆæœ€åè‹¥å¹²ä¸ª tokenï¼ˆå³ "åŠ¨ä½œ"ï¼‰æ—¶çš„å¯¹æ•°æ¦‚ç‡ï¼ˆlog probsï¼‰</strong>ï¼Œä¹‹æ‰€ä»¥è¦è¿™ä¹ˆå¤„ç†æ˜¯å› ä¸ºï¼Œåœ¨å¼ºåŒ–å­¦ä¹ æ¨¡å‹ä¸­ï¼ˆPPOã€DPOç­‰ï¼‰ä¸€èˆ¬è€Œè¨€æ¨¡å‹çš„è¾“å‡ºæ˜¯ä¸€ä¸ªåºåˆ—ï¼Œä½†ä¼˜åŒ–ç›®æ ‡ä¸æ˜¯â€œèƒ½ä¸èƒ½ç”Ÿæˆè¿™ä¸ªåºåˆ—â€ï¼Œè€Œæ˜¯ï¼šè¿™ä¸ªåºåˆ—ä¸­ï¼Œå“ªäº› token æ˜¯â€œå¥½â€çš„ï¼Ÿæ¨¡å‹å¯¹è¿™äº› token çš„æ¦‚ç‡åº”è¯¥æ›´é«˜ï¼æ¯”å¦‚è¯´åœ¨ <strong>DPO</strong>ä¸­ï¼š</p>
<p></p><div class="math display">\[L(Î¸) = E[ min(r(Î¸) * A, clip(r(Î¸), 1-Îµ, 1+Îµ) * A) ]
\]</div><p></p><p>é‡Œé¢çš„</p>
<p></p><div class="math display">\[r(\theta)=\pi_{\theta}(a|s)/\pi_{old}(a|s)
\]</div><p></p><p>å°±æ˜¯æ¦‚ç‡æ¯”å€¼ï¼Œä¸Šé¢ä»£ç ä¸­ï¼š</p>
<pre><code class="language-python">log_probs_from_logits(output["logits"][:, :-1, :], sequences[:, 1:], temperature=self.temperature)
</code></pre>
<p>è®¡ç®—çš„å°±æ˜¯ï¼š<span class="math inline">\(log(\pi_{\theta}(a|s))\)</span>ï¼Œåœ¨å…·ä½“ä»£ç ä¸­ï¼š</p>
<pre><code class="language-python">def log_probs_from_logits(logits: torch.Tensor, labels: torch.Tensor, temperature: float = 1.0) -&gt; torch.Tensor:
    if temperature != 1.0:
        logits.div_(temperature)
    if logits.dtype in [torch.float32, torch.float64]:
        batch_dim = logits.shape[:-1]
        last_dim = logits.shape[-1]
        try:
            from flash_attn.ops.triton.cross_entropy import cross_entropy_loss

            output = cross_entropy_loss(logits.reshape(-1, last_dim), labels.reshape(-1))
            log_probs_labels = -output[0].view(*batch_dim)
        except ImportError:
            logits_labels = torch.gather(logits, dim=-1, index=labels.unsqueeze(-1)).squeeze(-1)
            logsumexp_values = _logsumexp_by_chunk(logits.reshape(-1, last_dim))
            logsumexp_values = logsumexp_values.view(*batch_dim)
            log_probs_labels = logits_labels - logsumexp_values  # log_softmax(x_i) = x_i - logsumexp(x)
    else:
        log_probs_labels = []
        for row_logits, row_labels in zip(logits, labels):  # loop to reduce peak mem consumption
            row_log_probs = F.log_softmax(row_logits, dim=-1)
            row_log_probs_labels = row_log_probs.gather(dim=-1, index=row_labels.unsqueeze(-1)).squeeze(-1)
            log_probs_labels.append(row_log_probs_labels)
        log_probs_labels = torch.stack(log_probs_labels)
    return log_probs_labels
</code></pre>
<blockquote>
<p><strong>è¡¥å……-1</strong>ï¼š<br>
åœ¨ä½¿ç”¨ <code>AutoModelForCausalLM.from_pretrained</code>ä½¿ç”¨å¾—åˆ° <code>model</code>ä¹‹åï¼Œå…¶æ”¯æŒè¾“å…¥å‚æ•°ä¸ºï¼š</p>
</blockquote>
<pre><code class="language-python">outputs = model(
    input_ids=None,            # è¾“å…¥çš„tokenï¼ˆbatch_size, seq_lengthï¼‰
    attention_mask=None,       # æŒ‡ç¤ºå“ªäº› token æ˜¯æœ‰æ•ˆçš„ï¼ˆé paddingï¼‰ï¼Œå½¢çŠ¶åŒ input_ids
    position_ids=None,         # ä½ç½®ç¼–ç 
    past_key_values=None,
    inputs_embeds=None,
    use_cache=None,            # æ˜¯å¦ä½¿ç”¨k-v cache
    labels=None,               # è¾“å…¥æ ‡ç­¾å°±ç›´æ¥è®¡ç®—loss
    output_attentions=None,
    output_hidden_states=None,
    return_dict=None,
)
</code></pre>
<blockquote>
<p><strong>è¡¥å……-2</strong>ï¼š<br>
åœ¨LLMè®­ç»ƒè¿‡ç¨‹ä¸­é‡åˆ°è¿‡çŸ­çš„è¯­å¥ä¸ºäº†èŠ‚çº¦æ˜¾å­˜ï¼ˆå¦‚æœéƒ½å°†å†…å®¹è¡¥å……åˆ°ç›¸åŒé•¿åº¦ï¼Œé‚£ä¹ˆå°±ä¼šæœ‰è¾ƒå¤šçš„paddingé€ æˆæµªè´¹ï¼‰ï¼Œå› æ­¤å¯ä»¥å°†å‡ ä¸ªçŸ­çš„æ‹¼æ¥èµ·æ¥ï¼Œä½†æ˜¯ä¸ºäº†åŒºåˆ†é‚£äº›æ˜¯ä¸€ä¸ªå¥å­é‚£äº›ä¸æ˜¯çš„ï¼Œåœ¨ <strong>OpenRLHF</strong>ä¸­é€šè¿‡å‚æ•°ï¼š<code>self.packing_samples</code>ã€‚å¦‚æœæ²¡æœ‰ <code>packing</code>é‚£ä¹ˆç›´æ¥æ ¹æ® <code>attention_mask</code>å°†ä½ç½®ç¼–ç åœ¨å¤„ç†ä¸€ä¸‹</p>
</blockquote>
<pre><code class="language-python">if not self.packing_samples:
    position_ids = attention_mask.long().cumsum(-1) - 1
    position_ids.masked_fill_(attention_mask == 0, 1)
else:
    # convert attention_mask to position_ids
    if ring_attn_group is not None:
        labels = sequences
        sequences, attention_mask, position_ids = convert_ring_attn_params(
            sequences, attention_mask, packed_seq_lens, ring_attn_group
        )
    else:
        position_ids = reset_position_ids(attention_mask)
    # explicitly ignore attention_mask for packing_samples
    attention_mask = None
</code></pre>
<blockquote>
<p>å…¶ä¸­ <code>reset_position_ids</code>åšçš„å°±æ˜¯é‡æ–°åšä½ç½®ç¼–ç é‡æ–°å¤„ç†</p>
</blockquote>
<h3 id="2modelpy">2ã€model.py</h3>
<blockquote>
<p><a href="https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/models/model.py" target="_blank" rel="noopener nofollow">https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/models/model.py</a></p>
</blockquote>
<p><img src="https://img2024.cnblogs.com/blog/3395559/202504/3395559-20250422223842901-1120213596.png" alt="" loading="lazy"></p>
<p>ä¸»è¦åŠŸèƒ½è¿”å›æ‰€éœ€è¦çš„æ¨¡å‹ï¼Œä¸»è¦è¿”å›2ä¸ªæ¨¡å‹ï¼š1ã€<code>CriticModel</code>ï¼›2ã€<code>RewardModel</code> å›é¡¾ä¸€ä¸‹è¿™å‡ ç±»æ¨¡å‹çš„ä½œç”¨ï¼šæ— è®ºæ˜¯åœ¨GRPOè¿˜æ˜¯DPOä¸­éƒ½ä¼šè¾“å‡ºtokenç„¶åéœ€è¦å»å¯¹tokenè¿›è¡Œè¯„åˆ†ï¼Œèµ·è¯„åˆ†ä½œç”¨çš„å°±æ˜¯ <code>reward model</code> å¯¹åº”ä¸Šé¢å›¾ä¸­ <code>reward model</code>ï¼Œé™¤æ­¤ä¹‹å¤–éƒ½ä¼šè®¡ç®— <strong>ä¼˜åŠ¿å‡½æ•°</strong>ï¼ˆ<span class="math inline">\(Q(s,a)-V(s)\)</span>ï¼‰æ¥è¯„ä¼°ç­–ç•¥çš„å¥½åä¼˜åŠ¿å‡½æ•°é‡Œé¢è®¡ç®—å°±æ˜¯é€šè¿‡ <code>critic model</code>æ¥å¯¹æŸä¸€ä¸ªç­–ç•¥è¿›è¡Œè¯„ä¼°å¯¹åº”ä¸Šé¢å›¾åƒä¸­çš„ï¼š<code>value model</code></p>
<pre><code class="language-python">def _get_reward_model(base_pretrained_model, base_llm_model, value_head_prefix="score", packing_samples=False):
    class RewardModel(base_pretrained_model):
        def __init__(...):
            ...
            # åŠ è½½æ¨¡å‹
            setattr(self, self.base_model_prefix, base_llm_model(config))
            self.value_head_prefix = value_head_prefix
            setattr(self, value_head_prefix, nn.Linear(config.hidden_size, 1, bias=False) # è¾“å‡ºè¯„åˆ†
            ...
        def forward(self, input_ids: torch.LongTensor = None, attention_mask: Optional[torch.Tensor] = None, return_output=False, ring_attn_group=None,pad_sequence=False, packed_seq_lens=None,):
            ...# 1ã€å¤„ç†packing
            outputs = getattr(self, self.base_model_prefix)(
                input_ids, attention_mask=attention_mask, position_ids=position_ids
            )
            last_hidden_states = outputs["last_hidden_state"]
            values = getattr(self, self.value_head_prefix)(last_hidden_states).squeeze(-1)
            ...# 1ã€å¤„ç†packing
            else:
                # è¾“å‡ºæœ€åä¸€ä¸ªæœ‰æ•ˆtokençš„è¯„åˆ†ä»£æ›¿æ•´ä¸ªå¥å­è¯„åˆ†
                eos_indices = attention_mask.size(1) - 1 - attention_mask.long().fliplr().argmax(dim=1, keepdim=True)
                reward = values.gather(dim=1, index=eos_indices).squeeze(1)
            if not self.training and self.normalize_reward:
                reward = (reward - self.mean) / self.std
            return (reward, outputs) if return_output else reward
    return RewardModel

def _get_critic_model(base_pretrained_model, base_llm_model, value_head_prefix="score", packing_samples=False):
    class CriticModel(base_pretrained_model):
        def __init__(...):
            ...
        def forward(...):
            ...# 1ã€å¤„ç†packing
            outputs = getattr(self, self.base_model_prefix)(
                input_ids, attention_mask=attention_mask, position_ids=position_ids
            )
            last_hidden_states = outputs["last_hidden_state"]
            values = getattr(self, self.value_head_prefix)(last_hidden_states).squeeze(-1)
            ...
            if num_actions is None:
                assert return_output
                return outputs
            if not self.packing_samples:
                action_values = values[:, -num_actions:]
            else:
                assert isinstance(num_actions, list) and len(num_actions) == len(packed_seq_lens)
                action_values = []
                offset = 0
                for num_action, seq_len in zip(num_actions, packed_seq_lens):
                    start, end = max(0, offset + seq_len - num_action - 1), offset + seq_len - 1
                    action_values.append(values[:, start:end])
                    offset += seq_len
                action_values = torch.cat(action_values, dim=1)

            if return_output:
                return (action_values, outputs)
            else:
                return action_values

    return CriticModel
</code></pre>
<p>1ã€<code>reward model</code>: ä¼ å…¥ä¸€ä¸ª base_pretrained_modelï¼ˆæ¯”å¦‚ PreTrainedModelï¼‰ã€ä¸€ä¸ª base_llm_modelï¼ˆæ¯”å¦‚ AutoModelï¼‰ä»¥åŠä¸€äº›æ§åˆ¶å‚æ•°ã€‚å‡½æ•°å†…éƒ¨è¿”å›ä¸€ä¸ªå®šåˆ¶åŒ–çš„å¥–åŠ±æ¨¡å‹ç±» RewardModelï¼Œå®ƒå¯ä»¥åœ¨ç»™å®šè¾“å…¥å¥å­æ—¶ï¼Œ<strong>è¾“å‡ºä¸€ä¸ªæ•°å€¼ï¼ˆreward åˆ†æ•°ï¼‰ï¼Œåæ˜ è¾“å‡ºæ–‡æœ¬çš„è´¨é‡</strong>ã€‚åœ¨forwardè®¡ç®—ä¸­ï¼Œç›´æ¥å°†è¾“å…¥modelä½¿ç”¨çš„å‡ ä¸ªå‚æ•°ï¼ˆè§ä¸Šé¢çš„è¡¥å……æœ‰å…·ä½“è§£é‡Šï¼‰è®¡ç®—æœ€åå–æœ€åä¸€ä¸ªçŠ¶æ€çš„å€¼ï¼Œå¹¶ä¸”å°†è¿™ä¸ªå€¼å–è®¡ç®—è¯„åˆ†ã€‚ä¹Ÿå°±æ˜¯è¯´ reward modelï¼š<strong>é¦–å…ˆè®¡ç®—ä¸‹ä¸€ä¸ªé¢„æµ‹çš„tokenè€Œåå¯¹è¿™äº›tokenè¿›è¡Œæ‰“åˆ†</strong><br>
2ã€<code>critic model</code>ï¼šå…·ä½“è¾“å…¥å‚æ•°å’Œ <code>reward model</code>ç›¸åŒã€‚å‚è€ƒä¹‹å‰<a href="https://www.big-yellow-j.top/posts/2025/03/23/DPO-PPO.html#:~:text=%E5%AF%B9%E4%BA%8E%E4%B8%8A%E8%BF%B0%E5%85%AC%E5%BC%8F,%E5%B9%B3%E5%9D%87%E7%B4%AF%E8%AE%A1%E6%9C%9F%E6%9C%9B" target="_blank" rel="noopener nofollow">ä»‹ç»</a>ï¼Œä¸Šé¢ä»£ç ä¸­ç›´æ¥è¿”å›<code>action_values = values[:, -num_actions:]</code>ï¼ˆ <code>num_actions</code>å­˜åœ¨æ¡ä»¶ä¸‹ï¼‰è¿™æ ·å°±ä¼šå¾—åˆ°ä¸åŒçš„Q(s, a1), Q(s, a2), ...</p>
<blockquote>
<p><strong>æ€»ç»“ä¸Šé¢ä¸¤ç»„æ¨¡å‹</strong>ï¼Œåœ¨ LLM çš„å¼ºåŒ–å­¦ä¹ åœºæ™¯ä¸‹ï¼ŒReward Model å’Œ Critic Model éƒ½ä» last_hidden_state å¾—åˆ° token-level è¡¨è¾¾ï¼Œå†ç”¨ Linear å±‚è¾“å‡ºæ¯ä¸ª token çš„ scoreã€‚</p>
<ul>
<li><code>Reward Model</code> æœ€åæå–çš„æ˜¯ EOS token çš„ scoreï¼Œè¡¨ç¤ºæ•´å¥è¯çš„å¥–åŠ±ã€‚</li>
<li><code>Critic Model</code> ä¼šè¿›ä¸€æ­¥æå–æœ€å num_actions ä¸ª token çš„ valueï¼Œè¿™äº› token æ˜¯ Actor ç”Ÿæˆçš„åŠ¨ä½œï¼Œå¯¹åº”åˆ° PPO ä¸­çš„ï¼šğ´(ğ‘ ,ğ‘)=ğ‘„(ğ‘ ,ğ‘)âˆ’ğ‘‰(ğ‘ )ã€‚</li>
</ul>
</blockquote>
<p>ç†è§£ä¸Šé¢å†…å®¹ï¼Œå›é¡¾æœ€ä¸Šé¢çš„æ¡†æ¶è®¾è®¡ï¼Œç”¨ä¸‹é¢ä¾‹å­è¿›è¡Œè§£é‡Šã€‚<br>
Promptï¼š<code>"The capital of France is"</code><br>
Actor modelï¼š<code>"Paris is beautiful"</code>ã€‚é‚£ä¹ˆåˆå¹¶å¾—åˆ°ï¼š<code>input_ids = ["The", "capital", "of", "France", "is", " Paris", "is", "beautiful"]</code><br>
Reward modelï¼šå¯¹ä¸Šé¢æ¯ä¸ªå•è¯è¿›è¡Œè¯„åˆ†ï¼Œå‡è®¾ï¼š<code>values = [0.1, 0.2, 0.3, 0.2, 0.4, 0.7, 0.5, 0.8]  # æ¯ä¸ª token çš„ score</code> è€Œåè¾“å‡ºå¥å­ä¸­æ•´ä½“è¯„åˆ† 0.8<br>
Critic modelï¼šåªå¯¹æœ€åå‡ ä¸ª token çš„ action è®¡ç®— lossï¼Œäºæ˜¯ï¼š<code>action_values = values[:, -3:]  # å³å–å‡ºæœ€å 3 ä¸ªç”Ÿæˆ token çš„ Q å€¼</code>è¿™äº›å€¼ä¹Ÿå°±å¯¹åº”äº†æˆ‘ä»¬æ¨¡å‹çš„ç”Ÿæˆ</p>
<h3 id="3losspy">3ã€loss.py</h3>
<blockquote>
<p><a href="https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/models/loss.py" target="_blank" rel="noopener nofollow">https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/models/loss.py</a></p>
</blockquote>
<blockquote>
<p><strong>è¡¥å……-1ï¼š</strong><br>
è£å‰ªä½¿ç”¨çš„æ˜¯<code>torch.clamp</code>ï¼ˆ<a href="https://pytorch.org/docs/stable/generated/torch.clamp.html%EF%BC%89%E5%BC%BA%E5%88%B6%E5%B0%86%E8%8C%83%E5%9B%B4%E5%A4%96%E7%9A%84%E6%95%B0%E5%80%BC%E5%A4%84%E7%90%86%E4%B8%BA%E8%BE%B9%E7%95%8C%E5%80%BC%EF%BC%8C%E8%8C%83%E5%9B%B4%E5%86%85%E6%95%B0%E5%AD%97%E4%BF%9D%E6%8C%81%E4%B8%8D%E5%8F%98" target="_blank" rel="noopener nofollow">https://pytorch.org/docs/stable/generated/torch.clamp.htmlï¼‰å¼ºåˆ¶å°†èŒƒå›´å¤–çš„æ•°å€¼å¤„ç†ä¸ºè¾¹ç•Œå€¼ï¼ŒèŒƒå›´å†…æ•°å­—ä¿æŒä¸å˜</a></p>
</blockquote>
<p>1ã€<code>PolicyLoss</code>ï¼šPolicy Loss for PPO</p>
<p></p><div class="math display">\[\begin{align*}
r_t &amp;= \exp(\log \pi(a_t \mid s_t) - \log \pi_{\text{old}}(a_t \mid s_t)) \\
\mathcal{L}_{\text{clip}}(t) &amp;= \min\left(r_t \cdot A_t,\ \text{clip}(r_t,\ 1 - \epsilon,\ 1 + \epsilon) \cdot A_t\right) \\
\mathcal{L}_{\text{policy}} &amp;= -\mathbb{E}_t \left[ \mathcal{L}_{\text{clip}}(t) \right]
\end{align*}
\]</div><p></p><p>2ã€<code>ValueLoss</code>: Value Loss for PPO</p>
<p></p><div class="math display">\[\mathcal{L}_{\text{value}} = \frac{1}{2} \cdot \mathbb{E}_{t \sim \text{mask}} \left[ \max \left( 
(V_{\text{clip}, t} - R_t)^2, \, (V_t - R_t)^2 
\right) \right]\\
\text{å…¶ä¸­ï¼š}V_{\text{clip}} = V_{\text{old}} + \text{clip}(V - V_{\text{old}}, -\epsilon, \epsilon)
\]</div><p></p><h2 id="ä»£ç æµ‹è¯•">ä»£ç æµ‹è¯•</h2>
<p>ä¿®æ”¹äº†ä»£ç è§é“¾æ¥ï¼š<a href="https://www.big-yellow-j.top/_jupyter/OpenRLHF_model.py.txt" target="_blank" rel="noopener nofollow">https://www.big-yellow-j.top/_jupyter/OpenRLHF_model.py</a></p>
<h2 id="æ€»ç»“">æ€»ç»“</h2>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†åœ¨ <strong>OpenRLHF</strong>ä¸­æ¨¡å‹æ¡†æ¶è®¾è®¡ï¼Œä¸»è¦åˆ†ä¸º3ç±»æ¨¡å‹ï¼š1ã€<code>actor model</code>ï¼›2ã€<code>critic model</code>ï¼›3ã€<code>reward model</code>è¿™ä¸‰ç±»æ¨¡å‹ä¸­åˆ†åˆ«èµ·åˆ°ä½œç”¨ï¼š1ã€ç›´æ¥æ›´å…·promptè¾“å‡ºresponseï¼›2ã€è¾“å‡ºtokençš„è¯„åˆ†ï¼ˆ<code>action_values = values[:, -3:]</code>ï¼‰ï¼›3ã€è¿”å›æ•´å¥è¾“å‡ºè¯„åˆ†ï¼ˆæ‰¾å‡ºæœ€åä¸€ä¸ªæœ‰æ•ˆ token çš„ç´¢å¼•ï¼Œç„¶åä» value å‘é‡ä¸­æå–è¯¥ä½ç½®çš„å€¼ä½œä¸º rewardã€‚ï¼‰</p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.23398190319560186" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-04-22 22:40">2025-04-22 22:40</span>&nbsp;
<a href="https://www.cnblogs.com/Big-Yellow">Big-Yellow-J</a>&nbsp;
é˜…è¯»(<span id="post_view_count">5</span>)&nbsp;
è¯„è®º(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18841821);return false;">æ”¶è—</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18841821', targetLink: 'https://www.cnblogs.com/Big-Yellow/p/18841821', title: 'å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼šOpenRLHFæºç è§£è¯»ï¼Œæ¨¡å‹å¤„ç†' })">ä¸¾æŠ¥</a>
</div>
        