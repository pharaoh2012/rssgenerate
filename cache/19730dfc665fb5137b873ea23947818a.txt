
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/deali/p/18936652" title="发布于 2025-06-19 15:52">
    <span role="heading" aria-level="2">PVE折腾笔记 (3) 在原QNAP使用的硬盘上创建ZFS</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                    <div id="cnblogs_post_description" style="display: none">
        
        QNAP硬盘换ZFS？自愈功能挺好用，但折腾Linux兼容性够闹心。LVM残留导致擦盘失败？内核对象得一个个干掉。Pool创建时参数选错？那可真是后患无穷。
    </div>
<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h2 id="前言">前言</h2>
<p>在经过一番研究后，我决定使用ZFS作为俩机械硬盘的文件系统，本来也可以和QNAP一样直接ext4的，但ZFS比较安全，有自愈功能，可以处理比特位翻转的问题，总之就是好用。</p>
<p>如果追求灵活性可以使用Btrfs，这方面有个大佬讲得不错，可以参考一下: <a href="https://zhuanlan.zhihu.com/p/699986909" target="_blank" rel="noopener nofollow">数据硬盘分区：Btrfs文件系统 - 从零开始的NAS/小型服务器搭建</a></p>
<blockquote>
<p>一句话理解：ZFS 不只是文件系统，它是一套智能的存储解决方案。</p>
</blockquote>
<p>ZFS（Zettabyte File System）是由 Sun Microsystems 开发的一种开源的先进文件系统和逻辑卷管理器。</p>
<p>与传统文件系统（如 ext4、NTFS）不同，ZFS <strong>融合了文件系统 + RAID + 卷管理</strong>于一体。</p>
<p>关键词：<strong>自修复、快照、高可靠性、大容量支持</strong>。</p>
<h2 id="查看硬盘">查看硬盘</h2>
<p>主要使用 <code>fdisk -l</code> 和 <code>lsblk</code> 这两个命令。</p>
<ul>
<li><code>fdisk -l</code>：查看磁盘的 <strong>分区信息（偏底层、原始）</strong></li>
<li><code>lsblk</code>：查看磁盘的 <strong>块设备树状结构（更直观、面向人类）</strong></li>
</ul>
<h3 id="命令说明">命令说明：</h3>
<h4 id="-fdisk--l">🧱 <code>fdisk -l</code></h4>
<ul>
<li>
<p>全称是 <strong>"list partition table"</strong></p>
</li>
<li>
<p>显示所有磁盘和它们的 <strong>分区信息、容量、文件系统类型（MBR/GPT）等</strong></p>
</li>
<li>
<p>需要 root 权限（所以常用 <code>sudo fdisk -l</code>）</p>
</li>
<li>
<p>输出示例：</p>
<pre><code>pgsql复制编辑Disk /dev/sda: 512 GB
Device     Boot Start       End   Sectors   Size Type
/dev/sda1  *     2048    999423    997376   487M EFI System
/dev/sda2        999424 976773119 975773696 465G Linux filesystem
</code></pre>
</li>
<li>
<p>📌 更偏“底层”，有助于了解磁盘分区表结构。</p>
</li>
</ul>
<h4 id="-lsblk">🌳 <code>lsblk</code></h4>
<ul>
<li>
<p>全称是 <strong>"list block devices"</strong></p>
</li>
<li>
<p>显示所有的块设备（磁盘、分区、LVM、RAID 等）以及它们的挂载情况</p>
</li>
<li>
<p>类似树状结构，更加 <strong>直观</strong></p>
</li>
<li>
<p>不显示挂载点为空的 loop 设备（除非加参数）</p>
</li>
<li>
<p>默认不需要 root 权限</p>
</li>
<li>
<p>输出示例：</p>
<pre><code>pgsql复制编辑NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
sda      8:0    0   512G  0 disk
├─sda1   8:1    0   487M  0 part /boot/efi
└─sda2   8:2    0 511.5G  0 part /
</code></pre>
</li>
<li>
<p>📌 更适合用来 <strong>快速识别哪个设备挂载到哪个目录</strong>，比如你插了新 SSD，它就会以 <code>/dev/sdX</code> 或 <code>/dev/nvmeXn1</code> 的名字出现。</p>
</li>
</ul>
<h2 id="备份数据">备份数据</h2>
<p>参考我之前的内容</p>
<p>有几种方法：</p>
<ul>
<li>rsync: <code>rsync -a --info=progress2 /mnt/disk1/your-folder /mnt/disk2/</code></li>
<li>Midnight Commander (mc) (可视化)</li>
</ul>
<p>推荐用 mc，注意如果是 SSH 到服务器操作的话，一定得用 screen 之类的工具开启后台会话，防止复制到一半连接断了。</p>
<p>备份完成后请自行校验数据。方法也可以参考我之前的文章。</p>
<h2 id="格式化硬盘">格式化硬盘</h2>
<p>最简单的方法是只有用 PVE 里的 Wipe Disk 功能。</p>
<h3 id="报错">报错</h3>
<p>不过我在使用的时候遇到报错： <code>disk/partition '/dev/sda1' has a holder</code></p>
<p>分析下原因，<code>holder</code> 代表仍有内核对象在使用这块分区</p>
<p>常见持有者（holders）</p>
<table>
<thead>
<tr>
<th>设备类型</th>
<th>检查命令</th>
<th>典型出现在 QNAP 旧盘里的原因</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>LVM PV → dm-?</strong></td>
<td><code>pvs</code>, <code>lsblk -o NAME,TYPE</code></td>
<td>QTS 默认把整盘卷进 <code>vg1/lv1</code></td>
</tr>
<tr>
<td><strong>mdraid → md127</strong></td>
<td><code>mdadm --detail --scan</code></td>
<td>QTS 可能创建过软 RAID 缓存区</td>
</tr>
<tr>
<td><strong>dm-crypt / luks</strong></td>
<td><code>lsblk -f</code></td>
<td>如果开了加密共享</td>
</tr>
<tr>
<td><strong>挂载点</strong></td>
<td>`mount</td>
<td>grep sda1`</td>
</tr>
</tbody>
</table>
<p>只要还有 <strong>holders</strong>，Proxmox 的 <em>wipe_disk</em> 脚本（本质是 <code>sgdisk --zap-all</code> + <code>wipefs -a</code>）就会被内核拒绝写入，从而抛出 500 错误。</p>
<h3 id="排查方式">排查方式</h3>
<pre><code class="language-bash">lsblk -f
ls /sys/block/sda/sda1/holders
</code></pre>
<p>输出如 dm-0  dm-1  → 说明被 LVM/MD 等占着</p>
<h3 id="解决方法">解决方法</h3>
<p><strong>⚠️ 所有命令请确认磁盘名（这里用 /dev/sda 举例），误操作会立刻毁灭数据！</strong></p>
<table>
<thead>
<tr>
<th>步骤</th>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>1️⃣ 卸载挂载点</td>
<td><code>umount -f /dev/sda? 2&gt;/dev/null</code></td>
<td>保证分区不在用</td>
</tr>
<tr>
<td>2️⃣ 停用 LVM</td>
<td><code>vgchange -an vg1</code> <code>lvremove -f /dev/vg1/lv1</code> <code>pvremove /dev/sda1</code></td>
<td>vg/lv 名字以 <code>pvs</code>, <code>vgs</code> 查询为准</td>
</tr>
<tr>
<td>3️⃣ 停掉 mdraid</td>
<td><code>mdadm --stop /dev/md127</code> <code>mdadm --zero-superblock /dev/sda1</code></td>
<td>若 <code>lsblk</code> 显示有 <code>md*</code></td>
</tr>
<tr>
<td>4️⃣ 最终抹签名</td>
<td><code>sgdisk --zap-all /dev/sda</code> <code>wipefs -a /dev/sda</code></td>
<td>等同 PVE “Wipe Disk”</td>
</tr>
</tbody>
</table>
<p>做完再 <code>ls /sys/block/sda/sda1/holders</code> —— 目录应为空</p>
<p>这时回到 Web UI 点 <strong>Wipe Disk</strong> 就不会报错了。</p>
<h2 id="创建-zfs">创建 ZFS</h2>
<p>在 Web UI 里，选择节点（我的是叫 PVE） - 磁盘 - ZFS - 创建 ZFS</p>
<p><strong>太长不看：选择磁盘，填写名称，RAID 级别选单磁盘，如果想冗余的话可以选 Mirror，压缩选 lz4，ashift 选 12</strong></p>
<p>想深入了解的同学可以看以下说明</p>
<h3 id="推荐方案">推荐方案</h3>
<p><strong>RAID 级别</strong>：按容量、性能、容错、重建速度等权衡选择（mirror/raid10 偏性能+小规模，raidzN 偏容量+简单，draidN 偏容量+快速自愈）。</p>
<p><strong>压缩算法</strong>：若不特殊需求，直接选 <code>lz4</code>（或 <code>on</code>）；要极限压缩可考虑 <code>zstd</code> 或 <code>gzip</code>。</p>
<p><strong>ashift</strong>：现代盘选 <code>12</code>。</p>
<h3 id="这些参数都是啥意思">这些参数都是啥意思？</h3>
<h4 id="raid-级别pool-布局">RAID 级别（Pool 布局）</h4>
<table>
<thead>
<tr>
<th>类型</th>
<th>最少盘数</th>
<th>容错能力</th>
<th>特点与适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>mirror</strong></td>
<td>≥2</td>
<td>1 盘（每个 mirror 组内）</td>
<td>将每块盘数据完全镜像到另一块盘，读性能略优、写性能与单盘相当；推荐对性能和可靠性都较敏感的小规模部署。</td>
</tr>
<tr>
<td><strong>raid10</strong></td>
<td>≥4</td>
<td>最少可容忍 1 个盘（每个 mirror 组）</td>
<td>先成对镜像，再跨组做条带（stripe）；兼顾读写性能和容错，适合需要高 IOPS 的场景。</td>
</tr>
<tr>
<td><strong>raidz1</strong> (Proxmox 界面叫 “RAIDZ”)</td>
<td>≥3</td>
<td>1 盘（单重校验）</td>
<td>单块奇偶校验盘，空间利用率高于 mirror，但重建（resilver）时影响性能；适合容量优先、对重建时间不太敏感的场景。</td>
</tr>
<tr>
<td><strong>raidz2</strong></td>
<td>≥4</td>
<td>2 盘（双重校验）</td>
<td>双重奇偶校验，能容忍两块盘同时失效；比 raidz1 多一块校验盘，适合中大型容量池，对可靠性要求更高时。</td>
</tr>
<tr>
<td><strong>raidz3</strong></td>
<td>≥5</td>
<td>3 盘（3 重校验）</td>
<td>三重奇偶校验，能容忍三块盘同时失效；适合海量存储、对数据安全极度敏感的场景。</td>
</tr>
<tr>
<td><strong>draid1</strong></td>
<td>≥（P+1+1）<sup> 1 </sup></td>
<td>1 盘 校验 + 动态备用</td>
<td>动态 RAIDZ1，等同于 raidz1+集成热备；失效后可并行自愈（rebuild），重建速度更快。适合需快速恢复的大规模阵列。</td>
</tr>
<tr>
<td><strong>draid2</strong></td>
<td>≥（P+2+1）</td>
<td>2 盘 校验 + 动态备用</td>
<td>同理，双重校验 ＋ 动态热备；兼顾可靠性和重建性能。</td>
</tr>
<tr>
<td><strong>draid3</strong></td>
<td>≥（P+3+1）</td>
<td>3 盘 校验 + 动态备用</td>
<td>三重校验 ＋ 动态热备；极致可靠，支持并行重建。</td>
</tr>
</tbody>
</table>
<blockquote>
<p>注：</p>
<ol>
<li>对应公式：<code>总盘数 ≥ 校验盘数 P + 数据盘数 D + 1 (热备分区)</code>；Proxmox 界面只列出 <code>draid1/draid2/draid3</code>，无需手动指定 D，只管满足最少盘数即可。</li>
<li><strong>raidzN vs. draidN</strong> 对比：
<ul>
<li><strong>raidzN</strong>：专用奇偶盘，故障后单任务重建，影响阵列性能；</li>
<li><strong>draidN</strong>：集成热备，且重建过程并行分布在所有盘上，速度更快、对在线 I/O 影响更小。</li>
</ul>
</li>
</ol>
</blockquote>
<h4 id="压缩compression">压缩（compression）</h4>
<p>ZFS 支持在写入时对数据块做压缩，能节省空间并在多数情况下提升 I/O 性能（读取时少量 CPU 换取更少的磁盘 I/O）。</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>含义</th>
<th>优缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>off</strong></td>
<td>关闭压缩</td>
<td>写入最快，但空间占用大，读写 I/O 更多。</td>
</tr>
<tr>
<td><strong>on</strong></td>
<td>启用默认算法（Proxmox/ZFS 默认通常是 <code>lz4</code>）</td>
<td>综合性能好，推荐生产环境使用。</td>
</tr>
<tr>
<td><strong>lz4</strong></td>
<td>LZ4 算法：超快的压缩/解压，适合大多数场景</td>
<td>压缩率中等，CPU 占用低，读写性能几乎无损。</td>
</tr>
<tr>
<td><strong>gzip</strong></td>
<td>Gzip 算法（可指定级别，如 <code>gzip-1</code>…<code>gzip-9</code>）</td>
<td>压缩率高（尤其高级别），但 CPU 占用也高；适合只读或少写场景。</td>
</tr>
<tr>
<td><strong>zstd</strong></td>
<td>Zstandard：比 gzip 更好的压缩率与速度折中</td>
<td>高压缩率、解压速度快，写入性能略低于 lz4；适合对存储更敏感的场景。</td>
</tr>
<tr>
<td><strong>lzjb</strong></td>
<td>旧版 LZJB 算法</td>
<td>压缩率、性能均不及 lz4，现在基本已被淘汰。</td>
</tr>
<tr>
<td><strong>zle</strong></td>
<td>Zero-Length Encoding（零块跳过）</td>
<td>只跳过全 0 区块，无 CPU 或非常低开销，适合磁盘映像、稀疏文件等包含大量零块的工作负载。</td>
</tr>
</tbody>
</table>
<h4 id="ashift对齐大小">ashift（对齐大小）</h4>
<ul>
<li><strong>含义</strong><br>
ZFS 在物理盘上以 2 的幂次方字节大小做 I/O，<code>ashift</code> 就是这个幂次（<code>ashift = log2(块大小)</code>）。</li>
<li><strong>常见取值</strong>
<ul>
<li><code>ashift=9</code>：2^9 = 512 B，对应传统 512 字节扇区。</li>
<li><code>ashift=12</code>：2^12 = 4096 B，对应现代 4K 扇区（SSD 也常以 4K 物理写入）。</li>
</ul>
</li>
<li><strong>建议</strong>
<ol>
<li><strong>现代盘（尤其 SSD 或 4K 扇区盘）</strong> 一律用 <code>ashift=12</code>，避免跨扇区写入带来的性能和寿命惩罚。</li>
<li>如果你确认所有盘都是经典 512B 扇区盘，<code>ashift=9</code> 可以略微节省空间，但风险较大且现在少见。</li>
</ol>
</li>
</ul>
<blockquote>
<p><strong>注意</strong>：<code>ashift</code> 只能在创建 pool 时指定，一旦建立后无法更改，因此务必要一开始就选对。</p>
</blockquote>
<h2 id="常用命令">常用命令</h2>
<p>创建完 ZFS 可以用以下命令查看所有 ZVOL 和 Dataset</p>
<pre><code class="language-bash">zfs list
</code></pre>
<p>用以下命令可以查看所有 ZFS 池</p>
<pre><code class="language-bash">zpool list
</code></pre>
<h2 id="修改挂载点">修改挂载点</h2>
<p>默认挂载点是根目录，以 ZFS 池的名称为挂载点</p>
<p>比如 <code>/data1</code></p>
<p>可以手动修改</p>
<pre><code class="language-bash">sudo zfs set mountpoint=/storage/data1 data1
</code></pre>
<p>查看当前挂载点</p>
<pre><code class="language-bash">zfs get mountpoint data1
</code></pre>
<p>但是修改完还得同步修改 PVE 配置 <code>/etc/pve/storage.cfg</code></p>
<p>其中会有类似这样的配置</p>
<pre><code class="language-ini">zfspool: data1
    pool data1
    content rootdir,images
    mountpoint /data1

zfspool: data2
    pool data2
    content rootdir,images
    mountpoint /data2
</code></pre>
<p>需要修改对应的 <code>mountpoint</code> 到 <code>/storage/data1</code></p>
<p>保存退出即可，<code>/etc/pve</code> 是由 Proxmox 的集群配置文件系统（<code>pmxcfs</code>）自动托管并热更新的，PVE 相关服务（比如启动容器）会即时读取这个文件的变化。</p>
<p>可以使用 <code>pvesm status</code> 命令查看存储，不过这命令没有输出路径</p>
<pre><code class="language-bash">root@pve:/storage# pvesm status
Name             Type     Status           Total            Used       Available        %
backups           dir     active        98497780        13794364        79653868   14.00%
data1          zfspool     active      3771203584       650276484      3120927100   17.24%
data2          zfspool     active      3771203584      1778071764      1993131820   47.15%
local             dir     active        98497780        13794364        79653868   14.00%
local-lvm     lvmthin     active       847638528         2966734       844671793    0.35%
</code></pre>

</div>
<div id="MySignature" role="contentinfo">
    微信公众号：「程序设计实验室」
专注于互联网热门新技术探索与团队敏捷开发实践，包括架构设计、机器学习与数据分析算法、移动端开发、Linux、Web前后端开发等，欢迎一起探讨技术，分享学习实践经验。
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0" data-date-updated="2025-06-19 15:52">2025-06-19 15:52</span>&nbsp;
<a href="https://www.cnblogs.com/deali">程序设计实验室</a>&nbsp;
阅读(<span id="post_view_count">48</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18936652);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18936652', targetLink: 'https://www.cnblogs.com/deali/p/18936652', title: 'PVE折腾笔记 (3) 在原QNAP使用的硬盘上创建ZFS' })">举报</a>
</div>
        