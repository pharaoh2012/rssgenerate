
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/smartloli/p/18742244" title="发布于 2025-02-27 22:53">
    <span role="heading" aria-level="2">使用 DeepSeek R1 和 Ollama 开发 RAG 系统</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body blogpost-body-html">
<h1>1.概述</h1>
<p>掌握如何借助 DeepSeek R1 与 Ollama 搭建检索增强生成（RAG）系统。本文将通过代码示例，为你提供详尽的分步指南、设置说明，分享打造智能 AI 应用的最佳实践。</p>
<h1>2.内容</h1>
<h2>2.1 为什么选择DeepSeek R1？</h2>
<p>在这篇文章中，我们将探究性能上可与 OpenAI 的 o1 相媲美、但成本却低 95% 的 DeepSeek R1，如何为你的检索增强生成（RAG）系统带来强大助力。我们来深入剖析为何开发者们纷纷热衷于这项技术，以及你怎样利用它构建自己的 RAG 流程。</p>
<div class="auto-hide-last-sibling-br paragraph-JOTKXA paragraph-element br-paragraph-space">DeepSeek R1 的 15 亿参数模型在这方面表现出色，原因如下：</div>
<div class="auto-hide-last-sibling-br paragraph-JOTKXA paragraph-element br-paragraph-space">
<ul>
<li>精准检索：每个答案仅关联 3 个文档片段</li>
<li>严格提示：采用 “我不知道” 策略，避免模型产生幻觉</li>
<li>本地执行：与云 API 相比，实现零延迟</li>
</ul>
</div>
<p>环境：</p>
<table>
<thead>
<tr><th style="text-align: left">组件</th><th>成本</th></tr>
</thead>
<tbody>
<tr>
<td>DeepSeek R1 1.5B</td>
<td>免费</td>
</tr>
<tr>
<td>Ollama</td>
<td>免费</td>
</tr>
<tr>
<td>16GB 内存的个人电脑</td>
<td>0 元</td>
</tr>
</tbody>
</table>
<h2>2.2&nbsp;构建本地 RAG 系统所需的条件</h2>
<h3>1.Ollama</h3>
<p>Ollama 允许你在本地运行诸如 DeepSeek R1 之类的模型。</p>
<ul>
<li>下载：<a href="https://ollama.com/" target="_blank" rel="noopener nofollow">Ollama</a></li>
<li>设置：通过终端安装并运行以下命令。</li>
</ul>
<div class="cnblogs_code">
<pre>ollama run deepseek-r1  <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> For the 7B model (default)  </span></pre>
</div>
<p><img src="https://img2024.cnblogs.com/blog/666745/202502/666745-20250227223105878-1542989866.png" alt="" loading="lazy"></p>
<h3>&nbsp;2.DeepSeek R1 模型</h3>
<p>DeepSeek R1 的参数范围从 1.5B 到 671B。对于轻量级 RAG 应用程序，请从1.5B 模型开始。</p>
<div class="cnblogs_code">
<pre>ollama run deepseek-r1:1.5b </pre>
</div>
<p>提示：更大的模型（例如 70B）提供更好的推理能力，但需要更多的 RAM。</p>
<p><img src="https://img2024.cnblogs.com/blog/666745/202502/666745-20250227223325974-2030693614.png" alt="" loading="lazy"></p>
<h2>&nbsp;2.3&nbsp;构建 RAG 管道</h2>
<h3>1.导入库</h3>
<p>我们将使用：</p>
<ul>
<li><a href="https://github.com/langchain-ai/langchain" target="_blank" rel="noopener nofollow">LangChain</a> 用于文档处理和检索。</li>
<li><a href="https://streamlit.io/" target="_blank" rel="noopener nofollow">Streamlit</a> 具有用户友好的网络界面。</li>
</ul>
<div class="cnblogs_code">
<pre><span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> streamlit as st  
</span><span style="color: rgba(0, 0, 255, 1)">from</span> langchain_community.document_loaders <span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> PDFPlumberLoader  
</span><span style="color: rgba(0, 0, 255, 1)">from</span> langchain_experimental.text_splitter <span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> SemanticChunker  
</span><span style="color: rgba(0, 0, 255, 1)">from</span> langchain_community.embeddings <span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> HuggingFaceEmbeddings  
</span><span style="color: rgba(0, 0, 255, 1)">from</span> langchain_community.vectorstores <span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> FAISS  
</span><span style="color: rgba(0, 0, 255, 1)">from</span> langchain_community.llms <span style="color: rgba(0, 0, 255, 1)">import</span> Ollama  </pre>
</div>
<p><img src="https://img2024.cnblogs.com/blog/666745/202502/666745-20250227223626651-499940982.png" alt="" loading="lazy"></p>
<h3>&nbsp;2.上传并处理 PDF</h3>
<p>利用 Streamlit 的文件上传器选择本地 PDF。用于PDFPlumberLoader高效提取文本，无需手动解析。</p>
<div class="cnblogs_code">
<pre><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> Streamlit文件上传器</span>
uploaded_file = st.file_uploader(<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">Upload a PDF file</span><span style="color: rgba(128, 0, 0, 1)">"</span>, type=<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">pdf</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">)

</span><span style="color: rgba(0, 0, 255, 1)">if</span><span style="color: rgba(0, 0, 0, 1)"> uploaded_file:
    </span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 临时保存PDF文件</span>
    with open(<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">temp.pdf</span><span style="color: rgba(128, 0, 0, 1)">"</span>, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">wb</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">) as f:
        f.write(uploaded_file.getvalue())

    </span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 加载PDF文本</span>
    loader = PDFPlumberLoader(<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">temp.pdf</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">)
    docs </span>= loader.load()</pre>
</div>
<h3>3.策略性地整理文件</h3>
<p>我们打算使用递归字符文本分割器（RecursiveCharacterTextSplitter），该代码会将原始的 PDF 文本拆分成更小的片段（块）。下面我们来解释一下合理分块与不合理分块的概念：</p>
<p><img src="https://img2024.cnblogs.com/blog/666745/202502/666745-20250227224047558-1711174753.png" alt="" loading="lazy"></p>
<p>&nbsp;</p>
<p><strong>为什么要进行语义分块呢？</strong><br>语义分块能够将相关的句子归为一组（例如，“Milvus 如何存储数据” 这样的内容会保持完整），还能避免拆分表格或图表。</p>
<p>利用 Streamlit 的文件上传器选择本地 PDF。用于PDFPlumberLoader高效提取文本，无需手动解析。</p>
<div class="cnblogs_code">
<pre><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> 将文本拆分为语义块  </span>
text_splitter =<span style="color: rgba(0, 0, 0, 1)"> SemanticChunker(HuggingFaceEmbeddings())   
documents </span>= text_splitter.split_documents(docs)</pre>
</div>
<p>这一步通过让各文本片段稍有重叠来保留上下文信息，这有助于语言模型更准确地回答问题。小而明确的文档片段还能让搜索变得更高效、更具相关性。</p>
<h3>4.创建可搜索的知识库</h3>
<p>分割完成后，流程会为这些文本片段生成向量嵌入表示，并将它们存储在 FAISS 索引中。</p>
<div class="cnblogs_code">
<pre><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> Generate embeddings  </span>
embeddings =<span style="color: rgba(0, 0, 0, 1)"> HuggingFaceEmbeddings()  
vector_store </span>=<span style="color: rgba(0, 0, 0, 1)"> FAISS.from_documents(documents, embeddings)  

</span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> Connect retriever  </span>
retriever = vector_store.as_retriever(search_kwargs={<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">k</span><span style="color: rgba(128, 0, 0, 1)">"</span>: 3})  <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> Fetch top 3 chunks  </span></pre>
</div>
<p>这一过程将文本转换为一种数值表示形式，从而使查询变得更加容易。后续的查询操作将针对该索引展开，以找出上下文最为相关的文本片段。</p>
<h3>5.配置 DeepSeek R1</h3>
<p>在这里，你要使用 Deepseek R1 1.5B 参数模型作为本地大语言模型（LLM）来实例化一个检索问答（RetrievalQA）链。</p>
<div class="cnblogs_code">
<pre>llm = Ollama(model=<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">deepseek-r1:1.5b</span><span style="color: rgba(128, 0, 0, 1)">"</span>)  <span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> Our 1.5B parameter model  </span>

<span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> Craft the prompt template  </span>
prompt = <span style="color: rgba(128, 0, 0, 1)">"""</span><span style="color: rgba(128, 0, 0, 1)">  
1. Use ONLY the context below.  
2. If unsure, say "I don’t know".  
3. Keep answers under 4 sentences.  

Context: {context}  

Question: {question}  

Answer:  
</span><span style="color: rgba(128, 0, 0, 1)">"""</span><span style="color: rgba(0, 0, 0, 1)">  
QA_CHAIN_PROMPT </span>= PromptTemplate.from_template(prompt)  </pre>
</div>
<p>这个模板会迫使模型依据你 PDF 文档的内容来给出答案。通过将语言模型与和 FAISS 索引绑定的检索器相结合，任何通过该链发起的查询都会从 PDF 内容中查找相关上下文，从而让答案有原始材料作为依据。</p>
<h3>6.组装RAG链</h3>
<p>接下来，你可以将上传、分块和检索这几个步骤整合为一个连贯的流程。</p>
<div class="cnblogs_code">
<pre><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> Chain 1: Generate answers  </span>
llm_chain = LLMChain(llm=llm, prompt=<span style="color: rgba(0, 0, 0, 1)">QA_CHAIN_PROMPT)  

</span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> Chain 2: Combine document chunks  </span>
document_prompt =<span style="color: rgba(0, 0, 0, 1)"> PromptTemplate(  
    template</span>=<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">Context:\ncontent:{page_content}\nsource:{source}</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">,  
    input_variables</span>=[<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">page_content</span><span style="color: rgba(128, 0, 0, 1)">"</span>, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">source</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">]  
)  

</span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> Final RAG pipeline  </span>
qa =<span style="color: rgba(0, 0, 0, 1)"> RetrievalQA(  
    combine_documents_chain</span>=<span style="color: rgba(0, 0, 0, 1)">StuffDocumentsChain(  
        llm_chain</span>=<span style="color: rgba(0, 0, 0, 1)">llm_chain,  
        document_prompt</span>=<span style="color: rgba(0, 0, 0, 1)">document_prompt  
    ),  
    retriever</span>=<span style="color: rgba(0, 0, 0, 1)">retriever  
)</span></pre>
</div>
<p>这就是检索增强生成（RAG）设计的核心所在，它为大语言模型提供经过验证的上下文信息，而非让其单纯依赖自身的内部训练数据。</p>
<h3>7.启动 Web 接口</h3>
<p>最后，代码利用了 Streamlit 的文本输入和输出函数，这样用户就可以直接输入问题并立即查看回答。</p>
<div class="cnblogs_code">
<pre><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> Streamlit UI  </span>
user_input = st.text_input(<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">Ask your PDF a question:</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">)  

</span><span style="color: rgba(0, 0, 255, 1)">if</span><span style="color: rgba(0, 0, 0, 1)"> user_input:  
    with st.spinner(</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">Thinking...</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">):  
        response </span>= qa(user_input)[<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">result</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">]  
        st.write(response)  </span></pre>
</div>
<p>一旦用户输入查询内容，检索链就会找出最匹配的文本片段，将其输入到语言模型中，并显示答案。只要正确安装了langchain库，代码现在应该就能正常运行，不会再触发模块缺失的错误。<br>提出并提交问题，即可立即获得答案！</p>
<h3>8.完整示例代码</h3>
<div class="cnblogs_code">
<pre><span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> streamlit as st
</span><span style="color: rgba(0, 0, 255, 1)">from</span> langchain_community.document_loaders <span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> PDFPlumberLoader
</span><span style="color: rgba(0, 0, 255, 1)">from</span> langchain_experimental.text_splitter <span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> SemanticChunker
</span><span style="color: rgba(0, 0, 255, 1)">from</span> langchain_community.embeddings <span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> HuggingFaceEmbeddings
</span><span style="color: rgba(0, 0, 255, 1)">from</span> langchain_community.vectorstores <span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> FAISS
</span><span style="color: rgba(0, 0, 255, 1)">from</span> langchain_community.llms <span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> Ollama
</span><span style="color: rgba(0, 0, 255, 1)">from</span> langchain.prompts <span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> PromptTemplate
</span><span style="color: rgba(0, 0, 255, 1)">from</span> langchain.chains.llm <span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> LLMChain
</span><span style="color: rgba(0, 0, 255, 1)">from</span> langchain.chains.combine_documents.stuff <span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> StuffDocumentsChain
</span><span style="color: rgba(0, 0, 255, 1)">from</span> langchain.chains <span style="color: rgba(0, 0, 255, 1)">import</span><span style="color: rgba(0, 0, 0, 1)"> RetrievalQA

</span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> color palette</span>
primary_color = <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">#1E90FF</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">
secondary_color </span>= <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">#FF6347</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">
background_color </span>= <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">#F5F5F5</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">
text_color </span>= <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">#4561e9</span><span style="color: rgba(128, 0, 0, 1)">"</span>

<span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> Custom CSS</span>
st.markdown(f<span style="color: rgba(128, 0, 0, 1)">"""</span><span style="color: rgba(128, 0, 0, 1)">
    &lt;style&gt;
    .stApp {{
        background-color: {background_color};
        color: {text_color};
    }}
    .stButton&gt;button {{
        background-color: {primary_color};
        color: white;
        border-radius: 5px;
        border: none;
        padding: 10px 20px;
        font-size: 16px;
    }}
    .stTextInput&gt;div&gt;div&gt;input {{
        border: 2px solid {primary_color};
        border-radius: 5px;
        padding: 10px;
        font-size: 16px;
    }}
    .stFileUploader&gt;div&gt;div&gt;div&gt;button {{
        background-color: {secondary_color};
        color: white;
        border-radius: 5px;
        border: none;
        padding: 10px 20px;
        font-size: 16px;
    }}
    &lt;/style&gt;
</span><span style="color: rgba(128, 0, 0, 1)">"""</span>, unsafe_allow_html=<span style="color: rgba(0, 0, 0, 1)">True)

</span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> Streamlit app title</span>
st.title(<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">Build a RAG System with DeepSeek R1 &amp; Ollama</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">)

</span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> Load the PDF</span>
uploaded_file = st.file_uploader(<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">Upload a PDF file</span><span style="color: rgba(128, 0, 0, 1)">"</span>, type=<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">pdf</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">)

</span><span style="color: rgba(0, 0, 255, 1)">if</span> uploaded_file <span style="color: rgba(0, 0, 255, 1)">is</span> <span style="color: rgba(0, 0, 255, 1)">not</span><span style="color: rgba(0, 0, 0, 1)"> None:
    </span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> Save the uploaded file to a temporary location</span>
    with open(<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">temp.pdf</span><span style="color: rgba(128, 0, 0, 1)">"</span>, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">wb</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">) as f:
        f.write(uploaded_file.getvalue())

    </span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> Load the PDF</span>
    loader = PDFPlumberLoader(<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">temp.pdf</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">)
    docs </span>=<span style="color: rgba(0, 0, 0, 1)"> loader.load()

    </span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> Split into chunks</span>
    text_splitter =<span style="color: rgba(0, 0, 0, 1)"> SemanticChunker(HuggingFaceEmbeddings())
    documents </span>=<span style="color: rgba(0, 0, 0, 1)"> text_splitter.split_documents(docs)

    </span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> Instantiate the embedding model</span>
    embedder =<span style="color: rgba(0, 0, 0, 1)"> HuggingFaceEmbeddings()

    </span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> Create the vector store and fill it with embeddings</span>
    vector =<span style="color: rgba(0, 0, 0, 1)"> FAISS.from_documents(documents, embedder)
    retriever </span>= vector.as_retriever(search_type=<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">similarity</span><span style="color: rgba(128, 0, 0, 1)">"</span>, search_kwargs={<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">k</span><span style="color: rgba(128, 0, 0, 1)">"</span>: 3<span style="color: rgba(0, 0, 0, 1)">})

    </span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> Define llm</span>
    llm = Ollama(model=<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">deepseek-r1</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">)

    </span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> Define the prompt</span>
    prompt = <span style="color: rgba(128, 0, 0, 1)">"""</span><span style="color: rgba(128, 0, 0, 1)">
    1. Use the following pieces of context to answer the question at the end.
    2. If you don't know the answer, just say that "I don't know" but don't make up an answer on your own.\n
    3. Keep the answer crisp and limited to 3,4 sentences.

    Context: {context}

    Question: {question}

    Helpful Answer:</span><span style="color: rgba(128, 0, 0, 1)">"""</span><span style="color: rgba(0, 0, 0, 1)">

    QA_CHAIN_PROMPT </span>=<span style="color: rgba(0, 0, 0, 1)"> PromptTemplate.from_template(prompt)

    llm_chain </span>=<span style="color: rgba(0, 0, 0, 1)"> LLMChain(
        llm</span>=<span style="color: rgba(0, 0, 0, 1)">llm,
        prompt</span>=<span style="color: rgba(0, 0, 0, 1)">QA_CHAIN_PROMPT,
        callbacks</span>=<span style="color: rgba(0, 0, 0, 1)">None,
        verbose</span>=<span style="color: rgba(0, 0, 0, 1)">True)

    document_prompt </span>=<span style="color: rgba(0, 0, 0, 1)"> PromptTemplate(
        input_variables</span>=[<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">page_content</span><span style="color: rgba(128, 0, 0, 1)">"</span>, <span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">source</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">],
        template</span>=<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">Context:\ncontent:{page_content}\nsource:{source}</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">,
    )

    combine_documents_chain </span>=<span style="color: rgba(0, 0, 0, 1)"> StuffDocumentsChain(
        llm_chain</span>=<span style="color: rgba(0, 0, 0, 1)">llm_chain,
        document_variable_name</span>=<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">context</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">,
        document_prompt</span>=<span style="color: rgba(0, 0, 0, 1)">document_prompt,
        callbacks</span>=<span style="color: rgba(0, 0, 0, 1)">None)

    qa </span>=<span style="color: rgba(0, 0, 0, 1)"> RetrievalQA(
        combine_documents_chain</span>=<span style="color: rgba(0, 0, 0, 1)">combine_documents_chain,
        verbose</span>=<span style="color: rgba(0, 0, 0, 1)">True,
        retriever</span>=<span style="color: rgba(0, 0, 0, 1)">retriever,
        return_source_documents</span>=<span style="color: rgba(0, 0, 0, 1)">True)

    </span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> User input</span>
    user_input = st.text_input(<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">Ask a question related to the PDF :</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">)

    </span><span style="color: rgba(0, 128, 0, 1)">#</span><span style="color: rgba(0, 128, 0, 1)"> Process user input</span>
    <span style="color: rgba(0, 0, 255, 1)">if</span><span style="color: rgba(0, 0, 0, 1)"> user_input:
        with st.spinner(</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">Processing...</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">):
            response </span>= qa(user_input)[<span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">result</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">]
            st.write(</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">Response:</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(0, 0, 0, 1)">)
            st.write(response)
</span><span style="color: rgba(0, 0, 255, 1)">else</span><span style="color: rgba(0, 0, 0, 1)">:
    st.write(</span><span style="color: rgba(128, 0, 0, 1)">"</span><span style="color: rgba(128, 0, 0, 1)">Please upload a PDF file to proceed.</span><span style="color: rgba(128, 0, 0, 1)">"</span>)</pre>
</div>
<h1>3.总结</h1>
<p>本文详细介绍了利用 DeepSeek R1 和 Ollama 构建检索增强生成（RAG）系统的方法。首先说明了 DeepSeek R1 1.5B 模型的优势，如精准检索、避免幻觉、零延迟等。接着阐述了搭建流程，包括用 Ollama 本地运行模型、上传 PDF 文件、使用递归字符文本分割器进行语义分块、生成向量嵌入并存储于 FAISS 索引、实例化检索问答链，最后整合各步骤形成连贯流程。通过 Streamlit 实现用户输入问题并即时获取答案，且确保安装langchain库可避免错误。</p>
<h1>4.结束语</h1>
<p>这篇博客就和大家分享到这里，如果大家在研究学习的过程当中有什么问题，可以加群进行讨论或发送邮件给我，我会尽我所能为您解答，与君共勉！</p>
<p>另外，博主出新书了《<span style="color: rgba(0, 0, 255, 1)"><strong><a href="https://item.jd.com/14699434.html" rel="noopener nofollow" target="_blank"><span style="color: rgba(0, 0, 255, 1)">深入理解Hive</span></a></strong></span>》、同时已出版的《<span style="color: rgba(0, 0, 255, 1)"><strong><a href="https://item.jd.com/12455361.html" rel="noopener nofollow" target="_blank"><span style="color: rgba(0, 0, 255, 1)">Kafka并不难学</span></a></strong></span>》和《<span style="color: rgba(0, 0, 255, 1)"><strong><a href="https://item.jd.com/12371763.html" rel="noopener nofollow" target="_blank"><span style="color: rgba(0, 0, 255, 1)">Hadoop大数据挖掘从入门到进阶实战</span></a></strong></span>》也可以和新书配套使用，喜欢的朋友或同学， 可以<span style="color: rgba(255, 0, 0, 1)"><strong>在公告栏那里点击购买链接购买博主的书</strong></span>进行学习，在此感谢大家的支持。关注下面公众号，根据提示，可免费获取书籍的教学视频。</p>
</div>
<div id="MySignature" role="contentinfo">
    <div>
<b class="b1"></b><b class="b2 d1"></b><b class="b3 d1"></b><b class="b4 d1"></b>
<div class="b d1 k">  
联系方式：
<br>
邮箱：smartloli.org@gmail.com
<br>
<strong style="color: green">QQ群（Hive与AI实战【新群】）：935396818</strong>
<br>
QQ群（Hadoop - 交流社区1）：424769183
<br>
QQ群（Kafka并不难学）：825943084
<br>
温馨提示：请大家加群的时候写上加群理由（姓名＋公司/学校），方便管理员审核，谢谢！
<br>
<h3>热爱生活，享受编程，与君共勉！</h3>  
</div>
<b class="b4b d1"></b><b class="b3b d1"></b><b class="b2b d1"></b><b class="b1b"></b>
</div>
<br>
<div>
<b class="b1"></b><b class="b2 d1"></b><b class="b3 d1"></b><b class="b4 d1"></b>
<div class="b d1 k">
<h3>公众号：</h3>
<h3><img style="width: 8%; margin-left: 10px" src="https://www.cnblogs.com/images/cnblogs_com/smartloli/1324636/t_qr.png"></h3>
</div>
<b class="b4b d1"></b><b class="b3b d1"></b><b class="b2b d1"></b><b class="b1b"></b>
</div>
<br>
<div>
<b class="b1"></b><b class="b2 d1"></b><b class="b3 d1"></b><b class="b4 d1"></b>
<div class="b d1 k">
<h3>作者：哥不是小萝莉 ［<a style="color: green" href="http://www.kafka-eagle.org/" target="_blank">关于我</a>］［<a style="color: green" href="http://www.cnblogs.com/smartloli/p/4241701.html" target="_blank">犒赏</a>］</h3>
<h3>出处：<a style="color: green" href="http://www.cnblogs.com/smartloli/" target="_blank">http://www.cnblogs.com/smartloli/</a></h3>
<h3>转载请注明出处，谢谢合作！</h3>
</div>
<b class="b4b d1"></b><b class="b3b d1"></b><b class="b2b d1"></b><b class="b1b"></b>
</div>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.7308057720150463" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-02-27 22:53">2025-02-27 22:53</span>&nbsp;
<a href="https://www.cnblogs.com/smartloli">哥不是小萝莉</a>&nbsp;
阅读(<span id="post_view_count">113</span>)&nbsp;
评论(<span id="post_comment_count">1</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18742244" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18742244);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18742244', targetLink: 'https://www.cnblogs.com/smartloli/p/18742244', title: '使用 DeepSeek R1 和 Ollama 开发 RAG 系统' })">举报</a>
</div>
        