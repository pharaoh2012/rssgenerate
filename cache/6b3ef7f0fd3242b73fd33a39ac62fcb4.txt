
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/zczhaod/p/18869185" title="发布于 2025-05-10 08:04">
    <span role="heading" aria-level="2">爬虫入门（urllib与requests）</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h1 id="urllib与requests">urllib与requests</h1>
<h2 id="一urllib的学习">一、urllib的学习</h2>
<p><strong>学习目标</strong><br>
了解urllib的基本使用</p>
<h3 id="1urllib介绍">1、urllib介绍</h3>
<p>除了requests模块可以发送请求之外, urllib模块也可以实现请求的发送,只是操作方法略有不同!</p>
<p>urllib在python中分为urllib和urllib2，在python3中为urllib</p>
<p>下面以python3的urllib为例进行讲解</p>
<h3 id="-2urllib的基本方法介绍">### 2、urllib的基本方法介绍</h3>
<h4 id="21-urllibrequest">2.1 urllib.Request</h4>
<ol>
<li>构造简单请求</li>
</ol>
<pre><code class="language-python">import urllib
#构造请求
request = urllib.request.Request("http://www.baidu.com")
#发送请求获取响应
response = urllib.request.urlopen(request)
</code></pre>
<ol start="2">
<li>传入headers参数</li>
</ol>
<pre><code class="language-python">import urllib
#构造headers
headers = {"User-Agent" : "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)"} 
#构造请求
request = urllib.request.Request(url, headers=headers)
#发送请求
response = urllib.request.urlopen(request)
</code></pre>
<ol start="3">
<li>传入data参数 实现发送post请求（示例）</li>
</ol>
<pre><code class="language-python">import urllib.request
import urllib.parse
import json

url = 'https://ifanyi.iciba.com/index.php?c=trans&amp;m=fy&amp;client=6&amp;auth_user=key_ciba&amp;sign=99730f3bf66b2582'

headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Safari/605.1.15',

}
data = {
    'from': 'zh',
    'to': 'en',
    'q': 'lucky 是一个帅气的老'
}
# 使用post方式
# 需要
data = urllib.parse.urlencode(data).encode('utf-8')
req = urllib.request.Request(url, data=data, headers=headers)
res = urllib.request.urlopen(req)
print(res.getcode())
print(res.geturl())
data = json.loads(res.read().decode('utf-8'))
print(data)
</code></pre>
<h4 id="22-responseread">2.2 response.read()</h4>
<p>获取响应的html字符串,bytes类型</p>
<pre><code class="language-python">#发送请求
response = urllib.request.urlopen("http://www.baidu.com")
#获取响应
response.read()
</code></pre>
<h3 id="3urllib请求百度首页的完整例子">3、urllib请求百度首页的完整例子</h3>
<pre><code class="language-python">import urllib
import json

url = 'http://www.baidu.com'
#构造headers
headers = {"User-Agent" : "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)"}
#构造请求
request = urllib.request.Request(url, headers = headers)
#发送请求
response = urllib.request.urlopen(request)
#获取html字符串
html_str = response.read().decode('utf-8')
print(html_str)
</code></pre>
<h3 id="4小结">4、小结</h3>
<ol>
<li>urllib.request中实现了构造请求和发送请求的方法</li>
<li>urllib.request.Request(url,headers,data)能够构造请求</li>
<li>urllib.request.urlopen能够接受request请求或者url地址发送请求，获取响应</li>
<li>response.read()能够实现获取响应中的bytes字符串</li>
</ol>
<h1 id="requests模块的入门使用">requests模块的入门使用</h1>
<h2 id="一requests模块的入门使用">一、requests模块的入门使用</h2>
<p><strong>学习目标：</strong></p>
<ol>
<li>了解 requests模块的介绍</li>
<li>掌握 requests的基本使用</li>
<li>掌握 response常见的属性</li>
<li>掌握 requests.text和content的区别</li>
<li>掌握 解决网页的解码问题</li>
<li>掌握 requests模块发送带headers的请求</li>
<li>掌握 requests模块发送带参数的get请求</li>
</ol>
<h3 id="1为什么要重点学习requests模块而不是urllib">1、为什么要重点学习requests模块，而不是urllib</h3>
<ul>
<li>企业中用的最多的就是requests</li>
<li>requests的底层实现就是urllib</li>
<li>requests在python2 和python3中通用，方法完全一样</li>
<li>requests简单易用</li>
</ul>
<h3 id="2requests的作用与安装">2、requests的作用与安装</h3>
<p>作用：发送网络请求，返回响应数据<br>
命令： pip install requests<br>
requests模块发送简单的get请求、获取响应<br>
需求：通过requests向百度首页发送请求，获取百度首页的数据</p>
<pre><code class="language-python">import requests

# 目标url
url = 'https://www.baidu.com'

# 向目标url发送get请求
response = requests.get(url)

# 打印响应内容
print(response.text)
</code></pre>
<h3 id="3response的常用属性">3、response的常用属性：</h3>
<ul>
<li>
<p><code>response.text</code> 响应体 str类型</p>
</li>
<li>
<p><code>response.encoding</code>  从HTTP　header中猜测的响应内容的编码方式</p>
</li>
<li>
<p><code>respones.content</code> 响应体 bytes类型</p>
</li>
<li>
<p><code>response.status_code</code> 响应状态码</p>
</li>
<li>
<p><code>response.request.headers</code> 响应对应的请求头</p>
</li>
<li>
<p><code>response.headers</code> 响应头</p>
</li>
<li>
<p><code>response.cookies</code> 响应的cookie（经过了set-cookie动作）</p>
</li>
<li>
<p><code>response.url </code> 获取访问的url</p>
</li>
<li>
<p><code>response.json()</code>  获取json数据 得到内容为字典 (如果接口响应体的格式是json格式时)</p>
</li>
<li>
<p><code>response.ok</code></p>
<p>如果status_code小于等于200，response.ok返回True。</p>
<p>如果status_code大于200，response.ok返回False。</p>
</li>
</ul>
<p><strong>思考：text是response的属性还是方法呢？</strong></p>
<ul>
<li>一般来说名词，往往都是对象的属性，对应的动词是对象的方法</li>
</ul>
<h4 id="31-responsetext-和responsecontent的区别">3.1 response.text 和response.content的区别</h4>
<ul>
<li>response.text`
<ul>
<li>类型：str</li>
<li>解码类型： requests模块自动根据HTTP 头部对响应的编码作出有根据的推测，推测的文本编码</li>
<li>如何修改编码方式：<code>response.encoding="gbk/UTF-8"</code></li>
</ul>
</li>
<li><code>response.content</code>
<ul>
<li>类型：bytes</li>
<li>解码类型： 没有指定</li>
<li>如何修改编码方式：<code>response.content.deocde("utf-8")</code></li>
</ul>
</li>
</ul>
<p>获取网页源码的通用方式：</p>
<ol>
<li><code>response.content.decode()</code></li>
<li><code>response.content.decode("UTF-8")</code></li>
<li><code>response.text</code></li>
</ol>
<p>以上三种方法从前往后尝试，能够100%的解决所有网页解码的问题</p>
<p>所以：更推荐使用<code>response.content.deocde()</code>的方式获取响应的html页面</p>
<h4 id="32-练习把网络上的图片保存到本地">3.2 练习：把网络上的图片保存到本地</h4>
<p>我们来把<code>www.baidu.com</code>的图片保存到本地<br>
<strong>思考：</strong></p>
<ul>
<li>以什么方式打开文件</li>
<li>保存什么格式的内容</li>
</ul>
<p><strong>分析：</strong></p>
<ul>
<li>图片的url: <a href="https://www.baidu.com/img/bd_logo1.png" target="_blank" rel="noopener nofollow">https://www.baidu.com/img/bd_logo1.png</a></li>
<li>利用requests模块发送请求获取响应</li>
<li>以2进制写入的方式打开文件，并将response响应的二进制内容写入</li>
</ul>
<pre><code class="language-python">import requests

# 图片的url
url = 'https://www.baidu.com/img/bd_logo1.png'

# 响应本身就是一个图片,并且是二进制类型
response = requests.get(url)

# print(response.content)

# 以二进制+写入的方式打开文件
with open('baidu.png', 'wb') as f:
    # 写入response.content bytes二进制类型
    f.write(response.content)
</code></pre>
<h3 id="4发送带header的请求">4、发送带header的请求</h3>
<p>我们先写一个获取百度首页的代码</p>
<pre><code class="language-python">import requests

url = 'https://www.baidu.com'

response = requests.get(url)

print(response.content)

# 打印响应对应请求的请求头信息
print(response.request.headers)
</code></pre>
<h4 id="41-思考">4.1 思考</h4>
<p>对比浏览器上百度首页的网页源码和代码中的百度首页的源码，有什么不同？</p>
<p>代码中的百度首页的源码非常少，为什么？</p>
<h4 id="42-为什么请求需要带上header">4.2 为什么请求需要带上header？</h4>
<p>模拟浏览器，欺骗服务器，获取和浏览器一致的内容</p>
<h4 id="43-header的形式字典">4.3 header的形式：字典</h4>
<pre><code>headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36"}
</code></pre>
<h4 id="44-用法">4.4 用法</h4>
<pre><code>requests.get(url, headers=headers)
</code></pre>
<h4 id="45-完整的代码">4.5 完整的代码</h4>
<pre><code class="language-python">import requests

url = 'https://www.baidu.com'

headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36"}

# 在请求头中带上User-Agent，模拟浏览器发送请求
response = requests.get(url, headers=headers)

# print(response.content)

# 打印请求头信息
print(response.request.headers)
</code></pre>
<h3 id="5发送带参数的请求">5、发送带参数的请求</h3>
<p>我们在使用百度搜索的时候经常发现url地址中会有一个 <code>?</code>，那么该问号后边的就是请求参数，又叫做查询字符串</p>
<h4 id="51-什么叫做请求参数">5.1 什么叫做请求参数：</h4>
<p>例1： <a href="http://www.webkaka.com/tutorial/server/2015/021013/" target="_blank" rel="noopener nofollow">http://www.webkaka.com/tutorial/server/2015/021013/</a></p>
<p>例2：<a href="https://www.baidu.com/s?wd=python&amp;a=c" target="_blank" rel="noopener nofollow">https://www.baidu.com/s?wd=python&amp;a=c</a></p>
<p>例1中没有请求参数！例2中?后边的就是请求参数</p>
<h4 id="52-请求参数的形式字典">5.2 请求参数的形式：字典</h4>
<pre><code>kw = {'wd':'长城'}
</code></pre>
<h4 id="53-请求参数的用法">5.3 请求参数的用法</h4>
<pre><code>requests.get(url,params=kw)
</code></pre>
<h4 id="54-关于参数的注意点">5.4 关于参数的注意点</h4>
<p>在url地址中， 很多参数是没有用的，比如百度搜索的url地址，其中参数只有一个字段有用，其他的都可以删除 如何确定那些请求参数有用或者没用：挨个尝试！ 对应的,在后续的爬虫中，越到很多参数的url地址，都可以尝试删除参数</p>
<h4 id="55-两种方式发送带参数的请求">5.5 两种方式：发送带参数的请求</h4>
<ul>
<li>对<code>https://www.baidu.com/s?wd=python</code>发起请求可以使用<code>requests.get(url, params=kw)</code>的方式</li>
</ul>
<pre><code class="language-python"># 方式一：利用params参数发送带参数的请求
import requests

headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36"}

# 这是目标url
# url = 'https://www.baidu.com/s?wd=python'

# 最后有没有问号结果都一样
url = 'https://www.baidu.com/s'

# 请求参数是一个字典 即wd=python
kw = {'wd': 'python'}

# 带上请求参数发起请求，获取响应
response = requests.get(url, headers=headers, params=kw)

# 当有多个请求参数时，requests接收的params参数为多个键值对的字典，比如 '?wd=python&amp;a=c'--&gt;{'wd': 'python', 'a': 'c'}

print(response.content)
</code></pre>
<ul>
<li>也可以直接对<code>https://www.baidu.com/s?wd=python</code>完整的url直接发送请求，不使用params参数</li>
</ul>
<pre><code class="language-python"># 方式二：直接发送带参数的url的请求
import requests

headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36"}

url = 'https://www.baidu.com/s?wd=python'

# kw = {'wd': 'python'}

# url中包含了请求参数，所以此时无需params
response = requests.get(url, headers=headers)
</code></pre>
<h3 id="6小结">6、小结</h3>
<ol>
<li>requests模块的介绍：能够帮助我们发起请求获取响应</li>
<li>requests的基本使用：<code>requests.get(url)</code></li>
<li>以及response常见的属性：
<ul>
<li><code>response.text</code> 响应体 str类型</li>
<li><code>respones.content</code> 响应体 bytes类型</li>
<li><code>response.status_code</code> 响应状态码</li>
<li><code>response.request.headers</code> 响应对应的请求头</li>
<li><code>response.headers</code> 响应头</li>
<li><code>response.request._cookies</code> 响应对应请求的cookie</li>
<li><code>response.cookies</code> 响应的cookie（经过了set-cookie动作）</li>
</ul>
</li>
<li>掌握 requests.text和content的区别：text返回str类型，content返回bytes类型</li>
<li>掌握 解决网页的解码问题：
<ul>
<li><code>response.content.decode()</code></li>
<li><code>response.content.decode("UTF-8")</code></li>
<li><code>response.text</code></li>
</ul>
</li>
<li>掌握 requests模块发送带headers的请求：<code>requests.get(url, headers={})</code></li>
<li>掌握 requests模块发送带参数的get请求：<code>requests.get(url, params={})</code></li>
</ol>
<h2 id="二requests模块的深入使用">二、requests模块的深入使用</h2>
<p><strong>学习目标：</strong></p>
<ol>
<li>能够应用requests发送post请求的方法</li>
<li>能够应用requests模块使用代理的方法</li>
<li>了解代理ip的分类</li>
</ol>
<h3 id="1使用requests发送post请求">1、使用requests发送POST请求</h3>
<blockquote>
<p>思考：哪些地方我们会用到POST请求？</p>
</blockquote>
<ol>
<li>登录注册（ POST 比 GET 更安全）</li>
<li>需要传输大文本内容的时候（ POST 请求对数据长度没有要求）</li>
</ol>
<p>所以同样的，我们的爬虫也需要在这两个地方回去模拟浏览器发送post请求</p>
<h4 id="11-requests发送post请求语法">1.1 requests发送post请求语法：</h4>
<ul>
<li>用法：<pre><code class="language-python">response = requests.post("http://www.baidu.com/", data = data, headers=headers)
</code></pre>
</li>
<li>data 的形式：字典</li>
</ul>
<h4 id="12-post请求练习">1.2 POST请求练习</h4>
<p>下面面我们通过金山翻译的例子看看post请求如何使用：<br>
地址：<a href="https://www.iciba.com/fy" target="_blank" rel="noopener nofollow">https://www.iciba.com/fy</a><br>
<strong>思路分析</strong></p>
<ol>
<li>
<p>抓包确定请求的url地址<br>
<img src="https://img2024.cnblogs.com/blog/2595693/202505/2595693-20250510075508013-881689948.png" alt="image" loading="lazy"></p>
</li>
<li>
<p>确定请求的参数<br>
<img src="https://img2024.cnblogs.com/blog/2595693/202505/2595693-20250510075534781-145120038.png" alt="image" loading="lazy"></p>
</li>
<li>
<p>确定返回数据的位置<br>
<img src="https://img2024.cnblogs.com/blog/2595693/202505/2595693-20250510075601709-304102545.png" alt="image" loading="lazy"></p>
</li>
<li>
<p>模拟浏览器获取数据</p>
</li>
</ol>
<pre><code class="language-python">import requests
import json

headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36"}

url = 'https://ifanyi.iciba.com/index.php?c=trans&amp;m=fy&amp;client=6&amp;auth_user=key_ciba&amp;sign=99730f3bf66b2582'

data = {
    'from': 'zh',
    'to': 'en',
    'q': 'lucky 是一个帅气的老师'
}

res = requests.post(url, headers=headers, data=data)
# print(res.status_code)

# 返回的是json字符串 需要在进行转换为字典
data = json.loads(res.content.decode('UTF-8'))
# print(type(data))
print(data)
print(data['content']['out'])
</code></pre>
<h4 id="13-小结">1.3 小结</h4>
<p>在模拟登陆等场景，经常需要发送post请求，直接使用<code>requests.post(url,data)</code>即可</p>
<h3 id="2使用代理">2、使用代理</h3>
<h4 id="21-为什么要使用代理">2.1 为什么要使用代理</h4>
<ol>
<li>让服务器以为不是同一个客户端在请求</li>
<li>防止我们的真实地址被泄露，防止被追究</li>
</ol>
<h4 id="22-理解使用代理的过程">2.2 理解使用代理的过程</h4>
<p><img src="https://img2024.cnblogs.com/blog/2595693/202505/2595693-20250510075852466-644625989.png" alt="image" loading="lazy"></p>
<h4 id="23-理解正向代理和反向代理的区别">2.3 理解正向代理和反向代理的区别</h4>
<p><img src="https://img2024.cnblogs.com/blog/2595693/202505/2595693-20250510075921709-721875462.png" alt="image" loading="lazy"></p>
<p>通过上图可以看出：</p>
<ul>
<li>正向代理：对于浏览器知道服务器的真实地址，例如VPN</li>
<li>反向代理：浏览器不知道服务器的真实地址，例如nginx</li>
</ul>
<p><strong>详细讲解：</strong></p>
<p>正向代理是客户端与正向代理客户端在同一局域网，客户端发出请求，正向代理 替代客户端向服务器发出请求。服务器不知道谁是真正的客户端，正向代理隐藏了真实的请求客户端。<br>
反向代理：服务器与反向代理在同一个局域网，客服端发出请求，反向代理接收请求 ，反向代理服务器会把我们的请求分转发到真实提供服务的各台服务器Nginx就是性能非常好的反向代理服务器，用来做负载均衡<br>
<img src="https://img2024.cnblogs.com/blog/2595693/202505/2595693-20250510075950978-556299665.png" alt="image" loading="lazy"></p>
<h4 id="24-代理的使用">2.4 代理的使用</h4>
<ul>
<li>用法：</li>
</ul>
<pre><code>requests.get("http://www.baidu.com",  proxies = proxies)
</code></pre>
<ul>
<li>proxies的形式：字典</li>
<li>例如：</li>
</ul>
<pre><code>proxies = {
      "http": "http://12.34.56.79:9527",
      "https": "https://12.34.56.79:9527",
}
</code></pre>
<h4 id="25-代理ip的分类">2.5 代理IP的分类</h4>
<p>根据代理ip的匿名程度，代理IP可以分为下面四类：</p>
<ul>
<li>透明代理(Transparent Proxy)：透明代理的意思是客户端根本不需要知道有代理服务器的存在，但是它传送的仍然是真实的IP。使用透明代理时，对方服务器是可以知道你使用了代理的，并且他们也知道你的真实IP。你要想隐藏的话，不要用这个。透明代理为什么无法隐藏身份呢?因为他们将你的真实IP发送给了对方服务器，所以无法达到保护真实信息。</li>
<li>匿名代理(Anonymous Proxy)：匿名代理隐藏了您的真实IP，但是向访问对象可以检测是使用代理服务器访问他们的。会改变我们的请求信息，服务器端有可能会认为我们使用了代理。不过使用此种代理时，虽然被访问的网站不能知道你的ip地址，但仍然可以知道你在使用代理，当然某些能够侦测ip的网页也是可以查到你的ip。（<a href="https://wenku.baidu.com/view/9bf7b5bd3a3567ec102de2bd960590c69fc3d8cf.html%EF%BC%89" target="_blank" rel="noopener nofollow">https://wenku.baidu.com/view/9bf7b5bd3a3567ec102de2bd960590c69fc3d8cf.html）</a></li>
<li>高匿代理(Elite proxy或High Anonymity Proxy)：高匿名代理不改变客户机的请求，这样在服务器看来就像有个真正的客户浏览器在访问它，这时客户的真实IP是隐藏的，完全用代理服务器的信息替代了您的所有信息，就象您就是完全使用那台代理服务器直接访问对象，同时服务器端不会认为我们使用了代理。IPDIEA覆盖全球240＋国家地区ip高匿名代理不必担心被追踪。</li>
</ul>
<p>在使用的使用，毫无疑问使用高匿代理效果最好</p>
<p>从请求使用的协议可以分为：</p>
<ul>
<li>http代理</li>
<li>https代理</li>
<li>socket代理等</li>
</ul>
<p>不同分类的代理，在使用的时候需要根据抓取网站的协议来选择</p>
<h4 id="26-代理ip使用的注意点">2.6 代理IP使用的注意点</h4>
<ul>
<li>
<p>反反爬</p>
<p>使用代理ip是非常必要的一种<code>反反爬</code>的方式</p>
<p>但是即使使用了代理ip，对方服务器任然会有很多的方式来检测我们是否是一个爬虫，比如：</p>
<ul>
<li>
<p>一段时间内，检测IP访问的频率，访问太多频繁会屏蔽</p>
</li>
<li>
<p>检查Cookie，User-Agent，Referer等header参数，若没有则屏蔽</p>
</li>
<li>
<p>服务方购买所有代理提供商，加入到反爬虫数据库里，若检测是代理则屏蔽</p>
<p>所以更好的方式在使用代理ip的时候使用随机的方式进行选择使用，不要每次都用一个代理ip</p>
</li>
</ul>
</li>
<li>
<p>代理ip池的更新</p>
<p>购买的代理ip很多时候大部分(超过60%)可能都没办法使用，这个时候就需要通过程序去检测哪些可用，把不能用的删除掉。</p>
</li>
</ul>
<ul>
<li>
<p>代理服务器平台的使用：</p>
<p>当然还有很多免费的，但是大多都不可用需要自己尝试</p>
<ul>
<li><a href="http://www.66ip.cn" target="_blank" rel="noopener nofollow">http://www.66ip.cn</a></li>
<li><a href="https://ip.jiangxianli.com/?page=1" target="_blank" rel="noopener nofollow">https://ip.jiangxianli.com/?page=1</a></li>
<li><a href="https://www.zdaye.com" target="_blank" rel="noopener nofollow">https://www.zdaye.com</a></li>
<li><a href="https://www.kuaidaili.com/free" target="_blank" rel="noopener nofollow">https://www.kuaidaili.com/free</a></li>
</ul>
</li>
</ul>
<h3 id="3配置">3、配置</h3>
<ul>
<li>浏览器配置代理<br>
右边三点==&gt; 设置==&gt; 高级==&gt; 代理==&gt; 局域网设置==&gt; 为LAN使用代理==&gt; 输入ip和端口号即可<br>
参考网址：<a href="https://jingyan.baidu.com/article/a681b0dece76407a1843468d.html" target="_blank" rel="noopener nofollow">https://jingyan.baidu.com/article/a681b0dece76407a1843468d.html</a></li>
<li>代码配置<br>
urllib</li>
</ul>
<pre><code class="language-python">handler = urllib.request.ProxyHandler({'http': '114.215.95.188:3128'})
opener = urllib.request.build_opener(handler)
# 后续都使用opener.open方法去发送请求即可
</code></pre>
<p>requests</p>
<pre><code class="language-python"># 用到的库
import requests
# 写入获取到的ip地址到proxy
# 一个ip地址
proxy = {
    'http':'http://221.178.232.130:8080'
}
"""
# 多个ip地址
proxy = [
  {'http':'http://221.178.232.130:8080'},
  {'http':'http://221.178.232.130:8080'}
]
import random
proxy = random.choice(proxy)
"""

# 使用代理
proxy = {
    'http': 'http://58.20.184.187:9091'
}

result = requests.get("http://httpbin.org/ip", proxies=proxy)

print(result.text)
</code></pre>
<h3 id="4小结-1">4、小结</h3>
<ol>
<li>requests发送post请求使用requests.post方法，带上请求体，其中请求体需要时字典的形式，传递给data参数接收</li>
<li>在requests中使用代理，需要准备字典形式的代理，传递给proxies参数接收</li>
<li>不同协议的url地址，需要使用不同的代理去请求</li>
</ol>

</div>
<div id="MySignature" role="contentinfo">
    <p>本文来自博客园，作者：<a href="https://www.cnblogs.com/zczhaod/" target="_blank">生而自由爱而无畏</a>，转载请注明原文链接：<a href="https://www.cnblogs.com/zczhaod/p/18869185" target="_blank">https://www.cnblogs.com/zczhaod/p/18869185</a></p>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.12388926919907407" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-05-10 08:04">2025-05-10 08:04</span>&nbsp;
<a href="https://www.cnblogs.com/zczhaod">生而自由爱而无畏</a>&nbsp;
阅读(<span id="post_view_count">0</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18869185);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18869185', targetLink: 'https://www.cnblogs.com/zczhaod/p/18869185', title: '爬虫入门（urllib与requests）' })">举报</a>
</div>
        