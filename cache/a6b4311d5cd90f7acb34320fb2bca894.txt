
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/gzyatcnblogs/p/18684805" title="发布于 2025-01-22 00:43">
    <span role="heading" aria-level="2">大模型训练工具，小白也能轻松搞定！</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                    <div id="cnblogs_post_description" style="display: none">
        <img src="https://img2024.cnblogs.com/blog/2338485/202501/2338485-20250122004119746-1750666273.png" alt="大模型训练工具，小白也能轻松搞定！" class="desc_img">
        常见的大模型微调、训练工具，包括Axolotl、Llama-Factory、Firfly、Xtuner、Swift、Unsloth、Trainer。
    </div>
<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p></p><div class="toc"><div class="toc-container-header">目录</div><ul><li><a href="#axolotl" rel="noopener nofollow">Axolotl</a></li><li><a href="#llama-factory" rel="noopener nofollow">Llama-Factory</a></li><li><a href="#firfly" rel="noopener nofollow">Firfly</a></li><li><a href="#xtuner" rel="noopener nofollow">Xtuner</a></li><li><a href="#swift" rel="noopener nofollow">Swift</a></li><li><a href="#unsloth" rel="noopener nofollow">Unsloth</a></li><li><a href="#transformerstrainer" rel="noopener nofollow">transformers.Trainer</a></li><li><a href="#总结" rel="noopener nofollow">总结</a></li></ul></div><p></p>
<p><img src="https://img2024.cnblogs.com/blog/2338485/202501/2338485-20250122003824646-1221341470.png" alt="" loading="lazy"></p>
<div>
<div style="page-break-after: always"></div>
<h1 id="axolotl"><a href="https://github.com/axolotl-ai-cloud/axolotl" target="_blank" rel="noopener nofollow">Axolotl</a></h1>
<p><img src="https://img2024.cnblogs.com/blog/2338485/202501/2338485-20250122003851039-758908601.png" alt="" loading="lazy"></p>
<p>Axolotl 是一款旨在简化各种人工智能模型微调的工具，支持多种配置和架构。</p>
<p><strong>主要特点：</strong></p>
<ul>
<li>支持的常见开源大模型，多种训练方式，包括：全参微调、LoRA/QLoRA、xformers等。</li>
<li>可通过 yaml 或 CLI 自定义配置。</li>
<li>支持多种数据集格式以及自定义格式。</li>
<li>集成了 xformer、flash attention、liger kernel、rope 及 multipacking。</li>
<li>使用 Docker 在本地或云端轻松运行。</li>
<li>将结果和可选的检查点记录到 wandb 或 mlflow 中。</li>
</ul>
<p>示例：</p>
<pre><code class="language-shell"># finetune lora
accelerate launch -m axolotl.cli.train examples/openllama-3b/lora.yml
</code></pre>
<div>
<div style="page-break-after: always"></div>
<h1 id="llama-factory"><a href="https://github.com/hiyouga/LLaMA-Factory/tree/main" target="_blank" rel="noopener nofollow">Llama-Factory</a></h1>
<p><img src="https://img2024.cnblogs.com/blog/2338485/202501/2338485-20250122003911543-740283500.png" alt="" loading="lazy"></p>
<p>使用<span data-type="text" style="background-color: var(--b3-card-error-background); color: var(--b3-card-error-color)">零代码命令行</span>与 <span data-type="text" style="background-color: var(--b3-card-error-background); color: var(--b3-card-error-color)">Web UI</span> 轻松训练百余种大模型，并提供高效的训练和评估工具。</p>
<p><strong>主要特点：</strong></p>
<ul>
<li><strong>多种模型</strong>：LLaMA、LLaVA、Mistral、Mixtral-MoE、Qwen、Yi、Gemma、Baichuan、ChatGLM、Phi 等等。</li>
<li><strong>多种训练</strong>：预训练、（多模态）指令监督微调、奖励模型训练、PPO/DPO/KTO/ORPO 训练等等。</li>
<li><strong>多种精度</strong>：16-bit全参微调、冻结微调、LoRA/QLoRA 微调。</li>
<li><strong>先进算法</strong>：GaLore、BAdam、DoRA、LongLoRA、LLaMA Pro、Mixture-of-Depths、LoRA+、LoftQ、PiSSA 和 Agent 微调。</li>
<li><strong>实用技巧</strong>：FlashAttention-2、Unsloth、RoPE scaling、NEFTune 和 rsLoRA。</li>
<li><strong>实验监控</strong>：LlamaBoard、TensorBoard、Wandb、MLflow 等等。</li>
<li><strong>极速推理</strong>：基于 vLLM 的 OpenAI 风格 API、浏览器界面和命令行接口。</li>
</ul>
<p>示例：</p>
<pre><code class="language-shell">llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml
llamafactory-cli chat examples/inference/llama3_lora_sft.yaml
llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml
</code></pre>
<div>
<div style="page-break-after: always"></div>
<h1 id="firfly"><a href="https://github.com/yangjianxin1/Firefly" target="_blank" rel="noopener nofollow">Firfly</a></h1>
<p><img src="https://img2024.cnblogs.com/blog/2338485/202501/2338485-20250122003922003-1443343359.png" alt="" loading="lazy"></p>
<p><strong>Firefly</strong> 支持对主流的大模型进行预训练、指令微调和 DPO。</p>
<p><strong>主要特点：</strong></p>
<ul>
<li>
<p>支持预训练、SFT、DPO，支持全参数训练、LoRA/QLoRA 训练。</p>
</li>
<li>
<p>支持使用 <a href="https://github.com/yangjianxin1/unsloth" target="_blank" rel="noopener nofollow">Unsloth</a> 加速训练，降低显存需求。</p>
</li>
<li>
<p>支持绝大部分主流的开源大模型，如 Llama3、Gemma、MiniCPM、Llama、InternLM、Baichuan、ChatGLM、Yi、Deepseek、Qwen、Orion、Ziya、Xverse、Mistral、Mixtral-8x7B、Zephyr、Vicuna、Bloom，训练时与各个官方的 chat 模型的 template 对齐。</p>
</li>
<li>
<p>整理并开源指令微调数据集：firefly-train-1.1M 、moss-003-sft-data、ultrachat、 WizardLM_evol_instruct_V2_143k、school_math_0.25M。</p>
</li>
<li>
<p>在 Open LLM Leaderboard 上验证了 QLoRA 训练流程的有效性，开源<a href="https://huggingface.co/YeungNLP" target="_blank" rel="noopener nofollow">Firefly 系列指令微调模型权重</a>。</p>
</li>
</ul>
<p>示例：</p>
<pre><code class="language-shell">deepspeed --num_gpus={num_gpus} train.py --train_args_file train_args/sft/full/bloom-1b1-sft-full.json
torchrun --nproc_per_node={num_gpus} train.py --train_args_file train_args/pretrain/qlora/yi-6b-pretrain-qlora.json
</code></pre>
<div>
<div style="page-break-after: always"></div>
<h1 id="xtuner"><a href="https://github.com/InternLM/xtuner" target="_blank" rel="noopener nofollow">Xtuner</a></h1>
<p><img src="https://img2024.cnblogs.com/blog/2338485/202501/2338485-20250122003933062-1818906587.png" alt="" loading="lazy"></p>
<p>XTuner 由上海人工智能实验室发布，是一个高效、灵活、全能的轻量化大模型微调工具库。</p>
<p><strong>主要特点：</strong></p>
<ul>
<li>
<p><strong>高效</strong></p>
<ul>
<li>支持大语言模型 LLM、多模态图文模型 VLM 的预训练及轻量级微调。XTuner 支持在 8GB 显存下微调 7B 模型，同时也支持多节点跨设备微调更大尺度模型（70B+）。</li>
<li>自动分发高性能算子（如 FlashAttention、Triton kernels 等）加速训练吞吐。</li>
</ul>
</li>
<li>
<p><strong>灵活</strong></p>
<ul>
<li>支持多种大语言模型，包括但不限于 <a href="https://huggingface.co/internlm" target="_blank" rel="noopener nofollow">InternLM</a>、<a href="https://huggingface.co/mistralai" target="_blank" rel="noopener nofollow">Mixtral-8x7B</a>、<a href="https://huggingface.co/meta-llama" target="_blank" rel="noopener nofollow">Llama 2</a>、<a href="https://huggingface.co/THUDM" target="_blank" rel="noopener nofollow">ChatGLM</a>、<a href="https://huggingface.co/Qwen" target="_blank" rel="noopener nofollow">Qwen</a>、<a href="https://huggingface.co/baichuan-inc" target="_blank" rel="noopener nofollow">Baichuan</a>，及多模态图文模型 LLaVA 的预训练与微调。</li>
<li>兼容任意数据格式，开源数据或自定义数据皆可快速上手。</li>
<li>支持增量预训练、<a href="http://arxiv.org/abs/2305.14314" target="_blank" rel="noopener nofollow">QLoRA</a>、<a href="http://arxiv.org/abs/2106.09685" target="_blank" rel="noopener nofollow">LoRA</a>、指令微调、Agent微调、全量参数微调等多种训练方式。</li>
</ul>
</li>
<li>
<p><strong>全能</strong></p>
<ul>
<li>预定义众多开源对话模版，支持与开源或训练所得模型进行对话。</li>
<li>训练所得模型可无缝接入部署工具库 <a href="https://github.com/InternLM/lmdeploy" target="_blank" rel="noopener nofollow">LMDeploy</a>、大规模评测工具库 <a href="https://github.com/open-compass/opencompass" target="_blank" rel="noopener nofollow">OpenCompass</a> 及 <a href="https://github.com/open-compass/VLMEvalKit" target="_blank" rel="noopener nofollow">VLMEvalKit</a>。</li>
</ul>
</li>
</ul>
<p>示例：</p>
<pre><code class="language-shell">xtuner train internlm2_5_chat_7b_qlora_oasst1_e3 --deepspeed deepspeed_zero2 # 单卡
# 多卡
(DIST) NPROC_PER_NODE=${GPU_NUM} xtuner train internlm2_5_chat_7b_qlora_oasst1_e3 --deepspeed deepspeed_zero2
(SLURM) srun ${SRUN_ARGS} xtuner train internlm2_5_chat_7b_qlora_oasst1_e3 --launcher slurm --deepspeed deepspeed_zero2
</code></pre>
<div>
<div style="page-break-after: always"></div>
<h1 id="swift"><a href="https://github.com/modelscope/ms-swift/tree/main" target="_blank" rel="noopener nofollow">Swift</a></h1>
<p><img src="https://img2024.cnblogs.com/blog/2338485/202501/2338485-20250122003943697-255792829.png" alt="" loading="lazy"></p>
<p>ms-swift是魔塔提供的大模型与多模态大模型微调部署框架，支持450+大模型与150+多模态大模型的训练、推理、评测、量化与部署。</p>
<p><strong>主要特点：</strong></p>
<ul>
<li>
<p>🍎 <strong>模型类型</strong>：支持450+纯文本大模型、<strong>150+多模态大模型</strong>，All-to-All全模态模型的<strong>训练到部署全流程</strong>。</p>
</li>
<li>
<p><strong>数据集类型</strong>：内置150+预训练、微调、人类对齐、多模态等各种类型的数据集，并支持自定义数据集。</p>
</li>
<li>
<p><strong>多种训练：</strong></p>
<ul>
<li><strong>轻量训练</strong>：支持LoRA/QLoRA/DoRA/LoRA+/RS-LoRA、ReFT、LLaMAPro、Adapter、GaLore/Q-Galore、LISA、UnSloth、Liger-Kernel等轻量微调方式。支持对BNB、AWQ、GPTQ、AQLM、HQQ、EETQ量化模型进行训练。</li>
<li><strong>RLHF训练</strong>：支持文本和多模态大模型的DPO、CPO、SimPO、ORPO、KTO、RM、PPO等RLHF训练。</li>
<li><strong>多模态训练</strong>：支持对图像、视频和语音模态模型进行训练，支持VQA、Caption、OCR、Grounding任务的训练。</li>
<li><strong>界面训练</strong>：以界面的方式提供训练、推理、评测、量化的能力，完成大模型的全链路。</li>
</ul>
</li>
<li>
<p><strong>插件化与拓展</strong>：支持对loss、metric、trainer、loss-scale、callback、optimizer等组件进行自定义。</p>
</li>
<li>
<p><strong>模型评测</strong>：以EvalScope作为评测后端，支持100+评测数据集对纯文本和多模态模型进行评测。</p>
</li>
</ul>
<p>示例：</p>
<pre><code class="language-shell">CUDA_VISIBLE_DEVICES=0 swift sft --model Qwen/Qwen2.5-7B-Instruct \
    --train_type lora \
    --dataset 'AI-ModelScope/alpaca-gpt4-data-zh#500' \
    --lora_rank 8 --lora_alpha 32 \
    --target_modules all-linear \
    --warmup_ratio 0.05
</code></pre>
<div>
<div style="page-break-after: always"></div>
<h1 id="unsloth"><a href="https://github.com/unslothai/unsloth" target="_blank" rel="noopener nofollow">Unsloth</a></h1>
<p><img src="https://img2024.cnblogs.com/blog/2338485/202501/2338485-20250122003953386-1712899059.png" alt="" loading="lazy"></p>
<p>Unsloth是一个开源的大模型训练加速项目，使用OpenAI的Triton对模型的计算过程进行重写，<strong>大幅提升模型的训练速度，降低训练中的显存占用</strong>。Unsloth能够保证重写后的模型计算的一致性，实现中不存在近似计算，<strong>模型训练的精度损失为零</strong>。</p>
<p><strong>主要特点：</strong></p>
<ul>
<li>所有内核均使用OpenAI的Triton语言编写。采用手动反向传播引擎。</li>
<li>精度无损失——不采用近似方法——全部精确。</li>
<li>无需更改硬件。支持2018年及以后版本的NVIDIA GPU。最低CUDA Capability为7.0（V100、T4、Titan V、RTX 20/30/40、A100、H100、L40等）。GTX 1070、1080也可以使用，但速度较慢。</li>
<li>支持4bit和16bit GLorA/LoRA微调。</li>
<li>开源版本训练速度提高5倍，使用Unsloth Pro可获得高达30倍的训练加速！</li>
</ul>
<p>示例：</p>
<pre><code class="language-python">from unsloth import FastLanguageModel
# ... 导入其他包
max_seq_length = 2048 # Supports RoPE Scaling interally, so choose any!
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/llama-3-8b-bnb-4bit",
    max_seq_length = max_seq_length,
    dtype = None,
    load_in_4bit = True,
)
# 后续流程和使用 transformers.Trainer 类似
</code></pre>
<div>
<div style="page-break-after: always"></div>
<h1 id="transformerstrainer"><a href="https://huggingface.co/docs/transformers/zh/main_classes/trainer" target="_blank" rel="noopener nofollow">transformers.Trainer</a></h1>
<p><img src="https://img2024.cnblogs.com/blog/2338485/202501/2338485-20250122004004459-1119047213.png" alt="" loading="lazy"></p>
<p>最后不得不提下大名鼎鼎的transformers库的Trainer，上述的很多工具其实也是在其基础上构建的。</p>
<p>Trainer本身是一个高度封装的类，但相比刚刚提到的工具，居然还有点偏底层了😅。</p>
<p><strong>主要特点：</strong></p>
<ul>
<li><strong>通用性：</strong> Trainer是一个通用的训练接口，适用于各种NLP任务，如分类、回归、语言建模等。它提供了标准化的训练流程，使得用户无需从头开始编写训练代码。</li>
<li><strong>灵活性</strong>：用户可以通过自定义训练循环、损失函数、优化器、学习率调度器等方式来调整训练过程。</li>
<li><strong>高级功能：</strong> 混合精度训练、分布式训练、断点续训等。</li>
<li><strong>自定义回调函数</strong>：允许用户添加自定义回调函数，以便在训练过程的特定阶段执行自定义操作。</li>
</ul>
<p>示例：</p>
<pre><code class="language-python">from transformers import Trainer
# 加载模型、数据
trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)
trainer.train()
</code></pre>
<h1 id="总结">总结</h1>
<table>
<thead>
<tr>
<th><strong>工具名称</strong></th>
<th><strong>模型支持</strong></th>
<th><strong>训练方式</strong></th>
<th><strong>优化与加速</strong></th>
<th><strong>数据支持</strong></th>
<th><strong>工具与集成</strong></th>
<th><strong>其他特性</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Axolotl</strong></td>
<td>常见开源大模型</td>
<td>全参数微调、LoRA/QLoRA、xformers等</td>
<td>xformer、Flash Attention、liger kernel、rope、multipacking</td>
<td>多种数据集格式、支持自定义数据格式</td>
<td>wandb、MLflow</td>
<td>使用docker本地或云端运行</td>
</tr>
<tr>
<td><strong>Llama-Factory</strong></td>
<td>常见开源大模型、多模态模型</td>
<td>预训练、指令监督微调、奖励模型训练、PPO/DPO/KTO/ORPO 等</td>
<td>Flash Attention、Unsloth、NEFTune、rsLoRA等</td>
<td>支持多种数据格式</td>
<td>LlamaBoard、TensorBoard、Wandb、MLflow 等监控工具</td>
<td>零代码命令行、支持Web UI、OpenAI 风格 API、RoPE scaling</td>
</tr>
<tr>
<td><strong>Firefly</strong></td>
<td>常见开源大模型、多模态模型</td>
<td>预训练、全参数微调、指令微调、DPO、LoRA/QLoRA</td>
<td>Unsloth</td>
<td>整理并开源多个指令微调数据集、支持自定义数据格式</td>
<td></td>
<td>提供多种开源数据集、开源 Firefly 系列模型权重</td>
</tr>
<tr>
<td><strong>Xtuner</strong></td>
<td>常见开源大模型、多模态图文模型</td>
<td>增量预训练、QLoRA/LoRA、全量参数微调、指令微调、Agent 微调</td>
<td>Flash Attention、Triton kernels、多节点跨设备支持</td>
<td>兼容任意数据格式、支持开源和自定义数据、预定义开源对话模板</td>
<td>LMDeploy、OpenCompass、VLMEvalKit</td>
<td>8GB 显存微调 7B 模型</td>
</tr>
<tr>
<td><strong>Swift</strong></td>
<td>450+ 纯文本大模型、150+ 多模态大模型、All-to-All 全模态模型</td>
<td>LoRA/QLoRA、DoRA、ReFT 等轻量微调、RLHF 训练（DPO、CPO 等）、多模态训练（VQA、Caption 任务等）</td>
<td>支持 BNB、AWQ 等量化模型训练</td>
<td>内置 150+ 数据集、支持自定义数据集</td>
<td>EvalScope、支持插件化与拓展</td>
<td>以界面的方式提供训练、推理、评测、量化的能力，完成大模型的全链路</td>
</tr>
<tr>
<td><strong>Unsloth</strong></td>
<td>常见开源大模型</td>
<td>LoRA/QLoRA 微调等</td>
<td>Triton 重写计算过程、支持 4bit 和 16bit 微调</td>
<td>支持自定义数据格式</td>
<td>支持 2018 年及以后版本的 NVIDIA GPU</td>
<td>开源版本加速 5 倍、Pro 版本加速 30 倍、支持 RoPE Scaling</td>
</tr>
<tr>
<td><strong>Trainer</strong></td>
<td>常见开源大模型</td>
<td>通用训练接口<br>自定义训练循环、损失函数、优化器等<br></td>
<td>混合精度训练</td>
<td>适用于各种 NLP 数据集、支持自定义数据格式</td>
<td>支持自定义回调函数</td>
<td>适用于多种 NLP 任务</td>
</tr>
</tbody>
</table>
<p>图片版的总结：<br>
<img src="https://img2024.cnblogs.com/blog/2338485/202501/2338485-20250122004026256-2093369919.png" alt="" loading="lazy"></p>
</div></div></div></div></div></div></div>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.49565833369675927" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-01-22 00:43">2025-01-22 00:43</span>&nbsp;
<a href="https://www.cnblogs.com/gzyatcnblogs">Milkha</a>&nbsp;
阅读(<span id="post_view_count">33</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18684805" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18684805);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18684805', targetLink: 'https://www.cnblogs.com/gzyatcnblogs/p/18684805', title: '大模型训练工具，小白也能轻松搞定！' })">举报</a>
</div>
        