
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/qizhou/p/18913058" title="发布于 2025-06-05 20:11">
    <span role="heading" aria-level="2">论文解读：Locating and Editing Factual Associations in GPT（ROME）</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body blogpost-body-html">
<p data-first-child="" data-pid="DkMnPXgu">  论文发表于人工智能顶会NeurIPS（<a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/6f1d43d5a82a37e89b0665b33bf3a182-Abstract-Conference.html" target="_blank" rel="noopener nofollow">原文链接</a>），研究了GPT（Generative Pre-trained Transformer）中事实关联的存储和回忆，发现这些关联与局部化、可直接编辑的计算相对应。因此：</p>
<p>  1、开发了一种因果干预方法，用于识别对模型的事实预测起决定性作用的神经元。</p>
<p>  2、为了验证这些神经元是否对应于事实关联的回忆，使用秩一模型编辑 (Rank-One Model Editing, ROME) 修改前馈权重来更新特定的事实关联。</p>
<p>  3、提出一个反事实断言数据集来评估ROME。</p>
<p>  阅读本文请同时参考原始论文图表。</p>
<h1>方法</h1>
<p>  如图1所示，将自回归模型的中间表示与输出之间的关系表示为图节点的形式，从而可以分析输出与中间表示之间的因果关系。定义输入语言变量为<span class="math">$x=[x_1,…,x_T]$</span>，则第<span class="math">$i$</span>个输入变量的第<span class="math">$l$</span>层激活为<span class="math">$h_i^{(l)}$</span>。模型各层计算可以表示为文中式(1)：</p>
<p><img src="https://img2024.cnblogs.com/blog/1908255/202506/1908255-20250605200634033-635364116.png" alt="" height="127" width="482" style="display: block; margin-left: auto; margin-right: auto"></p>
<p>  本文使用最后一个输入的最后一层激活<span class="math">$h^{(L)}_T$</span>映射到词汇空间作为模型输出。实验使用包含事实三元组<span class="math">$(s,r,o)$</span>的头<span class="math">$s$</span>和关系<span class="math">$r$</span>的句子作为输入，预测尾实体<span class="math">$o$</span>。</p>
<h2>因果中介分析</h2>
<p>  为了分析哪些中间激活对正确事实预测的贡献最大，使用三种配置对模型<span class="math">$G$</span>推理三次：</p>
<p>  1、Clean run：输入干净的句子，得到干净的中间激活<span class="math">$\{h_i^{(l)}|i\in [1,T],l\in[1,L]\}$</span>和模型对尾实体<span class="math">$o$</span>的预测概率<span class="math">$\mathbb{P}[o]$</span>。</p>
<p>  2、Corrupted run：用噪声污染每个<span class="math">$s$</span>的词嵌入<span class="math">$h_i^{(0)}$</span>，得到污染的中间激活<span class="math">$\{h_{i*}^{(l)}|i\in [1,T],l\in[1,L]\}$</span>和模型对尾实体<span class="math">$o$</span>的预测概率<span class="math">$\mathbb{P}_*[o]$</span>。</p>
<p>  3、Corrupted-with-restoration run：使用干净的中间激活替换污染的中间激活，得到替换<span class="math">$h_i^{(l)}$</span>时模型对尾实体<span class="math">$o$</span>的预测概率<span class="math">$\mathbb{P}_{*,clean\, h_i^{(l)}}[o]$</span>。</p>
<p>  定义总效应 <span class="math">$\text{TE}=\mathbb{P}[o]-\mathbb{P}_*[o]$</span>，表示受污染的输入对模型性能的损害程度。</p>
<p>  定义间接效应 <span class="math">$\text{IE}=\mathbb{P}_{*,clean\, h_i^{(l)}}[o]-\mathbb{P}_*[o]$</span>，表示输入被污染后，恢复中间激活<span class="math">$h_i^{(l)}$</span>对模型性能的恢复程度。下面关于各层模块输出的影响程度都是通过Average IE (AIE) 来评估。</p>
<p>  其实验代码汇总并不是每次只恢复污染一个表示，而是一个窗口。如当窗口为10时，恢复层数为<span class="math">$[l-5,l+5]$</span>11个表示。</p>
<h3>分析结果1</h3>
<p>  图1e/f/g 可视化了模型在一个样本上分别恢复层激活<span class="math">$h_i^{l}$</span>、FFN激活<span class="math">$m_i^{(l)}$</span>、注意力激活<span class="math">$a_i^{(l)}$</span>的影响，以恢复模型中间状态在正确答案上的概率来衡量。</p>
<p>  图2a/b/c 可视化了模型在超过1000个样本上分别恢复三个激活对正确预测的平均影响程度，以IE来衡量。</p>
<p>  根据图1/2，可以看出：</p>
<p>  1、较前层的FFN在句子主体<span class="math">$s$</span>的最后一个token位置的激活<span class="math">$m_i^{(l)}$</span>对预测的影响较大。这说明本文提出的因果分析方法可以有效定位影响模型预测的中间激活，也和之前认为FFN保存知识的研究结果一致。</p>
<p>  2、较后层的注意力模块在句子的最后一个token位置的激活<span class="math">$a_i^{(l)}$</span>对预测影响较大，这是比较正常的现象。</p>
<p>  3、可以看出层激活<span class="math">$h_i^{(l)}$</span>对预测的影响是以上两者的综合。</p>
<p>  附录B.2 图7进一步展示了模型三类模块在句子各部分的激活对模型输出的影响。</p>
<h3>结果分析2</h3>
<p>  为了对FFN在句子的前半部分所起的作用有更清晰的理解，作者进一步修改因果图来研究其因果效应。如图3左所示，当把主体<span class="math">$s$</span>最后一个token的层激活<span class="math">$h_{i}^{(l)}$</span>修正（使用干净激活替换污染激活）时，对于其后面的隐藏状态<span class="math">$h^{(k)}_{i},k&gt;l$</span>，使用原始保存的没修正时（污染）的FFN输出<span class="math">$m_{i*}^{(k)}$</span>代替后续计算出的<span class="math">$m_{i}^{(k)}$</span>，从而在关于<span class="math">$h_i^{(l)}$</span>对模型预测的影响中去除FFN的效应。从右图可以看出，FFN计算的去除使得较前层的<span class="math">$h_i^{(l)}$</span>对模型预测的影响显著降低，说明<span class="math">$h_i^{(l)}$</span>后续的FFN计算（读取记忆）对预测是至关重要的，而注意力模块（粉色）则没有这个结果。</p>
<h2>秩一模型编辑 (ROME)</h2>
<p>  根据前面的分析，作者期望通过修改中间层FFN的权重来修改模型存储的事实。相较于之前的论文把FFN的第一层权重<span class="math">$W_{fc}^{(l)}$</span>和第二层权重<span class="math">$W_{prop}^{(l)}$</span>对应的行列向量看成键值对，本文：</p>
<p>  1、把FFN的第二层权重<span class="math">$W_{prop}^{(l)}\in\R^{d_2\times d_1}$</span>的工作机制看成Linear Associative Memory（线性相联存储器，不知道是不是这个意思)。</p>
<p>  2、把FFN第一层的输出<span class="math">$K=[k_1,...,k_t]\in \R^{d_1\times t}$</span>看成键。</p>
<p>  3、把通过第二层权重<span class="math">$W_{prop}^{(l)}$</span>的输出<span class="math">$V=WK=[v_1,…,v_t]\in\R^{d_2\times t}$</span>看成值。</p>
<p>  其中<span class="math">$t$</span>表示训练过程中出现的所有可能的键值对的数量。在已知<span class="math">$K,V$</span>的情况下，<span class="math">$W_{prop}^{(l)}$</span>可以直接通过广义逆计算<span class="math">$W_{prop}^{(l)}=VK^+$</span>。当我们需要新增一个知识时，就是给<span class="math">$K,V$</span>新增一个键<span class="math">$k_*$</span>和值<span class="math">$v_*$</span>。此时就是在满足<span class="math">$\hat{W}k_*=v_*$</span>的情况下，最小化<span class="math">$\|\hat{W}K-V\|$</span>。可以通过拉格朗日方法得到闭式解，如式(2)所示。</p>
<p><img src="https://img2024.cnblogs.com/blog/1908255/202506/1908255-20250605200634298-1492280872.png" alt="" height="30" width="702" style="display: block; margin-left: auto; margin-right: auto"></p>
<p>  其中<span class="math">$C=KK^T$</span>是一个通过预训练数据计算得到的常数矩阵，<span class="math">$W$</span>为原始矩阵，<span class="math">$\Lambda=(v_*-Wk_*)/(C^{-1}k_*)^Tk_*$</span>是相较于原始矩阵的残差。</p>
<p>  接下来就是计算新增知识的键值对<span class="math">$k_*,v_*$</span>，如图4所示：</p>
<p>  1、把句子输入模型，取主体<span class="math">$s$</span>的最后一个token在需要修改的第<span class="math">$l^*$</span>层的FFN的第一层激活作为<span class="math">$k_*$</span>。为了获得更鲁棒的结果，计算多个以<span class="math">$s$</span>结尾的句子的激活值的均值，如文中式(3)所示。</p>
<p>  2、通过优化相应位置的向量来获得<span class="math">$v_*$</span>，损失函数如式(4)所示，目的就是让<span class="math">$v_*$</span>编码<span class="math">$(r,o^*)$</span>。</p>
<p>  3、用式(2)和<span class="math">$k_*,v_*$</span>更新<span class="math">$W_{prop}^{(l)}$</span>。</p>
<p>  所谓的秩一，应该就是给<span class="math">$K,V$</span>都增加一列，从而更新的<span class="math">$W$</span>矩阵的秩可能加一。</p>
<h1>实验</h1>
<h2>zsRE上的对比</h2>
<p>  在Zero-Shot关系抽取(zsRE)数据集上对GPT-2 XL的编辑实验。数据示例如附录图22所示，实验流程就是把模型关于输入”src”的输出修改为”answer”。实验结果如表1所示，对比了元学习方法MEND和KE。其中”+L”表示微调的同时使用无穷范数限制参数的更新，”-zsRE”表示相关基于元学习的超网络方法先在zsRE训练集上进行训练后再进行修改。Efficacy是模型输入”src”时的判断准确率，Paraphrase是模型输入同义句子时的预测准确率，Specificity是模型输入不相关句子时的预测准确率。Efficacy和Paraphrase指标可以看出ROME的确能正确修改事实，但是Specificity指标看起来是一个没有意义的指标。</p>
<h2>反事实数据集上的对比</h2>
<p>  由于以上对比的指标差异并不显著，为了让方法效果更容易区分，作者构建了一个反事实 (COUNTERFACT) 数据集，旨在将模型携带的正确事实<span class="math">$(s,r,o^c)$</span>修改为错误事实<span class="math">$(s,r,o^*)$</span>。这是因为模型对正确事实的预测分数通常比错误事实高，如果编辑方法能使错误事实的预测分数比正确事实高，就能更好地说明方法的有效性。数据集汇总和对比如表2/3所示。</p>
<p>  定义<span class="math">$\mathbb{P}[o^*],\mathbb{P}[o^c]$</span>分别为模型编辑后对错误事实和正确事实的预测分数。评价指标如下：</p>
<p>  1、Efficacy Score (ES)为<span class="math">$\mathbb{P}[o^*]&gt;\mathbb{P}[o^c]$</span>的测试样本的比例，Efficacy Magnitude (EM)为<span class="math">$\mathbb{P}[o^*]-\mathbb{P}[o^c]$</span>的测试均值。</p>
<p>  2、同上定义在同义句子上测试的Paraphrase Scores (PS)和Paraphrase Magnitude (PM)。</p>
<p>  3、搜集有正确答案<span class="math">$o^c$</span>的不同事实<span class="math">$(s_n,r,o^c)$</span>，在将模型进行事实编辑<span class="math">$(s,r,o^*)$</span>后，测试模型对于<span class="math">$(s_n,r,?)$</span>的预测是否依旧有<span class="math">$\mathbb{P}[o^c]&gt;\mathbb{P}[o^*]$</span>，定义相应的Neighborhood Score (NS)和Neighborhood Magnitude (NM)。</p>
<p>  4、cos similarity (CS)：让编辑后的模型生成以<span class="math">$s$</span>开头的文本，计算其unigram TF-IDF，然后与包含<span class="math">$s,o^*$</span>的参考文本的unigram TF-IDF计算余弦相似度。</p>
<p>  5、GE：评估模型生成的流利退化程度。也就是计算生成句子<span class="math">$x$</span>的bi-/tri-gram熵<span class="math">$H(f(x))$</span>，其中<span class="math">$f(x)$</span>表示n-gram的频率分布。指标越高，生成句子的多样性越高，但不知道和流利度有什么关系。</p>
<p>  图5展示了对模型不同层和句子不同位置的组合对应的FFN权重进行编辑，得到的结果。可以看出对主体<span class="math">$s$</span>的最后一个token(红线)在模型中间层的激活进行编辑得到最好的结果，有最高的准确率和泛化率，以及对邻居事实最低的损害度。</p>
<p>  表4展示了各模型在反事实数据上的结果，其中”-CF” 表示先在其训练集上进行训练后再进行测试。本文方法有较好地修改效果的同时，能保持相邻事实的不变性(Specificity)，而其它方法都不能实现。</p>
<h2>与集成梯度归因的对比</h2>
<p>  图10：本文因果干预方法归因的中间激活对预测的影响可视化。</p>
<p>  图16：KN的集成梯度方法归因的中间激活对预测的影响可视化。</p>
<p>  可以看出集成梯度方法没有明确揭示出事实主体<span class="math">$s$</span>对预测的重要性，归因出几乎所有token的激活对最终预测都有影响，这是不合理的。当然，这里的归因与KN不同的是，KN论文中仅仅对[MASK]位置的激活进行归因，而且是在激活的元素层面上进行的，而这里是对所有激活向量进行归因。</p>
<h2>Attention模块对预测的影响</h2>
<p>  附录第I节，用微调对Att模块的各个注意力映射矩阵进行修改（取名为AttnEdit），与ROME方法进行对比，定性结果如表25所示。ROME和AttnEdit都能成功编辑，但是AttnEdit无法泛化到相近提问。</p>
<h1>总结</h1>
<p>  与KN的对比：</p>
<p>  1、KN通过集成梯度仅仅定位激活的一个元素，并修改FFN第二层权重对应的一个向量，并且直接通过翻倍或者置零实现，是一种很粗糙的编辑。</p>
<p>  2、ROME的定位比KN往上一个层级，用因果干预方法定位整个激活向量，然后修改FFN第二层的整个权重来实现编辑。简单来说就是把要编辑的事实对应于该权重的输入输出，加入模型原始训练数据对应于该权重的输入输出列表中，让这个权重重新适应这个列表。</p>
<p>  本文定位方法更有理论依据，编辑对其它知识的影响也可以从优化角度来量化。</p>
<p>&nbsp;</p>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.3251854743287037" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-06-05 20:27">2025-06-05 20:11</span>&nbsp;
<a href="https://www.cnblogs.com/qizhou">颀周</a>&nbsp;
阅读(<span id="post_view_count">19</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18913058);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18913058', targetLink: 'https://www.cnblogs.com/qizhou/p/18913058', title: '论文解读：Locating and Editing Factual Associations in GPT（ROME）' })">举报</a>
</div>
        