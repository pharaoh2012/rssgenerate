
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/deali/p/18722239" title="发布于 2025-02-18 16:22">
    <span role="heading" aria-level="2">数据不出内网：基于Ollama+OneAPI构建企业专属DeepSeek智能中台</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h2 id="前言">前言</h2>
<p>之前已经在Linux服务器上使用Ollama部署了DeepSeek</p>
<p>这次在没有外网（应该说是被限制比较多）的服务器上部署，遇到一些坑，记录一下</p>
<h2 id="ollama">ollama</h2>
<p>ollama 自然无法使用在线安装脚本了</p>
<p>根据 ollama 的文档</p>
<p>先在本地电脑根据服务器的系统和CPU架构下载安装包</p>
<pre><code class="language-bash">curl -L https://ollama.com/download/ollama-linux-amd64.tgz -o ollama-linux-amd64.tgz
</code></pre>
<p>然后使用 scp 等工具上传到服务器</p>
<pre><code class="language-bash">scp ollama-linux-amd64.tgz 服务器地址:/temp
</code></pre>
<p>连接到服务器上后解压安装，跟着 ollama 文档来就行（见第一个参考资料）</p>
<pre><code class="language-bash">sudo tar -C /usr -xzf ollama-linux-amd64.tgz
</code></pre>
<p>这时候已经能执行 ollama 程序了</p>
<pre><code class="language-bash">ollama serve
</code></pre>
<p>然后再添加到服务，这也是 ollama 官方推荐的做法，方便管理</p>
<pre><code class="language-bash">sudo useradd -r -s /bin/false -U -m -d /usr/share/ollama ollama
sudo usermod -a -G ollama $(whoami)
</code></pre>
<p>在 /etc/systemd/system 下新建 ollama.service 文件</p>
<pre><code class="language-ini">[Unit]
Description=Ollama Service
After=network-online.target

[Service]
ExecStart=/usr/bin/ollama serve
User=ollama
Group=ollama
Restart=always
RestartSec=3
Environment="PATH=$PATH"

[Install]
WantedBy=default.target
</code></pre>
<p>然后启用服务</p>
<pre><code class="language-bash">sudo systemctl daemon-reload
sudo systemctl enable ollama
</code></pre>
<p>到这里 ollama 的安装就搞定了</p>
<h2 id="模型部署">模型部署</h2>
<p>离线服务器是无法使用 ollama pull 拉取模型的</p>
<p>需要先在本地下载，可以在本地的电脑上执行 ollama pull 的操作</p>
<p>然后把模型文件找到并上传到服务器</p>
<p>大概思路就是这样，具体的接下来介绍</p>
<h3 id="找到本地模型文件">找到本地模型文件</h3>
<p>如果没有特别配置，ollama 默认的模型文件都在 <code>~/.ollama/models/blobs</code> 里</p>
<p>先执行命令看看指定模型的路径，比如说要找 deepseek-r1:32b 模型</p>
<pre><code class="language-bash">ollama show deepseek-r1:32b --modelfile
</code></pre>
<p>执行命令后的输出（节选）</p>
<pre><code class="language-dockerfile">FROM C:\Users\deali\.ollama\models\blobs\sha256-96c415656d377afbff962f6cdb2394ab092ccbcbaab4b82525bc4ca800fe8a49
TEMPLATE """{{- if .System }}{{ .System }}{{ end }}
{{- range $i, $_ := .Messages }}
{{- $last := eq (len (slice $.Messages $i)) 1}}
{{- if eq .Role "user" }}&lt;｜User｜&gt;{{ .Content }}
{{- else if eq .Role "assistant" }}&lt;｜Assistant｜&gt;{{ .Content }}{{- if not $last }}&lt;｜end▁of▁sentence｜&gt;{{- end }}
{{- end }}
{{- if and $last (ne .Role "assistant") }}&lt;｜Assistant｜&gt;{{- end }}
{{- end }}"""
PARAMETER stop &lt;｜begin▁of▁sentence｜&gt;
PARAMETER stop &lt;｜end▁of▁sentence｜&gt;
PARAMETER stop &lt;｜User｜&gt;
PARAMETER stop &lt;｜Assistant｜&gt;
</code></pre>
<p>可以看到这一行</p>
<pre><code>FROM C:\Users\deali\.ollama\models\blobs\sha256-96c415656d377afbff962f6cdb2394ab092ccbcbaab4b82525bc4ca800fe8a49
</code></pre>
<p>就是 ollama 下载到本地的模型的路径</p>
<p>把这个文件上传到服务器</p>
<h3 id="导出modelfile">导出Modelfile</h3>
<p>这个文件格式类似 Dockerfile</p>
<p>使用以下命令导出</p>
<pre><code class="language-bash">ollama show deepseek-r1:32b --modelfile &gt; Modelfile
</code></pre>
<p>然后这个文件也要上传到服务器上</p>
<h3 id="服务器上导入模型">服务器上导入模型</h3>
<p>模型文件和 Modelfile 上传之后，放在同一个目录下</p>
<p>先重命名一下，方便后续导入</p>
<pre><code class="language-bash">mv sha256-96c415656d377afbff962f6cdb2394ab092ccbcbaab4b82525bc4ca800fe8a49 deepseek-r1_32b.gguf
</code></pre>
<p>接着编辑一下 Modelfile 文件，把 FROM 这一行改成，也就是刚才修改之后的模型文件名称</p>
<pre><code class="language-dockerfile">FROM ./deepseek-r1_32b.gguf
</code></pre>
<p>然后执行以下命令导入</p>
<pre><code class="language-bash">ollama create deepseek-r1:32b -f Modelfile
</code></pre>
<p>如无意外就导入成功了，可以执行 <code>ollama list</code> 来查看是否已导入。</p>
<h2 id="one-api">one-api</h2>
<p>One API 是一款开源的 LLM（大语言模型）API 管理与分发系统，旨在通过标准的 OpenAI API 格式，统一访问多种大模型，开箱即用。 它支持多种主流大模型，包括 OpenAI ChatGPT 系列、Anthropic Claude 系列、Google PaLM2/Gemini 系列、Mistral 系列、字节跳动豆包大模型、百度文心一言系列模型、阿里通义千问系列模型、讯飞星火认知大模型、智谱 ChatGLM 系列模型、腾讯混元大模型等。</p>
<h3 id="docker部署">docker部署</h3>
<p>one-api是用go的gin框架开发的，部署很容易，我一般用docker部署，这块不再赘述</p>
<pre><code class="language-yaml">services:
  db:
    image: mysql:8.1.0
    container_name: mysql
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: mysql-password
    volumes:
      - ./data:/var/lib/mysql
  one-api:
    image: justsong/one-api
    container_name: one-api
    restart: always
    ports:
      - "3000:3000"
    depends:
      - db
    environment:
      - SQL_DSN=root:mysql-password@tcp(db:3306)/one_api
      - TZ=Asia/Shanghai
      - TIKTOKEN_CACHE_DIR=/TIKTOKEN_CACHE_DIR
    volumes:
      - ./data:/data
      - ./TIKTOKEN_CACHE_DIR:/TIKTOKEN_CACHE_DIR

networks:
  default:
    name: one-api
</code></pre>
<h3 id="解决-tiktoken-问题">解决 tiktoken 问题</h3>
<p>遇到的问题是它依赖了 tiktoken 这个库，tiktoken 需要联网下载 token encoder</p>
<p>解决方法是看错误日志，比如</p>
<pre><code>one-api  | [FATAL] 2025/02/17 - 10:47:21 | relay/adaptor/openai/token.go:26 [InitTokenEncoders] failed to get gpt-3.5-turbo token encoder: Get "https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken": dial tcp 57.150.97.129:443: i/o timeout, if you are using in offline environment, please set TIKTOKEN_CACHE_DIR to use exsited files
</code></pre>
<p>这里需要从 <a href="https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken" target="_blank" rel="noopener nofollow">https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken</a> 下载</p>
<p>我们先在本地下载这个文件，然后上传到服务器</p>
<p>但这时还不行</p>
<p>tiktoken 只认 URL 的 SHA-1</p>
<p>生成 SHA-1</p>
<pre><code class="language-bash">TIKTOKEN_URL=https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken
echo -n $TIKTOKEN_URL | sha1sum | head -c 40
</code></pre>
<p>也可以合成一行命令</p>
<pre><code class="language-bash">echo -n "https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken" | sha1sum | head -c 40
</code></pre>
<p>在这行命令中，<code>echo -n</code> 用于输出指定的 URL 字符串（其 <code>-n</code> 参数的作用是<strong>禁止在输出的末尾添加换行符</strong>），<code>sha1sum</code> 计算其 SHA-1 哈希值，<code>head -c 40</code> 截取前 40 个字符，即哈希值的前 40 位。</p>
<p>执行结果是</p>
<pre><code>9b5ad71b2ce5302211f9c61530b329a4922fc6a4
</code></pre>
<p>然后把 cl100k_base.tiktoken 文件重命名为输出的 <code>9b5ad71b2ce5302211f9c61530b329a4922fc6a4</code></p>
<p>在前面的 docker-compose.yaml 里，我们已经指定了 TIKTOKEN_CACHE_DIR 环境变量</p>
<p>然后把这个 9b5ad71b2ce5302211f9c61530b329a4922fc6a4 文件放在 TIKTOKEN_CACHE_DIR 目录里即可。</p>
<p>后续还有遇到类似报错，重复以上操作，直到没有报错为止。</p>
<p>我目前使用的版本只下载了两个 encoder</p>
<h2 id="在oneapi中添加ollama渠道">在OneApi中添加Ollama渠道</h2>
<p>这里因为docker网络的问题会有些麻烦</p>
<p>有多种思路，一种是让OneApi的容器跑在 host 网络模式下</p>
<p>一种是使用 host.docker.internal 这个地址</p>
<p>当然前提都是 ollama 的 host 设置为 <code>0.0.0.0</code> ，这个配置可以参考我之前的这篇文章: <a href="https://www.cnblogs.com/deali/p/18695132" target="_blank">LLM探索：本地部署DeepSeek-R1模型</a></p>
<p>在添加渠道的时候，类型选择 Ollama</p>
<p>自定义模型部分填入我们部署的 deepseek-r1:32b</p>
<p>然后代理填写 <code>http://host.docker.internal:11434</code></p>
<p><em>注意：在 Linux 环境中，<code>host.docker.internal</code> 可能无法工作，但你可以直接使用宿主机的 IP 地址。例如，如果宿主机的 IP 地址是 <code>192.168.1.100</code>，可以在OneApi中使用 <code>http://192.168.1.100:11434</code> 来访问 Ollama 服务。</em></p>
<h2 id="参考资料">参考资料</h2>
<ul>
<li><a href="https://github.com/ollama/ollama/blob/main/docs/linux.md" target="_blank" rel="noopener nofollow">https://github.com/ollama/ollama/blob/main/docs/linux.md</a></li>
<li><a href="https://stackoverflow.com/questions/76106366/how-to-use-tiktoken-in-offline-mode-computer" target="_blank" rel="noopener nofollow">https://stackoverflow.com/questions/76106366/how-to-use-tiktoken-in-offline-mode-computer</a></li>
<li><a href="https://www.cnblogs.com/cjdty/p/18659438" target="_blank">https://www.cnblogs.com/cjdty/p/18659438</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/20485169539" target="_blank" rel="noopener nofollow">https://zhuanlan.zhihu.com/p/20485169539</a></li>
</ul>

</div>
<div id="MySignature" role="contentinfo">
    微信公众号：「程序设计实验室」
专注于互联网热门新技术探索与团队敏捷开发实践，包括架构设计、机器学习与数据分析算法、移动端开发、Linux、Web前后端开发等，欢迎一起探讨技术，分享学习实践经验。
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.681241191363426" data-date-created="BlogServer.Application.Dto.BlogPost.BlogPostDto" data-date-updated="2025-02-18 16:41">2025-02-18 16:22</span>&nbsp;
<a href="https://www.cnblogs.com/deali">程序设计实验室</a>&nbsp;
阅读(<span id="post_view_count">258</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=18722239" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(18722239);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '18722239', targetLink: 'https://www.cnblogs.com/deali/p/18722239', title: '数据不出内网：基于Ollama+OneAPI构建企业专属DeepSeek智能中台' })">举报</a>
</div>
        